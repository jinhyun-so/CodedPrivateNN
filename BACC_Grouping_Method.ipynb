{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "\n",
    "import math\n",
    "\n",
    "from utils.sampling import mnist_iid, mnist_noniid, cifar_iid\n",
    "from utils.options import args_parser\n",
    "from models.Update import LocalUpdate\n",
    "from models.Nets import MLP, CNNMnist, CNNCifar, LeNet, CNNMnist2\n",
    "from models.Fed import FedAvg\n",
    "from models.Fed import FedQAvg, Quantization, Quantization_Finite, my_score, my_score_Finite\n",
    "from models.test import test_img\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "\n",
    "from utils.sampling import mnist_iid, mnist_noniid, cifar_iid\n",
    "from utils.options import args_parser\n",
    "from models.Update import LocalUpdate\n",
    "from models.Nets import MLP, CNNMnist, CNNCifar, LeNet, CNNMnist2, CNNMnist3\n",
    "from models.Fed import FedAvg\n",
    "from models.Fed import FedQAvg, Quantization, Quantization_Finite, my_score, my_score_Finite\n",
    "from models.test import test_img\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from utils.functions import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "class DatasetSplit(Dataset):\n",
    "    def __init__(self, dataset, idxs):\n",
    "        self.dataset = dataset\n",
    "        self.idxs = list(idxs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image, label = self.dataset[self.idxs[item]]\n",
    "        return image, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. G=2, N=4 (N_1=2, N_2=2), K=4 (K_1=2, K_2=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of X: (60000, 784)\n",
      "size of Y: (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "class my_argument:    \n",
    "    epochs = 200    #\"rounds of training\"\n",
    "    num_users = 15  # \"number of users: N\"\n",
    "    num_partition = 4 # \"number of users: K\"\n",
    "    frac = 0.5 #\"the fraction of clients: C\"\n",
    "    local_ep = 1 #\"the number of local epochs: E\"\n",
    "    local_bs = 200 #\"local batch size: B\"\n",
    "    bs=200 #\"test batch size\"\n",
    "    lr=0.01 #\"learning rate\"\n",
    "    momentum=0.5 # \"SGD momentum (default: 0.5)\"\n",
    "    split='user' # \"train-test split type, user or sample\"\n",
    "    opt='ADAM'\n",
    "    loss='Custom' # 'Custom' or 'Default'\n",
    "\n",
    "    # model arguments\n",
    "    model = 'cnn'\n",
    "    kernel_num=9 #, help='number of each kind of kernel')\n",
    "    kernel_sizes='3,4,5' #  help='comma-separated kernel size to use for convolution')\n",
    "    norm='None' #, help=\"batch_norm, layer_norm, or None\")\n",
    "    num_filters=32 #, help=\"number of filters for conv nets\")\n",
    "    max_pool='True' #help=\"Whether use max pooling rather than strided convolutions\")\n",
    "\n",
    "    # other arguments\n",
    "    dataset='mnist' #, help=\"name of dataset\")\n",
    "    iid=1\n",
    "    num_classes=10#, help=\"number of classes\")\n",
    "    num_channels=1#, help=\"number of channels of imges\")\n",
    "    gpu=1#, help=\"GPU ID, -1 for CPU\")\n",
    "    stopping_rounds=10#, help='rounds of early stopping')\n",
    "    verbose='False'#, help='verbose print')\n",
    "    seed=1#, help='random seed (default: 1)')\n",
    "    \n",
    "args = my_argument()\n",
    "\n",
    "args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "# load dataset and split users\n",
    "trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "dataset_train = datasets.MNIST('../data/mnist/', train=True, download=True, transform=trans_mnist)\n",
    "dataset_test = datasets.MNIST('../data/mnist/', train=False, download=True, transform=trans_mnist)\n",
    "\n",
    "dict_users = mnist_iid(dataset_train, args.num_partition)\n",
    "\n",
    "encoding_input_array_np = np.empty((len(dataset_train),28*28))\n",
    "encoding_label_array_np = np.empty((len(dataset_train),args.num_classes))\n",
    "print(\"size of X:\" ,encoding_input_array_np.shape)\n",
    "print(\"size of Y:\" ,encoding_label_array_np.shape)\n",
    "\n",
    "Size_submatrices = int(60000/args.num_partition)\n",
    "\n",
    "for i in range(args.num_partition):\n",
    "    \n",
    "    stt_pos = i*Size_submatrices\n",
    "    end_pos = (i+1)*Size_submatrices\n",
    "#     print(i,stt_pos,end_pos)\n",
    "    Temp_train = DataLoader(DatasetSplit(dataset_train, dict_users[i]), batch_size=Size_submatrices, shuffle=True)\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(Temp_train):\n",
    "        \n",
    "        images_np = images.detach().cpu().numpy()\n",
    "        encoding_input_array_np[stt_pos:end_pos,:] = np.reshape(images_np, (Size_submatrices,28*28))\n",
    "#         print(encoding_input_array_np[stt_pos:end_pos,:].shape)\n",
    "\n",
    "        onehot_labels = torch.nn.functional.one_hot(labels,num_classes=args.num_classes)\n",
    "        labels_np = onehot_labels.detach().cpu().numpy()\n",
    "#         print(labels_np.shape)\n",
    "        encoding_label_array_np[stt_pos:end_pos,:] = labels_np\n",
    "\n",
    "\n",
    "# print(labels_np[0:10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = 2\n",
    "N = 4 # N should be divisible by G\n",
    "K = 4 # N should be divisible by G\n",
    "\n",
    "size_per_group = int(60000/G)\n",
    "\n",
    "X_group = np.reshape(encoding_input_array_np, (G,size_per_group,28*28))\n",
    "y_group = np.reshape(encoding_label_array_np, (G,size_per_group,args.num_classes)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2 3\n",
      "##########################################\n",
      "###### 0 -th Trial!! ###########\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.2973 \n",
      "Accuracy: 2163/10000 (21.63%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.2949 \n",
      "Accuracy: 1899/10000 (18.99%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.8496 \n",
      "Accuracy: 7254/10000 (72.54%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3448 \n",
      "Accuracy: 9530/10000 (95.30%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3019 \n",
      "Accuracy: 9539/10000 (95.39%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2349 \n",
      "Accuracy: 9617/10000 (96.17%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2454 \n",
      "Accuracy: 9615/10000 (96.15%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2358 \n",
      "Accuracy: 9608/10000 (96.08%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2291 \n",
      "Accuracy: 9604/10000 (96.04%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2499 \n",
      "Accuracy: 9606/10000 (96.06%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2615 \n",
      "Accuracy: 9591/10000 (95.91%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2418 \n",
      "Accuracy: 9637/10000 (96.37%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2138 \n",
      "Accuracy: 9633/10000 (96.33%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2905 \n",
      "Accuracy: 9616/10000 (96.16%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2591 \n",
      "Accuracy: 9635/10000 (96.35%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2987 \n",
      "Accuracy: 9593/10000 (95.93%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2687 \n",
      "Accuracy: 9647/10000 (96.47%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2265 \n",
      "Accuracy: 9650/10000 (96.50%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2389 \n",
      "Accuracy: 9631/10000 (96.31%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2505 \n",
      "Accuracy: 9635/10000 (96.35%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2267 \n",
      "Accuracy: 9640/10000 (96.40%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2649 \n",
      "Accuracy: 9642/10000 (96.42%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2526 \n",
      "Accuracy: 9605/10000 (96.05%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2283 \n",
      "Accuracy: 9663/10000 (96.63%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2171 \n",
      "Accuracy: 9668/10000 (96.68%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2659 \n",
      "Accuracy: 9640/10000 (96.40%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2047 \n",
      "Accuracy: 9671/10000 (96.71%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.4531 \n",
      "Accuracy: 9568/10000 (95.68%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2314 \n",
      "Accuracy: 9604/10000 (96.04%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "N_i = int(N/G) # = 2\n",
    "K_i = int(K/G) # = 2\n",
    "T = 3\n",
    "sigma = 1\n",
    "Noise_Alloc = [0,2,4]\n",
    "m = N_i # number of selected workers (if there is no straggler, m=N_i)\n",
    "\n",
    "print(N_i,K_i,T)\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K_i+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K_i+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K_i+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "z_array = np.array([-0.81, 0.81])\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 30\n",
    "\n",
    "loss_test_arr_G2 = np.zeros((N_trials,N_epochs))\n",
    "acc_test_arr_G2  = np.zeros((N_trials,N_epochs))\n",
    "\n",
    "for trial_idx in range(N_trials):\n",
    "    \n",
    "    print('##########################################')\n",
    "    print('######',trial_idx,'-th Trial!! ###########')\n",
    "    \n",
    "    net_glob = CNNMnist2(args=args)\n",
    "    net_glob.cuda()\n",
    "    net_glob.train()\n",
    "    \n",
    "    # copy weights\n",
    "    w_glob = net_glob.state_dict()\n",
    "    \n",
    "    X_tilde = np.empty((N,Size_submatrices,28*28))\n",
    "    y_tilde = np.empty((N,Size_submatrices,10))\n",
    "    \n",
    "    for G_idx in range(G):\n",
    "        \n",
    "        _Noise_label = np.ones((size_per_group*T,10)) * 0.1\n",
    "        \n",
    "        X_tilde_tmp,a,b = BACC_Enc_Data_v3(X_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde_tmp,a,b = BACC_Enc_Data_v3(y_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "        \n",
    "        stt_pos = G_idx * N_i\n",
    "        end_pos = (G_idx+1) * N_i\n",
    "        \n",
    "        X_tilde[stt_pos:end_pos,:,:] = X_tilde_tmp\n",
    "        y_tilde[stt_pos:end_pos,:,:] = y_tilde_tmp\n",
    "        \n",
    "   \n",
    "\n",
    "    for iter in range(N_epochs): #args.epochs\n",
    "        \n",
    "        w_group_array = []\n",
    "        for G_idx in range(G):\n",
    "            w_locals, loss_locals = [], []\n",
    "            idxs_users = np.random.choice(range(N_i), m, replace=False)\n",
    "            idxs_users = np.sort(idxs_users)\n",
    "            print('selected users:',idxs_users)\n",
    "\n",
    "            coded_net = BACC_Enc_Model_withNoise_v3(net_glob.cuda(), N_i, K_i, T, 0.01, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "            dec_z_array = []\n",
    "            for idx in idxs_users: #for idx in range(N):\n",
    "        #         print(idx)\n",
    "                local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[G_idx*N_i+idx,:,:], label=y_tilde[G_idx*N_i+idx,:,:])\n",
    "                w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "#                 w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                w_locals.append(copy.deepcopy(w))\n",
    "                loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "            # update global weights\n",
    "            #w_glob = FedAvg(w_locals)\n",
    "            w_group = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "            \n",
    "            w_group_array.append(copy.deepcopy(w_group))\n",
    "        \n",
    "        w_glob = copy.deepcopy(w_group_array[0])\n",
    "        for k in w_glob.keys():\n",
    "            for G_idx in range(1,G):\n",
    "                w_glob[k] += w_group_array[G_idx][k]\n",
    "            w_glob[k] = torch.div(w_glob[k], len(w_group_array))\n",
    "        \n",
    "        # copy weight to net_glob\n",
    "        net_glob.load_state_dict(w_glob)\n",
    "\n",
    "        # print loss\n",
    "    #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "    #     loss_train_arr.append(loss_train)\n",
    "\n",
    "        acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "        acc_test_arr_G2[trial_idx][iter] = acc_test\n",
    "        loss_test_arr_G2[trial_idx][iter] = loss_test\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_group[G_idx,:,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. G=3, K=6 (K_i=2), N=6 (N_i=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of X: (60000, 784)\n",
      "size of Y: (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "class my_argument:    \n",
    "    epochs = 200    #\"rounds of training\"\n",
    "    num_users = 15  # \"number of users: N\"\n",
    "    num_partition = 6 # \"number of users: K\"\n",
    "    frac = 0.5 #\"the fraction of clients: C\"\n",
    "    local_ep = 1 #\"the number of local epochs: E\"\n",
    "    local_bs = 200 #\"local batch size: B\"\n",
    "    bs=200 #\"test batch size\"\n",
    "    lr=0.01 #\"learning rate\"\n",
    "    momentum=0.5 # \"SGD momentum (default: 0.5)\"\n",
    "    split='user' # \"train-test split type, user or sample\"\n",
    "    opt='ADAM'\n",
    "    loss='Custom' # 'Custom' or 'Default'\n",
    "\n",
    "    # model arguments\n",
    "    model = 'cnn'\n",
    "    kernel_num=9 #, help='number of each kind of kernel')\n",
    "    kernel_sizes='3,4,5' #  help='comma-separated kernel size to use for convolution')\n",
    "    norm='None' #, help=\"batch_norm, layer_norm, or None\")\n",
    "    num_filters=32 #, help=\"number of filters for conv nets\")\n",
    "    max_pool='True' #help=\"Whether use max pooling rather than strided convolutions\")\n",
    "\n",
    "    # other arguments\n",
    "    dataset='mnist' #, help=\"name of dataset\")\n",
    "    iid=1\n",
    "    num_classes=10#, help=\"number of classes\")\n",
    "    num_channels=1#, help=\"number of channels of imges\")\n",
    "    gpu=1#, help=\"GPU ID, -1 for CPU\")\n",
    "    stopping_rounds=10#, help='rounds of early stopping')\n",
    "    verbose='False'#, help='verbose print')\n",
    "    seed=1#, help='random seed (default: 1)')\n",
    "    \n",
    "args = my_argument()\n",
    "\n",
    "args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "# load dataset and split users\n",
    "trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "dataset_train = datasets.MNIST('../data/mnist/', train=True, download=True, transform=trans_mnist)\n",
    "dataset_test = datasets.MNIST('../data/mnist/', train=False, download=True, transform=trans_mnist)\n",
    "\n",
    "dict_users = mnist_iid(dataset_train, args.num_partition)\n",
    "\n",
    "encoding_input_array_np = np.empty((len(dataset_train),28*28))\n",
    "encoding_label_array_np = np.empty((len(dataset_train),args.num_classes))\n",
    "print(\"size of X:\" ,encoding_input_array_np.shape)\n",
    "print(\"size of Y:\" ,encoding_label_array_np.shape)\n",
    "\n",
    "Size_submatrices = int(60000/args.num_partition)\n",
    "\n",
    "for i in range(args.num_partition):\n",
    "    \n",
    "    stt_pos = i*Size_submatrices\n",
    "    end_pos = (i+1)*Size_submatrices\n",
    "#     print(i,stt_pos,end_pos)\n",
    "    Temp_train = DataLoader(DatasetSplit(dataset_train, dict_users[i]), batch_size=Size_submatrices, shuffle=True)\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(Temp_train):\n",
    "        \n",
    "        images_np = images.detach().cpu().numpy()\n",
    "        encoding_input_array_np[stt_pos:end_pos,:] = np.reshape(images_np, (Size_submatrices,28*28))\n",
    "#         print(encoding_input_array_np[stt_pos:end_pos,:].shape)\n",
    "\n",
    "        onehot_labels = torch.nn.functional.one_hot(labels,num_classes=args.num_classes)\n",
    "        labels_np = onehot_labels.detach().cpu().numpy()\n",
    "#         print(labels_np.shape)\n",
    "        encoding_label_array_np[stt_pos:end_pos,:] = labels_np\n",
    "\n",
    "\n",
    "# print(labels_np[0:10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = 3\n",
    "N = 6 # N should be divisible by G\n",
    "K = 6 # N should be divisible by G\n",
    "\n",
    "size_per_group = int(60000/G)\n",
    "\n",
    "X_group = np.reshape(encoding_input_array_np, (G,size_per_group,28*28))\n",
    "y_group = np.reshape(encoding_label_array_np, (G,size_per_group,args.num_classes)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2 3\n",
      "##########################################\n",
      "###### 0 -th Trial!! ###########\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 10000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 10000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 10000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 10000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 10000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 10000 \n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1027/10000 (10.27%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.2964 \n",
      "Accuracy: 2574/10000 (25.74%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.2028 \n",
      "Accuracy: 5098/10000 (50.98%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4135 \n",
      "Accuracy: 6138/10000 (61.38%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.8341 \n",
      "Accuracy: 8016/10000 (80.16%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.4538 \n",
      "Accuracy: 8951/10000 (89.51%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.4353 \n",
      "Accuracy: 9057/10000 (90.57%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.4178 \n",
      "Accuracy: 9011/10000 (90.11%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3867 \n",
      "Accuracy: 9143/10000 (91.43%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3976 \n",
      "Accuracy: 9128/10000 (91.28%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3632 \n",
      "Accuracy: 9188/10000 (91.88%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3798 \n",
      "Accuracy: 9189/10000 (91.89%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3676 \n",
      "Accuracy: 9186/10000 (91.86%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3669 \n",
      "Accuracy: 9174/10000 (91.74%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3746 \n",
      "Accuracy: 9148/10000 (91.48%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3653 \n",
      "Accuracy: 9212/10000 (92.12%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3733 \n",
      "Accuracy: 9189/10000 (91.89%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3773 \n",
      "Accuracy: 9161/10000 (91.61%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3742 \n",
      "Accuracy: 9180/10000 (91.80%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3901 \n",
      "Accuracy: 9140/10000 (91.40%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3975 \n",
      "Accuracy: 9138/10000 (91.38%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3835 \n",
      "Accuracy: 9136/10000 (91.36%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3650 \n",
      "Accuracy: 9167/10000 (91.67%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3753 \n",
      "Accuracy: 9104/10000 (91.04%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3893 \n",
      "Accuracy: 9137/10000 (91.37%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3831 \n",
      "Accuracy: 9179/10000 (91.79%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3602 \n",
      "Accuracy: 9173/10000 (91.73%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3802 \n",
      "Accuracy: 9144/10000 (91.44%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3609 \n",
      "Accuracy: 9185/10000 (91.85%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3711 \n",
      "Accuracy: 9164/10000 (91.64%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "N_i = int(N/G)\n",
    "K_i = int(K/G)\n",
    "T = 3\n",
    "sigma = 1\n",
    "Noise_Alloc = [0,2,4]\n",
    "m = N_i # number of selected workers (if there is no straggler, m=N_i)\n",
    "\n",
    "print(N_i,K_i,T)\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K_i+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K_i+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K_i+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "z_array = np.array([-0.81, 0.81])\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 30\n",
    "\n",
    "loss_test_arr_G3 = np.zeros((N_trials,N_epochs))\n",
    "acc_test_arr_G3  = np.zeros((N_trials,N_epochs))\n",
    "\n",
    "for trial_idx in range(N_trials):\n",
    "    \n",
    "    print('##########################################')\n",
    "    print('######',trial_idx,'-th Trial!! ###########')\n",
    "    \n",
    "    net_glob = CNNMnist2(args=args)\n",
    "    net_glob.cuda()\n",
    "    net_glob.train()\n",
    "    \n",
    "    # copy weights\n",
    "    w_glob = net_glob.state_dict()\n",
    "    \n",
    "    X_tilde = np.empty((N,Size_submatrices,28*28))\n",
    "    y_tilde = np.empty((N,Size_submatrices,10))\n",
    "    \n",
    "    for G_idx in range(G):\n",
    "        \n",
    "        _Noise_label = np.ones((size_per_group*T,10)) * 0.1\n",
    "        \n",
    "        X_tilde_tmp,a,b = BACC_Enc_Data_v3(X_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde_tmp,a,b = BACC_Enc_Data_v3(y_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "        \n",
    "        stt_pos = G_idx * N_i\n",
    "        end_pos = (G_idx+1) * N_i\n",
    "        \n",
    "        X_tilde[stt_pos:end_pos,:,:] = X_tilde_tmp\n",
    "        y_tilde[stt_pos:end_pos,:,:] = y_tilde_tmp\n",
    "        \n",
    "   \n",
    "\n",
    "    for iter in range(N_epochs): #args.epochs\n",
    "        \n",
    "        w_group_array = []\n",
    "        for G_idx in range(G):\n",
    "            w_locals, loss_locals = [], []\n",
    "            idxs_users = np.random.choice(range(N_i), m, replace=False)\n",
    "            idxs_users = np.sort(idxs_users)\n",
    "            print('selected users:',idxs_users)\n",
    "\n",
    "            coded_net = BACC_Enc_Model_withNoise_v3(net_glob.cuda(), N_i, K_i, T, 0.01, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "            dec_z_array = []\n",
    "            for idx in idxs_users: #for idx in range(N):\n",
    "        #         print(idx)\n",
    "                local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[G_idx*N_i+idx,:,:], label=y_tilde[G_idx*N_i+idx,:,:])\n",
    "                w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "                w_locals.append(copy.deepcopy(w))\n",
    "                loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "            # update global weights\n",
    "            #w_glob = FedAvg(w_locals)\n",
    "            w_group = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "            \n",
    "            w_group_array.append(copy.deepcopy(w_group))\n",
    "        \n",
    "        w_glob = copy.deepcopy(w_group_array[0])\n",
    "        for k in w_glob.keys():\n",
    "            for G_idx in range(1,G):\n",
    "                w_glob[k] += w_group_array[G_idx][k]\n",
    "            w_glob[k] = torch.div(w_glob[k], len(w_group_array))\n",
    "        \n",
    "        # copy weight to net_glob\n",
    "        net_glob.load_state_dict(w_glob)\n",
    "\n",
    "        # print loss\n",
    "    #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "    #     loss_train_arr.append(loss_train)\n",
    "\n",
    "        acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "        acc_test_arr_G3[trial_idx][iter] = acc_test\n",
    "        loss_test_arr_G3[trial_idx][iter] = loss_test\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. K=2, Without Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of X: (60000, 784)\n",
      "size of Y: (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "class my_argument:    \n",
    "    epochs = 200    #\"rounds of training\"\n",
    "    num_users = 15  # \"number of users: N\"\n",
    "    num_partition = 2 # \"number of users: K\"\n",
    "    frac = 0.5 #\"the fraction of clients: C\"\n",
    "    local_ep = 1 #\"the number of local epochs: E\"\n",
    "    local_bs = 200 #\"local batch size: B\"\n",
    "    bs=200 #\"test batch size\"\n",
    "    lr=0.01 #\"learning rate\"\n",
    "    momentum=0.5 # \"SGD momentum (default: 0.5)\"\n",
    "    split='user' # \"train-test split type, user or sample\"\n",
    "    opt='ADAM'\n",
    "    loss='Custom' # 'Custom' or 'Default'\n",
    "\n",
    "    # model arguments\n",
    "    model = 'cnn'\n",
    "    kernel_num=9 #, help='number of each kind of kernel')\n",
    "    kernel_sizes='3,4,5' #  help='comma-separated kernel size to use for convolution')\n",
    "    norm='None' #, help=\"batch_norm, layer_norm, or None\")\n",
    "    num_filters=32 #, help=\"number of filters for conv nets\")\n",
    "    max_pool='True' #help=\"Whether use max pooling rather than strided convolutions\")\n",
    "\n",
    "    # other arguments\n",
    "    dataset='mnist' #, help=\"name of dataset\")\n",
    "    iid=1\n",
    "    num_classes=10#, help=\"number of classes\")\n",
    "    num_channels=1#, help=\"number of channels of imges\")\n",
    "    gpu=1#, help=\"GPU ID, -1 for CPU\")\n",
    "    stopping_rounds=10#, help='rounds of early stopping')\n",
    "    verbose='False'#, help='verbose print')\n",
    "    seed=1#, help='random seed (default: 1)')\n",
    "    \n",
    "args = my_argument()\n",
    "\n",
    "args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "# load dataset and split users\n",
    "trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "dataset_train = datasets.MNIST('../data/mnist/', train=True, download=True, transform=trans_mnist)\n",
    "dataset_test = datasets.MNIST('../data/mnist/', train=False, download=True, transform=trans_mnist)\n",
    "\n",
    "dict_users = mnist_iid(dataset_train, args.num_partition)\n",
    "\n",
    "encoding_input_array_np = np.empty((len(dataset_train),28*28))\n",
    "encoding_label_array_np = np.empty((len(dataset_train),args.num_classes))\n",
    "print(\"size of X:\" ,encoding_input_array_np.shape)\n",
    "print(\"size of Y:\" ,encoding_label_array_np.shape)\n",
    "\n",
    "Size_submatrices = int(60000/args.num_partition)\n",
    "\n",
    "for i in range(args.num_partition):\n",
    "    \n",
    "    stt_pos = i*Size_submatrices\n",
    "    end_pos = (i+1)*Size_submatrices\n",
    "#     print(i,stt_pos,end_pos)\n",
    "    Temp_train = DataLoader(DatasetSplit(dataset_train, dict_users[i]), batch_size=Size_submatrices, shuffle=True)\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(Temp_train):\n",
    "        \n",
    "        images_np = images.detach().cpu().numpy()\n",
    "        encoding_input_array_np[stt_pos:end_pos,:] = np.reshape(images_np, (Size_submatrices,28*28))\n",
    "#         print(encoding_input_array_np[stt_pos:end_pos,:].shape)\n",
    "\n",
    "        onehot_labels = torch.nn.functional.one_hot(labels,num_classes=args.num_classes)\n",
    "        labels_np = onehot_labels.detach().cpu().numpy()\n",
    "#         print(labels_np.shape)\n",
    "        encoding_label_array_np[stt_pos:end_pos,:] = labels_np\n",
    "\n",
    "\n",
    "# print(labels_np[0:10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z_array: [-0.81  0.81]\n",
      "0.4838626198316927\n",
      "0.48386261983169315\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 30000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 30000 \n",
      "\n",
      "(T, sigma)= 3 1 )  0 -th Trial!!\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.1504 \n",
      "Accuracy: 5550/10000 (55.50%)\n",
      "\n",
      "Round   0, Average loss 2.150 Test accuracy 55.500\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.7712 \n",
      "Accuracy: 8177/10000 (81.77%)\n",
      "\n",
      "Round   1, Average loss 0.771 Test accuracy 81.770\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2180 \n",
      "Accuracy: 9692/10000 (96.92%)\n",
      "\n",
      "Round   2, Average loss 0.218 Test accuracy 96.920\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1758 \n",
      "Accuracy: 9740/10000 (97.40%)\n",
      "\n",
      "Round   3, Average loss 0.176 Test accuracy 97.400\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1905 \n",
      "Accuracy: 9728/10000 (97.28%)\n",
      "\n",
      "Round   4, Average loss 0.191 Test accuracy 97.280\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1868 \n",
      "Accuracy: 9750/10000 (97.50%)\n",
      "\n",
      "Round   5, Average loss 0.187 Test accuracy 97.500\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1771 \n",
      "Accuracy: 9752/10000 (97.52%)\n",
      "\n",
      "Round   6, Average loss 0.177 Test accuracy 97.520\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1665 \n",
      "Accuracy: 9737/10000 (97.37%)\n",
      "\n",
      "Round   7, Average loss 0.166 Test accuracy 97.370\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1977 \n",
      "Accuracy: 9736/10000 (97.36%)\n",
      "\n",
      "Round   8, Average loss 0.198 Test accuracy 97.360\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1733 \n",
      "Accuracy: 9732/10000 (97.32%)\n",
      "\n",
      "Round   9, Average loss 0.173 Test accuracy 97.320\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1734 \n",
      "Accuracy: 9756/10000 (97.56%)\n",
      "\n",
      "Round  10, Average loss 0.173 Test accuracy 97.560\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1853 \n",
      "Accuracy: 9765/10000 (97.65%)\n",
      "\n",
      "Round  11, Average loss 0.185 Test accuracy 97.650\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3139 \n",
      "Accuracy: 9485/10000 (94.85%)\n",
      "\n",
      "Round  12, Average loss 0.314 Test accuracy 94.850\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1789 \n",
      "Accuracy: 9738/10000 (97.38%)\n",
      "\n",
      "Round  13, Average loss 0.179 Test accuracy 97.380\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2365 \n",
      "Accuracy: 9737/10000 (97.37%)\n",
      "\n",
      "Round  14, Average loss 0.236 Test accuracy 97.370\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2020 \n",
      "Accuracy: 9748/10000 (97.48%)\n",
      "\n",
      "Round  15, Average loss 0.202 Test accuracy 97.480\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1793 \n",
      "Accuracy: 9749/10000 (97.49%)\n",
      "\n",
      "Round  16, Average loss 0.179 Test accuracy 97.490\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1965 \n",
      "Accuracy: 9742/10000 (97.42%)\n",
      "\n",
      "Round  17, Average loss 0.196 Test accuracy 97.420\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1861 \n",
      "Accuracy: 9749/10000 (97.49%)\n",
      "\n",
      "Round  18, Average loss 0.186 Test accuracy 97.490\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3015 \n",
      "Accuracy: 9566/10000 (95.66%)\n",
      "\n",
      "Round  19, Average loss 0.302 Test accuracy 95.660\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1838 \n",
      "Accuracy: 9762/10000 (97.62%)\n",
      "\n",
      "Round  20, Average loss 0.184 Test accuracy 97.620\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1723 \n",
      "Accuracy: 9744/10000 (97.44%)\n",
      "\n",
      "Round  21, Average loss 0.172 Test accuracy 97.440\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2225 \n",
      "Accuracy: 9748/10000 (97.48%)\n",
      "\n",
      "Round  22, Average loss 0.223 Test accuracy 97.480\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1853 \n",
      "Accuracy: 9747/10000 (97.47%)\n",
      "\n",
      "Round  23, Average loss 0.185 Test accuracy 97.470\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1962 \n",
      "Accuracy: 9755/10000 (97.55%)\n",
      "\n",
      "Round  24, Average loss 0.196 Test accuracy 97.550\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2181 \n",
      "Accuracy: 9743/10000 (97.43%)\n",
      "\n",
      "Round  25, Average loss 0.218 Test accuracy 97.430\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2300 \n",
      "Accuracy: 9750/10000 (97.50%)\n",
      "\n",
      "Round  26, Average loss 0.230 Test accuracy 97.500\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1930 \n",
      "Accuracy: 9749/10000 (97.49%)\n",
      "\n",
      "Round  27, Average loss 0.193 Test accuracy 97.490\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1894 \n",
      "Accuracy: 9753/10000 (97.53%)\n",
      "\n",
      "Round  28, Average loss 0.189 Test accuracy 97.530\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2037 \n",
      "Accuracy: 9753/10000 (97.53%)\n",
      "\n",
      "Round  29, Average loss 0.204 Test accuracy 97.530\n",
      "z_array: [-0.81 -0.22  0.22  0.81]\n",
      "0.4838626198316927\n",
      "0.46852880899669475\n",
      "0.468528808996694\n",
      "0.48386261983169315\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 30000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 30000 \n",
      "\n",
      "(T, sigma)= 3 1 )  0 -th Trial!!\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.2978 \n",
      "Accuracy: 4081/10000 (40.81%)\n",
      "\n",
      "Round   0, Average loss 2.298 Test accuracy 40.810\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.2801 \n",
      "Accuracy: 3493/10000 (34.93%)\n",
      "\n",
      "Round   1, Average loss 2.280 Test accuracy 34.930\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.2261 \n",
      "Accuracy: 7954/10000 (79.54%)\n",
      "\n",
      "Round   2, Average loss 2.226 Test accuracy 79.540\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.1914 \n",
      "Accuracy: 7929/10000 (79.29%)\n",
      "\n",
      "Round   3, Average loss 2.191 Test accuracy 79.290\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.1369 \n",
      "Accuracy: 6894/10000 (68.94%)\n",
      "\n",
      "Round   4, Average loss 2.137 Test accuracy 68.940\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.0815 \n",
      "Accuracy: 8826/10000 (88.26%)\n",
      "\n",
      "Round   5, Average loss 2.081 Test accuracy 88.260\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.0964 \n",
      "Accuracy: 8591/10000 (85.91%)\n",
      "\n",
      "Round   6, Average loss 2.096 Test accuracy 85.910\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.1099 \n",
      "Accuracy: 9409/10000 (94.09%)\n",
      "\n",
      "Round   7, Average loss 2.110 Test accuracy 94.090\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.1391 \n",
      "Accuracy: 8650/10000 (86.50%)\n",
      "\n",
      "Round   8, Average loss 2.139 Test accuracy 86.500\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.0060 \n",
      "Accuracy: 9606/10000 (96.06%)\n",
      "\n",
      "Round   9, Average loss 2.006 Test accuracy 96.060\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9598 \n",
      "Accuracy: 9456/10000 (94.56%)\n",
      "\n",
      "Round  10, Average loss 1.960 Test accuracy 94.560\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9703 \n",
      "Accuracy: 9485/10000 (94.85%)\n",
      "\n",
      "Round  11, Average loss 1.970 Test accuracy 94.850\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9826 \n",
      "Accuracy: 9690/10000 (96.90%)\n",
      "\n",
      "Round  12, Average loss 1.983 Test accuracy 96.900\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.0128 \n",
      "Accuracy: 9711/10000 (97.11%)\n",
      "\n",
      "Round  13, Average loss 2.013 Test accuracy 97.110\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8374 \n",
      "Accuracy: 9722/10000 (97.22%)\n",
      "\n",
      "Round  14, Average loss 1.837 Test accuracy 97.220\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9075 \n",
      "Accuracy: 9455/10000 (94.55%)\n",
      "\n",
      "Round  15, Average loss 1.908 Test accuracy 94.550\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9255 \n",
      "Accuracy: 9717/10000 (97.17%)\n",
      "\n",
      "Round  16, Average loss 1.925 Test accuracy 97.170\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8705 \n",
      "Accuracy: 9569/10000 (95.69%)\n",
      "\n",
      "Round  17, Average loss 1.871 Test accuracy 95.690\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8759 \n",
      "Accuracy: 9661/10000 (96.61%)\n",
      "\n",
      "Round  18, Average loss 1.876 Test accuracy 96.610\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9110 \n",
      "Accuracy: 9700/10000 (97.00%)\n",
      "\n",
      "Round  19, Average loss 1.911 Test accuracy 97.000\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8551 \n",
      "Accuracy: 9721/10000 (97.21%)\n",
      "\n",
      "Round  20, Average loss 1.855 Test accuracy 97.210\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.0106 \n",
      "Accuracy: 9707/10000 (97.07%)\n",
      "\n",
      "Round  21, Average loss 2.011 Test accuracy 97.070\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.0318 \n",
      "Accuracy: 9667/10000 (96.67%)\n",
      "\n",
      "Round  22, Average loss 2.032 Test accuracy 96.670\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9992 \n",
      "Accuracy: 9299/10000 (92.99%)\n",
      "\n",
      "Round  23, Average loss 1.999 Test accuracy 92.990\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8857 \n",
      "Accuracy: 9527/10000 (95.27%)\n",
      "\n",
      "Round  24, Average loss 1.886 Test accuracy 95.270\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8113 \n",
      "Accuracy: 9700/10000 (97.00%)\n",
      "\n",
      "Round  25, Average loss 1.811 Test accuracy 97.000\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8391 \n",
      "Accuracy: 9736/10000 (97.36%)\n",
      "\n",
      "Round  26, Average loss 1.839 Test accuracy 97.360\n",
      "selected users: [0 1 2 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.9353 \n",
      "Accuracy: 9652/10000 (96.52%)\n",
      "\n",
      "Round  27, Average loss 1.935 Test accuracy 96.520\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.7971 \n",
      "Accuracy: 9755/10000 (97.55%)\n",
      "\n",
      "Round  28, Average loss 1.797 Test accuracy 97.550\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9200 \n",
      "Accuracy: 9682/10000 (96.82%)\n",
      "\n",
      "Round  29, Average loss 1.920 Test accuracy 96.820\n",
      "z_array: [-0.9  -0.81 -0.22  0.22  0.81  0.9 ]\n",
      "0.039572349708934106\n",
      "0.4838626198316927\n",
      "0.46852880899669475\n",
      "0.468528808996694\n",
      "0.48386261983169315\n",
      "0.039572349708934106\n",
      "@BACC_Enc: N,K,T, m_i= 6 2 3 30000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 6 2 3 30000 \n",
      "\n",
      "(T, sigma)= 3 1 )  0 -th Trial!!\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 2.2223 \n",
      "Accuracy: 6257/10000 (62.57%)\n",
      "\n",
      "Round   1, Average loss 2.222 Test accuracy 62.570\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 2.2161 \n",
      "Accuracy: 6335/10000 (63.35%)\n",
      "\n",
      "Round   2, Average loss 2.216 Test accuracy 63.350\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 1.6638 \n",
      "Accuracy: 8681/10000 (86.81%)\n",
      "\n",
      "Round   3, Average loss 1.664 Test accuracy 86.810\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 1.3783 \n",
      "Accuracy: 9617/10000 (96.17%)\n",
      "\n",
      "Round   4, Average loss 1.378 Test accuracy 96.170\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 1.2237 \n",
      "Accuracy: 9644/10000 (96.44%)\n",
      "\n",
      "Round   5, Average loss 1.224 Test accuracy 96.440\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.8223 \n",
      "Accuracy: 9638/10000 (96.38%)\n",
      "\n",
      "Round   6, Average loss 0.822 Test accuracy 96.380\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.7091 \n",
      "Accuracy: 9713/10000 (97.13%)\n",
      "\n",
      "Round   7, Average loss 0.709 Test accuracy 97.130\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.6716 \n",
      "Accuracy: 9713/10000 (97.13%)\n",
      "\n",
      "Round   8, Average loss 0.672 Test accuracy 97.130\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.6693 \n",
      "Accuracy: 9678/10000 (96.78%)\n",
      "\n",
      "Round   9, Average loss 0.669 Test accuracy 96.780\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.5860 \n",
      "Accuracy: 9650/10000 (96.50%)\n",
      "\n",
      "Round  10, Average loss 0.586 Test accuracy 96.500\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.5678 \n",
      "Accuracy: 9689/10000 (96.89%)\n",
      "\n",
      "Round  11, Average loss 0.568 Test accuracy 96.890\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.6501 \n",
      "Accuracy: 9699/10000 (96.99%)\n",
      "\n",
      "Round  12, Average loss 0.650 Test accuracy 96.990\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.5906 \n",
      "Accuracy: 9678/10000 (96.78%)\n",
      "\n",
      "Round  13, Average loss 0.591 Test accuracy 96.780\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.6501 \n",
      "Accuracy: 9738/10000 (97.38%)\n",
      "\n",
      "Round  14, Average loss 0.650 Test accuracy 97.380\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.5508 \n",
      "Accuracy: 9722/10000 (97.22%)\n",
      "\n",
      "Round  15, Average loss 0.551 Test accuracy 97.220\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.5475 \n",
      "Accuracy: 9593/10000 (95.93%)\n",
      "\n",
      "Round  16, Average loss 0.547 Test accuracy 95.930\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.5397 \n",
      "Accuracy: 9471/10000 (94.71%)\n",
      "\n",
      "Round  17, Average loss 0.540 Test accuracy 94.710\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.8024 \n",
      "Accuracy: 9654/10000 (96.54%)\n",
      "\n",
      "Round  18, Average loss 0.802 Test accuracy 96.540\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.6738 \n",
      "Accuracy: 9694/10000 (96.94%)\n",
      "\n",
      "Round  19, Average loss 0.674 Test accuracy 96.940\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.5364 \n",
      "Accuracy: 9693/10000 (96.93%)\n",
      "\n",
      "Round  20, Average loss 0.536 Test accuracy 96.930\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.6852 \n",
      "Accuracy: 9707/10000 (97.07%)\n",
      "\n",
      "Round  21, Average loss 0.685 Test accuracy 97.070\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.6254 \n",
      "Accuracy: 9715/10000 (97.15%)\n",
      "\n",
      "Round  22, Average loss 0.625 Test accuracy 97.150\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.5700 \n",
      "Accuracy: 9576/10000 (95.76%)\n",
      "\n",
      "Round  23, Average loss 0.570 Test accuracy 95.760\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.7501 \n",
      "Accuracy: 9319/10000 (93.19%)\n",
      "\n",
      "Round  24, Average loss 0.750 Test accuracy 93.190\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.8517 \n",
      "Accuracy: 9267/10000 (92.67%)\n",
      "\n",
      "Round  25, Average loss 0.852 Test accuracy 92.670\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.8802 \n",
      "Accuracy: 9491/10000 (94.91%)\n",
      "\n",
      "Round  26, Average loss 0.880 Test accuracy 94.910\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.8818 \n",
      "Accuracy: 8799/10000 (87.99%)\n",
      "\n",
      "Round  27, Average loss 0.882 Test accuracy 87.990\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.8674 \n",
      "Accuracy: 9460/10000 (94.60%)\n",
      "\n",
      "Round  28, Average loss 0.867 Test accuracy 94.600\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.6362 \n",
      "Accuracy: 9709/10000 (97.09%)\n",
      "\n",
      "Round  29, Average loss 0.636 Test accuracy 97.090\n",
      "z_array: [-0.9  -0.81 -0.22 -0.16  0.16  0.22  0.81  0.9 ]\n",
      "0.039572349708934106\n",
      "0.4838626198316927\n",
      "0.46852880899669475\n",
      "0.23252578774407598\n",
      "0.23252578774407545\n",
      "0.468528808996694\n",
      "0.48386261983169315\n",
      "0.039572349708934106\n",
      "@BACC_Enc: N,K,T, m_i= 8 2 3 30000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 8 2 3 30000 \n",
      "\n",
      "(T, sigma)= 3 1 )  0 -th Trial!!\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 2.2079 \n",
      "Accuracy: 4164/10000 (41.64%)\n",
      "\n",
      "Round   0, Average loss 2.208 Test accuracy 41.640\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.6987 \n",
      "Accuracy: 7686/10000 (76.86%)\n",
      "\n",
      "Round   1, Average loss 1.699 Test accuracy 76.860\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.5383 \n",
      "Accuracy: 9540/10000 (95.40%)\n",
      "\n",
      "Round   2, Average loss 0.538 Test accuracy 95.400\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.5397 \n",
      "Accuracy: 9462/10000 (94.62%)\n",
      "\n",
      "Round   3, Average loss 0.540 Test accuracy 94.620\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.6978 \n",
      "Accuracy: 9447/10000 (94.47%)\n",
      "\n",
      "Round   4, Average loss 0.698 Test accuracy 94.470\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.3977 \n",
      "Accuracy: 9543/10000 (95.43%)\n",
      "\n",
      "Round   5, Average loss 0.398 Test accuracy 95.430\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.8793 \n",
      "Accuracy: 9627/10000 (96.27%)\n",
      "\n",
      "Round   6, Average loss 0.879 Test accuracy 96.270\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.2739 \n",
      "Accuracy: 9606/10000 (96.06%)\n",
      "\n",
      "Round   7, Average loss 0.274 Test accuracy 96.060\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.2858 \n",
      "Accuracy: 9585/10000 (95.85%)\n",
      "\n",
      "Round   8, Average loss 0.286 Test accuracy 95.850\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.4189 \n",
      "Accuracy: 9596/10000 (95.96%)\n",
      "\n",
      "Round   9, Average loss 0.419 Test accuracy 95.960\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.4356 \n",
      "Accuracy: 9599/10000 (95.99%)\n",
      "\n",
      "Round  10, Average loss 0.436 Test accuracy 95.990\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.3830 \n",
      "Accuracy: 9478/10000 (94.78%)\n",
      "\n",
      "Round  11, Average loss 0.383 Test accuracy 94.780\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.3993 \n",
      "Accuracy: 9545/10000 (95.45%)\n",
      "\n",
      "Round  12, Average loss 0.399 Test accuracy 95.450\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.4343 \n",
      "Accuracy: 9518/10000 (95.18%)\n",
      "\n",
      "Round  13, Average loss 0.434 Test accuracy 95.180\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.2578 \n",
      "Accuracy: 9606/10000 (96.06%)\n",
      "\n",
      "Round  14, Average loss 0.258 Test accuracy 96.060\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.2952 \n",
      "Accuracy: 9710/10000 (97.10%)\n",
      "\n",
      "Round  15, Average loss 0.295 Test accuracy 97.100\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.3339 \n",
      "Accuracy: 9661/10000 (96.61%)\n",
      "\n",
      "Round  16, Average loss 0.334 Test accuracy 96.610\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.5458 \n",
      "Accuracy: 9503/10000 (95.03%)\n",
      "\n",
      "Round  17, Average loss 0.546 Test accuracy 95.030\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.4201 \n",
      "Accuracy: 9622/10000 (96.22%)\n",
      "\n",
      "Round  18, Average loss 0.420 Test accuracy 96.220\n",
      "selected users: [0 1 2 3 4 5 6 7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.5977 \n",
      "Accuracy: 9224/10000 (92.24%)\n",
      "\n",
      "Round  19, Average loss 0.598 Test accuracy 92.240\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.3295 \n",
      "Accuracy: 9644/10000 (96.44%)\n",
      "\n",
      "Round  20, Average loss 0.330 Test accuracy 96.440\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.3735 \n",
      "Accuracy: 9571/10000 (95.71%)\n",
      "\n",
      "Round  21, Average loss 0.373 Test accuracy 95.710\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.3915 \n",
      "Accuracy: 9354/10000 (93.54%)\n",
      "\n",
      "Round  22, Average loss 0.391 Test accuracy 93.540\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.4134 \n",
      "Accuracy: 9209/10000 (92.09%)\n",
      "\n",
      "Round  23, Average loss 0.413 Test accuracy 92.090\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.4039 \n",
      "Accuracy: 9634/10000 (96.34%)\n",
      "\n",
      "Round  24, Average loss 0.404 Test accuracy 96.340\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.4398 \n",
      "Accuracy: 9725/10000 (97.25%)\n",
      "\n",
      "Round  25, Average loss 0.440 Test accuracy 97.250\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.4958 \n",
      "Accuracy: 9551/10000 (95.51%)\n",
      "\n",
      "Round  26, Average loss 0.496 Test accuracy 95.510\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.4113 \n",
      "Accuracy: 9572/10000 (95.72%)\n",
      "\n",
      "Round  27, Average loss 0.411 Test accuracy 95.720\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.2503 \n",
      "Accuracy: 9627/10000 (96.27%)\n",
      "\n",
      "Round  28, Average loss 0.250 Test accuracy 96.270\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.3805 \n",
      "Accuracy: 9541/10000 (95.41%)\n",
      "\n",
      "Round  29, Average loss 0.381 Test accuracy 95.410\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "K = 2\n",
    "T = 3\n",
    "sigma = 1\n",
    "Noise_Alloc = [0,2,4]\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "\n",
    "N_array = [2,4,6,8]\n",
    "B_array = [0.5]\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 30\n",
    "\n",
    "\n",
    "\n",
    "loss_test_arr_v1 = np.zeros((len(N_array),len(B_array),N_trials,N_epochs))\n",
    "acc_test_arr_v1  = np.zeros((len(N_array),len(B_array),N_trials,N_epochs))\n",
    "\n",
    "for N_idx in range(len(N_array)):\n",
    "    \n",
    "    N = N_array[N_idx]\n",
    "    \n",
    "    \n",
    "           \n",
    "        \n",
    "    # print(\"alpha_array: \",alpha_array,'\\n')\n",
    "    \n",
    "    \n",
    "    for B_idx in range(len(B_array)):\n",
    "        \n",
    "        B = B_array[B_idx]\n",
    "        z_array = []\n",
    "#         while(len(z_array)<N):\n",
    "#             z_tmp = np.random.uniform(-1,1,1)\n",
    "#             MIS_tmp = MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_tmp], 1,sigma)\n",
    "#             if MIS_tmp < B and MIS_tmp > 0.1:\n",
    "#                 z_array.append(z_tmp[0])\n",
    "#         \n",
    "#         z_array = np.sort(z_array)\n",
    "\n",
    "        if N==2:\n",
    "            z_array = np.array([-0.81, 0.81])\n",
    "        elif N ==4:\n",
    "            z_array = np.array([-0.81, -0.22, 0.22, 0.81])\n",
    "        elif N ==5:\n",
    "            z_array = np.array([-0.81, -0.22, 0, 0.22, 0.81])\n",
    "        elif N ==6:\n",
    "            z_array = np.array([-0.9, -0.81, -0.22, 0.22, 0.81, 0.9])\n",
    "        elif N ==7:\n",
    "            z_array = np.array([-0.9, -0.82, -0.21, 0, 0.21, 0.82, 0.9])\n",
    "        else:\n",
    "            z_array = np.array([-0.9, -0.81, -0.22, -0.16, 0.16, 0.22, 0.81, 0.9])\n",
    "#             z_array = np.array([-0.9, -0.81, -0.22, -0.20, 0.20, 0.22, 0.81, 0.9])\n",
    "            \n",
    "        print('z_array:',z_array)\n",
    "        for j in range(len(z_array)):\n",
    "            print(MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_array[j]], 1,sigma))\n",
    "        \n",
    "        \n",
    "        _Noise_label = np.ones((30000*T,10)) * 0.1\n",
    "\n",
    "        X_tilde,a,b = BACC_Enc_Data_v3(encoding_input_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde,a,b = BACC_Enc_Data_v3(encoding_label_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "\n",
    "        m = N # m is the number of received result @ master\n",
    "    #     print('number of results:',m)\n",
    "\n",
    "        for trial_idx in range(N_trials):\n",
    "            print('(T, sigma)=',T,sigma,') ',trial_idx,'-th Trial!!')\n",
    "\n",
    "            net_glob = CNNMnist2(args=args)\n",
    "            net_glob.cuda()\n",
    "            net_glob.train()\n",
    "\n",
    "            # copy weights\n",
    "            w_glob = net_glob.state_dict()\n",
    "\n",
    "            for iter in range(N_epochs): #args.epochs\n",
    "                w_locals, loss_locals = [], []\n",
    "                idxs_users = np.random.choice(range(N), m, replace=False)\n",
    "                idxs_users = np.sort(idxs_users)\n",
    "                print('selected users:',idxs_users)\n",
    "                \n",
    "                coded_net = BACC_Enc_Model_withNoise_v3(net_glob.cuda(), N, K, T, 0.01, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "                dec_z_array = []\n",
    "                for idx in idxs_users: #for idx in range(N):\n",
    "            #         print(idx)\n",
    "                    local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[idx,:,:], label=y_tilde[idx,:,:])\n",
    "                    w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "                    w_locals.append(copy.deepcopy(w))\n",
    "                    loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                    dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "                # update global weights\n",
    "                #w_glob = FedAvg(w_locals)\n",
    "                w_glob = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "\n",
    "                # copy weight to net_glob\n",
    "                net_glob.load_state_dict(w_glob)\n",
    "\n",
    "                # print loss\n",
    "            #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "            #     loss_train_arr.append(loss_train)\n",
    "\n",
    "                acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "                acc_test_arr_v1[N_idx][B_idx][trial_idx][iter] = acc_test\n",
    "                loss_test_arr_v1[N_idx][B_idx][trial_idx][iter] = loss_test\n",
    "                if iter % 1 ==0:\n",
    "                    print('Round {:3d}, Average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_test, acc_test))\n",
    "                #print(loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "B_sel = 0\n",
    "\n",
    "# plt.plot(acc_test_arr_v1[0,B_sel,0,0:30],label='N=2, B='+str(B_array[B_sel]))\n",
    "plt.plot(acc_test_arr_v1[1,B_sel,0,0:30],label='N=4, B='+str(B_array[B_sel]))\n",
    "# plt.plot(acc_test_arr_v1[2,B_sel,0,0:30],label='N=6, B='+str(B_array[B_sel]))\n",
    "# plt.plot(acc_test_arr_v1[3,B_sel,0,0:30],label='N=8, B='+str(B_array[B_sel]))\n",
    "plt.plot(acc_test_arr_G2[0,0:30],label='Grouping, N=4, G=2, B='+str(B_array[B_sel]))\n",
    "# plt.plot(acc_test_arr_G3[0,0:30],label='Grouping, N=6, G=3, B='+str(B_array[B_sel]))\n",
    "\n",
    "# plt.plot(acc_test_arr_v1[4,B_sel,0,0:30],label='N=7, B='+str(B_array[B_sel]))\n",
    "# plt.plot(acc_test_arr_v1[5,B_sel,0,0:30],label='N=8, B='+str(B_array[B_sel]))\n",
    "\n",
    "# plt.plot(plot_acc_v2[2,sigma_sel,:],label='T=10')\n",
    "# plt.plot(plot_acc_v2[3,sigma_sel,:],label='T=11')\n",
    "\n",
    "# plt.plot(acc_test_arr_uncoded,'r',label='uncoded')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "# plt.title(title_name)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Test accuracy(%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. K=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of X: (60000, 784)\n",
      "size of Y: (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "class my_argument:    \n",
    "    epochs = 200    #\"rounds of training\"\n",
    "    num_users = 15  # \"number of users: N\"\n",
    "    num_partition = 4 # \"number of users: K\"\n",
    "    frac = 0.5 #\"the fraction of clients: C\"\n",
    "    local_ep = 1 #\"the number of local epochs: E\"\n",
    "    local_bs = 200 #\"local batch size: B\"\n",
    "    bs=200 #\"test batch size\"\n",
    "    lr=0.01 #\"learning rate\"\n",
    "    momentum=0.5 # \"SGD momentum (default: 0.5)\"\n",
    "    split='user' # \"train-test split type, user or sample\"\n",
    "    opt='ADAM'\n",
    "    loss='Custom' # 'Custom' or 'Default'\n",
    "\n",
    "    # model arguments\n",
    "    model = 'cnn'\n",
    "    kernel_num=9 #, help='number of each kind of kernel')\n",
    "    kernel_sizes='3,4,5' #  help='comma-separated kernel size to use for convolution')\n",
    "    norm='None' #, help=\"batch_norm, layer_norm, or None\")\n",
    "    num_filters=32 #, help=\"number of filters for conv nets\")\n",
    "    max_pool='True' #help=\"Whether use max pooling rather than strided convolutions\")\n",
    "\n",
    "    # other arguments\n",
    "    dataset='mnist' #, help=\"name of dataset\")\n",
    "    iid=1\n",
    "    num_classes=10#, help=\"number of classes\")\n",
    "    num_channels=1#, help=\"number of channels of imges\")\n",
    "    gpu=1#, help=\"GPU ID, -1 for CPU\")\n",
    "    stopping_rounds=10#, help='rounds of early stopping')\n",
    "    verbose='False'#, help='verbose print')\n",
    "    seed=1#, help='random seed (default: 1)')\n",
    "    \n",
    "args = my_argument()\n",
    "\n",
    "args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "# load dataset and split users\n",
    "trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "dataset_train = datasets.MNIST('../data/mnist/', train=True, download=True, transform=trans_mnist)\n",
    "dataset_test = datasets.MNIST('../data/mnist/', train=False, download=True, transform=trans_mnist)\n",
    "\n",
    "dict_users = mnist_iid(dataset_train, args.num_partition)\n",
    "\n",
    "encoding_input_array_np = np.empty((len(dataset_train),28*28))\n",
    "encoding_label_array_np = np.empty((len(dataset_train),args.num_classes))\n",
    "print(\"size of X:\" ,encoding_input_array_np.shape)\n",
    "print(\"size of Y:\" ,encoding_label_array_np.shape)\n",
    "\n",
    "Size_submatrices = int(60000/args.num_partition)\n",
    "\n",
    "for i in range(args.num_partition):\n",
    "    \n",
    "    stt_pos = i*Size_submatrices\n",
    "    end_pos = (i+1)*Size_submatrices\n",
    "#     print(i,stt_pos,end_pos)\n",
    "    Temp_train = DataLoader(DatasetSplit(dataset_train, dict_users[i]), batch_size=Size_submatrices, shuffle=True)\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(Temp_train):\n",
    "        \n",
    "        images_np = images.detach().cpu().numpy()\n",
    "        encoding_input_array_np[stt_pos:end_pos,:] = np.reshape(images_np, (Size_submatrices,28*28))\n",
    "#         print(encoding_input_array_np[stt_pos:end_pos,:].shape)\n",
    "\n",
    "        onehot_labels = torch.nn.functional.one_hot(labels,num_classes=args.num_classes)\n",
    "        labels_np = onehot_labels.detach().cpu().numpy()\n",
    "#         print(labels_np.shape)\n",
    "        encoding_label_array_np[stt_pos:end_pos,:] = labels_np\n",
    "\n",
    "\n",
    "# print(labels_np[0:10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "395 137 77 66 325\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.9967233991100619 0.012570500902590362\n",
      "-0.9948458393310273 0.009186154041335111\n",
      "-0.9903708272549521 0.0030278602507773783\n",
      "-0.9881497879818113 0.0011322378640799269\n",
      "-0.9856357664431257 7.237229030193549e-05\n",
      "-0.9819502816066774 0.0009151342222115455\n",
      "-0.9743801406025652 0.013807862953069145\n",
      "-0.9732429036029051 0.017308391808860374\n",
      "-0.9716499425996106 0.02300816009848999\n",
      "-0.9707810034884226 0.026529720146499475\n",
      "-0.9629358119852338 0.07356572119174355\n",
      "-0.9592848094889013 0.10648755223399524\n",
      "-0.9578307990518302 0.12188755258432364\n",
      "-0.9571486494917996 0.12959372414089768\n",
      "-0.9548728694739659 0.15764285382604115\n",
      "-0.9543836651467361 0.16416119397904505\n",
      "-0.9512302584551071 0.21060896428961837\n",
      "-0.9501286871582235 0.22873571263139653\n",
      "-0.9493079992209263 0.24291445045342466\n",
      "-0.9468712143720244 0.2885550929930391\n",
      "-0.9462045027048012 0.3020009958277021\n",
      "-0.9450226360856129 0.3268894006886918\n",
      "-0.942889051744954 0.37535344112475444\n",
      "-0.9420255449368415 0.3963084326299616\n",
      "-0.9399882819365448 0.44893946653047034\n",
      "-0.9393210692612008 0.4671793063599652\n",
      "-0.9361401480934441 0.5612547387172819\n",
      "-0.9334249487300454 0.6513177462798181\n",
      "-0.932257382371722 0.6929463184727821\n",
      "-0.9289758144235429 0.8197277601845057\n",
      "-0.927738182324308 0.8714218799219998\n",
      "-0.9271053605255939 0.8987001744079706\n",
      "-0.9235110987778754 1.0648437210320687\n",
      "-0.9234480005306531 1.0679348900683752\n",
      "-0.9233213357776953 1.0741587323924215\n",
      "-0.918849424826937 1.3102067934801664\n",
      "-0.9186182427332463 1.3232967588171969\n",
      "-0.9181858737213797 1.348019590780991\n",
      "-0.9168905756695997 1.4239895832414762\n",
      "-0.9163210916120146 1.4583072724489874\n",
      "-0.9150248839348085 1.5385496585068086\n",
      "-0.9135509049600627 1.6334782867338973\n",
      "-0.9123670637792722 1.712638254922767\n",
      "-0.9113186540835647 1.7849729240484424\n",
      "-0.9111252443304267 1.7985504623655058\n",
      "-0.9077342856705073 2.0488775378617987\n",
      "-0.9067488000118127 2.1261699951421535\n",
      "-0.9015728242039807 2.5692475464313973\n",
      "-0.9003597325562589 2.6829564783466524\n",
      "-0.8992964395654581 2.7860290679332906\n",
      "-0.8967889938292337 3.0426685631404755\n",
      "-0.8929303360009146 3.4801107236187185\n",
      "-0.8924259873123535 3.5416352736229997\n",
      "-0.8872975815773814 4.238337724143214\n",
      "-0.887075149848507 4.271965774429419\n",
      "-0.8869893920991923 4.285018736673706\n",
      "-0.8859689029392013 4.4442512946490895\n",
      "-0.8856471272842785 4.496022920882679\n",
      "-0.884814672454697 4.633690574454463\n",
      "-0.8834943859785533 4.864050930487142\n",
      "-0.8817329488836674 5.197882779004097\n",
      "-0.8813696754392899 5.271034956378789\n",
      "-0.881088643191996 5.328739739698576\n",
      "-0.8775609575033796 6.153928115797533\n",
      "-0.8760981634708074 6.568295483658911\n",
      "-0.8754728537973833 6.763118659869037\n",
      "-0.8733950435939555 7.5127198081558895\n",
      "-0.8718467400753278 8.217762885071172\n",
      "-0.8714329360779989 8.437082411993401\n",
      "-0.8713564142457995 8.479417893396516\n",
      "-0.8712801101631871 8.522224832496745\n",
      "-0.8712097850693972 8.562215870307737\n",
      "-0.8666359479299288 14.80640707622643\n",
      "-0.8602151865217669 8.405030664570349\n",
      "-0.8600162091893928 8.310905203003161\n",
      "-0.8580565555548707 7.526195737502597\n",
      "-0.8569598373455518 7.170603684365414\n",
      "-0.8555582684023841 6.7766703909722255\n",
      "-0.8554187656610377 6.740534281306398\n",
      "-0.8540222373097082 6.40420279824693\n",
      "-0.8505471285755672 5.72077682444265\n",
      "-0.848945743793434 5.459382023095871\n",
      "-0.8467526399397753 5.141357544923404\n",
      "-0.8429089171957409 4.668701539822457\n",
      "-0.8422189458196252 4.592981744328978\n",
      "-0.8418261566465528 4.550939645186029\n",
      "-0.8417852772103849 4.546607054711651\n",
      "-0.8367232578105628 4.0642245395150844\n",
      "-0.834410090789593 3.873466400053546\n",
      "-0.8343203722934223 3.866379642597188\n",
      "-0.8310231824514018 3.6202061676836648\n",
      "-0.8294629931577833 3.512459030807207\n",
      "-0.8294020994957441 3.5083565990694052\n",
      "-0.8292733419235208 3.4997067416723864\n",
      "-0.8261884715825596 3.3018595563598474\n",
      "-0.8260959127523366 3.296185379640544\n",
      "-0.8258111998665689 3.2788212977309\n",
      "-0.82540027336506 3.2539953691909878\n",
      "-0.8250963128562394 3.235807760371835\n",
      "-0.8248502784936174 3.2211939702272296\n",
      "-0.8196862915317098 2.934706144983515\n",
      "-0.819578855140563 2.929120554213317\n",
      "-0.8186380719997193 2.8807998822912695\n",
      "-0.815231582175499 2.7141329293319383\n",
      "-0.8151963431046056 2.712472512892968\n",
      "-0.8143493646763478 2.672932926152529\n",
      "-0.8124459782330016 2.5865638686058836\n",
      "-0.8110944291961184 2.5272120431152176\n",
      "-0.8109950273995756 2.522908883236551\n",
      "-0.8105533292123301 2.503887688958898\n",
      "-0.8095115288225925 2.4596585635288637\n",
      "-0.808791969177798 2.42961617858205\n",
      "-0.8071662407339542 2.363200193245918\n",
      "-0.8021155956991857 2.168699221820976\n",
      "-0.8015400712014591 2.147581174860692\n",
      "-0.8013005178213168 2.1388501584690376\n",
      "-0.7989153213248099 2.053747798058474\n",
      "-0.7975091540029895 2.0050729299582093\n",
      "-0.7913588869582644 1.8040366771494918\n",
      "-0.7886943895249638 1.7224090309312383\n",
      "-0.7871336949955006 1.676003377828127\n",
      "-0.783267205880174 1.5652476604607213\n",
      "-0.7830570752980781 1.5593935260597984\n",
      "-0.7829021036494994 1.5550866807477157\n",
      "-0.7815194669977168 1.517054876663595\n",
      "-0.7777251329885562 1.416199384521821\n",
      "-0.7759213435984083 1.3699862599288903\n",
      "-0.7744764671980322 1.3337425978377153\n",
      "-0.7737215575768588 1.315074393709174\n",
      "-0.7730350618723734 1.2982554611440067\n",
      "-0.7725850862483015 1.2873118177954865\n",
      "-0.7703319726417956 1.2334597223699109\n",
      "-0.7647822277315575 1.1072924532084405\n",
      "-0.7618916653743606 1.045074544119761\n",
      "-0.7585065060770475 0.9751280584900909\n",
      "-0.7563976312193976 0.9331019970079399\n",
      "-0.7561289395600914 0.9278314025810692\n",
      "-0.7550962992799191 0.9077502440770584\n",
      "-0.7549996807497663 0.9058855033848836\n",
      "-0.7536199576644498 0.8795190693028361\n",
      "-0.7516124249421976 0.8420227740366935\n",
      "-0.7445224665933863 0.717617184815701\n",
      "-0.7435870512292879 0.7021154665021572\n",
      "-0.7433009089907185 0.6974151850858306\n",
      "-0.7411016830869268 0.6619367490609414\n",
      "-0.7399306425846846 0.6435087323886064\n",
      "-0.7354891683059677 0.5764957813011693\n",
      "-0.7293167001632472 0.4907529018704423\n",
      "-0.7274519376681967 0.46649571953718183\n",
      "-0.726627761782827 0.4560136164016924\n",
      "-0.7245764823076315 0.43055428501280063\n",
      "-0.723467786783724 0.41716437297170067\n",
      "-0.721062520086799 0.3889985160910504\n",
      "-0.7203097425701162 0.3804293786795524\n",
      "-0.7195556876469855 0.37196218555747046\n",
      "-0.7175204468539162 0.34968583793166336\n",
      "-0.716525693067974 0.3391015919381231\n",
      "-0.7155282724817407 0.3286871846087321\n",
      "-0.7124722903169596 0.2979987633090837\n",
      "-0.7105865274255168 0.279965785348844\n",
      "-0.707637906451057 0.25312315903940424\n",
      "-0.705087772582756 0.23121340120606423\n",
      "-0.702776323158554 0.2123772915900445\n",
      "-0.6991041898082215 0.18440324852291753\n",
      "-0.6957852406367315 0.1611212717184458\n",
      "-0.6953277269784104 0.158057434486189\n",
      "-0.6948007031785346 0.15457121447150815\n",
      "-0.6917068192081144 0.1350236765822276\n",
      "-0.6887511671699826 0.11778312075528802\n",
      "-0.6874382580663505 0.11056294634804917\n",
      "-0.6866715044466281 0.1064687716213853\n",
      "-0.6844188136538432 0.09495479354327546\n",
      "-0.6739442791711638 0.05102410665193244\n",
      "-0.6727773851515093 0.047059908978024347\n",
      "-0.6652171827386859 0.025575688568246904\n",
      "-0.6631520792460948 0.020921449982884317\n",
      "-0.6622901569822615 0.01912694250626523\n",
      "-0.6617180665905069 0.01798336981783097\n",
      "-0.6593349101083004 0.013621631190405197\n",
      "-0.6531431635857339 0.00521897765877584\n",
      "-0.6525174448999624 0.0045973664838485145\n",
      "-0.6484888205967232 0.0015569859086773817\n",
      "-0.6479258162110044 0.0012622859980153933\n",
      "-0.6438280483255558 5.106797153315618e-05\n",
      "-0.6421145350287583 2.1255568856977764e-05\n",
      "-0.6415082450847731 7.66488825261708e-05\n",
      "-0.6389178445267303 0.0006956644510220618\n",
      "-0.6379725785996695 0.001073938353363978\n",
      "-0.6379527402896892 0.0010827409612590556\n",
      "-0.6364502780070158 0.0018518596990942122\n",
      "-0.6344714373770466 0.0031703679109744043\n",
      "-0.6287453322405203 0.00889470001635452\n",
      "-0.6286940052977006 0.008958592201225034\n",
      "-0.6280394144575618 0.009792737832132793\n",
      "-0.6277200973328232 0.010212606665850499\n",
      "-0.627415612552287 0.010620867097582825\n",
      "-0.6272424801552983 0.01085643806165269\n",
      "-0.6264699304532122 0.011937830942327386\n",
      "-0.6263718619390497 0.01207862939826423\n",
      "-0.618422634298947 0.026087069155648943\n",
      "-0.6153093658250433 0.03294413332116032\n",
      "-0.6100851692462712 0.04614540649902589\n",
      "-0.6078406290027223 0.05246278055953142\n",
      "-0.6066055501778782 0.05610342229623562\n",
      "-0.604545635512397 0.06243445228816902\n",
      "-0.6037394527551716 0.06500025915116753\n",
      "-0.6017944330303504 0.07139430112844833\n",
      "-0.5989062581182882 0.0814200888689244\n",
      "-0.5968145184413918 0.0890779143543549\n",
      "-0.5931188516938424 0.10342462564693505\n",
      "-0.5922797607991401 0.10682785054597058\n",
      "-0.5914530351857801 0.11023390020356422\n",
      "-0.588184472874028 0.12421719684875779\n",
      "-0.5872953403163661 0.1281644849098929\n",
      "-0.5859359035684486 0.13431901595991924\n",
      "-0.5790143140501938 0.16791910492029907\n",
      "-0.577173098046589 0.1775039578806022\n",
      "-0.5765191041231907 0.18097490510149647\n",
      "-0.5752881386070603 0.18760312001716922\n",
      "-0.5746699779796673 0.1909787143969019\n",
      "-0.5674900803661798 0.2325332794481938\n",
      "-0.5668364487036504 0.23653476387024377\n",
      "-0.5647786434909048 0.2493758964723925\n",
      "-0.5646114795662982 0.2504353687724136\n",
      "-0.5636368526893165 0.25666172602209664\n",
      "-0.5635156706739048 0.2574417859511214\n",
      "-0.5630198814626606 0.26064685768562706\n",
      "-0.5618968720837092 0.26798800158510794\n",
      "-0.5604746582561781 0.2774482740049406\n",
      "-0.5604734589915665 0.2774563287304097\n",
      "-0.5594892890069421 0.28411067343916374\n",
      "-0.5574320835027928 0.2983081438500432\n",
      "-0.556727131337279 0.3032637456977752\n",
      "-0.5559483231115714 0.3087927399800869\n",
      "-0.5553155593729171 0.3133270577570157\n",
      "-0.5552210518963121 0.31400754403624176\n",
      "-0.5517135856529489 0.33986792949612527\n",
      "-0.5487463163809616 0.3626831659506652\n",
      "-0.5482188255367728 0.3668307962901925\n",
      "-0.5451185120719841 0.3917790703681211\n",
      "-0.5429474153298361 0.4098406131187936\n",
      "-0.5416670180921854 0.4207249192192375\n",
      "-0.5376694754592712 0.4558425922245338\n",
      "-0.5341983649906659 0.4877690746831675\n",
      "-0.5328401462433701 0.5006343930936883\n",
      "-0.5324273015506786 0.5045871765098606\n",
      "-0.5320467153929183 0.5082486623958143\n",
      "-0.5275771146433694 0.5525312155152399\n",
      "-0.5268492352510659 0.5599704397111616\n",
      "-0.5266842936960068 0.5616652289004272\n",
      "-0.5237625541138808 0.5922455676443196\n",
      "-0.523136816425315 0.598934149296835\n",
      "-0.5225549934258247 0.6051980094765629\n",
      "-0.5156399227388082 0.6830319363353229\n",
      "-0.5152286859737194 0.6878627237403888\n",
      "-0.5088064961592806 0.7663847158815406\n",
      "-0.5087541104570823 0.7670495857182532\n",
      "-0.5081858901519469 0.7742872767500081\n",
      "-0.5075176920551989 0.7828594762908545\n",
      "-0.5072362092534513 0.7864904250422926\n",
      "-0.5022548684568737 0.8527336444599126\n",
      "-0.49724309191039207 0.9233152660349804\n",
      "-0.49386415288059626 0.973234366714796\n",
      "-0.4908332770103576 1.0196757955816567\n",
      "-0.4896886597724268 1.0376352074510848\n",
      "-0.48841825994605736 1.057843620169181\n",
      "-0.4864499267173197 1.089735695632759\n",
      "-0.48509828089886375 1.112052178784966\n",
      "-0.47798716246581385 1.235267373509185\n",
      "-0.47640013972383866 1.2641496745904428\n",
      "-0.47501414326444547 1.2898029006164609\n",
      "-0.47338448410949496 1.320487582126624\n",
      "-0.46842115544020024 1.4175277553766783\n",
      "-0.4677088254097046 1.431911307741003\n",
      "-0.4640696682671972 1.507252952855446\n",
      "-0.4628819273713862 1.5325319041709038\n",
      "-0.46075307870709614 1.5787153216697023\n",
      "-0.4601000956801953 1.5931102037923057\n",
      "-0.4598799142467562 1.5979886255779925\n",
      "-0.4586543516328454 1.6253712052438065\n",
      "-0.4575156086804133 1.6511653984895958\n",
      "-0.45720268275134424 1.658313653051073\n",
      "-0.45420177376951165 1.728205885658245\n",
      "-0.45387026657802565 1.7360784306869257\n",
      "-0.4506284086538601 1.814706276023717\n",
      "-0.44597614170289357 1.9329628220423474\n",
      "-0.445927366131305 1.934237888478609\n",
      "-0.44546994666963413 1.946232062393676\n",
      "-0.44457051292816474 1.9700105271812967\n",
      "-0.4425698484161493 2.0238418487327765\n",
      "-0.44158802717927004 2.0507437035697076\n",
      "-0.4378751241504053 2.155469822440791\n",
      "-0.43781405769279447 2.1572329185690675\n",
      "-0.43409659638002385 2.267152041089097\n",
      "-0.4320527578071214 2.329834043984122\n",
      "-0.4309483052854275 2.364399259810747\n",
      "-0.4303926483814069 2.381977478484739\n",
      "-0.4293802793682715 2.414333415186353\n",
      "-0.4281083174972491 2.4556008494383907\n",
      "-0.4265963165650084 2.5055696620344508\n",
      "-0.42405842755668965 2.591756062705796\n",
      "-0.42258063012252367 2.6433309340533704\n",
      "-0.4218834730046279 2.6680287490008423\n",
      "-0.41447599816508296 2.9460941144212436\n",
      "-0.4111859279774295 3.0796492106244955\n",
      "-0.40881815766723917 3.180033168148034\n",
      "-0.40498198336225477 3.3509391329294904\n",
      "-0.4042264302507641 3.3858873023581104\n",
      "-0.4039990470399246 3.396491858781821\n",
      "-0.40300449326680465 3.443357440824359\n",
      "-0.40142322003152575 3.519534045694932\n",
      "-0.4009803669391929 3.5412459370253373\n",
      "-0.3987532163600276 3.6530601120555843\n",
      "-0.3935178888633313 3.934754100334473\n",
      "-0.392672132981309 3.9829961965648666\n",
      "-0.39168536306781343 4.040329593659016\n",
      "-0.3911045637477306 4.074619364838942\n",
      "-0.390812729408456 4.092004864393874\n",
      "-0.38695280836607027 4.332414934303315\n",
      "-0.37788855104032715 4.990369029667964\n",
      "-0.3775599084890875 5.017255324053604\n",
      "-0.3737838966463589 5.345320703844731\n",
      "-0.3695826300457081 5.759493839935885\n",
      "-0.36833779108207865 5.894354227764473\n",
      "-0.3633806112358118 6.502885342282357\n",
      "-0.3613997520597134 6.786381059007433\n",
      "-0.3603216160793523 6.953037541928173\n",
      "-0.3592215607104656 7.13345383806893\n",
      "-0.3536968840806478 8.25937987306142\n",
      "-0.34961986085815133 9.504987405021756\n",
      "-0.3463469237460557 11.135605845722305\n",
      "-0.3447431506493912 12.474458710149976\n",
      "-0.3441482837436842 13.186673017094954\n",
      "-0.34349478819800594 14.24622854454702\n",
      "-0.3385145116512762 11.756233665757003\n",
      "-0.3384266379112326 11.684954021015294\n",
      "-0.33708204849987067 10.770216930730768\n",
      "-0.3369386750907646 10.687891294562107\n",
      "-0.33662271191416937 10.51440145351341\n",
      "-0.3344983560334225 9.560631575523201\n",
      "-0.3338948412729934 9.339039843272328\n",
      "-0.333663321362452 9.258397190957126\n",
      "-0.329770536563172 8.162194342180536\n",
      "-0.3259641090093597 7.388661232030878\n",
      "-0.3222268586994175 6.792120747775825\n",
      "-0.32147990822528616 6.686704042575233\n",
      "-0.31525668760893355 5.935512956758782\n",
      "-0.3063819934528411 5.127533605555235\n",
      "-0.3063259458859686 5.123117275096936\n",
      "-0.30419692283966016 4.960449738100935\n",
      "-0.30317087733357373 4.885399934068709\n",
      "-0.3018606791166387 4.792490846124613\n",
      "-0.30148717514448764 4.766578816267149\n",
      "-0.30110911067632085 4.740600996006331\n",
      "-0.2981000598585959 4.542308348175664\n",
      "-0.2955033303391843 4.382219672084914\n",
      "-0.2945662464547889 4.326723679430936\n",
      "-0.2943317244349821 4.313013615005469\n",
      "-0.2904935577576906 4.098137676777759\n",
      "-0.2824059235891967 3.6956715661554904\n",
      "-0.28166409063512754 3.6616814270771125\n",
      "-0.2697172623999189 3.1683951009070057\n",
      "-0.2675630015335777 3.0888102076565773\n",
      "-0.2666551252274023 3.0560129289985314\n",
      "-0.26420231159232865 2.969502438643957\n",
      "-0.2631813715677094 2.934363595856836\n",
      "-0.2590003138694845 2.79546657996625\n",
      "-0.2578029788213363 2.757098842491229\n",
      "-0.2577269198994818 2.754681876852246\n",
      "-0.25739187411494835 2.7440634416566514\n",
      "-0.2554122338486642 2.6822547110240764\n",
      "-0.25492856233464556 2.667390444094457\n",
      "-0.2533071214759133 2.618218826115725\n",
      "-0.25221476225083483 2.5856510117809157\n",
      "-0.2521160952711565 2.5827310453816184\n",
      "-0.24664759657196988 2.4262643772533266\n",
      "-0.24502045489197255 2.3816464407810867\n",
      "-0.23896757886590936 2.2228287800545283\n",
      "-0.23757752823333922 2.1878609898664645\n",
      "-0.23373409093749853 2.0939085752144737\n",
      "-0.232167302511344 2.0567153970262293\n",
      "-0.23070917765018106 2.0226555649687397\n",
      "-0.23055466918423528 2.0190771940306607\n",
      "-0.22506087714622036 1.8955271008575336\n",
      "-0.2182848443713279 1.7524006894524609\n",
      "-0.2151771905640909 1.6899282463096463\n",
      "-0.21414718248165454 1.6696396571521932\n",
      "-0.20435235202494195 1.4864706076318306\n",
      "-0.1947935243975425 1.3233773160006388\n",
      "-0.19234323798513064 1.2838570181366222\n",
      "-0.1923093625724268 1.2833169014866863\n",
      "-0.19160934125795626 1.2721933727513328\n",
      "-0.1902605826732664 1.2509628520782228\n",
      "-0.17382611904963263 1.0123422137919764\n",
      "-0.17323514358677206 1.0044135714395326\n",
      "-0.16832487398277607 0.9401757372438373\n",
      "-0.16472278358954173 0.8948663769781041\n",
      "-0.16459988744472143 0.8933469713093496\n",
      "-0.16441810726881512 0.8911027308752573\n",
      "-0.16404946193716974 0.8865630573799362\n",
      "-0.16070412247455756 0.8460692354001869\n",
      "-0.16068599526325333 0.845853226870656\n",
      "-0.16002845830809398 0.8380424628023702\n",
      "-0.15899954531334592 0.8259159193662394\n",
      "-0.15882027511338426 0.8238149732024225\n",
      "-0.15751995629946358 0.8086809510785641\n",
      "-0.15687040193639668 0.8011897305234104\n",
      "-0.15669340959696765 0.7991564015506707\n",
      "-0.15627627284437717 0.7943775791086931\n",
      "-0.1515414074764152 0.7414301397922214\n",
      "-0.14744875848973837 0.6975425654340138\n",
      "-0.1458101248269026 0.6804471755060603\n",
      "-0.14402383110809236 0.6621164625781965\n",
      "-0.14400815491040042 0.6619569937486922\n",
      "-0.14342853368985575 0.6560776864748508\n",
      "-0.14228947548105797 0.6446198347645276\n",
      "-0.14066928913572663 0.6285399775109662\n",
      "-0.14031328089926554 0.6250407316994595\n",
      "-0.13770017859005734 0.5997278236152631\n",
      "-0.13748694770237635 0.5976909411051644\n",
      "-0.1370917355104877 0.5939270475799174\n",
      "-0.13493917429631064 0.5736842466131212\n",
      "-0.13464437290986675 0.5709456080677111\n",
      "-0.13334577664471992 0.5589778135050038\n",
      "-0.13230253506304424 0.5494759407204453\n",
      "-0.1322690707601959 0.5491727993274489\n",
      "-0.12996243577212407 0.5285236496215138\n",
      "-0.12979087939731415 0.5270071317315963\n",
      "-0.12807103721540614 0.5119503070449642\n",
      "-0.1265563455365002 0.49890820425744253\n",
      "-0.12435875867259671 0.4803463560656056\n",
      "-0.12225588120702335 0.4629793284854724\n",
      "-0.11608425206435635 0.41419539640159286\n",
      "-0.11522608676777257 0.4076657969960914\n",
      "-0.11515207506195702 0.40710552204867445\n",
      "-0.11419605398391264 0.39990910596693074\n",
      "-0.11339495848664805 0.39393693578623556\n",
      "-0.11273755940725283 0.3890753903738322\n",
      "-0.11125758355540771 0.37825998154493357\n",
      "-0.10961108335696079 0.36643645748262915\n",
      "-0.10959828181151754 0.36634538637350994\n",
      "-0.10818220683256796 0.3563525024518876\n",
      "-0.10696639396010266 0.34790059355051106\n",
      "-0.10495785991882167 0.33419447271733915\n",
      "-0.10335528611011724 0.3234859024236075\n",
      "-0.10040264475091898 0.3042783932677299\n",
      "-0.10025882702975353 0.30335998828156163\n",
      "-0.09808491443367262 0.2896700160841939\n",
      "-0.09713743555236776 0.2838156961271181\n",
      "-0.09642270762372651 0.27944434105813176\n",
      "-0.0961724566526132 0.2779228630337643\n",
      "-0.09479449995707223 0.2696292289307203\n",
      "-0.091694157610126 0.2514852415479754\n",
      "-0.08637959210380663 0.22202029522193387\n",
      "-0.08592162377883228 0.21957666879339763\n",
      "-0.08490309280180108 0.21419571279234745\n",
      "-0.08440608632555824 0.21159682479232597\n",
      "-0.0762815176061209 0.17157150622321146\n",
      "-0.07069188534107429 0.1466699250446\n",
      "-0.07054238735856933 0.14603283803527287\n",
      "-0.06726805826435944 0.13245249605434814\n",
      "-0.0648643637417099 0.12293400623678798\n",
      "-0.06455385391741952 0.1217320292331845\n",
      "-0.06337602373754736 0.11722984359330556\n",
      "-0.06210545336192941 0.11247426233207981\n",
      "-0.06208317386674422 0.11239180622350818\n",
      "-0.05624019309530026 0.09186758523929046\n",
      "-0.05519587938854276 0.08842837069327045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.05391121245331343 0.08429198796341066\n",
      "-0.05116035452277945 0.07578293577457994\n",
      "-0.04994750389353486 0.07218121551212613\n",
      "-0.047126382974370884 0.06415583985005885\n",
      "-0.046510482267415165 0.06246897693064006\n",
      "-0.045224290722228 0.059021297802406635\n",
      "-0.04302562861680004 0.05336170018208458\n",
      "-0.04279176091843295 0.052777010177149704\n",
      "-0.042151974953842375 0.05119444792253114\n",
      "-0.0419389393921108 0.05067299418835479\n",
      "-0.037184159104619585 0.03974693693077096\n",
      "-0.03599721971014902 0.03723101295809522\n",
      "-0.034571804105439696 0.03432061510526045\n",
      "-0.032943207737284874 0.03114315714139796\n",
      "-0.02942909211734679 0.02482133992077939\n",
      "-0.02763593275015186 0.021875507639314232\n",
      "-0.0229133855453163 0.015016784374369168\n",
      "-0.02193911643328339 0.013763363491504177\n",
      "-0.020419263130643017 0.011917939772911697\n",
      "-0.01960321250414343 0.010982258618801446\n",
      "-0.018245328503861957 0.009510612438407\n",
      "-0.01776810438720222 0.009018683061269081\n",
      "-0.0156854533165125 0.007025478286782084\n",
      "-0.013339936301562716 0.005079418947050514\n",
      "-0.012433484066946665 0.004411964873108684\n",
      "-0.012078548019103907 0.004163450579494197\n",
      "-0.010949848546780494 0.0034211586919105853\n",
      "-0.009839345247192766 0.002762039623793556\n",
      "-0.008385491644375742 0.0020057937399096735\n",
      "-0.006302925707973639 0.0011330125374496849\n",
      "-0.0033408157747367717 0.0003182597496368227\n",
      "0.0026206227164513596 0.00019582790980456004\n",
      "0.003951404380300838 0.000445236849573699\n",
      "0.005937108257771939 0.0010052839017599647\n",
      "0.0071492633271668105 0.0014578145640921755\n",
      "0.011838489064855517 0.0039994633293697155\n",
      "0.01251225130836775 0.004468094392859734\n",
      "0.01264586540763979 0.004564121422331663\n",
      "0.013070954346615427 0.00487643934118596\n",
      "0.019658682287173823 0.011044640220268026\n",
      "0.02048258482147358 0.011992154907082027\n",
      "0.022177063362815774 0.01406440369511512\n",
      "0.023626132196444827 0.01596866978438043\n",
      "0.024723830779374234 0.01749246604787067\n",
      "0.02812912237378673 0.02266692422582319\n",
      "0.028619075071516065 0.023467266573199044\n",
      "0.029868035058375986 0.025571210913701917\n",
      "0.030215050366033447 0.026172058808321186\n",
      "0.03305273118623919 0.031351909316757756\n",
      "0.034282904413683646 0.03374547467036253\n",
      "0.035120380798741024 0.03542637569387634\n",
      "0.03564194767348283 0.03649430143290084\n",
      "0.03679461533545836 0.03891195388580108\n",
      "0.038912884218214705 0.043562077755897315\n",
      "0.04069484169380133 0.047682620341594054\n",
      "0.041292909168272995 0.0491084848827851\n",
      "0.041945496830263496 0.05068900402457782\n",
      "0.04684589677512241 0.06338473939549585\n",
      "0.04800957798645644 0.06661549333269745\n",
      "0.04938974434439958 0.07055552118216392\n",
      "0.05437770803453579 0.08578200051061538\n",
      "0.05716083138810757 0.09495678162503753\n",
      "0.058281322536268165 0.09878926085669265\n",
      "0.05925222857764423 0.10217486248987774\n",
      "0.06147863163038125 0.11016665115490032\n",
      "0.06170364888705193 0.11099211952103825\n",
      "0.062274990890096804 0.11310277052865059\n",
      "0.06557350850480326 0.12570273888094666\n",
      "0.06660102768290121 0.12977302404010574\n",
      "0.06800992237855419 0.13546706718279142\n",
      "0.06865974630178018 0.13813755712481618\n",
      "0.07311273888150538 0.15719523209624\n",
      "0.0763844399172866 0.17204993153159118\n",
      "0.07695880233917807 0.174733142292527\n",
      "0.07936906712760661 0.18624076448850824\n",
      "0.07983893452115942 0.18853095061795414\n",
      "0.08350106664325896 0.20690942356721714\n",
      "0.0842408470545104 0.21073666208401573\n",
      "0.08462353802260414 0.21273173709581802\n",
      "0.08496020591604303 0.2144954874037581\n",
      "0.08500396841882374 0.214725344618892\n",
      "0.08666613541543122 0.22355687366268834\n",
      "0.08809208945352642 0.23129126205611122\n",
      "0.08918549006692578 0.23732130943521376\n",
      "0.08959761258644039 0.23961664255647278\n",
      "0.08965322428449007 0.23992731970508163\n",
      "0.08974605558244719 0.24044642711421388\n",
      "0.09078341954327152 0.24629004429446197\n",
      "0.0911325850697875 0.24827463124678803\n",
      "0.09281880141297094 0.25798475946040966\n",
      "0.09418216751294484 0.26598923219274795\n",
      "0.09431116818382645 0.2667537528619966\n",
      "0.0969940095855506 0.28293540035880205\n",
      "0.09889986094758396 0.2947598655916852\n",
      "0.09911624365050065 0.2961197948445661\n",
      "0.10049076300725157 0.3048418908867959\n",
      "0.10549993989380901 0.3378622563356632\n",
      "0.10678895950019474 0.3466769536172976\n",
      "0.11262711736149078 0.38826213009186916\n",
      "0.11267333991144768 0.388602376952146\n",
      "0.11547178366838495 0.40952900137622783\n",
      "0.11593365694520408 0.41304511917173165\n",
      "0.11643624737087288 0.41689138293654615\n",
      "0.12144177731789552 0.4563585625464889\n",
      "0.12176139941124142 0.4589511158520638\n",
      "0.1229859957838455 0.4689656619504625\n",
      "0.12510714460164563 0.48661995725753143\n",
      "0.1304387611150042 0.5327481865515638\n",
      "0.1306541369643055 0.5346650961696962\n",
      "0.13079795700320251 0.5359474779964233\n",
      "0.13151299036010888 0.5423510065652053\n",
      "0.1329086561356425 0.5549843416273026\n",
      "0.1334544583798014 0.5599734449678545\n",
      "0.1337764993512034 0.5629300525540104\n",
      "0.13635038814752853 0.5869063517031461\n",
      "0.13779467167575432 0.6006318421676312\n",
      "0.14067936567254113 0.6286391987446588\n",
      "0.14278187469920778 0.6495573299991992\n",
      "0.14448902282994136 0.6668597461160974\n",
      "0.14531273748892026 0.6753112259961691\n",
      "0.15694887463394425 0.8020923239769199\n",
      "0.16058367892061165 0.8446346826833939\n",
      "0.16213939677859335 0.8632885388825667\n",
      "0.1648420325382225 0.8963423446868045\n",
      "0.16859591043001632 0.943646333292539\n",
      "0.1692124385007201 0.9515732771223747\n",
      "0.17020885150094656 0.9644800872917714\n",
      "0.1711326493020382 0.9765524843377844\n",
      "0.1719145456260558 0.9868509245265236\n",
      "0.17662150350389805 1.0504338865365264\n",
      "0.17800209982881832 1.0696100793463346\n",
      "0.1861330188719108 1.1876065418583077\n",
      "0.1863929768961643 1.1915262135086062\n",
      "0.18685850452316677 1.1985689208756158\n",
      "0.18819696450735912 1.218986448475961\n",
      "0.19004892306111798 1.2476550732904426\n",
      "0.1901001029234708 1.2484543115748268\n",
      "0.190702137719152 1.2578842109380242\n",
      "0.19101042552891734 1.2627333488176526\n",
      "0.1928076186628862 1.2912782774235658\n",
      "0.19345414129418037 1.3016635667071101\n",
      "0.19442798344246404 1.3174244913616369\n",
      "0.19627429399566165 1.3476992690754173\n",
      "0.19709811425683532 1.361376309680021\n",
      "0.19782908112186148 1.3735999487395705\n",
      "0.19783333191028385 1.3736712766189312\n",
      "0.19839906067078616 1.3831894361111732\n",
      "0.1996788132013827 1.4049071951679253\n",
      "0.20847297510698604 1.5614372443068099\n",
      "0.20857934372706532 1.5634116811439882\n",
      "0.20992736323788397 1.588609170968679\n",
      "0.2124592721011478 1.6368295809850768\n",
      "0.2126616481588297 1.640735071989899\n",
      "0.21865818354521438 1.7600358177531235\n",
      "0.2191550826226163 1.7702419222715535\n",
      "0.2196173302007729 1.7797817650286465\n",
      "0.2204122873314558 1.7962914062019408\n",
      "0.22134491507973086 1.8158285814156343\n",
      "0.22168206994928497 1.822936675213218\n",
      "0.22576938150140213 1.9110685401623273\n",
      "0.22707795233624628 1.9400724911627674\n",
      "0.22841562921557124 1.9701306211395138\n",
      "0.22851978873305834 1.9724887344164277\n",
      "0.22879561799643744 1.9787457558529444\n",
      "0.23366042800440812 2.092145888461686\n",
      "0.23380901738829651 2.095702931242435\n",
      "0.23497744885085692 2.12387339303028\n",
      "0.23821149161488497 2.2037419805574787\n",
      "0.24243001375079087 2.312333217579675\n",
      "0.24483830774152748 2.376704450374554\n",
      "0.24727755296817278 2.443769402097315\n",
      "0.24845348270102963 2.4767986778293114\n",
      "0.25325806697345543 2.6167467725131877\n",
      "0.25630080808982125 2.7098030979044188\n",
      "0.2605416732488801 2.8457621790449927\n",
      "0.261286588035756 2.8704440329200125\n",
      "0.26278228976297546 2.9207624521220525\n",
      "0.2647662631844032 2.989128266027264\n",
      "0.2649642707400961 2.996055990763148\n",
      "0.26586767238250464 3.0279106224311643\n",
      "0.2695064602668775 3.1604952282912766\n",
      "0.2715343369744163 3.2375366131966095\n",
      "0.272270937912406 3.2661128147689475\n",
      "0.2730873899945332 3.2981678438119832\n",
      "0.2741620008984129 3.3409846193533053\n",
      "0.27457094021168493 3.357469464950264\n",
      "0.27481946598609563 3.367540175255004\n",
      "0.27747525067118706 3.4777101252761446\n",
      "0.27932843309294286 3.5574873791677146\n",
      "0.28215910485320217 3.684313057493631\n",
      "0.28245363407904645 3.6978729272597435\n",
      "0.282858327449123 3.716620862677849\n",
      "0.2861181953210876 3.872770041579564\n",
      "0.28664108093530394 3.898708122489592\n",
      "0.29063707491009305 4.105866580155759\n",
      "0.29255472428867346 4.211363879459851\n",
      "0.2954734466821236 4.380431973945371\n",
      "0.2965429552212777 4.445168371090213\n",
      "0.2971744822172806 4.484142801014351\n",
      "0.2990114886029607 4.600843217512429\n",
      "0.2992347166736138 4.61537606606202\n",
      "0.30077938525719916 4.718146904973282\n",
      "0.30413716904941257 4.956021701133615\n",
      "0.30502205113908953 5.0223449369816535\n",
      "0.3146855489011675 5.875745147394209\n",
      "0.31560009570100744 5.972080022067387\n",
      "0.3168792237595046 6.112689877847305\n",
      "0.31850278869928994 6.30205878175313\n",
      "0.32123546234328537 6.653049411195009\n",
      "0.3281741287474875 7.811734736601667\n",
      "0.3323491892136019 8.839404678237512\n",
      "0.3343506486599541 9.504788373433286\n",
      "0.3355850862039138 10.008907455384032\n",
      "0.3356892666089035 10.055814782447435\n",
      "0.3369903394735638 10.717285514219235\n",
      "0.3389651343219675 12.15243491397265\n",
      "0.342494659614176 17.519606584308523\n",
      "0.3425232813892498 17.350564218870268\n",
      "0.34281224308513836 16.040615473857976\n",
      "0.3442738774297103 13.021013649571039\n",
      "0.3451608698107316 12.061965371440753\n",
      "0.3481915186166433 10.107992808823814\n",
      "0.3494658346449162 9.564313546632249\n",
      "0.3570661886078741 7.522849396196333\n",
      "0.35961684816671036 7.067337784983792\n",
      "0.36244476038164763 6.633413564386415\n",
      "0.3624734398741254 6.62932596859903\n",
      "0.36599783546799136 6.165962588537028\n",
      "0.36948054898930227 5.770322494056096\n",
      "0.36952149687242386 5.765973982537143\n",
      "0.37061622874926714 5.652044879206219\n",
      "0.37494987943473945 5.240040007149541\n",
      "0.37525031216324356 5.213515153578784\n",
      "0.3772553129506726 5.0423970868597845\n",
      "0.37868485139958086 4.926231419478706\n",
      "0.3940845355756015 3.9028823825742305\n",
      "0.39461686181765443 3.8732620956069295\n",
      "0.3953585057830873 3.832501676840589\n",
      "0.395529728948961 3.8231737392420895\n",
      "0.39554790636595527 3.822185260445919\n",
      "0.3959891629221355 3.7982948562229426\n",
      "0.39699060240375506 3.7448100238768562\n",
      "0.39732615012704287 3.727112585448833\n",
      "0.40065300507690926 3.557404542886796\n",
      "0.4101397551283468 3.1235451077167764\n",
      "0.4104803362650702 3.109176467881906\n",
      "0.4119321615960603 3.0487688160213455\n",
      "0.4137422318064876 2.975307935675125\n",
      "0.41382806959015195 2.971873846093307\n",
      "0.41432312701012597 2.952154082235906\n",
      "0.41600596654526534 2.886194879593756\n",
      "0.41623230462840355 2.877447619884907\n",
      "0.4170970294625522 2.8442935688995905\n",
      "0.4193262752001021 2.760708560190755\n",
      "0.4198860693421689 2.740133896928122\n",
      "0.42020653498428784 2.7284286373653375\n",
      "0.42300425187401425 2.628439362260425\n",
      "0.4259196579137834 2.5282610540532633\n",
      "0.4274894193039025 2.4759327772204185\n",
      "0.4294947914702185 2.410651984461252\n",
      "0.4296008774262636 2.4072463760412104\n",
      "0.43252012137919693 2.3153551955176277\n",
      "0.4362451178983866 2.202996354434115\n",
      "0.4419052176172449 2.0420173230690972\n",
      "0.44333667732644044 2.0030540262103345\n",
      "0.44533483905531956 1.949787449937105\n",
      "0.4537931503586565 1.737914161882596\n",
      "0.4557849936727787 1.6910268586028374\n",
      "0.4575759350829247 1.649790335718938\n",
      "0.45934650561755497 1.6098587147928447\n",
      "0.4595863363017789 1.6045126216077843\n",
      "0.46083334775894413 1.5769532946386215\n",
      "0.46565951300121466 1.4739513774788247\n",
      "0.4673709838777407 1.4387741573100015\n",
      "0.4691234529533648 1.403460935286817\n",
      "0.4698782516524307 1.3884677652520414\n",
      "0.4728932407109485 1.3298492727392663\n",
      "0.48969306340642094 1.0375656643448572\n",
      "0.49782947808124733 0.9148465421578392\n",
      "0.499234135015048 0.8947894882068718\n",
      "0.5011945462614062 0.8673303385270582\n",
      "0.505924694275836 0.803564502454691\n",
      "0.5075651443504325 0.7822485336395003\n",
      "0.5103642660856129 0.7467969439291351\n",
      "0.5103849132431446 0.7465396883470584\n",
      "0.5105668486096253 0.7442755048756067\n",
      "0.5133796535119459 0.7098721446287078\n",
      "0.5239389792915141 0.5903686996519396\n",
      "0.5287351410041683 0.5408285910364513\n",
      "0.5289449921542715 0.5387252538391866\n",
      "0.5297225083433292 0.5309782899248338\n",
      "0.5305067451463605 0.5232374612762866\n",
      "0.5313210580277254 0.5152768690020098\n",
      "0.5327119827957181 0.5018593771282844\n",
      "0.5329098678946096 0.49996879424351565\n",
      "0.5371824376703473 0.4602407808480188\n",
      "0.5409283126975386 0.42708393875122186\n",
      "0.5425060496181802 0.413572900592166\n",
      "0.5428202213770112 0.41091408157812687\n",
      "0.5461642255330741 0.3832543570132613\n",
      "0.5475431787125902 0.37218432355585035\n",
      "0.5484082771477461 0.36533793064869635\n",
      "0.5519659926470484 0.33796720668587216\n",
      "0.5588211464570838 0.288678857963198\n",
      "0.5593909383335796 0.28478053407517673\n",
      "0.5597295103426627 0.28247827103985584\n",
      "0.5602369850537705 0.27904714050254126\n",
      "0.5613400652129248 0.27166994685016516\n",
      "0.5637123038958631 0.25617669955978317\n",
      "0.5658043632318135 0.2429288295035961\n",
      "0.568603947023774 0.2257994033875916\n",
      "0.5734222426138285 0.19788866576851794\n",
      "0.5737146494627516 0.19625771222642235\n",
      "0.5743073691093799 0.19297352084205016\n",
      "0.5749281900281478 0.18956485531570122\n",
      "0.5753688565482482 0.18716467085371188\n",
      "0.5763027670259779 0.1821307755146501\n",
      "0.5764620401723608 0.1812794205552469\n",
      "0.5776967338688388 0.17475002545888127\n",
      "0.5780316072209801 0.17300054286440003\n",
      "0.5837106335706304 0.14470650308275795\n",
      "0.5840011684450592 0.1433281399396617\n",
      "0.586833816857121 0.13023772066922842\n",
      "0.5891341972538755 0.12006885043629713\n",
      "0.5896349966019632 0.11790957522768966\n",
      "0.5927669484473883 0.10484530316900584\n",
      "0.5944817847658965 0.09801197437310233\n",
      "0.5955476881634694 0.09387808899428911\n",
      "0.5959896128905247 0.09218965751329097\n",
      "0.5991500517958912 0.08054926132724402\n",
      "0.6000570444909008 0.07734923832530369\n",
      "0.6067815078106868 0.05557763352707068\n",
      "0.6075529434031137 0.05330037676938917\n",
      "0.6075931311193703 0.05318299003750771\n",
      "0.6089348193331896 0.04933489541513746\n",
      "0.6153236428943321 0.0329109557067011\n",
      "0.6160200880359814 0.03131184571957372\n",
      "0.6184630536170794 0.02600305511869127\n",
      "0.6185044299799554 0.02591718543696677\n",
      "0.6190759990773953 0.024744836294575828\n",
      "0.6192472997269358 0.02439851571983268\n",
      "0.6193574187581639 0.024177114570213407\n",
      "0.6195679833162988 0.02375643880941311\n",
      "0.622328220270536 0.018568657045695453\n",
      "0.6265921797303435 0.011763426225690467\n",
      "0.626812561312482 0.01145213887580022\n",
      "0.6350134991676617 0.0027749315981174544\n",
      "0.6385986281048479 0.000814374521545946\n",
      "0.6399183415583001 0.00038362657848637076\n",
      "0.6407556480525971 0.00019289716669685966\n",
      "0.6423305572248796 9.807875478158869e-06\n",
      "0.6450812361169733 0.0002491827992312181\n",
      "0.6476443306986648 0.0011267161874287611\n",
      "0.6513966378161065 0.003585359208787746\n",
      "0.6525741889740784 0.0046520579059692166\n",
      "0.6581904101745633 0.011753885493639355\n",
      "0.66462830901782 0.02419704162020164\n",
      "0.6685080396566192 0.03405162569532099\n",
      "0.6696031401589044 0.03716796174102168\n",
      "0.6721608537579935 0.04503725547731151\n",
      "0.6724880742477601 0.046104615972841406\n",
      "0.6726899074136137 0.04676991147233452\n",
      "0.6743350406372173 0.05239170765294506\n",
      "0.6753739209237939 0.05612642653217372\n",
      "0.6759007906834122 0.05807578348180802\n",
      "0.6772248047613891 0.0631405194864422\n",
      "0.6799455898052249 0.07430684640408257\n",
      "0.6858487364204897 0.1021749356236247\n",
      "0.6925142768816344 0.13997537863533074\n",
      "0.6925282396405272 0.1400619289350094\n",
      "0.7049018368375224 0.2296625109470454\n",
      "0.7069626571814871 0.24720503349612086\n",
      "0.7075237153397189 0.25211640347072867\n",
      "0.7077569882764958 0.2541756160776253\n",
      "0.7080236380283991 0.2565418703465377\n",
      "0.7089817563386598 0.2651539044071107\n",
      "0.7098622658458418 0.27322054743627044\n",
      "0.7156607207911847 0.33005874689177306\n",
      "0.7169699616290861 0.3438041558495179\n",
      "0.7184622158177234 0.35988956343829154\n",
      "0.7207069062959053 0.3849359177828705\n",
      "0.7226183810719835 0.40708031806046086\n",
      "0.7226224462036788 0.40712822019540057\n",
      "0.7247040306672146 0.43211131750043846\n",
      "0.7261912595260027 0.4505209860280233\n",
      "0.7271570146601551 0.4627280545924879\n",
      "0.7295400177785933 0.49370844819263365\n",
      "0.7357410503412496 0.5801756008277614\n",
      "0.7362106716373187 0.587074855683452\n",
      "0.7363645031612869 0.5893457042493527\n",
      "0.7386058136919913 0.6230451372531528\n",
      "0.7392146840283853 0.632399388902408\n",
      "0.7403348337248965 0.6498330919424783\n",
      "0.7408864730629412 0.6585260736483748\n",
      "0.7414482443601729 0.6674519152895879\n",
      "0.742222228766168 0.6798710240642192\n",
      "0.7426954928243206 0.6875344867439047\n",
      "0.7440945344642316 0.7104995513176611\n",
      "0.7441324697715823 0.7111287460107186\n",
      "0.7451194267325849 0.7276195082451711\n",
      "0.7467489110389285 0.7553594815259633\n",
      "0.7488728484653411 0.792487171263842\n",
      "0.7492398647651048 0.7990151543841878\n",
      "0.7537945587112451 0.8828286880708557\n",
      "0.7551063594450635 0.9079445443169013\n",
      "0.7556990797647427 0.9194385133770994\n",
      "0.7584419675124316 0.9738244675531313\n",
      "0.7588918141011676 0.9829338546734917\n",
      "0.7628771098948945 1.0660241293586634\n",
      "0.7630414949682123 1.0695449743850796\n",
      "0.7642861543135528 1.0964483696012857\n",
      "0.7652049306785609 1.116587710086584\n",
      "0.7667041175900975 1.1499669463071511\n",
      "0.7720503406085564 1.2743888710311138\n",
      "0.7731203359168279 1.3003365452426365\n",
      "0.7744973859337396 1.33426249996924\n",
      "0.7753767488095034 1.3562456534747223\n",
      "0.7761879372157812 1.3767481893662943\n",
      "0.7770077045410513 1.3976885532324015\n",
      "0.7780341593693263 1.4242264281394004\n",
      "0.7789547231096596 1.4483319228367793\n",
      "0.7799247081792975 1.4740492475547553\n",
      "0.7812123775064954 1.5087027055846933\n",
      "0.7885609501863153 1.7184016123583044\n",
      "0.7897869450888602 1.7555056916510476\n",
      "0.7904117231158996 1.7746639361097267\n",
      "0.7904409326355748 1.775563806483641\n",
      "0.7918558737824015 1.819610109858545\n",
      "0.7960215129687449 1.9547242067134758\n",
      "0.7962383708224972 1.9619921831160887\n",
      "0.7964918570506034 1.9705183942713032\n",
      "0.7974577123266349 2.003312507966239\n",
      "0.7990163259754932 2.057285843903567\n",
      "0.8013716316826123 2.141438455611601\n",
      "0.8017343420606513 2.154687161656672\n",
      "0.8023614735922877 2.1777831382269524\n",
      "0.8027057986479473 2.1905671187778615\n",
      "0.8065431401711987 2.3382620645404413\n",
      "0.8148903085222436 2.6981045350173263\n",
      "0.8154059232611741 2.7223659480757214\n",
      "0.8160973797604512 2.755322858829493\n",
      "0.8168100852716713 2.7898120909505697\n",
      "0.8200590988398573 2.954197863908956\n",
      "0.8270199159662139 3.3534863754191218\n",
      "0.8276228462587256 3.3916837627012297\n",
      "0.8293038004616591 3.501749921894906\n",
      "0.8293652347938587 3.5058766447188616\n",
      "0.8319621942652997 3.6876291506379895\n",
      "0.8340576027302526 3.845749958056787\n",
      "0.8363839059744493 4.035223482560703\n",
      "0.8392288284687781 4.290478098968781\n",
      "0.8438178194952126 4.772302849202502\n",
      "0.8438614665452029 4.7773938034403\n",
      "0.8486306291142391 5.411057795549757\n",
      "0.8520429651323951 5.992580371307452\n",
      "0.856233775963394 6.9591325750561825\n",
      "0.857201596931999 7.2449772123192195\n",
      "0.8587028686829035 7.7604982894179075\n",
      "0.859351377493115 8.018244881581099\n",
      "0.8656411747420587 16.15825000193857\n",
      "0.8659888074985806 22.937204549609486\n",
      "0.8669430280462134 13.625970106148985\n",
      "0.8697781424503921 9.517336504817084\n",
      "0.8700413384214754 9.317589431147166\n",
      "0.8748367786605682 6.974312177031384\n",
      "0.8783314193747447 5.955236694660734\n",
      "0.8785674381715123 5.8967086841745635\n",
      "0.8795608928403085 5.66116369596308\n",
      "0.8799310106416811 5.577530239380281\n",
      "0.8799998751274642 5.562199195977348\n",
      "0.8801604538904846 5.5267224236027905\n",
      "0.8816775869185682 5.208928765539675\n",
      "0.8835046594111335 4.8621970973661925\n",
      "0.8838950574704998 4.79248694181836\n",
      "0.8878324491417531 4.158786584896114\n",
      "0.8880488229765939 4.127116875805464\n",
      "0.8894572739221553 3.92774471169818\n",
      "0.8911570405208478 3.7014476571592962\n",
      "0.8930443065045035 3.4663581233876166\n",
      "0.8965376411599628 3.069515044907278\n",
      "0.8985535088816465 2.860022799187699\n",
      "0.8987154582526631 2.8437509268306873\n",
      "0.8993340626139186 2.7823258638421935\n",
      "0.9020989369916335 2.5211684333521838\n",
      "0.9039155217064578 2.3606370213629075\n",
      "0.9044566899064126 2.3143969194295417\n",
      "0.9055302613286971 2.224728364109934\n",
      "0.9083203788547769 2.0039055306929363\n",
      "0.9100926237744587 1.8722941572623994\n",
      "0.9103262486823254 1.8554237811327088\n",
      "0.9132115910807461 1.6558980047322676\n",
      "0.9183031940925435 1.3412800308410058\n",
      "0.9232445012730006 1.077946193227693\n",
      "0.9258162038128452 0.9560743744873579\n",
      "0.9300635619294226 0.7760713652337551\n",
      "0.9312605797660702 0.7299110730544554\n",
      "0.9314865682577889 0.7214142314785215\n",
      "0.9355108005551793 0.5813083521553027\n",
      "0.9368642376436371 0.5387814047824355\n",
      "0.9414562203411698 0.41055978010222305\n",
      "0.9417611790670963 0.40288261783495555\n",
      "0.9434720292721548 0.3616487124972864\n",
      "0.9461114260919699 0.3039118672816688\n",
      "0.9496387581010401 0.2371296989071135\n",
      "0.9579959403858966 0.12006887879259644\n",
      "0.9584281105991843 0.1153947198256853\n",
      "0.9591237053712096 0.10812666404325144\n",
      "0.959836496854304 0.10099842918826729\n",
      "0.9609315416888069 0.09065889202266351\n",
      "0.9618242863108875 0.08275983037376387\n",
      "0.9651540859887777 0.05723170506832543\n",
      "0.9665148307658236 0.04846264339061356\n",
      "0.9685842739926298 0.03681872007074643\n",
      "0.969604924448507 0.03178404404831495\n",
      "0.9696825399977858 0.03141950490200762\n",
      "0.9703130386755512 0.02855193011513684\n",
      "0.9708956723675464 0.026047766715865384\n",
      "0.9730470665381177 0.017958047968366288\n",
      "0.9730490783542025 0.017951302781695228\n",
      "0.9761660384939395 0.009206212250337431\n",
      "0.9776349628455692 0.006190148988716695\n",
      "0.9793104344567629 0.0035369407821729142\n",
      "0.9800417430011463 0.0026267191031790474\n",
      "0.9816309918637716 0.0011369926581744105\n",
      "0.9831071153519164 0.00031808695468072985\n",
      "0.9831427672942374 0.0003047130491231175\n",
      "0.9834466122701795 0.00020264102580844673\n",
      "0.9856681831621861 7.8109018143563e-05\n",
      "0.9877582318879015 0.0008880338495862145\n",
      "0.9899061899504238 0.0025620877740545286\n",
      "0.9969925500351093 0.013089983201700068\n",
      "0.9974367451796156 0.013965256868339949\n"
     ]
    }
   ],
   "source": [
    "K = 4\n",
    "T = 5\n",
    "Noise_Alloc = [0,2,4,6,8]\n",
    "sigma = 1\n",
    "\n",
    "N = 10\n",
    "\n",
    "j_array = np.array(range(K+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K+T)))\n",
    "\n",
    "i_array = np.array(range(N))\n",
    "beta_array = np.cos(i_array*2*math.pi/(N-1)/2) # np.cos(i_array*2*math.pi/N/2)\n",
    "# print(\"z_array: \",z_array,'\\n')\n",
    "\n",
    "N = 1000\n",
    "z_array = np.random.uniform(-1,1,N) #np.cos(i_array[1:]*2*math.pi/(K+T)/2) # np.cos(i_array*2*math.pi/N/2)\n",
    "z_array = np.sort(z_array)\n",
    "MIS_array = np.zeros((N))\n",
    "MIS_LCC_array = np.zeros((N))\n",
    "# print(z_array)\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "B = [0.5, 1, 1.5, 2]\n",
    "\n",
    "z_array_0 = []\n",
    "z_array_1 = []\n",
    "z_array_2 = []\n",
    "z_array_3 = []\n",
    "z_array_4 = []\n",
    "\n",
    "for j in range(len(z_array)):\n",
    "    MIS_array[j] = MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_array[j]], 1,sigma)\n",
    "    MIS_LCC_array[j] = MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_array[j]], 1,sigma, _is_LCC=True)\n",
    "    \n",
    "    if MIS_array[j] < B[0]:\n",
    "        z_array_0.append(z_array[j])\n",
    "    elif MIS_array[j] < B[1]:\n",
    "        z_array_1.append(z_array[j])\n",
    "    elif MIS_array[j] < B[2]:\n",
    "        z_array_2.append(z_array[j])\n",
    "    elif MIS_array[j] < B[3]:\n",
    "        z_array_3.append(z_array[j])\n",
    "    else:\n",
    "        z_array_4.append(z_array[j])\n",
    "#     print('(beta index, MIS) = ',j,',',MIS_array[j])\n",
    "#     print()\n",
    "\n",
    "\n",
    "\n",
    "print(len(z_array_0),len(z_array_1),len(z_array_2),len(z_array_3),len(z_array_4))\n",
    "\n",
    "\n",
    "plt.plot(z_array, MIS_array, label='Mutual Information Security, BACC')\n",
    "plt.plot(z_array, MIS_LCC_array, label='Mutual Information Security, LCC')\n",
    "plt.plot(alpha_array[Signal_Alloc],0*np.ones(len(Signal_Alloc)),'g*',label='alpha_i, for X')\n",
    "plt.plot(alpha_array[Noise_Alloc],0*np.ones(len(Noise_Alloc)),'r*',label='alpha_i, for N')\n",
    "# plt.plot(beta_array,0*np.ones(len(beta_array)),'b.',label='beta_i')\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('MIS')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "    \n",
    "for i in range(len(z_array)):\n",
    "    print(z_array[i],MIS_array[i])\n",
    "    \n",
    "# print(alpha_array[Signal_Alloc])\n",
    "# print(alpha_array[Noise_Alloc])\n",
    "# print(alpha_array)\n",
    "\n",
    "# plt.plot((2*j_array[Signal_Alloc]+1)/(K+T),alpha_array[Signal_Alloc],'g*',label='alpha_i, for X')\n",
    "# plt.plot((2*j_array[Noise_Alloc]+1)/(K+T),alpha_array[Noise_Alloc],'r*',label='alpha_i, for N')\n",
    "# plt.plot(2*i_array[1:]/(K+T), z_array,'b.',label='beta_i')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.44862362 0.49983047 0.4896348  0.48571877 0.48571877 0.4896348\n",
      " 0.49983047 0.44862362]\n"
     ]
    }
   ],
   "source": [
    "z_array_ = np.array([-0.94, -0.73, -0.534, -0.125, 0.125, 0.534, 0.73, 0.94])\n",
    "MIS_array_ = np.zeros(len(z_array_))\n",
    "for j in range(len(z_array_)):\n",
    "    MIS_array_[j] = MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_array_[j]], 1,sigma)\n",
    "\n",
    "print(MIS_array_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 !!!\n",
      "z_array: [-0.94  -0.534  0.534  0.94 ]\n",
      "0.4486236179368535\n",
      "0.48963480280841937\n",
      "0.4896348028084205\n",
      "0.4486236179368556\n",
      "@BACC_Enc: N,K,T, m_i= 4 4 5 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 4 5 15000 \n",
      "\n",
      "(T, sigma)= 5 1 )  0 -th Trial!!\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.1096 \n",
      "Accuracy: 5721/10000 (57.21%)\n",
      "\n",
      "Round   0, Average loss 2.110 Test accuracy 57.210\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.1266 \n",
      "Accuracy: 7100/10000 (71.00%)\n",
      "\n",
      "Round   1, Average loss 1.127 Test accuracy 71.000\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.5521 \n",
      "Accuracy: 8830/10000 (88.30%)\n",
      "\n",
      "Round   2, Average loss 0.552 Test accuracy 88.300\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.5766 \n",
      "Accuracy: 8792/10000 (87.92%)\n",
      "\n",
      "Round   3, Average loss 0.577 Test accuracy 87.920\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.5347 \n",
      "Accuracy: 8754/10000 (87.54%)\n",
      "\n",
      "Round   4, Average loss 0.535 Test accuracy 87.540\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.4962 \n",
      "Accuracy: 8952/10000 (89.52%)\n",
      "\n",
      "Round   5, Average loss 0.496 Test accuracy 89.520\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.5815 \n",
      "Accuracy: 8530/10000 (85.30%)\n",
      "\n",
      "Round   6, Average loss 0.581 Test accuracy 85.300\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.5218 \n",
      "Accuracy: 8721/10000 (87.21%)\n",
      "\n",
      "Round   7, Average loss 0.522 Test accuracy 87.210\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.4822 \n",
      "Accuracy: 8970/10000 (89.70%)\n",
      "\n",
      "Round   8, Average loss 0.482 Test accuracy 89.700\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.4971 \n",
      "Accuracy: 8965/10000 (89.65%)\n",
      "\n",
      "Round   9, Average loss 0.497 Test accuracy 89.650\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.4843 \n",
      "Accuracy: 8931/10000 (89.31%)\n",
      "\n",
      "Round  10, Average loss 0.484 Test accuracy 89.310\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.4700 \n",
      "Accuracy: 9055/10000 (90.55%)\n",
      "\n",
      "Round  11, Average loss 0.470 Test accuracy 90.550\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.5940 \n",
      "Accuracy: 8268/10000 (82.68%)\n",
      "\n",
      "Round  12, Average loss 0.594 Test accuracy 82.680\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.5906 \n",
      "Accuracy: 8421/10000 (84.21%)\n",
      "\n",
      "Round  13, Average loss 0.591 Test accuracy 84.210\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.4872 \n",
      "Accuracy: 8947/10000 (89.47%)\n",
      "\n",
      "Round  14, Average loss 0.487 Test accuracy 89.470\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.4743 \n",
      "Accuracy: 9090/10000 (90.90%)\n",
      "\n",
      "Round  15, Average loss 0.474 Test accuracy 90.900\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.4518 \n",
      "Accuracy: 9072/10000 (90.72%)\n",
      "\n",
      "Round  16, Average loss 0.452 Test accuracy 90.720\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.6107 \n",
      "Accuracy: 8392/10000 (83.92%)\n",
      "\n",
      "Round  17, Average loss 0.611 Test accuracy 83.920\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.4972 \n",
      "Accuracy: 9029/10000 (90.29%)\n",
      "\n",
      "Round  18, Average loss 0.497 Test accuracy 90.290\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.5020 \n",
      "Accuracy: 8896/10000 (88.96%)\n",
      "\n",
      "Round  19, Average loss 0.502 Test accuracy 88.960\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.4898 \n",
      "Accuracy: 8988/10000 (89.88%)\n",
      "\n",
      "Round  20, Average loss 0.490 Test accuracy 89.880\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.5119 \n",
      "Accuracy: 8797/10000 (87.97%)\n",
      "\n",
      "Round  21, Average loss 0.512 Test accuracy 87.970\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.5865 \n",
      "Accuracy: 8486/10000 (84.86%)\n",
      "\n",
      "Round  22, Average loss 0.586 Test accuracy 84.860\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.4493 \n",
      "Accuracy: 9085/10000 (90.85%)\n",
      "\n",
      "Round  23, Average loss 0.449 Test accuracy 90.850\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.5316 \n",
      "Accuracy: 8671/10000 (86.71%)\n",
      "\n",
      "Round  24, Average loss 0.532 Test accuracy 86.710\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.4957 \n",
      "Accuracy: 8876/10000 (88.76%)\n",
      "\n",
      "Round  25, Average loss 0.496 Test accuracy 88.760\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.4819 \n",
      "Accuracy: 9052/10000 (90.52%)\n",
      "\n",
      "Round  26, Average loss 0.482 Test accuracy 90.520\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.5505 \n",
      "Accuracy: 8454/10000 (84.54%)\n",
      "\n",
      "Round  27, Average loss 0.551 Test accuracy 84.540\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.4558 \n",
      "Accuracy: 9063/10000 (90.63%)\n",
      "\n",
      "Round  28, Average loss 0.456 Test accuracy 90.630\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.5085 \n",
      "Accuracy: 8865/10000 (88.65%)\n",
      "\n",
      "Round  29, Average loss 0.508 Test accuracy 88.650\n",
      "1 !!!\n",
      "z_array: [-0.94 -0.73  0.73  0.94]\n",
      "0.4486236179368535\n",
      "0.49983046821646326\n",
      "0.4998304682164621\n",
      "0.4486236179368556\n",
      "@BACC_Enc: N,K,T, m_i= 4 4 5 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 4 5 15000 \n",
      "\n",
      "(T, sigma)= 5 1 )  0 -th Trial!!\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 14.9017 \n",
      "Accuracy: 1010/10000 (10.10%)\n",
      "\n",
      "Round   0, Average loss 14.902 Test accuracy 10.100\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 6.1003 \n",
      "Accuracy: 913/10000 (9.13%)\n",
      "\n",
      "Round   1, Average loss 6.100 Test accuracy 9.130\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 3.1299 \n",
      "Accuracy: 2403/10000 (24.03%)\n",
      "\n",
      "Round   2, Average loss 3.130 Test accuracy 24.030\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.1211 \n",
      "Accuracy: 5713/10000 (57.13%)\n",
      "\n",
      "Round   3, Average loss 2.121 Test accuracy 57.130\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6793 \n",
      "Accuracy: 6092/10000 (60.92%)\n",
      "\n",
      "Round   4, Average loss 1.679 Test accuracy 60.920\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8428 \n",
      "Accuracy: 4490/10000 (44.90%)\n",
      "\n",
      "Round   5, Average loss 1.843 Test accuracy 44.900\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.4765 \n",
      "Accuracy: 5939/10000 (59.39%)\n",
      "\n",
      "Round   6, Average loss 1.477 Test accuracy 59.390\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.5836 \n",
      "Accuracy: 3956/10000 (39.56%)\n",
      "\n",
      "Round   7, Average loss 1.584 Test accuracy 39.560\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.9048 \n",
      "Accuracy: 6955/10000 (69.55%)\n",
      "\n",
      "Round   8, Average loss 0.905 Test accuracy 69.550\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.2221 \n",
      "Accuracy: 5199/10000 (51.99%)\n",
      "\n",
      "Round   9, Average loss 2.222 Test accuracy 51.990\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.3952 \n",
      "Accuracy: 3415/10000 (34.15%)\n",
      "\n",
      "Round  10, Average loss 2.395 Test accuracy 34.150\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.0757 \n",
      "Accuracy: 5288/10000 (52.88%)\n",
      "\n",
      "Round  11, Average loss 2.076 Test accuracy 52.880\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.0978 \n",
      "Accuracy: 6371/10000 (63.71%)\n",
      "\n",
      "Round  12, Average loss 1.098 Test accuracy 63.710\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.9247 \n",
      "Accuracy: 6976/10000 (69.76%)\n",
      "\n",
      "Round  13, Average loss 0.925 Test accuracy 69.760\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8458 \n",
      "Accuracy: 5300/10000 (53.00%)\n",
      "\n",
      "Round  14, Average loss 1.846 Test accuracy 53.000\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.2624 \n",
      "Accuracy: 6462/10000 (64.62%)\n",
      "\n",
      "Round  15, Average loss 1.262 Test accuracy 64.620\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.8284 \n",
      "Accuracy: 7170/10000 (71.70%)\n",
      "\n",
      "Round  16, Average loss 0.828 Test accuracy 71.700\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.3174 \n",
      "Accuracy: 5706/10000 (57.06%)\n",
      "\n",
      "Round  17, Average loss 1.317 Test accuracy 57.060\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6789 \n",
      "Accuracy: 5641/10000 (56.41%)\n",
      "\n",
      "Round  18, Average loss 1.679 Test accuracy 56.410\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.3697 \n",
      "Accuracy: 5732/10000 (57.32%)\n",
      "\n",
      "Round  19, Average loss 1.370 Test accuracy 57.320\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8367 \n",
      "Accuracy: 5033/10000 (50.33%)\n",
      "\n",
      "Round  20, Average loss 1.837 Test accuracy 50.330\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.0057 \n",
      "Accuracy: 6839/10000 (68.39%)\n",
      "\n",
      "Round  21, Average loss 1.006 Test accuracy 68.390\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.9745 \n",
      "Accuracy: 7327/10000 (73.27%)\n",
      "\n",
      "Round  22, Average loss 0.974 Test accuracy 73.270\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.2776 \n",
      "Accuracy: 6115/10000 (61.15%)\n",
      "\n",
      "Round  23, Average loss 1.278 Test accuracy 61.150\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.8756 \n",
      "Accuracy: 7395/10000 (73.95%)\n",
      "\n",
      "Round  24, Average loss 0.876 Test accuracy 73.950\n",
      "selected users: [0 1 2 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.9437 \n",
      "Accuracy: 4549/10000 (45.49%)\n",
      "\n",
      "Round  25, Average loss 1.944 Test accuracy 45.490\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9621 \n",
      "Accuracy: 4538/10000 (45.38%)\n",
      "\n",
      "Round  26, Average loss 1.962 Test accuracy 45.380\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.5629 \n",
      "Accuracy: 5165/10000 (51.65%)\n",
      "\n",
      "Round  27, Average loss 1.563 Test accuracy 51.650\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.5378 \n",
      "Accuracy: 4532/10000 (45.32%)\n",
      "\n",
      "Round  28, Average loss 1.538 Test accuracy 45.320\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.7706 \n",
      "Accuracy: 3791/10000 (37.91%)\n",
      "\n",
      "Round  29, Average loss 1.771 Test accuracy 37.910\n",
      "2 !!!\n",
      "z_array: [-0.94  -0.125  0.125  0.94 ]\n",
      "0.4486236179368535\n",
      "0.4857187736123886\n",
      "0.48571877361238697\n",
      "0.4486236179368556\n",
      "@BACC_Enc: N,K,T, m_i= 4 4 5 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 4 5 15000 \n",
      "\n",
      "(T, sigma)= 5 1 )  0 -th Trial!!\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1009/10000 (10.09%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 10.090\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.2424 \n",
      "Accuracy: 5070/10000 (50.70%)\n",
      "\n",
      "Round   1, Average loss 2.242 Test accuracy 50.700\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.4146 \n",
      "Accuracy: 7038/10000 (70.38%)\n",
      "\n",
      "Round   2, Average loss 1.415 Test accuracy 70.380\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.3003 \n",
      "Accuracy: 9427/10000 (94.27%)\n",
      "\n",
      "Round   3, Average loss 0.300 Test accuracy 94.270\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.3013 \n",
      "Accuracy: 9451/10000 (94.51%)\n",
      "\n",
      "Round   4, Average loss 0.301 Test accuracy 94.510\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.2650 \n",
      "Accuracy: 9476/10000 (94.76%)\n",
      "\n",
      "Round   5, Average loss 0.265 Test accuracy 94.760\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.3133 \n",
      "Accuracy: 9405/10000 (94.05%)\n",
      "\n",
      "Round   6, Average loss 0.313 Test accuracy 94.050\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.2627 \n",
      "Accuracy: 9508/10000 (95.08%)\n",
      "\n",
      "Round   7, Average loss 0.263 Test accuracy 95.080\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.3314 \n",
      "Accuracy: 9370/10000 (93.70%)\n",
      "\n",
      "Round   8, Average loss 0.331 Test accuracy 93.700\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.2940 \n",
      "Accuracy: 9458/10000 (94.58%)\n",
      "\n",
      "Round   9, Average loss 0.294 Test accuracy 94.580\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.2614 \n",
      "Accuracy: 9506/10000 (95.06%)\n",
      "\n",
      "Round  10, Average loss 0.261 Test accuracy 95.060\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.3157 \n",
      "Accuracy: 9466/10000 (94.66%)\n",
      "\n",
      "Round  11, Average loss 0.316 Test accuracy 94.660\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.2752 \n",
      "Accuracy: 9434/10000 (94.34%)\n",
      "\n",
      "Round  12, Average loss 0.275 Test accuracy 94.340\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.2416 \n",
      "Accuracy: 9519/10000 (95.19%)\n",
      "\n",
      "Round  13, Average loss 0.242 Test accuracy 95.190\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.2472 \n",
      "Accuracy: 9494/10000 (94.94%)\n",
      "\n",
      "Round  14, Average loss 0.247 Test accuracy 94.940\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.2721 \n",
      "Accuracy: 9484/10000 (94.84%)\n",
      "\n",
      "Round  15, Average loss 0.272 Test accuracy 94.840\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.2630 \n",
      "Accuracy: 9476/10000 (94.76%)\n",
      "\n",
      "Round  16, Average loss 0.263 Test accuracy 94.760\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.2521 \n",
      "Accuracy: 9499/10000 (94.99%)\n",
      "\n",
      "Round  17, Average loss 0.252 Test accuracy 94.990\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.2482 \n",
      "Accuracy: 9498/10000 (94.98%)\n",
      "\n",
      "Round  18, Average loss 0.248 Test accuracy 94.980\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.2595 \n",
      "Accuracy: 9466/10000 (94.66%)\n",
      "\n",
      "Round  19, Average loss 0.259 Test accuracy 94.660\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.3803 \n",
      "Accuracy: 9427/10000 (94.27%)\n",
      "\n",
      "Round  20, Average loss 0.380 Test accuracy 94.270\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.3021 \n",
      "Accuracy: 9477/10000 (94.77%)\n",
      "\n",
      "Round  21, Average loss 0.302 Test accuracy 94.770\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.2993 \n",
      "Accuracy: 9461/10000 (94.61%)\n",
      "\n",
      "Round  22, Average loss 0.299 Test accuracy 94.610\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.2820 \n",
      "Accuracy: 9473/10000 (94.73%)\n",
      "\n",
      "Round  23, Average loss 0.282 Test accuracy 94.730\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.2762 \n",
      "Accuracy: 9470/10000 (94.70%)\n",
      "\n",
      "Round  24, Average loss 0.276 Test accuracy 94.700\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.2685 \n",
      "Accuracy: 9438/10000 (94.38%)\n",
      "\n",
      "Round  25, Average loss 0.269 Test accuracy 94.380\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.2634 \n",
      "Accuracy: 9491/10000 (94.91%)\n",
      "\n",
      "Round  26, Average loss 0.263 Test accuracy 94.910\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.2612 \n",
      "Accuracy: 9494/10000 (94.94%)\n",
      "\n",
      "Round  27, Average loss 0.261 Test accuracy 94.940\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.2629 \n",
      "Accuracy: 9461/10000 (94.61%)\n",
      "\n",
      "Round  28, Average loss 0.263 Test accuracy 94.610\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.2946 \n",
      "Accuracy: 9421/10000 (94.21%)\n",
      "\n",
      "Round  29, Average loss 0.295 Test accuracy 94.210\n",
      "3 !!!\n",
      "z_array: [-0.94  -0.73  -0.534 -0.125  0.125  0.534  0.73   0.94 ]\n",
      "0.4486236179368535\n",
      "0.49983046821646326\n",
      "0.48963480280841937\n",
      "0.4857187736123886\n",
      "0.48571877361238697\n",
      "0.4896348028084205\n",
      "0.4998304682164621\n",
      "0.4486236179368556\n",
      "@BACC_Enc: N,K,T, m_i= 8 4 5 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 8 4 5 15000 \n",
      "\n",
      "(T, sigma)= 5 1 )  0 -th Trial!!\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 2.3025 \n",
      "Accuracy: 1570/10000 (15.70%)\n",
      "\n",
      "Round   0, Average loss 2.302 Test accuracy 15.700\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 2.3023 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   1, Average loss 2.302 Test accuracy 9.800\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 2.2951 \n",
      "Accuracy: 3325/10000 (33.25%)\n",
      "\n",
      "Round   2, Average loss 2.295 Test accuracy 33.250\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 2.2834 \n",
      "Accuracy: 4603/10000 (46.03%)\n",
      "\n",
      "Round   3, Average loss 2.283 Test accuracy 46.030\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 2.2469 \n",
      "Accuracy: 6849/10000 (68.49%)\n",
      "\n",
      "Round   4, Average loss 2.247 Test accuracy 68.490\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 2.2372 \n",
      "Accuracy: 6985/10000 (69.85%)\n",
      "\n",
      "Round   5, Average loss 2.237 Test accuracy 69.850\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 2.1586 \n",
      "Accuracy: 8297/10000 (82.97%)\n",
      "\n",
      "Round   6, Average loss 2.159 Test accuracy 82.970\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 2.0744 \n",
      "Accuracy: 8772/10000 (87.72%)\n",
      "\n",
      "Round   7, Average loss 2.074 Test accuracy 87.720\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 2.0148 \n",
      "Accuracy: 8916/10000 (89.16%)\n",
      "\n",
      "Round   8, Average loss 2.015 Test accuracy 89.160\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.8957 \n",
      "Accuracy: 8740/10000 (87.40%)\n",
      "\n",
      "Round   9, Average loss 1.896 Test accuracy 87.400\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.8660 \n",
      "Accuracy: 9333/10000 (93.33%)\n",
      "\n",
      "Round  10, Average loss 1.866 Test accuracy 93.330\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.9060 \n",
      "Accuracy: 9321/10000 (93.21%)\n",
      "\n",
      "Round  11, Average loss 1.906 Test accuracy 93.210\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.8647 \n",
      "Accuracy: 9156/10000 (91.56%)\n",
      "\n",
      "Round  12, Average loss 1.865 Test accuracy 91.560\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.9895 \n",
      "Accuracy: 8688/10000 (86.88%)\n",
      "\n",
      "Round  13, Average loss 1.989 Test accuracy 86.880\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.9847 \n",
      "Accuracy: 8912/10000 (89.12%)\n",
      "\n",
      "Round  14, Average loss 1.985 Test accuracy 89.120\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.8959 \n",
      "Accuracy: 9271/10000 (92.71%)\n",
      "\n",
      "Round  15, Average loss 1.896 Test accuracy 92.710\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.8891 \n",
      "Accuracy: 9292/10000 (92.92%)\n",
      "\n",
      "Round  16, Average loss 1.889 Test accuracy 92.920\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.8449 \n",
      "Accuracy: 9199/10000 (91.99%)\n",
      "\n",
      "Round  17, Average loss 1.845 Test accuracy 91.990\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.8692 \n",
      "Accuracy: 9353/10000 (93.53%)\n",
      "\n",
      "Round  18, Average loss 1.869 Test accuracy 93.530\n",
      "selected users: [0 1 2 3 4 5 6 7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.8736 \n",
      "Accuracy: 9199/10000 (91.99%)\n",
      "\n",
      "Round  19, Average loss 1.874 Test accuracy 91.990\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.9324 \n",
      "Accuracy: 9358/10000 (93.58%)\n",
      "\n",
      "Round  20, Average loss 1.932 Test accuracy 93.580\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.8969 \n",
      "Accuracy: 9094/10000 (90.94%)\n",
      "\n",
      "Round  21, Average loss 1.897 Test accuracy 90.940\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.8659 \n",
      "Accuracy: 9164/10000 (91.64%)\n",
      "\n",
      "Round  22, Average loss 1.866 Test accuracy 91.640\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.8866 \n",
      "Accuracy: 8781/10000 (87.81%)\n",
      "\n",
      "Round  23, Average loss 1.887 Test accuracy 87.810\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.8758 \n",
      "Accuracy: 9261/10000 (92.61%)\n",
      "\n",
      "Round  24, Average loss 1.876 Test accuracy 92.610\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.8242 \n",
      "Accuracy: 9403/10000 (94.03%)\n",
      "\n",
      "Round  25, Average loss 1.824 Test accuracy 94.030\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.9040 \n",
      "Accuracy: 9103/10000 (91.03%)\n",
      "\n",
      "Round  26, Average loss 1.904 Test accuracy 91.030\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.9271 \n",
      "Accuracy: 9135/10000 (91.35%)\n",
      "\n",
      "Round  27, Average loss 1.927 Test accuracy 91.350\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.9110 \n",
      "Accuracy: 9250/10000 (92.50%)\n",
      "\n",
      "Round  28, Average loss 1.911 Test accuracy 92.500\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.8761 \n",
      "Accuracy: 9361/10000 (93.61%)\n",
      "\n",
      "Round  29, Average loss 1.876 Test accuracy 93.610\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "K = 4\n",
    "T = 5\n",
    "sigma = 1\n",
    "Noise_Alloc = [0,2,4,6,8]\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "\n",
    "N_array = [4, 4, 4, 8]\n",
    "B_array = [0.5]\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 30\n",
    "\n",
    "\n",
    "\n",
    "loss_test_arr_K4_G1 = np.zeros((len(N_array),len(B_array),N_trials,N_epochs))\n",
    "acc_test_arr_K4_G1  = np.zeros((len(N_array),len(B_array),N_trials,N_epochs))\n",
    "\n",
    "for N_idx in range(len(N_array)):\n",
    "    \n",
    "    N = N_array[N_idx]\n",
    "    \n",
    "    \n",
    "           \n",
    "        \n",
    "    # print(\"alpha_array: \",alpha_array,'\\n')\n",
    "    \n",
    "    \n",
    "    for B_idx in range(len(B_array)):\n",
    "        \n",
    "        B = B_array[B_idx]\n",
    "        z_array = []\n",
    "#         while(len(z_array)<N):\n",
    "#             z_tmp = np.random.uniform(-1,1,1)\n",
    "#             MIS_tmp = MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_tmp], 1,sigma)\n",
    "#             if MIS_tmp < B and MIS_tmp > 0.1:\n",
    "#                 z_array.append(z_tmp[0])\n",
    "#         \n",
    "#         z_array = np.sort(z_array)\n",
    "        print(N_idx,'!!!')\n",
    "        if N_idx==0:\n",
    "            z_array = np.array([-0.94,-0.534,0.534, 0.94])\n",
    "        elif N_idx==1:\n",
    "            z_array = np.array([-0.94, -0.73, 0.73, 0.94])\n",
    "        elif N_idx==2:\n",
    "            z_array = np.array([-0.94, -0.125, 0.125, 0.94])\n",
    "        else:\n",
    "            z_array = np.array([-0.94, -0.73, -0.534, -0.125, 0.125, 0.534, 0.73, 0.94])\n",
    "#             z_array = np.array([-0.9, -0.81, -0.22, -0.20, 0.20, 0.22, 0.81, 0.9])\n",
    "            \n",
    "        print('z_array:',z_array)\n",
    "        for j in range(len(z_array)):\n",
    "            print(MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_array[j]], 1,sigma))\n",
    "        \n",
    "        \n",
    "        _Noise_label = np.ones((15000*T,10)) * 0.1\n",
    "\n",
    "        X_tilde,a,b = BACC_Enc_Data_v3(encoding_input_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde,a,b = BACC_Enc_Data_v3(encoding_label_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "\n",
    "        m = N # m is the number of received result @ master\n",
    "    #     print('number of results:',m)\n",
    "\n",
    "        for trial_idx in range(N_trials):\n",
    "            print('(T, sigma)=',T,sigma,') ',trial_idx,'-th Trial!!')\n",
    "\n",
    "            net_glob = CNNMnist2(args=args)\n",
    "            net_glob.cuda()\n",
    "            net_glob.train()\n",
    "\n",
    "            # copy weights\n",
    "            w_glob = net_glob.state_dict()\n",
    "\n",
    "            for iter in range(N_epochs): #args.epochs\n",
    "                w_locals, loss_locals = [], []\n",
    "                idxs_users = np.random.choice(range(N), m, replace=False)\n",
    "                idxs_users = np.sort(idxs_users)\n",
    "                print('selected users:',idxs_users)\n",
    "                \n",
    "                coded_net = BACC_Enc_Model_withNoise_v3(net_glob.cuda(), N, K, T, 0.01, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "                dec_z_array = []\n",
    "                for idx in idxs_users: #for idx in range(N):\n",
    "            #         print(idx)\n",
    "                    local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[idx,:,:], label=y_tilde[idx,:,:])\n",
    "                    w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "#                     w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                    w_locals.append(copy.deepcopy(w))\n",
    "                    loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                    dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "                # update global weights\n",
    "                #w_glob = FedAvg(w_locals)\n",
    "                w_glob = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "\n",
    "                # copy weight to net_glob\n",
    "                net_glob.load_state_dict(w_glob)\n",
    "\n",
    "                # print loss\n",
    "            #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "            #     loss_train_arr.append(loss_train)\n",
    "\n",
    "                acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "                acc_test_arr_K4_G1[N_idx][B_idx][trial_idx][iter] = acc_test\n",
    "                loss_test_arr_K4_G1[N_idx][B_idx][trial_idx][iter] = loss_test\n",
    "                if iter % 1 ==0:\n",
    "                    print('Round {:3d}, Average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_test, acc_test))\n",
    "                #print(loss_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. With Grouping, N=4, N_i=2, G=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2 3\n",
      "##########################################\n",
      "###### 0 -th Trial!! ###########\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.2922 \n",
      "Accuracy: 2742/10000 (27.42%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.2554 \n",
      "Accuracy: 3894/10000 (38.94%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.3262 \n",
      "Accuracy: 7200/10000 (72.00%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.6624 \n",
      "Accuracy: 8873/10000 (88.73%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.4450 \n",
      "Accuracy: 9446/10000 (94.46%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3529 \n",
      "Accuracy: 9474/10000 (94.74%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3906 \n",
      "Accuracy: 9476/10000 (94.76%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3426 \n",
      "Accuracy: 9502/10000 (95.02%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3325 \n",
      "Accuracy: 9516/10000 (95.16%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3553 \n",
      "Accuracy: 9527/10000 (95.27%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3242 \n",
      "Accuracy: 9565/10000 (95.65%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3068 \n",
      "Accuracy: 9569/10000 (95.69%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3130 \n",
      "Accuracy: 9546/10000 (95.46%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3697 \n",
      "Accuracy: 9507/10000 (95.07%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3316 \n",
      "Accuracy: 9528/10000 (95.28%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2945 \n",
      "Accuracy: 9530/10000 (95.30%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3098 \n",
      "Accuracy: 9529/10000 (95.29%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2964 \n",
      "Accuracy: 9515/10000 (95.15%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2905 \n",
      "Accuracy: 9521/10000 (95.21%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3015 \n",
      "Accuracy: 9524/10000 (95.24%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2967 \n",
      "Accuracy: 9518/10000 (95.18%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2678 \n",
      "Accuracy: 9563/10000 (95.63%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2688 \n",
      "Accuracy: 9555/10000 (95.55%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2546 \n",
      "Accuracy: 9568/10000 (95.68%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2670 \n",
      "Accuracy: 9570/10000 (95.70%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2567 \n",
      "Accuracy: 9520/10000 (95.20%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2519 \n",
      "Accuracy: 9562/10000 (95.62%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2475 \n",
      "Accuracy: 9556/10000 (95.56%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2605 \n",
      "Accuracy: 9556/10000 (95.56%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3143 \n",
      "Accuracy: 9488/10000 (94.88%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "G = 2\n",
    "N = 4 # N should be divisible by G\n",
    "K = 4 # N should be divisible by G\n",
    "\n",
    "size_per_group = int(60000/G)\n",
    "\n",
    "X_group = np.reshape(encoding_input_array_np, (G,size_per_group,28*28))\n",
    "y_group = np.reshape(encoding_label_array_np, (G,size_per_group,args.num_classes)) \n",
    "\n",
    "N_i = int(N/G) # = 2\n",
    "K_i = int(K/G) # = 2\n",
    "T = 3\n",
    "sigma = 1\n",
    "Noise_Alloc = [0,2,4]\n",
    "m = N_i # number of selected workers (if there is no straggler, m=N_i)\n",
    "\n",
    "print(N_i,K_i,T)\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K_i+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K_i+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K_i+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "z_array = np.array([-0.81, 0.81])\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 30\n",
    "\n",
    "loss_test_arr_G2 = np.zeros((N_trials,N_epochs))\n",
    "acc_test_arr_G2  = np.zeros((N_trials,N_epochs))\n",
    "\n",
    "for trial_idx in range(N_trials):\n",
    "    \n",
    "    print('##########################################')\n",
    "    print('######',trial_idx,'-th Trial!! ###########')\n",
    "    \n",
    "    net_glob = CNNMnist2(args=args)\n",
    "    net_glob.cuda()\n",
    "    net_glob.train()\n",
    "    \n",
    "    # copy weights\n",
    "    w_glob = net_glob.state_dict()\n",
    "    \n",
    "    X_tilde = np.empty((N,Size_submatrices,28*28))\n",
    "    y_tilde = np.empty((N,Size_submatrices,10))\n",
    "    \n",
    "    for G_idx in range(G):\n",
    "        \n",
    "        _Noise_label = np.ones((size_per_group*T,10)) * 0.1\n",
    "        \n",
    "        X_tilde_tmp,a,b = BACC_Enc_Data_v3(X_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde_tmp,a,b = BACC_Enc_Data_v3(y_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "        \n",
    "        stt_pos = G_idx * N_i\n",
    "        end_pos = (G_idx+1) * N_i\n",
    "        \n",
    "        X_tilde[stt_pos:end_pos,:,:] = X_tilde_tmp\n",
    "        y_tilde[stt_pos:end_pos,:,:] = y_tilde_tmp\n",
    "        \n",
    "   \n",
    "\n",
    "    for iter in range(N_epochs): #args.epochs\n",
    "        \n",
    "        w_group_array = []\n",
    "        for G_idx in range(G):\n",
    "            w_locals, loss_locals = [], []\n",
    "            idxs_users = np.random.choice(range(N_i), m, replace=False)\n",
    "            idxs_users = np.sort(idxs_users)\n",
    "            print('selected users:',idxs_users)\n",
    "\n",
    "            coded_net = BACC_Enc_Model_withNoise_v3(net_glob.cuda(), N_i, K_i, T, 0.01, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "            dec_z_array = []\n",
    "            for idx in idxs_users: #for idx in range(N):\n",
    "        #         print(idx)\n",
    "                local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[G_idx*N_i+idx,:,:], label=y_tilde[G_idx*N_i+idx,:,:])\n",
    "                w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "#                 w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                w_locals.append(copy.deepcopy(w))\n",
    "                loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "            # update global weights\n",
    "            #w_glob = FedAvg(w_locals)\n",
    "            w_group = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "            \n",
    "            w_group_array.append(copy.deepcopy(w_group))\n",
    "        \n",
    "        w_glob = copy.deepcopy(w_group_array[0])\n",
    "        for k in w_glob.keys():\n",
    "            for G_idx in range(1,G):\n",
    "                w_glob[k] += w_group_array[G_idx][k]\n",
    "            w_glob[k] = torch.div(w_glob[k], len(w_group_array))\n",
    "        \n",
    "        # copy weight to net_glob\n",
    "        net_glob.load_state_dict(w_glob)\n",
    "\n",
    "        # print loss\n",
    "    #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "    #     loss_train_arr.append(loss_train)\n",
    "\n",
    "        acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "        acc_test_arr_G2[trial_idx][iter] = acc_test\n",
    "        loss_test_arr_G2[trial_idx][iter] = loss_test\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. With Grouping, N=8, N_i=4, G=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2 3\n",
      "##########################################\n",
      "###### 0 -th Trial!! ###########\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 15000 \n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.3017 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.3014 \n",
      "Accuracy: 1828/10000 (18.28%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.3012 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.2969 \n",
      "Accuracy: 3021/10000 (30.21%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.2764 \n",
      "Accuracy: 5190/10000 (51.90%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.2188 \n",
      "Accuracy: 5264/10000 (52.64%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.1921 \n",
      "Accuracy: 5235/10000 (52.35%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.1554 \n",
      "Accuracy: 5730/10000 (57.30%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.0841 \n",
      "Accuracy: 7809/10000 (78.09%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.0817 \n",
      "Accuracy: 7686/10000 (76.86%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9836 \n",
      "Accuracy: 9151/10000 (91.51%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9573 \n",
      "Accuracy: 8965/10000 (89.65%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9295 \n",
      "Accuracy: 9351/10000 (93.51%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9021 \n",
      "Accuracy: 9367/10000 (93.67%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9236 \n",
      "Accuracy: 9385/10000 (93.85%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8886 \n",
      "Accuracy: 9350/10000 (93.50%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8534 \n",
      "Accuracy: 9420/10000 (94.20%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8475 \n",
      "Accuracy: 9457/10000 (94.57%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8547 \n",
      "Accuracy: 9360/10000 (93.60%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8596 \n",
      "Accuracy: 9410/10000 (94.10%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9589 \n",
      "Accuracy: 9301/10000 (93.01%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9261 \n",
      "Accuracy: 9333/10000 (93.33%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8444 \n",
      "Accuracy: 9399/10000 (93.99%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8707 \n",
      "Accuracy: 9403/10000 (94.03%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9353 \n",
      "Accuracy: 9406/10000 (94.06%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8539 \n",
      "Accuracy: 9320/10000 (93.20%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8646 \n",
      "Accuracy: 9359/10000 (93.59%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8296 \n",
      "Accuracy: 9402/10000 (94.02%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8863 \n",
      "Accuracy: 9373/10000 (93.73%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "N = 8\n",
    "K = 4\n",
    "G = 2\n",
    "\n",
    "N_i = int(N/G) # = 2\n",
    "K_i = int(K/G) # = 2\n",
    "T = 3\n",
    "sigma = 1\n",
    "Noise_Alloc = [0,2,4]\n",
    "m = N_i # number of selected workers (if there is no straggler, m=N_i)\n",
    "\n",
    "print(N_i,K_i,T)\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K_i+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K_i+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K_i+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "z_array = np.array([-0.81, -0.22, 0.22, 0.81])\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 30\n",
    "\n",
    "loss_test_arr_K4_G2_N8 = np.zeros((N_trials,N_epochs))\n",
    "acc_test_arr_K4_G2_N8  = np.zeros((N_trials,N_epochs))\n",
    "\n",
    "for trial_idx in range(N_trials):\n",
    "    \n",
    "    print('##########################################')\n",
    "    print('######',trial_idx,'-th Trial!! ###########')\n",
    "    \n",
    "    net_glob = CNNMnist2(args=args)\n",
    "    net_glob.cuda()\n",
    "    net_glob.train()\n",
    "    \n",
    "    # copy weights\n",
    "    w_glob = net_glob.state_dict()\n",
    "    \n",
    "    X_tilde = np.empty((N,Size_submatrices,28*28))\n",
    "    y_tilde = np.empty((N,Size_submatrices,10))\n",
    "    \n",
    "    for G_idx in range(G):\n",
    "        \n",
    "        _Noise_label = np.ones((size_per_group*T,10)) * 0.1\n",
    "        \n",
    "        X_tilde_tmp,a,b = BACC_Enc_Data_v3(X_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde_tmp,a,b = BACC_Enc_Data_v3(y_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "        \n",
    "        stt_pos = G_idx * N_i\n",
    "        end_pos = (G_idx+1) * N_i\n",
    "        \n",
    "        X_tilde[stt_pos:end_pos,:,:] = X_tilde_tmp\n",
    "        y_tilde[stt_pos:end_pos,:,:] = y_tilde_tmp\n",
    "        \n",
    "   \n",
    "\n",
    "    for iter in range(N_epochs): #args.epochs\n",
    "        \n",
    "        w_group_array = []\n",
    "        for G_idx in range(G):\n",
    "            w_locals, loss_locals = [], []\n",
    "            idxs_users = np.random.choice(range(N_i), m, replace=False)\n",
    "            idxs_users = np.sort(idxs_users)\n",
    "            print('selected users:',idxs_users)\n",
    "\n",
    "            coded_net = BACC_Enc_Model_withNoise_v3(net_glob.cuda(), N_i, K_i, T, 0.01, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "            dec_z_array = []\n",
    "            for idx in idxs_users: #for idx in range(N):\n",
    "        #         print(idx)\n",
    "                local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[G_idx*N_i+idx,:,:], label=y_tilde[G_idx*N_i+idx,:,:])\n",
    "                w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "#                 w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                w_locals.append(copy.deepcopy(w))\n",
    "                loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "            # update global weights\n",
    "            #w_glob = FedAvg(w_locals)\n",
    "            w_group = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "            \n",
    "            w_group_array.append(copy.deepcopy(w_group))\n",
    "        \n",
    "        w_glob = copy.deepcopy(w_group_array[0])\n",
    "        for k in w_glob.keys():\n",
    "            for G_idx in range(1,G):\n",
    "                w_glob[k] += w_group_array[G_idx][k]\n",
    "            w_glob[k] = torch.div(w_glob[k], len(w_group_array))\n",
    "        \n",
    "        # copy weight to net_glob\n",
    "        net_glob.load_state_dict(w_glob)\n",
    "\n",
    "        # print loss\n",
    "    #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "    #     loss_train_arr.append(loss_train)\n",
    "\n",
    "        acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "        acc_test_arr_K4_G2_N8[trial_idx][iter] = acc_test\n",
    "        loss_test_arr_K4_G2_N8[trial_idx][iter] = loss_test\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. With Grouping, N=8, N_i=2, G=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "464 86 75 63 312\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.9960065472726245 0.024246284094422223\n",
      "-0.9945806304055294 0.02379138948912273\n",
      "-0.993493820294377 0.02344658388246322\n",
      "-0.9915948270662338 0.02284811569959494\n",
      "-0.9898654400800284 0.02230760460521098\n",
      "-0.9897014327908784 0.022256570490235782\n",
      "-0.988038212984107 0.021741257534331246\n",
      "-0.9855355914753883 0.020973626381079663\n",
      "-0.9836359338139788 0.02039727443095157\n",
      "-0.9833772703799692 0.0203192250498566\n",
      "-0.9788318268364826 0.018964774051525496\n",
      "-0.9759626600991576 0.01812685312430741\n",
      "-0.9747165405337235 0.01776714244762389\n",
      "-0.9733447938473296 0.0173741628752302\n",
      "-0.9711924567729937 0.016763972611583335\n",
      "-0.9674330021665809 0.015717351079513994\n",
      "-0.9652858824012636 0.015130801675177045\n",
      "-0.9652457929172171 0.015119928807473135\n",
      "-0.9642087780812734 0.014839687334542639\n",
      "-0.9638938590998354 0.014754971483028234\n",
      "-0.9621416084416963 0.014286926257770411\n",
      "-0.9599432872341076 0.013707798213239565\n",
      "-0.9593561339584071 0.01355465651173764\n",
      "-0.9586057977696858 0.013359906725444062\n",
      "-0.9575513224099064 0.01308803759138225\n",
      "-0.9566128944599299 0.012847890368682946\n",
      "-0.9563593223783295 0.012783293384640643\n",
      "-0.9537677825661035 0.012130327459587198\n",
      "-0.9527619326889922 0.011880475625196098\n",
      "-0.9522503461694536 0.011754175491524728\n",
      "-0.9509199093815408 0.01142819117370528\n",
      "-0.9489235102356512 0.010945804989857163\n",
      "-0.9489146201023797 0.010943675227370754\n",
      "-0.9486355149673011 0.010876894699892867\n",
      "-0.9441450555372011 0.009824941106922351\n",
      "-0.9440315481445896 0.009798905742771381\n",
      "-0.9431204409975924 0.009590930512940648\n",
      "-0.9384849004561346 0.008560945000614325\n",
      "-0.9353661165618867 0.007895070065038822\n",
      "-0.9334356784710904 0.007494116361763292\n",
      "-0.9294193710605347 0.0066880396284560405\n",
      "-0.9287312968042334 0.006553819169319708\n",
      "-0.928156674453134 0.0064426103624047855\n",
      "-0.9273253207631929 0.00628314226921327\n",
      "-0.9272479915889329 0.006268395297950148\n",
      "-0.926270605288434 0.0060832734056211375\n",
      "-0.9254229122526492 0.005924630609969325\n",
      "-0.9250762187559896 0.005860263437083356\n",
      "-0.9214114790749022 0.005198392006075601\n",
      "-0.9177640507391578 0.004573921969528565\n",
      "-0.9172985630024091 0.004496736153564054\n",
      "-0.9112896952048579 0.0035526844484318463\n",
      "-0.9091511367721421 0.0032406434437721975\n",
      "-0.9034026079726294 0.0024663826552649196\n",
      "-0.9013394443102645 0.00221200583366624\n",
      "-0.9010903885902473 0.0021821554107416294\n",
      "-0.9006432276255458 0.0021290275425957404\n",
      "-0.9002751889997387 0.0020857509141149026\n",
      "-0.8994749412387182 0.0019930617240613863\n",
      "-0.8981987633555839 0.0018492672416088919\n",
      "-0.8972842525999662 0.0017492852211808015\n",
      "-0.8964077288501675 0.0016558731535465897\n",
      "-0.8957239258660441 0.0015846521046735621\n",
      "-0.8931840390980881 0.0013329209213731224\n",
      "-0.8926223416768824 0.0012800010250540923\n",
      "-0.8892747773126126 0.0009855996930602762\n",
      "-0.8887842081837298 0.0009455084210809261\n",
      "-0.8884247278384758 0.0009166305561977698\n",
      "-0.8867946122879025 0.0007910215281170476\n",
      "-0.8851400399451881 0.0006725578743725231\n",
      "-0.880891316408106 0.00041078989552159026\n",
      "-0.87937271812583 0.00033230809892147997\n",
      "-0.8775446635187523 0.0002485586337917972\n",
      "-0.8755708451189232 0.00017145301569292318\n",
      "-0.8707915993619828 4.321952808778851e-05\n",
      "-0.8685560448678977 1.2247210583772913e-05\n",
      "-0.8643350148690097 5.518008338818848e-06\n",
      "-0.8612335742189288 4.4660676367535035e-05\n",
      "-0.8571527552412392 0.00015457165832655917\n",
      "-0.8546747818618503 0.0002544221293141915\n",
      "-0.8518040963103946 0.000402053514377647\n",
      "-0.851664959918669 0.0004100913672900415\n",
      "-0.8512928415299652 0.0004319924550760918\n",
      "-0.8500832937076315 0.0005072585243121605\n",
      "-0.8479140148393618 0.0006580015111916468\n",
      "-0.8442432408577909 0.0009599009703416575\n",
      "-0.8434562824219709 0.0010323990487838594\n",
      "-0.8428518071099351 0.001089971289832426\n",
      "-0.8405397325276998 0.0013254209790844376\n",
      "-0.8380872582453796 0.0016018925008366549\n",
      "-0.8371934918094168 0.0017095742422622636\n",
      "-0.836460007064002 0.0018007340570339954\n",
      "-0.8357818406296116 0.0018872670524583402\n",
      "-0.8278943177475091 0.003055646222840166\n",
      "-0.8275899571624084 0.0031068240161004714\n",
      "-0.8240557203448955 0.0037351042046638895\n",
      "-0.8216817246043513 0.004192757968841471\n",
      "-0.8195179063070686 0.004635271869851411\n",
      "-0.8177724153023145 0.005010109634543575\n",
      "-0.8164174746976263 0.005312206208265919\n",
      "-0.8142707831538196 0.005810966139795478\n",
      "-0.8059256466885918 0.007990353696330305\n",
      "-0.8019371062704233 0.009171271579242048\n",
      "-0.8017446892860718 0.009230579659577784\n",
      "-0.8013944013020895 0.00933910542184131\n",
      "-0.7970035116462422 0.010761172953931442\n",
      "-0.7969361592288569 0.010783884841507413\n",
      "-0.7940070679801889 0.011798269214752967\n",
      "-0.7939610874013523 0.01181461113046833\n",
      "-0.7927153943622698 0.012262303368612575\n",
      "-0.7863302406476715 0.014709689156291101\n",
      "-0.7861940942187877 0.014764698033685061\n",
      "-0.7853926551141446 0.01509093930699445\n",
      "-0.7842452503044544 0.015565262916058191\n",
      "-0.7830628469977039 0.01606304269113221\n",
      "-0.7830500703703882 0.016068471578967385\n",
      "-0.7810265913171464 0.01694185733776075\n",
      "-0.7804914637184772 0.017177372651483544\n",
      "-0.7794706145264236 0.017631964253136758\n",
      "-0.7786030561473591 0.018023792415954507\n",
      "-0.7785080123421619 0.01806702669259122\n",
      "-0.7760643149244848 0.019199678585332797\n",
      "-0.7742617693183347 0.020061342994392238\n",
      "-0.7701685425557581 0.022101941589016728\n",
      "-0.7694329978469616 0.02248114662604093\n",
      "-0.767579793057886 0.02345367735544513\n",
      "-0.7668040118536148 0.023868121684107752\n",
      "-0.7651254659080755 0.024779773447290543\n",
      "-0.7648859088000144 0.024911555041193478\n",
      "-0.7645290573075263 0.0251086393510418\n",
      "-0.7635757843336592 0.025639701110646458\n",
      "-0.7628983612310787 0.026021157043338246\n",
      "-0.7620154870436571 0.026523399895848745\n",
      "-0.757772519383723 0.02901849204912195\n",
      "-0.757481213388365 0.02919478966023204\n",
      "-0.7559160898739841 0.030153112015065996\n",
      "-0.7545433815745475 0.031009139192518163\n",
      "-0.7475224521927215 0.03561853363378694\n",
      "-0.7465585098410124 0.03628210263608596\n",
      "-0.7459387255602143 0.03671273721777908\n",
      "-0.7414940593976542 0.03989326936050083\n",
      "-0.741437548267436 0.039934760949785486\n",
      "-0.7326446150276911 0.04672083355407704\n",
      "-0.7317940448154432 0.04741269325351549\n",
      "-0.7210418906744682 0.056717446888047585\n",
      "-0.7148932233935394 0.06251886853903253\n",
      "-0.7146049351014072 0.0627997170925192\n",
      "-0.7145111054256839 0.062891298112328\n",
      "-0.7114435933815635 0.06593230483927998\n",
      "-0.7097220307267329 0.06767929619455494\n",
      "-0.7074023096331026 0.07007967128176974\n",
      "-0.6970855801038922 0.08141619155850607\n",
      "-0.6947921525509828 0.08408670133949604\n",
      "-0.6921672983538785 0.08721197443837737\n",
      "-0.6893527387235037 0.09064580486856683\n",
      "-0.6864914090679413 0.0942256056207767\n",
      "-0.683553882181652 0.09799530149421107\n",
      "-0.6826721667300948 0.0991456935419845\n",
      "-0.6814176574245694 0.10079763794054841\n",
      "-0.6799889449522871 0.10270078451043177\n",
      "-0.6786446101527721 0.10451287961147472\n",
      "-0.6762104301813785 0.10784713077452608\n",
      "-0.6676235657319698 0.12016618085248865\n",
      "-0.6650152538426515 0.12408372509790837\n",
      "-0.6634195543838055 0.12652150812529012\n",
      "-0.6615698044166243 0.1293868433626009\n",
      "-0.6602325724985059 0.1314848282605226\n",
      "-0.6585565732295964 0.13414601030507908\n",
      "-0.6514602391744215 0.1458104441564064\n",
      "-0.6513647297798824 0.1459718708980515\n",
      "-0.6501186411277327 0.14808889657163296\n",
      "-0.643203992237068 0.16020978903060298\n",
      "-0.6377400748632449 0.17024396943213083\n",
      "-0.6346415843825848 0.17611712080554234\n",
      "-0.6337943205602881 0.17774645596111224\n",
      "-0.6320728254001602 0.1810881243556788\n",
      "-0.62748887609959 0.19019175664576085\n",
      "-0.6240219703212386 0.19727808871487731\n",
      "-0.6233245847798896 0.1987247139764284\n",
      "-0.6231766967743015 0.1990324028879655\n",
      "-0.6230651059382086 0.1992647861969731\n",
      "-0.6224091628088331 0.20063446175332628\n",
      "-0.6197305879878638 0.20629355701675203\n",
      "-0.6191547876292742 0.20752397544620524\n",
      "-0.6177277058047326 0.21059483135573492\n",
      "-0.6161742619036339 0.21397234074418423\n",
      "-0.610931585581834 0.22564122333174286\n",
      "-0.6075027642322661 0.23350138931777653\n",
      "-0.6074498485912232 0.23362412357313334\n",
      "-0.6046500364021239 0.24018064536748676\n",
      "-0.6043485397890436 0.24089403276815513\n",
      "-0.6042614544922738 0.24110035708540936\n",
      "-0.6018530636413688 0.2468539697461398\n",
      "-0.6015866397907907 0.24749611629839705\n",
      "-0.59789365931517 0.2565142202608112\n",
      "-0.5942821248406844 0.26554669232205597\n",
      "-0.5930460516646412 0.26868705835118406\n",
      "-0.5923510056038308 0.27046392656345114\n",
      "-0.5921840133199785 0.27089202470628837\n",
      "-0.5869088932332016 0.2846535471817776\n",
      "-0.5861945933176951 0.2865527584108762\n",
      "-0.5842322713036341 0.291814590949219\n",
      "-0.5830157038360027 0.29510951770965504\n",
      "-0.5825356275426228 0.29641668044693714\n",
      "-0.5801751170304696 0.302901276805625\n",
      "-0.5765552894905575 0.3130318547632427\n",
      "-0.5743885379619535 0.3192048046904517\n",
      "-0.5736081288314454 0.3214482865901416\n",
      "-0.571346630645098 0.32801011911905253\n",
      "-0.5681707745812321 0.3373781217446336\n",
      "-0.5678978300195912 0.3381916404876065\n",
      "-0.5678430046428715 0.3383552099611822\n",
      "-0.5673716969851934 0.3397635615092783\n",
      "-0.5644270697162543 0.34865312890562394\n",
      "-0.5627060674489635 0.3539213154984553\n",
      "-0.5618101849680588 0.3566850469539195\n",
      "-0.5614232963785988 0.35788309967736553\n",
      "-0.554883217383197 0.3785522125334278\n",
      "-0.552944100435542 0.3848332342693606\n",
      "-0.5499902081355448 0.39453700435745737\n",
      "-0.5476835498436403 0.40222936963635664\n",
      "-0.5464829932922062 0.40627314287923594\n",
      "-0.5442888863208757 0.4137347615148996\n",
      "-0.5432285325259116 0.4173739511651098\n",
      "-0.5415709629018419 0.42310634832034916\n",
      "-0.5408864858871654 0.4254890256855376\n",
      "-0.540410372270542 0.42715175731994304\n",
      "-0.5403469032916357 0.42737374331861305\n",
      "-0.5361532640670228 0.44221572788433683\n",
      "-0.5330092383225757 0.45357004336362666\n",
      "-0.5309692632268748 0.46104220355981596\n",
      "-0.5303886676924152 0.4631840362896655\n",
      "-0.5270167036666229 0.4757570708496776\n",
      "-0.5266484757785208 0.47714395826125205\n",
      "-0.522599860098774 0.49257403466058186\n",
      "-0.5225551519951219 0.49274629028409733\n",
      "-0.5218497722484856 0.4954694567942743\n",
      "-0.519211943052208 0.5057434575593702\n",
      "-0.5191534512761686 0.5059728983459365\n",
      "-0.5175374351775714 0.5123398599756712\n",
      "-0.5142325385138304 0.5255295665440983\n",
      "-0.5137875073409865 0.5273230586933103\n",
      "-0.513545998780268 0.5282980808946802\n",
      "-0.5131296752135721 0.5299817354107504\n",
      "-0.512710969327054 0.531678686917808\n",
      "-0.5109534148153929 0.5388419451883917\n",
      "-0.509068390576086 0.546597064443393\n",
      "-0.5080358489539891 0.5508768705913415\n",
      "-0.5078201626621157 0.5517737284126838\n",
      "-0.501827009299058 0.5770911135987797\n",
      "-0.5003856780324394 0.5832948623502393\n",
      "-0.4956710529334396 0.6039020216476434\n",
      "-0.4951368473640885 0.6062675338692287\n",
      "-0.4927853323011153 0.6167546462893548\n",
      "-0.49275816931783756 0.6168764956170353\n",
      "-0.49201446737305354 0.6202189520104672\n",
      "-0.4836105196062168 0.6588419261310104\n",
      "-0.48355336204677535 0.6591100136801915\n",
      "-0.4818595747738603 0.6670878104935191\n",
      "-0.47893149038432603 0.6810320609958078\n",
      "-0.47444715355328304 0.7027655975452836\n",
      "-0.4725600490620754 0.7120492181109345\n",
      "-0.4665942392213387 0.7419393913341935\n",
      "-0.4642325151898896 0.7540011514217717\n",
      "-0.46065042389299604 0.7725455254784569\n",
      "-0.4592407374866623 0.7799264787221886\n",
      "-0.4591626973460954 0.7803364629916602\n",
      "-0.4587769612020025 0.7823650515584469\n",
      "-0.4549960317532731 0.80243639249341\n",
      "-0.45481813689092276 0.8033891640795605\n",
      "-0.45399900018611383 0.8077860852869073\n",
      "-0.44571606928766405 0.8531553463773425\n",
      "-0.44526131687029014 0.8556944316028253\n",
      "-0.4427212122214994 0.8699699850236307\n",
      "-0.4414437666742008 0.8772091200041714\n",
      "-0.4410216297193983 0.8796101457505631\n",
      "-0.44021159586416125 0.8842297452919285\n",
      "-0.4379499237322817 0.8972138047929733\n",
      "-0.4361370301778944 0.907713014873376\n",
      "-0.4345826243394526 0.9167803409682991\n",
      "-0.4307931605691939 0.9391386632744325\n",
      "-0.4243439791720751 0.9780221542788481\n",
      "-0.4226027310826319 0.9887018275231834\n",
      "-0.42161613694457634 0.9947873855345551\n",
      "-0.41988596209208695 1.0055198700417467\n",
      "-0.41809649524846537 1.016701253142603\n",
      "-0.41774285030088665 1.0189207700749578\n",
      "-0.4117355263462621 1.0571197265951335\n",
      "-0.4113619361575829 1.0595263964373087\n",
      "-0.41101783123935287 1.0617463606174542\n",
      "-0.41046704311529614 1.0653061938992021\n",
      "-0.4087174032615364 1.0766673324921106\n",
      "-0.4080213632950276 1.0812094447268774\n",
      "-0.4077573232119336 1.082935825147162\n",
      "-0.40736723541775155 1.0854897196945412\n",
      "-0.39978955482000145 1.1359035545647436\n",
      "-0.39712766332974314 1.153978313970794\n",
      "-0.3962794393680853 1.159778162594294\n",
      "-0.39323960510663847 1.1807240009111453\n",
      "-0.3931861889116923 1.1810943140447385\n",
      "-0.3924837752772019 1.1859711220027909\n",
      "-0.39244628556880623 1.1862317896734145\n",
      "-0.39105152900460793 1.1959569565785213\n",
      "-0.38769738043135793 1.2195632837185655\n",
      "-0.38535528780769135 1.2362312518235492\n",
      "-0.38072653016984326 1.2696225108939192\n",
      "-0.3792451433917341 1.2804360896862004\n",
      "-0.3738393303111449 1.3204241283823652\n",
      "-0.3734097370387994 1.3236376725202375\n",
      "-0.3729388585101441 1.3271661267694437\n",
      "-0.3707127955870413 1.3439330774815226\n",
      "-0.3699635550468685 1.3496085798372328\n",
      "-0.369303248564963 1.354623867963035\n",
      "-0.368952999268942 1.3572892768392821\n",
      "-0.3670433867761347 1.371884169016349\n",
      "-0.3669799773320439 1.3723706190545828\n",
      "-0.3654442711288932 1.3841877519938346\n",
      "-0.3644566753153331 1.3918236889200108\n",
      "-0.3609613004871304 1.419079960672264\n",
      "-0.36004654930870417 1.4262726771210876\n",
      "-0.35992098314885235 1.427261947684112\n",
      "-0.3578552198677427 1.4436044501194005\n",
      "-0.35731669106067687 1.4478857608511495\n",
      "-0.354756383311424 1.4683593125523386\n",
      "-0.3540866852163338 1.4737471498370474\n",
      "-0.349096550456397 1.5143225421990296\n",
      "-0.34634747421525147 1.5370014468030329\n",
      "-0.34378497024055776 1.558351860325525\n",
      "-0.3404196416406271 1.5867028933707237\n",
      "-0.3383589879477815 1.6042389149452967\n",
      "-0.3378085559383639 1.6089458449220568\n",
      "-0.3376117989775531 1.610630718727604\n",
      "-0.3355411954980907 1.6284366758584918\n",
      "-0.3352739137668146 1.6307451394638055\n",
      "-0.3349862817963887 1.6332319262193051\n",
      "-0.3345310947562985 1.637172781214005\n",
      "-0.33442875233064195 1.6380597449853374\n",
      "-0.3338133167620987 1.6434006111582524\n",
      "-0.3287552212039948 1.687761027186107\n",
      "-0.3276465201264118 1.6975962827013158\n",
      "-0.327633055765451 1.6977159734267402\n",
      "-0.32654402004660343 1.7074167199581978\n",
      "-0.32592272883109863 1.712968542035171\n",
      "-0.32476485061089577 1.7233494735336699\n",
      "-0.32396980247278684 1.7305033177423959\n",
      "-0.32254903413838276 1.7433400416108689\n",
      "-0.32202977426240187 1.748048489204444\n",
      "-0.3188077517531045 1.7774681529661898\n",
      "-0.31510749948055716 1.8116912561156953\n",
      "-0.3132277449533565 1.8292577910577432\n",
      "-0.308027684023779 1.8784974441977023\n",
      "-0.3067932451441473 1.8903270611147485\n",
      "-0.30677556291498376 1.8904969053764822\n",
      "-0.3049451760220707 1.9081391501142595\n",
      "-0.30395440934387774 1.917739058837561\n",
      "-0.30137002137140967 1.942947987117845\n",
      "-0.2977848667504477 1.9783247691521337\n",
      "-0.2948617775618263 2.0075226697876403\n",
      "-0.29380456599837057 2.018162098639523\n",
      "-0.29318739090251533 2.0243927472945105\n",
      "-0.2914217078855723 2.04229832447579\n",
      "-0.28983430901471063 2.058498112204863\n",
      "-0.2880050714717346 2.0772869028810064\n",
      "-0.28597432672823286 2.09829855705168\n",
      "-0.28434766494466523 2.1152465335358044\n",
      "-0.2835281127880942 2.1238251607535696\n",
      "-0.28198195465772335 2.140082656531876\n",
      "-0.281651088433589 2.1435741167854117\n",
      "-0.2727956528163631 2.2386868796014436\n",
      "-0.2726955807023106 2.2397803785193444\n",
      "-0.2721297266650784 2.245971512966047\n",
      "-0.2704900352898958 2.2639887129570506\n",
      "-0.2659518590859742 2.314459412304207\n",
      "-0.2646838637608355 2.328722281229118\n",
      "-0.26443134873795016 2.3315711598295907\n",
      "-0.26378574913362196 2.338867714439303\n",
      "-0.26355622186805583 2.3414663025621127\n",
      "-0.2627292536935675 2.350848337686692\n",
      "-0.26200584398906934 2.35908065556728\n",
      "-0.25885240306186796 2.3952433777709383\n",
      "-0.2583409882907328 2.4011509704833545\n",
      "-0.2567285361837659 2.419856302489756\n",
      "-0.25625942944665914 2.425320886960245\n",
      "-0.255809748865226 2.4305688288468072\n",
      "-0.253500245499799 2.4576714830450554\n",
      "-0.25327262700584385 2.4603563184214368\n",
      "-0.25117882886851883 2.4851695731555026\n",
      "-0.2503407737105041 2.4951603795910953\n",
      "-0.2500801890422957 2.4982738594104004\n",
      "-0.24909555551143225 2.5100681775534204\n",
      "-0.24689972009653638 2.5365421494324654\n",
      "-0.23561940096194722 2.676400361274393\n",
      "-0.2338762111524022 2.698610350389738\n",
      "-0.23340449589817402 2.704648868441771\n",
      "-0.2331465652912721 2.707955833601758\n",
      "-0.2308052542511594 2.7381417957234846\n",
      "-0.22916896775884799 2.759419306512311\n",
      "-0.2277217625686483 2.7783641022592658\n",
      "-0.22702909271317795 2.7874738363033007\n",
      "-0.2266819120529251 2.792050186846071\n",
      "-0.22589938949627553 2.8023904738290777\n",
      "-0.22339400198960102 2.8357365950975115\n",
      "-0.2218844411443266 2.856007211525604\n",
      "-0.22101706879474636 2.86771601472127\n",
      "-0.21763593257562452 2.913794466175185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.21601643520851632 2.936114907964704\n",
      "-0.21482291810189036 2.952669700116319\n",
      "-0.2138619533665631 2.9660645131722596\n",
      "-0.21102352244253209 3.0059761127642646\n",
      "-0.20869144038840615 3.0391627126063656\n",
      "-0.20825602812902444 3.0453989380058952\n",
      "-0.20780185836614273 3.0519173954357286\n",
      "-0.20194191698088693 3.1372908490862805\n",
      "-0.20051748488093213 3.158407685309725\n",
      "-0.19999485756051416 3.1661920921550553\n",
      "-0.19584463945328867 3.228719489839688\n",
      "-0.19121690454129636 3.299975417605105\n",
      "-0.1907532743252467 3.3072060677754025\n",
      "-0.19039175227583716 3.3128560868763848\n",
      "-0.1899624246263356 3.319579310327534\n",
      "-0.18739512473707354 3.360092530780653\n",
      "-0.18698528972676431 3.3666095921654775\n",
      "-0.18373507469116102 3.4187881242197378\n",
      "-0.18113884958256787 3.4611126560919256\n",
      "-0.17937794199732138 3.490154460430079\n",
      "-0.1775175647730476 3.521137433791579\n",
      "-0.1745138247141691 3.571830065883337\n",
      "-0.17318041639174186 3.5946038161512948\n",
      "-0.1730496697540147 3.5968459868799765\n",
      "-0.17061225489905563 3.6389472109770202\n",
      "-0.1698676542007398 3.651924402190272\n",
      "-0.16667876290010897 3.7081297470809536\n",
      "-0.16663412975778447 3.7089237611529584\n",
      "-0.16388165747089478 3.758290455082035\n",
      "-0.16134293921286758 3.8045377296306246\n",
      "-0.16034731916147615 3.8228666364919985\n",
      "-0.1554701777973686 3.9142696661660907\n",
      "-0.15512134965426072 3.920912714623039\n",
      "-0.1544262087080721 3.9341940155859514\n",
      "-0.1539574225631617 3.943183221557536\n",
      "-0.15279295834292972 3.965627238502236\n",
      "-0.1516188767038722 3.9884244338004335\n",
      "-0.1482706420078459 4.054385999217995\n",
      "-0.14762107676972103 4.067349157680982\n",
      "-0.14654864303450688 4.088872443662735\n",
      "-0.14508284864670773 4.118537784877769\n",
      "-0.14408219020792434 4.1389563478436955\n",
      "-0.14222510908742825 4.177216261262762\n",
      "-0.12885473270156678 4.467857176836912\n",
      "-0.12737067958085557 4.50190168754186\n",
      "-0.12632826669855102 4.526045767072451\n",
      "-0.12406074435451364 4.57924037942648\n",
      "-0.12247947894484512 4.616897609555609\n",
      "-0.1169004065042405 4.753644377320308\n",
      "-0.11630875617268432 4.768516691945354\n",
      "-0.11607167470248547 4.774496884930057\n",
      "-0.11479492265584978 4.806907831832677\n",
      "-0.11158944547215865 4.889853803291047\n",
      "-0.10419776844347028 5.090341577361408\n",
      "-0.10379673365458353 5.101612546236675\n",
      "-0.10358249658595997 5.107651070796307\n",
      "-0.10251548897725349 5.137908932444713\n",
      "-0.09966063265246916 5.2204060686476\n",
      "-0.09824516007456552 5.262170581134257\n",
      "-0.09706961332727704 5.29730619687294\n",
      "-0.0951856808744187 5.35449331855257\n",
      "-0.09407358907338503 5.38877404137709\n",
      "-0.09284685401211656 5.427052674749528\n",
      "-0.09093529378432064 5.487700662917937\n",
      "-0.08730555768937709 5.6063867288654246\n",
      "-0.08510451776671468 5.680741695531406\n",
      "-0.0801506465236177 5.855259288894152\n",
      "-0.07888958321340844 5.901381029369314\n",
      "-0.07631976640692417 5.997661310693022\n",
      "-0.07567026022164325 6.022501680225787\n",
      "-0.07543517202023664 6.031544500964281\n",
      "-0.07497431934511112 6.049352456746846\n",
      "-0.07132025665411179 6.194498676691586\n",
      "-0.0712619061214772 6.19687556480098\n",
      "-0.0621838204196532 6.592292320004527\n",
      "-0.06203819019250889 6.599091176774213\n",
      "-0.055287633123298985 6.932965208187793\n",
      "-0.05453721695020941 6.972550740576291\n",
      "-0.05243467003212965 7.086411908066587\n",
      "-0.050261274425404734 7.208978269131467\n",
      "-0.045652230365068824 7.487334158461812\n",
      "-0.04523180575881125 7.514101748198834\n",
      "-0.045087233079868616 7.52336361932249\n",
      "-0.04427356831635443 7.576047522424207\n",
      "-0.04215724103713603 7.717723646730618\n",
      "-0.039781448938477126 7.885461501709488\n",
      "-0.03960164309941194 7.898559621976151\n",
      "-0.03257617562948956 8.463009167744369\n",
      "-0.03067377227334589 8.636861753397543\n",
      "-0.02997496963135604 8.703437196023318\n",
      "-0.02827959695965121 8.871619136838884\n",
      "-0.027216226754827044 8.98232082297187\n",
      "-0.025025479790692362 9.224678245496804\n",
      "-0.02258289218899079 9.521235679660549\n",
      "-0.02191536999979249 9.607867008260929\n",
      "-0.018736229700298646 10.060339380014078\n",
      "-0.011889406571026884 11.373049727370466\n",
      "-0.010544020774902574 11.71961089707352\n",
      "-0.009863888801644682 11.91203092463786\n",
      "-0.009293672982454204 12.08386722878808\n",
      "-0.00783568280350333 12.576295005224482\n",
      "-0.007012023682803514 12.89677477410104\n",
      "-0.00690052395097851 12.943027676867725\n",
      "-0.0061579210698161635 13.271571389275236\n",
      "-0.005604410009902194 13.543345650063864\n",
      "-0.0022732828729954413 16.14696669909923\n",
      "-0.0017633563720784728 16.879876946948773\n",
      "-0.00026180848783052824 22.38336338754723\n",
      "0.002802618448648264 15.54296526589659\n",
      "0.009876200162191706 11.908431377321065\n",
      "0.010933640050372118 11.614897481498566\n",
      "0.014844983466974382 10.73230043397808\n",
      "0.015919841871257168 10.53053592684028\n",
      "0.020358756396169797 9.820580257654312\n",
      "0.02811680640249503 8.888294308457313\n",
      "0.029968146499587123 8.704094845492644\n",
      "0.032516459645937745 8.468310686183575\n",
      "0.03542662360800253 8.220606872879955\n",
      "0.03671724327091885 8.117182398363834\n",
      "0.038367435431398755 7.990098013629239\n",
      "0.04049942488909308 7.833741098174609\n",
      "0.04293464508257405 7.664874924058176\n",
      "0.0429568296366627 7.663380810795576\n",
      "0.04407933166547817 7.588766543242127\n",
      "0.04520045709836551 7.516107558657922\n",
      "0.04630837562208168 7.446044766588089\n",
      "0.047502152497545636 7.372394294226124\n",
      "0.048306894628800334 7.323776708597522\n",
      "0.05273546345350244 7.069847774241199\n",
      "0.05464380090438414 6.966895464333344\n",
      "0.05888736403206929 6.750196098423754\n",
      "0.06093572478289566 6.651079401417009\n",
      "0.06481547849790825 6.472075349369398\n",
      "0.06755254647627296 6.352063364264896\n",
      "0.0693855654241522 6.274350813792097\n",
      "0.07877917161550152 5.905453714163623\n",
      "0.07927669118386582 5.887146251032372\n",
      "0.0796363156007307 5.873983335059137\n",
      "0.08305679689098033 5.751635404837313\n",
      "0.08449266350475493 5.701747212273808\n",
      "0.08476681418849119 5.69231693127598\n",
      "0.08728497050608963 5.6070736247937285\n",
      "0.08740180598489222 5.603177493348104\n",
      "0.08844938921207302 5.568470477164282\n",
      "0.08888795681976314 5.554060259254287\n",
      "0.08956943200818435 5.531806691752851\n",
      "0.0901342082695733 5.513489560780984\n",
      "0.09354407349523353 5.405236464277287\n",
      "0.09365209602353053 5.401870681424807\n",
      "0.09400412123939184 5.390928582237116\n",
      "0.09501950418288718 5.359590759682905\n",
      "0.0960885824180957 5.326948500885703\n",
      "0.09617257055793771 5.3243991816061325\n",
      "0.09832367026322175 5.259838698161833\n",
      "0.10768418440239769 4.994105101551049\n",
      "0.10937260524713288 4.948587314435092\n",
      "0.11121584616794111 4.899671869666019\n",
      "0.11147005860928605 4.89298776218009\n",
      "0.11293317166453876 4.854804054573368\n",
      "0.11565104955747696 4.785136145189147\n",
      "0.11831972654693201 4.71826345084252\n",
      "0.1201589652525088 4.673023953049451\n",
      "0.1204806338009985 4.665181159422316\n",
      "0.12247418912355723 4.617024375544194\n",
      "0.12469156256453728 4.564347621046434\n",
      "0.12610814605197151 4.531168859252067\n",
      "0.12647968035263446 4.522526789912983\n",
      "0.12835977184098746 4.4791692234724705\n",
      "0.13005398902059073 4.440622669132593\n",
      "0.13029862270281267 4.435097076424212\n",
      "0.13051466188743888 4.430225710799677\n",
      "0.14007982186335965 4.2220198493484515\n",
      "0.1403481738943837 4.216379233592974\n",
      "0.14622114810714115 4.0954754649580245\n",
      "0.14691240063698707 4.081554979041206\n",
      "0.14701469798735856 4.079500286355925\n",
      "0.14950867503754695 4.0298301860842125\n",
      "0.14963720423241966 4.027292131691493\n",
      "0.15222021444551315 3.9767270233994854\n",
      "0.15265506551945807 3.9682959274798346\n",
      "0.15596152566996846 3.904936847519393\n",
      "0.15693889425603613 3.886456458243003\n",
      "0.1581969146123079 3.8628322224431337\n",
      "0.15825772764507362 3.8616948193826417\n",
      "0.1587720900447398 3.852091390725342\n",
      "0.16051769393691084 3.8197223417677586\n",
      "0.16080589670673184 3.814410834944607\n",
      "0.16093834017900277 3.8119730135192005\n",
      "0.16106940391467184 3.809562492929111\n",
      "0.16525378559064974 3.733581630916407\n",
      "0.16639681020011032 3.7131490743365787\n",
      "0.16797104396102336 3.685228846201484\n",
      "0.16802105855518623 3.6843459430658125\n",
      "0.16846379479765794 3.6765413981530988\n",
      "0.16994728934098702 3.650533871582063\n",
      "0.1703839674859946 3.6429200747600907\n",
      "0.17319910220046064 3.59428350712772\n",
      "0.17374430250098327 3.584952426825959\n",
      "0.1746806335163016 3.5689929069720576\n",
      "0.1757871246084819 3.5502391554729997\n",
      "0.17742658581396498 3.5226606395177633\n",
      "0.18071796683584473 3.4680291911116146\n",
      "0.18306950040820125 3.429583180113075\n",
      "0.18369395656380538 3.419453929440993\n",
      "0.18662632458173567 3.372329099891738\n",
      "0.19095420602939872 3.304070304252776\n",
      "0.19393343922220496 3.257947401447322\n",
      "0.19537204992344548 3.2359209530254858\n",
      "0.19537330282175236 3.235901838647824\n",
      "0.1995260575307236 3.1731915946549996\n",
      "0.20132721645714047 3.1463858001709055\n",
      "0.20185418755864037 3.138587222374562\n",
      "0.20448288563789685 3.09997898490421\n",
      "0.2102598585995883 3.0168037674572585\n",
      "0.2130471239937033 2.9774686353266677\n",
      "0.21507688206884468 2.9491395274632133\n",
      "0.215911942089271 2.9375607016309337\n",
      "0.21978633860868646 2.8844077468932814\n",
      "0.22347272047661093 2.8346832616510396\n",
      "0.22572637193010148 2.8046815190528473\n",
      "0.22593168556431653 2.801963011499392\n",
      "0.2270175536530974 2.7876258266924903\n",
      "0.22752969899624453 2.7808872970404925\n",
      "0.22759117428652909 2.7800794487969163\n",
      "0.2283186254692131 2.7705363803067327\n",
      "0.23155096298458933 2.728494614240234\n",
      "0.23328999958027463 2.706116392408996\n",
      "0.23524356529206236 2.681174983776032\n",
      "0.23911108074853726 2.632402650242875\n",
      "0.24050450375464316 2.6150239939145195\n",
      "0.2425837190959954 2.5892794335391964\n",
      "0.24287239374239378 2.5857226727465052\n",
      "0.24419992715564653 2.5694208039269117\n",
      "0.24426047339734502 2.5686794403128985\n",
      "0.24654492629335478 2.540842090376418\n",
      "0.24710001453667418 2.5341174341564874\n",
      "0.2503013162779584 2.4956316072009015\n",
      "0.2510439031257099 2.486775784248998\n",
      "0.2518741359523393 2.4769062726387454\n",
      "0.25453826509788113 2.4454588819423577\n",
      "0.2546592651643049 2.4440385999570946\n",
      "0.25934837303371805 2.3895256781085217\n",
      "0.25998329062484515 2.3822225920334477\n",
      "0.2622755103003074 2.3560091320537566\n",
      "0.26371684603372403 2.339647552503328\n",
      "0.26441352546739605 2.331772349234485\n",
      "0.264928614473416 2.32596369905762\n",
      "0.2658818684119928 2.315244842039165\n",
      "0.2674049715530804 2.2982012876028386\n",
      "0.268266817341009 2.2886021355052417\n",
      "0.2697899254198861 2.2717166945525005\n",
      "0.271471127650287 2.25319452324439\n",
      "0.27311162921431364 2.235236951843816\n",
      "0.27696202368511424 2.1935338331937215\n",
      "0.2770202877428307 2.1929075252565546\n",
      "0.27862416236435217 2.1757216786226987\n",
      "0.2789456884334147 2.1722891782150624\n",
      "0.2796428067757972 2.164861505154649\n",
      "0.28481737845285493 2.1103418796323314\n",
      "0.2859308832277183 2.098749828623797\n",
      "0.28732166584707586 2.0843398853036974\n",
      "0.2882085758736461 2.07519019402161\n",
      "0.2898555029598435 2.0582811822853504\n",
      "0.2944203218137511 2.011960188863356\n",
      "0.2963681622972796 1.9924357955652459\n",
      "0.2989479694548831 1.9667956587936293\n",
      "0.2994259257479108 1.9620725275264745\n",
      "0.30184141506546336 1.9383316905753543\n",
      "0.3025660076479568 1.9312516960519588\n",
      "0.3084195540784276 1.8747535180203407\n",
      "0.30907048735803966 1.8685465639012506\n",
      "0.3112727932446886 1.8476576967155196\n",
      "0.31178312325031876 1.842841590840189\n",
      "0.31219589895782063 1.8389528079033137\n",
      "0.31315714702685726 1.8299199306712257\n",
      "0.3161719021553935 1.8017985357701698\n",
      "0.317712579086036 1.7875483202262448\n",
      "0.3194910524360177 1.7711996514275994\n",
      "0.32418237493158597 1.728588523998266\n",
      "0.3280309871664362 1.6941811043728006\n",
      "0.32846177883957295 1.6903602111168057\n",
      "0.33029404584547084 1.6741770217254235\n",
      "0.33224982099019784 1.6570239365625916\n",
      "0.33297621547700373 1.6506847530427475\n",
      "0.3354702186616856 1.6290494663491173\n",
      "0.3372630274856583 1.6136203572006913\n",
      "0.3379424604443646 1.6077998953781203\n",
      "0.3413536764324152 1.5787985056158087\n",
      "0.34219669950713016 1.5716878815574054\n",
      "0.34367831035920005 1.55924496647834\n",
      "0.3468413633604517 1.5329098521700306\n",
      "0.34751199557175827 1.5273661090501274\n",
      "0.3485720492657485 1.518631521603418\n",
      "0.34962953266858277 1.5099525389190969\n",
      "0.35011136479163807 1.5060094165259015\n",
      "0.35182443467695235 1.4920477971903399\n",
      "0.3553417109009891 1.4636613372029246\n",
      "0.35964781849270855 1.4294156897475971\n",
      "0.3601235190181604 1.4256665044554704\n",
      "0.36169019375416167 1.4133664136873774\n",
      "0.36422992929647613 1.3935808933615266\n",
      "0.3647759264018666 1.3893521605277317\n",
      "0.36521921801590773 1.3859253121706891\n",
      "0.37021683234329483 1.3476881871182098\n",
      "0.37112828940033005 1.3407926972817352\n",
      "0.3712937324982515 1.3395436335969757\n",
      "0.37175202901939275 1.3360877020704989\n",
      "0.3725167887940455 1.3303342489167975\n",
      "0.37418402656769945 1.317849472763068\n",
      "0.37736427582453036 1.2942550813390943\n",
      "0.38184624667063094 1.2614899894906355\n",
      "0.3843640023051551 1.2433318583296986\n",
      "0.3854417966494812 1.2356128847558363\n",
      "0.38591854523875346 1.232208809897723\n",
      "0.3873835492271578 1.2217879003081105\n",
      "0.3912582049845519 1.1945125083984565\n",
      "0.3920974400111996 1.18865917328376\n",
      "0.39213543128307426 1.1883946556948848\n",
      "0.4094257673389814 1.0720579188190702\n",
      "0.4100294550072692 1.068140079444556\n",
      "0.4103686423425059 1.065943013192788\n",
      "0.41071478324384625 1.0637040232251573\n",
      "0.4114189428896915 1.0591589221874702\n",
      "0.4136915637248517 1.0445785661183449\n",
      "0.4145935227327362 1.0388293045690755\n",
      "0.41502150872544763 1.036108653990854\n",
      "0.41546950115864045 1.0332659309168895\n",
      "0.4161373259356358 1.0290379525314062\n",
      "0.42052650167920946 1.0015375531450637\n",
      "0.4249208822766948 0.9745008818428749\n",
      "0.4264656783755445 0.9651135609208945\n",
      "0.4268687760877481 0.9626740207528753\n",
      "0.42934160749259775 0.9477985919629565\n",
      "0.4334531490985021 0.9234067516824126\n",
      "0.43392830150174255 0.9206152389031715\n",
      "0.43570237254985233 0.9102424409784801\n",
      "0.43613087223388547 0.9077488172754725\n",
      "0.4362126109127118 0.9072736639383205\n",
      "0.4369610484928841 0.9029306650709633\n",
      "0.4377065485031242 0.8986185481735762\n",
      "0.4404784816239202 0.8827059162319472\n",
      "0.4409197756339993 0.8801901275437\n",
      "0.4427917644167525 0.8695713423179345\n",
      "0.4437252527392488 0.8643083276174137\n",
      "0.44665754103123567 0.8479147196491461\n",
      "0.45698149107896 0.7918539628595902\n",
      "0.4588606778085056 0.7819244854865438\n",
      "0.45983472221752275 0.776810707834711\n",
      "0.4606318263914311 0.7726425937061758\n",
      "0.4620031035693484 0.7655072231842007\n",
      "0.46646615263509106 0.7425902081163172\n",
      "0.4683915167100978 0.7328476383832598\n",
      "0.4805712407738896 0.6731992539493197\n",
      "0.48369118926807153 0.658463684153779\n",
      "0.4845761703204876 0.6543238050655996\n",
      "0.48483503686348817 0.6531161684231381\n",
      "0.48500750995623876 0.6523123993210008\n",
      "0.48564521691649176 0.6493463077761091\n",
      "0.4858908046036854 0.6482064625640998\n",
      "0.4893052187270366 0.6324985701380745\n",
      "0.49420361639846155 0.6104149627085437\n",
      "0.4986062030542717 0.5910160160739965\n",
      "0.500312383675724 0.5836115331926854\n",
      "0.504530503533192 0.5655753793223713\n",
      "0.5093001890734048 0.5456393810372712\n",
      "0.5126998888762337 0.5317236442292723\n",
      "0.5135810921445243 0.5281563256202303\n",
      "0.5201157843173336 0.5022069993486918\n",
      "0.5221138468105979 0.49444878632686534\n",
      "0.5261689953320778 0.47895397643369836\n",
      "0.5276802886798131 0.4732646709730768\n",
      "0.5303043325454844 0.46349571209253243\n",
      "0.5310818958348702 0.46062747958051553\n",
      "0.5422837200742594 0.4206348902574886\n",
      "0.5455466350317084 0.409446166723819\n",
      "0.5461586081914414 0.40737048066560644\n",
      "0.546574466932986 0.4059640676098541\n",
      "0.5466231764389204 0.40579955103648024\n",
      "0.5475208304445627 0.40277583709790915\n",
      "0.5479430398469594 0.4013589576256241\n",
      "0.5498119200566094 0.39512796732225963\n",
      "0.5499554767170685 0.3946520797131756\n",
      "0.5507656389039157 0.39197371873465736\n",
      "0.5555638372910645 0.376364262942\n",
      "0.5589728041283386 0.36553509062547335\n",
      "0.5607806310808159 0.3598792509731057\n",
      "0.5613352703801999 0.35815606618269324\n",
      "0.5632216395367231 0.3523374433225608\n",
      "0.5637904218191014 0.35059571317131877\n",
      "0.5690578408624936 0.3347433999582873\n",
      "0.5745209866222261 0.3188251089502244\n",
      "0.5773769532627715 0.31071239570744014\n",
      "0.5791600973298348 0.30571908355649885\n",
      "0.5867646937334576 0.28503625923620357\n",
      "0.5869172067115 0.2846314934910631\n",
      "0.5876253419974997 0.28275723944757736\n",
      "0.588618555319911 0.28014262128345035\n",
      "0.5903936074810268 0.27551084904395373\n",
      "0.5914906812133514 0.2726743626120916\n",
      "0.5919330325693264 0.2715362997846082\n",
      "0.5924951854192624 0.2700946800326043\n",
      "0.5953377884465898 0.262884494328763\n",
      "0.5961270197141983 0.26090607642504926\n",
      "0.5965196856689357 0.25992553272061003\n",
      "0.6004161935356973 0.2503306068160269\n",
      "0.6005535202247325 0.2499969056412168\n",
      "0.6005967006002515 0.24989204058462428\n",
      "0.6028866116526583 0.24437356275645047\n",
      "0.6049154677002571 0.23955378083854892\n",
      "0.6071231746380792 0.23438278979312424\n",
      "0.6089635735381929 0.23013036368331036\n",
      "0.6090734069371762 0.22987824834436887\n",
      "0.6095669003973556 0.22874777482792574\n",
      "0.6114556260023352 0.22445593570687633\n",
      "0.6118402268124132 0.22358872264996912\n",
      "0.6138662603751597 0.21905774931694588\n",
      "0.61668276569971 0.21286274806587818\n",
      "0.619460315329009 0.20687048292382007\n",
      "0.6211007184206399 0.203385572915639\n",
      "0.6222674233196046 0.2009312602965873\n",
      "0.6241003775899898 0.19711588970041638\n",
      "0.6244997255951175 0.1962911643591726\n",
      "0.6278911658943194 0.18938075849033426\n",
      "0.6280926071579165 0.18897553842725373\n",
      "0.630339823519154 0.18449453339897162\n",
      "0.6320069772751633 0.18121677672428574\n",
      "0.6327843364377146 0.17970190660776847\n",
      "0.6349351143596309 0.1755549950327094\n",
      "0.637303852228317 0.17106274980467043\n",
      "0.638788577847327 0.16828671324509994\n",
      "0.6401016403349398 0.16585695117880167\n",
      "0.646286564032335 0.15472752973548204\n",
      "0.6471395244353486 0.15323303405650365\n",
      "0.6493585820081933 0.149390187529304\n",
      "0.6499894677787699 0.14830951774364093\n",
      "0.6510544719552602 0.14649708004302398\n",
      "0.6536894572509679 0.14207639808802117\n",
      "0.6572690505047105 0.13621447077240423\n",
      "0.660196148227318 0.13154228768805104\n",
      "0.6662514746679526 0.12221663335793087\n",
      "0.6669545929519265 0.12116303072131267\n",
      "0.6671153820070723 0.12092293902833126\n",
      "0.669656701931628 0.11716982718084473\n",
      "0.6703655316204462 0.11613688931746914\n",
      "0.6721538131090754 0.11355763628223295\n",
      "0.6733244559431926 0.11188980944268946\n",
      "0.6757658489419254 0.10846354015327826\n",
      "0.6853088036609887 0.09573163352429721\n",
      "0.685634866223835 0.09531484303999324\n",
      "0.6888585081752172 0.09125769610547563\n",
      "0.6902679367014981 0.08951978337426601\n",
      "0.6933133558532802 0.08583833121388022\n",
      "0.694061291810331 0.08494947943919791\n",
      "0.6954574714455886 0.08330624649266245\n",
      "0.6969160341818024 0.08161170872027852\n",
      "0.7055486722107545 0.07203642527853875\n",
      "0.7063408248448251 0.07119597850864567\n",
      "0.7145618397791342 0.06284176908102151\n",
      "0.720048358083244 0.05763069805821803\n",
      "0.7202577940364505 0.05743741762036483\n",
      "0.7207754526275565 0.05696145079005376\n",
      "0.7217161618360592 0.05610290359136878\n",
      "0.7239118927348109 0.05413086105180329\n",
      "0.7248914303632332 0.05326542131519905\n",
      "0.7257505980735701 0.05251354625308315\n",
      "0.7265732349732827 0.05179992953543536\n",
      "0.7286028102691879 0.050065467096016954\n",
      "0.7317855637312196 0.0474196238396466\n",
      "0.734732095342437 0.04504966897178722\n",
      "0.7363811418141699 0.043756208218944055\n",
      "0.7380515021750591 0.042469828712316825\n",
      "0.7385679130163303 0.04207694334622932\n",
      "0.7398841799574092 0.0410857441220603\n",
      "0.7399377663739493 0.0410457013255184\n",
      "0.7404522772098328 0.04066245918641884\n",
      "0.7408242824937377 0.04038674946722024\n",
      "0.7418271746662228 0.039649230979473\n",
      "0.7423062881882057 0.03929985588747066\n",
      "0.7452013913385016 0.03722912283198204\n",
      "0.7515680947177303 0.032914888391857446\n",
      "0.7526399821339138 0.03222032705097406\n",
      "0.7528132808088497 0.032108880782881884\n",
      "0.7547001406515756 0.03091064607165872\n",
      "0.7553444336653734 0.030507829958512707\n",
      "0.762817602218796 0.02606685855966507\n",
      "0.7636934370404038 0.02557379609615743\n",
      "0.7636965669040272 0.02557204424611027\n",
      "0.7641816669966854 0.025301394859700392\n",
      "0.7645660114817103 0.025088186767807035\n",
      "0.7646753365854988 0.02502773849620889\n",
      "0.7649520407814843 0.024875133582805593\n",
      "0.7660284896110592 0.024286778546994816\n",
      "0.7678725653664507 0.02329839743748599\n",
      "0.7686308435194058 0.022899085436308295\n",
      "0.7767452280444751 0.018879989510792068\n",
      "0.7784580776150682 0.018089765835528175\n",
      "0.781736163104735 0.01663250380532327\n",
      "0.7836514527084426 0.015814101312802493\n",
      "0.7847922286671338 0.01533808066007989\n",
      "0.7848511360381354 0.01531373021953819\n",
      "0.7869553707503443 0.01445864210684504\n",
      "0.7886727388817123 0.013781845678713012\n",
      "0.7942766521436357 0.011702718045377397\n",
      "0.7979972303512031 0.01042925887014701\n",
      "0.7983212627854466 0.010322311225069958\n",
      "0.8042643628361015 0.008471046752470807\n",
      "0.8088162867665252 0.007191441627173008\n",
      "0.8123813444662091 0.006270617984692927\n",
      "0.8140213595522947 0.005870530582875094\n",
      "0.8156476706908002 0.005488209644346817\n",
      "0.817762413342924 0.0050123038971291835\n",
      "0.819623661171275 0.004613076486085116\n",
      "0.8213952396725062 0.004249947907915284\n",
      "0.8217659176560765 0.0041760315479531405\n",
      "0.8223212109719262 0.004066629587560421\n",
      "0.8256252098215271 0.003448329744890442\n",
      "0.828850116984367 0.00289791769913034\n",
      "0.8317263954793466 0.002450364032889065\n",
      "0.8335629993698412 0.002185604784872869\n",
      "0.8348571373336566 0.0020087565814626658\n",
      "0.8382762390786622 0.0015796001458026883\n",
      "0.839039887369246 0.0014912056926547775\n",
      "0.8409611270252606 0.001280697756233774\n",
      "0.841819546947904 0.0011920947457897096\n",
      "0.8453229576274839 0.0008649256915657723\n",
      "0.8478483707494306 0.0006628809319913537\n",
      "0.8488116750020487 0.0005931557178341773\n",
      "0.8511137318380408 0.0004427439489191146\n",
      "0.8546374536708161 0.00025612043734032875\n",
      "0.8557200798872964 0.0002092123593089299\n",
      "0.8557611816264874 0.00020752708727110386\n",
      "0.8606680261594242 5.5897852753423475e-05\n",
      "0.8624000794773359 2.5494408548185157e-05\n",
      "0.8625265070801225 2.374031775389704e-05\n",
      "0.878332010933659 0.00028318454726602803\n",
      "0.8788859640609843 0.000308858231453394\n",
      "0.8794498174566139 0.0003360986658285375\n",
      "0.879490019419823 0.0003380834412447583\n",
      "0.8811789449547314 0.00042655762588741003\n",
      "0.882606916427517 0.0005090641402433842\n",
      "0.8845928882592098 0.0006354030535131446\n",
      "0.8862455656421757 0.0007506972449434463\n",
      "0.8896093955224502 0.0010133968303042216\n",
      "0.8917096365264148 0.0011961566039335688\n",
      "0.8946035636270688 0.0014711127550036905\n",
      "0.8959103991057544 0.0016039299817053644\n",
      "0.8968014086929474 0.0016975343898431646\n",
      "0.8981538556754096 0.001844297689890355\n",
      "0.8993395218266853 0.0019775683522352838\n",
      "0.9011342322624016 0.0021873968065363983\n",
      "0.9030624811284789 0.002423579164509354\n",
      "0.9030718855881914 0.0024247580896431717\n",
      "0.9038449945397582 0.00252256447350216\n",
      "0.9054989522083197 0.002737683704900016\n",
      "0.9056728532697418 0.002760764176312562\n",
      "0.9076477195976158 0.003028986203155395\n",
      "0.9098314155066434 0.003338515033886175\n",
      "0.9113122764005368 0.0035560473952984395\n",
      "0.9115636118538628 0.0035935734658824555\n",
      "0.9116721629404627 0.00360983499296198\n",
      "0.9117780006230563 0.003625721435753835\n",
      "0.9145157026145758 0.004047348647333318\n",
      "0.9175268070356801 0.004534511230437829\n",
      "0.9211762663795904 0.005157079926563941\n",
      "0.9303375464148569 0.006868925714954736\n",
      "0.9314199991453447 0.007084771727936622\n",
      "0.9360937479334985 0.00804843685022384\n",
      "0.9404007622257176 0.00898087584510627\n",
      "0.9420025338179523 0.00933821011423656\n",
      "0.9434762334976703 0.009671932551309692\n",
      "0.9466195983245747 0.010399373309324349\n",
      "0.9472700460051295 0.010552519045351436\n",
      "0.9527721108857488 0.011882993743507153\n",
      "0.953057651550447 0.011953722006672949\n",
      "0.9573078553106082 0.013025569826948746\n",
      "0.9574852997498045 0.013071086472944235\n",
      "0.9585530394606165 0.013346253710959634\n",
      "0.9620939529711914 0.014274276165215746\n",
      "0.9633551962163862 0.01461048726666087\n",
      "0.9707722878254457 0.016645778526475318\n",
      "0.9740666834466365 0.017580577217771596\n",
      "0.9792184497679182 0.019078704744572895\n",
      "0.9802109911298529 0.019372284160606337\n",
      "0.9806016378749247 0.019488262100239295\n",
      "0.9810505397500717 0.019621833733342132\n",
      "0.9840015642274929 0.020507776148642003\n",
      "0.98827027186536 0.021812911047697923\n",
      "0.9886217402598401 0.021921586745220635\n",
      "0.9910157348471242 0.022666641495663283\n",
      "0.9911928070090803 0.022722080553522195\n",
      "0.9949691347252534 0.023915049496871992\n",
      "0.9969040849883399 0.024534057301401943\n",
      "0.9972295144072054 0.024638671082411145\n",
      "0.9976229353971329 0.02476533466586868\n",
      "0.9990659726658317 0.025231723059649564\n"
     ]
    }
   ],
   "source": [
    "K = 1\n",
    "T = 2\n",
    "Noise_Alloc = [0,2]\n",
    "sigma = 1\n",
    "\n",
    "N = 2\n",
    "\n",
    "j_array = np.array(range(K+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K+T)))\n",
    "\n",
    "i_array = np.array(range(N))\n",
    "beta_array = np.cos(i_array*2*math.pi/(N-1)/2) # np.cos(i_array*2*math.pi/N/2)\n",
    "# print(\"z_array: \",z_array,'\\n')\n",
    "\n",
    "N = 1000\n",
    "z_array = np.random.uniform(-1,1,N) #np.cos(i_array[1:]*2*math.pi/(K+T)/2) # np.cos(i_array*2*math.pi/N/2)\n",
    "z_array = np.sort(z_array)\n",
    "MIS_array = np.zeros((N))\n",
    "MIS_LCC_array = np.zeros((N))\n",
    "# print(z_array)\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "B = [0.5, 1, 1.5, 2]\n",
    "\n",
    "z_array_0 = []\n",
    "z_array_1 = []\n",
    "z_array_2 = []\n",
    "z_array_3 = []\n",
    "z_array_4 = []\n",
    "\n",
    "for j in range(len(z_array)):\n",
    "    MIS_array[j] = MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_array[j]], 1,sigma)\n",
    "    MIS_LCC_array[j] = MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_array[j]], 1,sigma, _is_LCC=True)\n",
    "    \n",
    "    if MIS_array[j] < B[0]:\n",
    "        z_array_0.append(z_array[j])\n",
    "    elif MIS_array[j] < B[1]:\n",
    "        z_array_1.append(z_array[j])\n",
    "    elif MIS_array[j] < B[2]:\n",
    "        z_array_2.append(z_array[j])\n",
    "    elif MIS_array[j] < B[3]:\n",
    "        z_array_3.append(z_array[j])\n",
    "    else:\n",
    "        z_array_4.append(z_array[j])\n",
    "#     print('(beta index, MIS) = ',j,',',MIS_array[j])\n",
    "#     print()\n",
    "\n",
    "\n",
    "\n",
    "print(len(z_array_0),len(z_array_1),len(z_array_2),len(z_array_3),len(z_array_4))\n",
    "\n",
    "\n",
    "plt.plot(z_array, MIS_array, label='Mutual Information Security, BACC')\n",
    "plt.plot(z_array, MIS_LCC_array, label='Mutual Information Security, LCC')\n",
    "plt.plot(alpha_array[Signal_Alloc],0*np.ones(len(Signal_Alloc)),'g*',label='alpha_i, for X')\n",
    "plt.plot(alpha_array[Noise_Alloc],0*np.ones(len(Noise_Alloc)),'r*',label='alpha_i, for N')\n",
    "# plt.plot(beta_array,0*np.ones(len(beta_array)),'b.',label='beta_i')\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('MIS')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "    \n",
    "for i in range(len(z_array)):\n",
    "    print(z_array[i],MIS_array[i])\n",
    "    \n",
    "# print(alpha_array[Signal_Alloc])\n",
    "# print(alpha_array[Noise_Alloc])\n",
    "# print(alpha_array)\n",
    "\n",
    "# plt.plot((2*j_array[Signal_Alloc]+1)/(K+T),alpha_array[Signal_Alloc],'g*',label='alpha_i, for X')\n",
    "# plt.plot((2*j_array[Noise_Alloc]+1)/(K+T),alpha_array[Noise_Alloc],'r*',label='alpha_i, for N')\n",
    "# plt.plot(2*i_array[1:]/(K+T), z_array,'b.',label='beta_i')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1 2\n",
      "##########################################\n",
      "###### 0 -th Trial!! ###########\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 958/10000 (9.58%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3025 \n",
      "Accuracy: 794/10000 (7.94%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 974/10000 (9.74%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3024 \n",
      "Accuracy: 1135/10000 (11.35%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 974/10000 (9.74%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3027 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3025 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3024 \n",
      "Accuracy: 974/10000 (9.74%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3025 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 974/10000 (9.74%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3025 \n",
      "Accuracy: 1030/10000 (10.30%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1028/10000 (10.28%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3024 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3025 \n",
      "Accuracy: 974/10000 (9.74%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3011 \n",
      "Accuracy: 1048/10000 (10.48%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3006 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.2846 \n",
      "Accuracy: 4591/10000 (45.91%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.1880 \n",
      "Accuracy: 6945/10000 (69.45%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.9889 \n",
      "Accuracy: 8943/10000 (89.43%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.9235 \n",
      "Accuracy: 9103/10000 (91.03%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.9263 \n",
      "Accuracy: 9118/10000 (91.18%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.9325 \n",
      "Accuracy: 9095/10000 (90.95%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.9255 \n",
      "Accuracy: 9203/10000 (92.03%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.9287 \n",
      "Accuracy: 9197/10000 (91.97%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.9232 \n",
      "Accuracy: 9227/10000 (92.27%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.9331 \n",
      "Accuracy: 9181/10000 (91.81%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.9318 \n",
      "Accuracy: 9214/10000 (92.14%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "N = 8\n",
    "K = 4\n",
    "G = 4\n",
    "\n",
    "size_per_group = int(60000/G)\n",
    "\n",
    "X_group = np.reshape(encoding_input_array_np, (G,size_per_group,28*28))\n",
    "y_group = np.reshape(encoding_label_array_np, (G,size_per_group,args.num_classes)) \n",
    "\n",
    "\n",
    "N_i = int(N/G) # = 2\n",
    "K_i = int(K/G) # = 2\n",
    "T = 2\n",
    "sigma = 1\n",
    "Noise_Alloc = [0,2]\n",
    "m = N_i # number of selected workers (if there is no straggler, m=N_i)\n",
    "\n",
    "print(N_i,K_i,T)\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K_i+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K_i+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K_i+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "z_array = np.array([-0.521, 0.521])\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 30\n",
    "\n",
    "loss_test_arr_K4_G4_N8 = np.zeros((N_trials,N_epochs))\n",
    "acc_test_arr_K4_G4_N8  = np.zeros((N_trials,N_epochs))\n",
    "\n",
    "for trial_idx in range(N_trials):\n",
    "    \n",
    "    print('##########################################')\n",
    "    print('######',trial_idx,'-th Trial!! ###########')\n",
    "    \n",
    "    net_glob = CNNMnist2(args=args)\n",
    "    net_glob.cuda()\n",
    "    net_glob.train()\n",
    "    \n",
    "    # copy weights\n",
    "    w_glob = net_glob.state_dict()\n",
    "    \n",
    "    X_tilde = np.empty((N,Size_submatrices,28*28))\n",
    "    y_tilde = np.empty((N,Size_submatrices,10))\n",
    "    \n",
    "    for G_idx in range(G):\n",
    "        \n",
    "        _Noise_label = np.ones((size_per_group*T,10)) * 0.1\n",
    "        \n",
    "        X_tilde_tmp,a,b = BACC_Enc_Data_v3(X_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde_tmp,a,b = BACC_Enc_Data_v3(y_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "        \n",
    "        stt_pos = G_idx * N_i\n",
    "        end_pos = (G_idx+1) * N_i\n",
    "        \n",
    "        X_tilde[stt_pos:end_pos,:,:] = X_tilde_tmp\n",
    "        y_tilde[stt_pos:end_pos,:,:] = y_tilde_tmp\n",
    "        \n",
    "   \n",
    "\n",
    "    for iter in range(N_epochs): #args.epochs\n",
    "        \n",
    "        w_group_array = []\n",
    "        for G_idx in range(G):\n",
    "            w_locals, loss_locals = [], []\n",
    "            idxs_users = np.random.choice(range(N_i), m, replace=False)\n",
    "            idxs_users = np.sort(idxs_users)\n",
    "            print('selected users:',idxs_users)\n",
    "\n",
    "            coded_net = BACC_Enc_Model_withNoise_v3(net_glob.cuda(), N_i, K_i, T, 0.01, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "            dec_z_array = []\n",
    "            for idx in idxs_users: #for idx in range(N):\n",
    "        #         print(idx)\n",
    "                local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[G_idx*N_i+idx,:,:], label=y_tilde[G_idx*N_i+idx,:,:])\n",
    "                w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "#                 w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                w_locals.append(copy.deepcopy(w))\n",
    "                loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "            # update global weights\n",
    "            #w_glob = FedAvg(w_locals)\n",
    "            w_group = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "            \n",
    "            w_group_array.append(copy.deepcopy(w_group))\n",
    "        \n",
    "        w_glob = copy.deepcopy(w_group_array[0])\n",
    "        for k in w_glob.keys():\n",
    "            for G_idx in range(1,G):\n",
    "                w_glob[k] += w_group_array[G_idx][k]\n",
    "            w_glob[k] = torch.div(w_glob[k], len(w_group_array))\n",
    "        \n",
    "        # copy weight to net_glob\n",
    "        net_glob.load_state_dict(w_glob)\n",
    "\n",
    "        # print loss\n",
    "    #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "    #     loss_train_arr.append(loss_train)\n",
    "\n",
    "        acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "        acc_test_arr_K4_G4_N8[trial_idx][iter] = acc_test\n",
    "        loss_test_arr_K4_G4_N8[trial_idx][iter] = loss_test\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test_arr_K4_G2_N4 = acc_test_arr_G2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEGCAYAAABhMDI9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOydd3wcxdn4v3NdJ526LBdZknsvuNtgI7AxBLDpJRgwhJgfIdhvCv1NQgoBkkCICW8KnQDBNqYXg8FYBAhY2JZ7tyQXdaucrred3x97OktWl9W9X3s/uze7M/PM3mmemXlmnhFSSjQ0NDQ0NGrRdbcAGhoaGho9C00xaGhoaGjUQ1MMGhoaGhr10BSDhoaGhkY9NMWgoaGhoVEPQ3cLcDokJyfLzMzMdsV1uVxER0d3rEDdTF8rU18rD/S9MvW18kDfK1Nj5dmyZcsJKWVKk5GklJ1yAC8AZcCuOmGJwKfAwfA5IRwugKeAQ8AOYEpr8pg6dapsLxs3bmx33J5KXytTXyuPlH2vTH2tPFL2vTI1Vh5gs2ymbu3MoaSXgItOCbsf2CClHAFsCH8G+B4wInzcDvy9E+XS0NDQ0GiGTlMMUsr/AJWnBF8GvBy+fhm4vE74v8LK7FsgXggxoLNk09DQ0NBoGiE7ceWzECIT+EBKOT78uVpKGV/nfpWUMkEI8QHwmJTyq3D4BuA+KeXmRtK8HbVXQWpq6tRVq1a1Szan00lMTEy74vZU+lqZ+lp5oO+Vqa+VB/pemRorz3nnnbdFSjmtqTg9xfgsGglrVGNJKZ8BngGYNm2azMrKaleG2dnZtDduT6WvlamvlQf6Xpn6Wnmg75WpPeXp6umqpbVDROFzWTj8ODC4znNpQFEXy6ahoaGhQdcrhveApeHrpcC7dcJvFiqzALuUsriLZdPQ0NDQoBOHkoQQrwNZQLIQ4jjwEPAYsEYIcRtwFLgm/PhHwMWo01XdwK2dJZeGhoaGRvN0mmKQUn6/iVvzG3lWAj/uLFk0NM40pJRI5MkzEvX/KeGNPFc7lz0kQyhSiRyNfS7yF3Gw6iA6oUMIgUCgEzp06ECghofDauUCVY6657r36iKEan4UnHKuEy7EyfRr8xYiHM5JuRSpIJHqOVze2jApT15XBasocZXUy+vUfGupfRdSShROXodkKJJmbXhtWHPvNaSECCpBAkqAgBKIXDd2zhqcxfjk8R3wa2lITzE+a/RwpJQElACeoAdfyIcv6MMb8uIL+fAG1bMn6MET9OANefEE1LM36K0X7g16UaQS+YOuPQP1wxCUlpfyXvZ7kT+EoBIkJEOR69o/kpAMoUOHXqdHL/QYdUb0Oj0GnQG90KPX6TEKNay2oqqbT20lUvczQEAJ4A/58YV8Dc51j5ASUvPQGTEIAwbdyUOv02MQBvWezoDD7uDV9a+iF6osBmFAJ3QR2fRCLQMQeb+177tunt6gF3/IjzfkrVe51a1ou4z3uj7LTmdtdwvQMv2s/TTFoNF1eINe3jv8Hm8ceIMSV0mkImpPpaMXeqIMUVgMFix6CxaDBZ3QRSoyoEFrFdSWpNvvxl5tr1fRGnQGTDoTVoP1ZOUr9Eik2tqSwUirKyiDBEKBkwolfO/UfOq2FuvKY9KbMOlNmHVmTHo1zwRzghqmV8Nqy1M3z4jCOiUsqASRSHxBX72WYlAGUZSTn0MyBIBZb8asN2MxWDAbzMSZ4yJhZoMZi96CUa8qI6Ceoq39p/4XjSriiJJsRDnrhC6iqGqvG/u8Z/cexo4b2+A9NnZ9auu/lrqt8Lr36n4fdTn1e1KkAtB0jyB8Xbf3Urd3c2rj4MD+A4wcNbLe76Ne/nV+O3XfSe17q01XL/SRNGuf0ev0Jz/r1N5N3fDa33RtQ+LUc+11bdqdhaYYNCJUeCpYtX8Vq/etpspXxdiksVyYeWGkQrcYLPUqq9rw2rAoQ9RJJWCwEGWIwqgztluevjZtEPpemcwFZrIys7pbjA4luyibrJFZ3S1Gt6IpBg3yqvP4155/8f7h9wkoAc4dfC5Lxy5laurUTm2VaGho9Ew0xdDDsfvsOANOPAEP7qBbPQInz56gJ3I+YT+Bt8BLWkwaaTFpxJnjmqzYpZRsKtnEy7tf5qvCrzDrzVw+/HJuGnsTFvqTe7SKx7fvZ3CClaxR/egfZ+m0MkopcfqClNb4KKvxUurwUlrjY89BP/nGfJJizCRFm0iKMZEYbSLRasKg79iZ1lUuP/tKHOwrqWFfsYMDZQ6khBizgRizgWizAZvFQLRZT4zZSIxZT4zFQLTJgNVkwKgXGPQ6THodBr3AqNdhDJ8NeoFRp8No0OEPSYIhBb1OtErpSinxBRXc/hAuXxCXP4jLF8LtD6qffSGU8NDGyaEaqE06ckZg0Aviooz1DpvFiF6nKf+24A8qlNZ4qXL7USThobLaoS1QlPBnJLWjUNFmA/FRRuKtveOda4qhh+ALhnD51D9+py+I2x/k/YLVvFnwt9YlII0gArz/xfuRoCi9lTRbGoNtaaTZ0hgUM4g0WxrVvmpe2fMK+yr3kWhO5PKMH5Ass9h3WOH72Ycptu8G1Eql9oc9dkAs541O4bxR/TgrPaHNP+xqt589xWqlW1TtodTho7TGS1mNlzKHD7c/1CCOAN47vKfR9OKtRlVZRJtJijERbzUSF2WKVHjq5zqH1YjNbCAQkhwud7K/xMHesBLYV1JDaY0vknZitIlRqTYMeoHTF6TM4cXpVb8Xpy+Icrr23U/XAWDQCfQ6ETkb9brIZwm4fEHc/hDBNmaoU0LYAm7ifC5i/S5i/W7i/C4kcCg+jYLY/gR1tXYJsJkNxNV5X/FRJhKijSRGqwo5MdqknptRzN5AiNIaLyV2LyU13vC1+h2X1Hgpc3gRCCxGHWaDvsHZbNRhMeoxG3QYdIJASFWIgZCCv87ZX+dzICQjZVDH8kEnQCdEJEwXtrGYDLoGSrFema0nFWW5W+G7gkqKqj0U29UyFVV7KKnxUlTt5YTTx+lQ+87jraZIvvFWU0RxqO/aTHL4nSdFm0mwGju8MdQcmmLoBu55Yzs7jttxRlqAwciPvBahdxA97DlC3qEE7FNAMSMVE0ahjt1Hm6zEGKOJNkUTZ7ZiNRnZf/wIVTio9pcgTJX4jVU47JUcNu9BGr5CCn8kfZtuEImeJRw/MIZXthmAIgYnRjE9M5HJg+M5Kz2eMQNiOVLhZuP+Mj7fV8Y/vsjj/zYeJt5qZN6IFM4f3Y95I1NIjDZF0lUUydFKN3uLa9hTXKOei2oosnsjz1iMOvrHWugXa2FCWjypNjOpsRb6xZrpZ7OQGmumX6yFnP9+yeQZZ1Pp8nHC6afS5afC6aPC5afC6afC5aPC6edgmZNqd4AaTwB/SGnyvevClUUoXNGa9DqG94vh7OHJjO5vY3T/WEYPsJESY262p+UNKDh8gYgid/mCBBUZqayCIbUCC4YkQUWJ9BICIYUDhw6TkTGEgCIJKQpBRRIKSYKK+mxIkZHfQozZgNWkJ9psILr2rFOwVZQSU16EuaQQQ/FxqKpEVlcj7dXI6mpw1DT7+5MGI570odSkD6d84FCO98vgmC2Jar+C3ROg2F5DlctPtSfAKXZXjKEgiV47GdJFesiF4vfy+4+KOShsJ7snYawmPf1jLaTGWpiSnoBOCLyBEN5AKNwLClLlVsJhCr6ggi+gKkKTQYdRr8NsUHtetZ9rzzYDJHqrMIaCSCUEigRFUQ8UCEmQClKRCKkQDIZw+wIU+IK4vQGQEoFESIkuXEi9DDFAVLLTOpS8bEdEedosBgbEWRgQF8W4gbH0j41iQLyFRKsJna5WAdUqJlUZUaugwu/C5Q9S7Q6oh0f9rVa71Xdsd/qoLipFVFXic3lwGqNwGSy4jFEE9CcVeHyUUVXS4R7092ekM29k01sqnA6aYuhiymq8vLHlOBMGxTFzYKL6x242EGPW17k28H7hX/mmLMTKhb9nZOLQ8DCGAWMzrYbs7Bqysq7A4Q1QcMJN3gkneeUu8k+4yDvhJL+yGA8nEEikMoxJg+O5dF48Zw1OYHJ6PMkx5gZpjupvY1R/G3ecOwy7J8CXB8vZuK+cLw6U8d72IoSAyYPjGZVq42CZk33FNbjCrX+9TjA0OZrpQxIZMyCWsQNiGTMgluQYU6uGUXRCqC3UaBPD+7X8bqWUeAIh7J4Ado/6R1h7bQ9fA4xIjWHMgFiGJEc3+T4Vl4tgZSXS50Px+pB+H9LrRfH5kD4fJq8Xo89PnM+r3vd6UbxepNcT/uxB8XhRvB6k1xe+52VWKEh82mD08fHo4+PC5/CRWHudAELgLziCv6Cg3hEoKqJebZ2YiCE1FUNCPPr0QejjE9An1B7xGGqvbTFIdzXevEK8u3bh2bmLmM1f0N/5IRMAYbFgGTMGy/jxWMaORQYD+EtKcRcV4y0uJVRaCifK0Tvsjb4vX1wi3lHj0E+YRNz0qaROnUhsTNRp2agUjwf/sWMEjh7FfyQf/7Gj4eujBIqLw0qgc5B6HSJjKNYxo4kZMRrzyFFYRg9Bn5zcqjIpfj+hykpClZUEK6sIVZwgeKKCYEXFyesTJ9TPlZVNlkUxmghFReOzWPGao3AZo/Do9RiEGwJXw8ibOrrogKYYupxdReof1i8uGcPMoUmNPpNXnceDuR9x3ajrmD+87fOUbRYjE9LimJAWVy9cSkm5w4fDFyQzKbrNw0FxUUYunTiQSycORFEkOwvtfL6vjOz9ZazbVcKoVBtXT01j7EBVAYxMtWEx6tssPwAFXzFu1yNQ+hzozaA3gcGknmsPgxn0RjBYIG0GIm0a1vCY/4C4qPblC9R8sp6iBx5Aut1tiieiotBZLAiLRT1HWdCZLeiiLOgTEhBmE45jxwm5nPgLj6NU2wnV1NCgWX4KuuhoTJmZRE2eTNzll2PKzAwfGehttqYjBjxw+HPY8zLsXwe+GkyDZxA7ZzH88LfI2DT8R47g3bVLVRa7dlO9di3S44kkoU9KwpzaD2NGGoYZUzEk2DCGCjE4dmCwbwdFwWOejduXgWfXfgI5XxJ6HootFqonTCBqyhSizpqMdfJkMBhQ7GqZQ3Y7IXsNoRo7Sk0NofC7CNXYCZaXEzh6jGBZWb3i6OPiMGZkEHXWWcRdthhj2mB0VivoBEKnI9x8R+j1IHQnw0U4XCfUZxzFcGwT4ti3ULoDZAjMsYiM2cjkUTg/fxVZ7sArq/BtzqHsww9OypCYiHnUSCyjRmNISSFUXUWwspJQZZWqBKqqCFVUoLhcjf9GTCb0yUkYklMwDhxI1MQJ6JOSMCQlY0hJRpjNKE4XIUcNSo0DxeEgdKKQUOlRlMpSQvY8FLePUEBHvxObAE0x9Al2Hq9BCBg3KK7JZ57c8iRRhijumHRHh+YthKBfrIVWNL5bRKcTTBocz6TB8fz0gpEdkGIdKg7D6zcQpwCiBkL+k0fQB6EAhHygBOvHi8+A8VfBhKshdVybs5WKwomnn+bE3/5O1KRJxF93HTqLGWGxIExm9dpsCZ9PvW56+CnCkW9wfLOOIUMHg98FATfS6yRkrw5Xlg5CNS5CTg+gwzRsNKYJM9GPnoNIHgn6Vvy5+pxwcD3sfQ8OrIeACyzxMPoSiB8M+z6C9f8L6/8XMfAszGMWY55zGXGLFqnvIBTCf+QIOrMZQ0oKwmQCdyXsfR92vw35/1Er0sShMHcFR47kk1H0IQkyBx64k8DIG/HsOYwndyvurblUPP88BIMtCA0YDOjj4tDHxqJPSiT67LMxZaRjSk/HODgdU/pg9HFN/800ixKCo9/CgXVw4BM4cUANTxkD0++AUd+DtOmgUxsx36XMYF7oK/jmb2BNInjOr/DpRuLbvx/v/v349h+g6vXXkT4fGI1qrywxEUNiAlFpaZFrfUIi+sQEDImJauWfnIwuJqb530koCCU74OhROPoNOL6FqDLIBEbHw+CZkD4L0mfDwLPa9z5agaYYupidhXaGJEcTY2781ecU55B9PJufTPkJCZaELpauB+BzwKobQKdn6+THmPW965p+VlFUZeEPV4Y718LXK+GrP6t/9BOuUhVF4tAWsw05nRTdex/Ozz8n7sor6f/rh9CZTC3GaxUBD6y7D7a+zBiAfSdvCaMVg9GKwRQN0dEQbwVTEgTcUPI+bFwDGwFDFPQfDwMmwYDJ6jlltNqL8lSrFd6ed+HwBgh6IToFJl4DYxbDkHlqzwrgvAdVxbv3fVV5bPiNevQbB2MXI8ZehnnIaPBUwa7VqjLIy1aVQcIQOPt/YNwV0H8CCEF+djYZV/0WNvwOvnoS49Z/Ycx6gNh77wa9EcXjwbNjJ96dO0Do0MfHoYuNRR8bhz4uNqIMhNXaOVOjgz5YfaP6+9AZIfMcmHYbjLwQEoc0GkXRW2D+wzDhGnhvBYZP7sIwYiHRi5+A+JsBkMEgisdTr6KXUhIKKPg8QXzuID5PEK8rQMAbwuQyYDVAtAxgiTGiq+2tBzxQuAWO/BeOfA3HvlOVOUB8Ogw776QiSB4FOh1Skfi9QXRSR/tXCTVPp27U09lMmzZNbt7cYC+fVtFdC41mPbKBmUMTWXl9Q22vSIXrP7ieKl8V71/+PhZD26aI9vrFU4oCa25Shz1uepvso7Lt5XGdUCuzXW+qLS6AQVNh/NVqhRbbcGNA/9GjHLvzTvz5BaTedx8JN93YcZVUxWFYsxRKd8I5P+XbwGhmzT0fTNFqZa9rZqaJEoITB6F4e/jYBsU7wO9Q7+tNkDgMKg6BEgDbQBizCMYuVisSXSuG8aqPwb4PYM974fclIS4dHEVqjywhU31v466A/hMbGJjr/eaKtsH6X0DBl5A0Ai74rdoa7661MEEfrL4JDn4CCx+GKUvBElvvESkljgovxYftlOTZKSuowV7twBarVvg6HQhXKaLmKDoURGI6uvjBCL0ORZH4a5WAO4DPE0QJtlyfCiGJMvuJ1lVhDRUSravEqqvCGh+FMSUdX8wIvObB+IIWvOG0va4gPlcArzuA3x1ESshaMopxcwe1mF9j9YIQolds1HNGUO7wUVLjZUITw0gf5X/E3sq9PHLOI21WCn2Cr55QK6kLH4Gh58LR7BajSCkpf+opDPHxxC5ejCEhGWYsU4/qY7D7LbUn8ckD8MmDMPIiuPKfYFG/A+fXX1P4s58jgPTnniV69uyOK8/ut+Hd5eoQ0A1vwMiFeLOzIaaVg3k6PfQbrR6Twj0nRYGq/LCS2A6le2DEAhhzmaoAm1M0jRE/GGb9SD0cper7P7QBxoeVwYDJra/YB06Gpe/DgY9h/S9h1fchcy4s/F3DYY9QEOxHVcVZcVhVbhWH1GtLHFz5DKSObVtZ6hL0qwr54Cdw6ZMw7QfhbBXKjzkoOWyn5LCd4jw7brs6W89o0dMvIxaTDWISLEhFIhWJYhmEtCWhnDiMPHEMaa9GictAmKOxWA3YkiyYowyYrQZMUQbMRgWzwYtZ78EknJiUanzH9uM+loerogZ3KA63kojLkIHLOJKyQAwetwAXUBiWX1RgthqwWI3qOdpIXEoUFqsBc7Qa1n9oO4fWWoGmGLqQXYWq4Xl8I4rBF/Lx1NanGJM4hkuGXtLVonU/Bz6Bz38PE66FWXe2Oppv714q/v4PAMoefwLbBRcQf83VWGfORMQPVoc+zv4fteW9Yw189SS8cgVyyZtUrnmXsj/+CfOwYaT97f8wDR7cQm6tJOhTW845z0DaDLj6BbUC7gh0Okgaph7jr+qYNGuxpcL029SjvQih9hKGL4AtL0H2o/BMljosE5MKlXmqAqjMV3s5tZhjcdkmc4ibKM8LEnrsHYL9jxOK6kfQHyIUUAgFFYIBJXIdCijojTqMZj1GswGTRR++1mEs/i/GmoEYR7yA6cRUfG8dUnsERxyEAuoMoNhkC2mjEhgwLI7+w+JIHBiDTifCLeyJDcsmZ6nKft194K6AjIvUoUxvNVRWQ1G1OqxXt1y16M2QNg2mz4GMOervwnxyu00lpOBxBgj6Q5itRsxRBtVY3k1oiqELqVUM4wbGNrj32t7XKHYV87uzfxdxIXzGUHEY3lymjlsvWtmmoYea9etBpyP9hRdwbNiA/b33qPnoI4yDBxN/1VXEXXEFxtR+kDwCzv9fGDQF5fWbKVlyPva9XmwXLGDgY4+hi47umLJUFcAbt0BRLsy+Cxb8+uT4/pmE3qj22iZeqyrjb/6mfq+JQyFllGoMTxqOyzyMvGMJHNrlomi3HSRExxkwyRL0x46ij/NgSByMMcaI3qDDYNShN+rQG/Xo9YJQUCHgCxHwhfB7Q3hdARwFRwl4EwgYvkdgpwFlex46vSAl3cb4cwcxYKiqCKLjGk7PbhYhYPyV6rj/ht+qthdzLETFQ1yaauSPig+fE+pfJ48EY9OjADq9ru3ydCKaYuhCag3PNkv9iqLKW8WzO55lXto8Zg6Y2U3SdRN1jM1c9yqYrG2K7vj0M6zTphE9aybRs2bS7+6f41j/KdVr11L+l79Q/te/EnPuucRffTUx8+YSjJ/K8a1T8R44QvIsK8mPPoToKKWw70N450fqbuXXvQZjLu2YdHszljhVOWY9oBp/dTrcNX7ytpVzaGMphQerQXpIHBjNjEuHMGxKPxIHREPACx/+HLa9CgO+pw4tWRo2qOoRCsCbt4HvXbj2DzDrunCwAgL0hg5qcEUlqMNTfRhNMXQhuwrtTM1MbBD+zx3/xB1087OpP+sGqboRRYG371CHeW56GxIy2hTdd/gw/sOHSfj+yT2hdGYzcYsuJW7RpfiPHKF67ZtUv/M2zs8/x5CSovq1cbsZ9OBtxB75I7xyOdz8LkQ3vqakVYQC8Nmv4Zun1TH5a15qcsbLmYrXq+NwbjGHtpRRuL8KKSE+1cq0izMZPrUfSQNj6kcwWuCyp9XZVx/fD8/Nh+tfh+ThjWcQCsJby9SZWRc+ArNOTvXWG8+wHngHoCmGLqLC6aPI7uWWQfVbPUdqjrB632quHHElw+KHdZN0p0HAq85tdxTBqItbb1gF+PIUY3MbcaxfD4DtggWN3jdlZNDv5z8jZcVynP/5D9Vr3iBkt9P/N7/BMmokHJ4Cr38fXl6kKoeYdrgXqDisKrfjOTB9GVz4e3XhnUYEvzfIv3+7CU+Nn7iUKKZclMHwqakkDYpufvaXEDDzdug3Bt5YCs+eB1c9DyMX1n8uFIS3/586/n/B72C2thnk6aIphi5iZxOG579s+QtGvZEfT+5FP2ZnuTrbY/86OLzx5LzrD34GIy6ASd9XDZDNVZAHPoGNbTc216Vm/adETZ6MMTW12eeE0Yht/nxs80/ZVXbY+XDDGvj3dfDypeqMmtYqtuLt6tj5nnfBGA1Xv6iOP2s0oOSwHU+Nnwt+MJYR01PbPhV4yFy4PRtWLYF/Xwvzfwnn/ExVHEpIHb7btVYdsjp7RccX4AxEUwxdRGMzknLLcvns6GfcOflOkqOSu0u0lpESyverK0f3r4NjOYBU581Pul7tKdhSYecb6syfAx+rRrfxV8KkG9TZGHUrgxOH4M0ftsvYXIv/2DF8e/fS7957T69sQ8+FJW+oFc5Ll6jKwda/8WelVBciffVnOPSZang8+39UxdaWntIZRtGhaoROkDmxdX6GGiU+HX7wCby3XDX8Fu9Qh5o+ugd2roHzfwnn/LRjBT+D0RRDF7GrsIbMJCuxYcOzlJLHNz9OSlQKS8cu7WbpmqBwC+x8E/Z/pM6dB3UMPet+tUdw6oKn/hNg/kPqbI3tr8O2f8PmF9TFTpOuVw9LHKxeAjpDu4zNtUSGkRZecJqFRG2R3vgmvHr1SeUQO/DkfUVRe0hfPQnHNqmriuf/Cqb/MLIeQqNpig/ZSRkcg8lymtWNyQpXPQcDJqo2nbyN4LXDef8L8+7uEFk1VDTF0EXsLLRzVnp85PP6I+vZUb6D3875LVZj+yrHTmX7anjnDnUmydBzYc5ydXFYXAsrLXV6GD5fPbw1sOcd2PY6fP47+PxhiB2kOjFrh7G5LjXr12MZOxZTWlq706hHxhy46S149aqwcvhAnXe/+y1VIZTtUVcEX/w4nHUjGNvvpO9MIhRQKC2oYfy8llfotgoh1F5a6jh49y51SvC5p9lr1GiAphi6gCqXn8JqDzfNVitCf8jPX7b8hREJI1g8bHE3S9cItUoh8xx12mVL0wSbwhILU25Wj8o8Nd0978Lcn7bL2FxLoKQE7/YdpPzkJ+1Oo1HSZ6kK69Wr4MWLAAHVR1SfRFf8U11QdiauSTgNyo6qC8oGDO/gntXwBfCzvd3nbqOPoymGLqDW8FzrCmP1/tUcdx7nHwv+gb41/my6krpK4fur2z3U04DEoXDeA+pxmjjWfwqAbeHCFp5sB4NnwE3vwGtXqX6ILnpM7Sm11dWEBgDFh6oBGDAsvoUn24GmFDoNTTF0AZEZSQPjkFLy7I5nmTVgFmcPOrubJTuFzlIKHYxj/XrMI4ZjHtpJawXSpsLdh1rn5lqjWYoOVROfasUa20GeajW6BK0Z1AXsKrSTnmglzmqkxFVCla+KCzI6wGjakdQqhYyze7RSCJ44gXvLFmwXdPL705TCaSMVSclhOwM7ehhJo9PRFEMXsLPQHhlGyrPnATAkrgetjN2x5qRSuGFNj1UKAI4Nn4OUnTOMpNGhVBa78LmDDBjRCcNIGp2Kphg6mWq3n+NVnsj6hVrFMDSu5c1juoQda9RVo71AKYA6jGRMT8c8alR3i6LRAkUHO9G+oNGpaIqhk9lVWANQr8cQZ44j0dLQZ1KXU08p9Nzho1pCdjuuTZuIXXhB5+z2pdGhFB+qJjrORGzyGbi3SC9HUwydzM5TXG3n2/MZGje0+yu2BkqhgzyMdiKOzzdCMKgNI/UCpJQUHbIzYER89//WNdqMphg6mV2FdtISokiIVmdl5Nvzu9++0AuVAqjDSIYBA7BMmNDdomi0gKPCi6vax8Dh2jBSb0RTDJ1MXcNztbeaSm9l99oXDn7WK5VCyOnC9fXX2C5YoLVAewGR9QuaYuiVaIqhE7G7AxytdDcwPHdrj+Gbv0JsWq9SCgDOL7KRfj+x2jBSr6DokB1TlIHEgVLRkH4AACAASURBVL3nN6ZxEk0xdCK7i+qveO72GUk1RZD3herMrhcpBVBXO+uTk4k666yWH9bodooPVTNgWBy6bty3WKP9aIqhEznVFUaePQ+L3sLAmIHNRetEgdYCUlUMvQjF48H5n/9gWzAfoe9hLkQ0GuBx+KkqcXe8fySNLqNbFIMQ4qdCiN1CiF1CiNeFEBYhxBAhxCYhxEEhxGohRK9fQ7+z0M6g+JOG5zx7HplxmehEN+njHath0DRI6l07xTm/+grp8WjDSL2E4sNqg0gzPPdeuryGEkIMAlYA06SU4wE9cD3wB+BJKeUIoAq4ratl62h2FdoZX2crzwJ7QffZF0p2QekumHhd9+R/GjjWf4o+Lg7r9OndLYpGKyg6VI3eoKNfRju98mp0O901lGQAooQQBsAKFAPnA2vD918GLu8m2TqEGm+Aggp3ZBjJE/RQ5CzqPsWwY7W6OU4v235S+v04N24kZv58hFFzed0bKD5YTb9MG3qjNlLdW+nyb05KWQg8DhxFVQh2YAtQLaUMhh87DnTQzh7dw6lbeRbYC5DI7jE8KyF1283hCyC6B28h2giub79FcTqxXbCgu0XRaAV+b5DyY05tGKmX0+UuJIUQCcBlwBCgGngD+F4jj8om4t8O3A6QmppKdnZ2u+RwOp3tjtsa1uUHAKg5spvs4j1sdm0GoOJABdkFnZNvU2WKr9rOZEcxuwffSHknlrmjcTqdHHj7bcwWC1uCQehFsjdFZ//uuppTy+MskUhFUu45Snb2se4T7DTo699Ra+gO38ILgHwpZTmAEOItYA4QL4QwhHsNaUBRY5GllM8AzwBMmzZNZmVltUuI7Oxs2hu3NbxVnMvAuEoWLzwPgJ25O9FV6Lh6/tWY9J1jV2+yTG+vBnMs4674ea/akjJ7wwZidu8hev58xnW2m+0uorN/d13NqeXZ9H4eR0UBF14+F1NU73Rd3te/o9bQHYOAR4FZQgirUJewzgf2ABuBq8PPLAXe7QbZOgzV8Hxyul6+PZ/BtsGdphSaxO+Gve/B2MW9SikAGA8eJFRdrflG6kUUH7KTlBbTa5WChkqXf3tSyk1CiLXAViAI5KL2AD4EVgkhHg6HPd/VsnUUDm+AvBMurjjrpJkkrzqvewzP+z8CvxMmdv/aBRkMkn+Favy2zppF9KyZWKdPRx/b+OwVS24uwmIhZu45XSmmRjsJhRRK8+2MObub1ulodBjdotallA8BD50SnAfM6AZxOpzdRaqr7doeQ1AJcsRxhHMHn9v1wuxYrbrAyOj+bUR9Bw7gO3gQ84jhVL/xBlWvvAI6HZZx44ieNQvrrJlYp0xBFxWFVBTM27YTM3cuOmvPdgeuoVJ+1EHQr2iG5z6A1t/rBE6dkXTccZygEuz6HoOzDA5tgLNX9IjN7N25uQAM/sc/0Kek4N2+Hdc33+LatImKF1+k4tlnEUYjUZMnYxo6FL3drg0j9SKKD6q/e23Fc+9HUwydwM5CO/1jLaTYzEA3+kja9SbIUI9Z1ObZmoshNRXDwIEIIbBOn451+nRSWI7icuHeuhXXt9/i/nYT1WvWoJjNxJyX1d1ia7SSokPVxKZEER1n7m5RNE4TTTF0AqcanrvNq+qO1dB/IvQb07X5NoEnN5eos85q1G22LjqamLlziZk7F4BQdTVff/45+piYrhZTox1IRVJy2E7mxKTuFkWjA+j+8YU+htMXJO+EK7LiGdQZSf2i+mEz2bpOkPIDUJTbYxzmBUpLCRQVYZ3SOu+o+vh4lMQesP2pRquoKnHjdQW0/Rf6CJpi6GD2FNUgJUxIOznTJq86jyHx3dBbEDoYf1XX5tsEnrB9QXOb3TcpCm/Moxme+waaYuhgdp5ieJZSkl+T37X2BUVRt+8ceh7Y+nddvs3gCU89tYwe3d2iaHQCxYeqiYo1Edevd62V0WgcTTF0MLsK7fSzmelnswBQ5i7DFXB1rWI49i3Yj/YYozOAe2suURMmaI7w+ihFh6oZOCxO23a1j6Aphg6m7h7P0E2G5+2rwBgNYy7tujybQfF48O7dqw0j9VEclV6clT7NvtCH0BRDB+L2Bzlc7mx0RlKX9RgCXtj9DoxZ1GO27/Ts3AnBIFGtNDxr9C6Ka+0LIzTF0FfQFEMHEjE8nzIjyWa0kRzVRe6uD34CPjtMvLZr8msFntxtAFgnT+5mSTQ6g6JDdowWPUlp2tTivoKmGDqQyB7PafV7DEPih3Td2Ov21RCTCkOzuia/VuDZuhXTsGHo47UWZV+k+FA1A4bGodNp9oW+gqYYOpCdhXZSbGZSYy2RsLzqvC4bRjIEauDgephwDej0XZJnS0hFwbNtG1Fnab2FvkjQJ6kscmluMPoYmmLoQHadYni2++xUeCu6TDH0K/salECPmo3kLyggZLdj1QzPfRL3CfWsGZ77Fppi6CDc/iCHypwN9mCArjM8p5ZmQ8oY6D+hS/JrDZ6tWwGIOmtKN0ui0Rm4yyU6vSA1s3HX6Rq9k2Z9JQkhBgDXAXOBgYAH2IW6d8J6KWWj22+eiewtrkGRMH7gyT+QWsXQJVNVK/OIq9kHC34NPWguuTs3F318PKYhmd0tikYn4C6HfhmxGEw9Y+hSo2NosscghHgWeDX8zErgVuBnwFfA5cDXQghtB5UwO483bng26UwMihnUVLSOY8caJEK1L/QgPFubdpyn0bsJ+EN4KjU3232R5noMT0sptzcSvg1YI4SwAOmdI1bvY/ORKgbEWehf1/BszyMjLgN9ZxmCpYSKQ3DwU9j8AtXx40mIS+ucvNpBsKoKf34+cVdc0d2i1CMQCHD8+HG8Xm+X5x0XF8fevXu7PN/OIBhQmL4kgSibt8+UCfrWd2SxWNrVKGtSMTSmFIQQGYBVSrlXSukFDrQ5xz6IlJKc/EpmD0uq9yXkVecxLnlcx2bmd0PBV+rso0OfQlWBGp48kvzBS0jo2NxOi8j6hR42I+n48ePYbDYyMzO7vCfjcDiw2brQy24n4qr24bL7SE6LQafvO+bKvvIdSSmpqKggOrrtC11bvR+DEOI+YBqgCCE8Uspb2pxbH+VIhZsyh48ZQ066ifYGvRQ6C1k0bNHpZ1BxWFUEBz9VlULIB0YrDJkHc5bD8AsgIYOa7OzTz6sD8eTmgsGAZULPMYYDeL3eblEKfY2AP4TQ06eUQl9CCEFSUhLHjh1rc9wmFYMQ4kfAP6WUSjhoipTymvC9He2StI+SU1AJwIzMk4rhSM0RJPL0ZiQd+QbevRMqVbcaJI2A6bfBiAsgfQ4YLc3H72Y8ublYxo5FZ+l5cmpK4fQJ+kM9ZbmMRhO093feXI/BA3wshHhSSrkO2CCE+BwQwIZ25dZHycmvJDHaxPB+J10CdMiMpO2vg7McLn4chi+AxC7e0+E0kH4/np07Sbi+Z2wUpNGxhIIKSkhi1Hbx7JM02QeUUr6EOvtolhDibeC/wGXA1VLKn3aNeL2DnPxKpmcm1Lcv2PMQCDJiM9qfcOEWGDwDZizrVUoBwLtvH9Ln0zyqngaPPvoor732Wquf//jjj5kxYwajR49m8uTJXHfddRw9erRTZAv6Q4C6wP7iiy+murq6w/OIqbOt60cffcSIESPaVJ6amhoGDRrEXXfd1eKzt9xyC4MGDcLn8wFw4sQJMjMz2yzz8uXL68ndHDk5OWRlZTFixAimTJnCJZdcws6dO1ud10UXXUR8fDyXXtrxXpRbGhwcDLwM3AX8HPgjoHUe61Bi93K00s30zPrbUObZ8xgUMwiLoZ3DKH4XlO2BQVM7QMquxx1Z2KYphvayfv16Fi5c2Kpnd+3axfLly3n55ZfZt28f27ZtY8mSJRQUFDR4NhgMnrZsAb86wiwMaqUd34l+sDZs2MDy5cv5+OOPSU9v/UTIX/7yl5x77rmtfl6v1/PCCy+0R0QANm/e3GoFWVpayrXXXssjjzzCwYMH2bp1Kw888ACHDx9udX733HMPr7zySnvFbZbm1jE8D/wGeBK4S0p5K/A88KIQ4oFOkaYXUmtfmDmk/iboefY8hsafhn2heDtIpdcqBk/uNoyDBmFM7dfdovQ4/vjHP/LUU08B8NOf/pTzzz8fUCvAG2+8EVBbu36/n5SUFI4cOcL8+fOZOHEi8+fPb7TV/Ic//IEHH3yQMWPGRMIWL17MvHnzAMjKyuLBBx/k3HPPZeXKlU2mecstt7B27dpIGrWt3+zsbObNm8cVV1zB2LFjWf4/P0bo1THszMxMTpw4QUFBAWPGjGHZsmWMGzeOhQsX4vF4APjuu++YOHEis2fP5p577mH8+PGteldffvkly5Yt48MPP2TYsGGtfsdbtmyhtLS01YoV4Cc/+QlPPvlkuxRnKBTinnvu4Y9//GOrnn/66adZunQpc+bMiYSdc845XH755a3Oc/78+Z02e6o5G8M0KeUkACFELvCAlHIzcIkQomdsJNwDyMmvIMZsYMyAk19QSAlxxH6Eswee3f6EC7eo50G9z5WElBLP1q1YZ87sblFa5Dfv72ZPUU2Hpjl2YCwPLWp6mvK8efN44oknWLFiBZs3b8bn8xEIBPjqq6+YO3cuAJ999hnz588H4K677uLmm29m6dKlvPDCC6xYsYJ33nmnXpq7d+/m7rvvblau6upqvvjiCwAWLVrUYpqnkpOTw549e0hPT2fB+QtZt/4Drrim/qy7gwcP8vrrr/Pss89y7bXX8uabb3LjjTdy66238swzzzBnzhzuv//+ZvOpxefzcdlll5Gdnc3oOlvCvvbaa/zpT39q8Pzw4cNZu3YtiqLw85//nFdeeYUNG1pvDk1PT+ecc85h1apVXHPNyYWiDocj8r2cyr///W/Gjh3L008/zeLFixkwYECr8tq9ezdLly5t8n5LZexsmlMMn4WNzSZgdd0bUso3O1WqXkROfiVTMxIw1JmyV+gsxK/4T29GUuEWiEuHmN7X4g4UFhEsL9c25mmCqVOnsmXLFhwOB2azmSlTprB582a+/PLLSE/i448/5tZbbwXgm2++4a233gLgpptu4t577202/YqKCubPn4/b7eb222+PKIzrrjvpXLGtaQLMmDGDoUOHEgoqXL7oKjZt/qaBYhgyZAiTw/tuTJ06lYKCAqqrq3E4HJHW8Q033MAHH3zQYn5Go5E5c+bw/PPPs3Llykj4kiVLWLJkSZPx/va3v3HxxRczePDgFvM4lQcffJBLL72Uq6462fa12Wxs27atyThFRUW88cYbZJ/GdPGZM2dSU1PDwoULWblyZYtl7GyaW+D2cyFEIhCSUtq7UKZeQ5XLz4FSJ5dNru/yokNmJBVu6ZW9BQBPrmpf6A0eVZtr2XcWRqORzMxMXnzxRebMmcPEiRPZuHEjhw8fjgwF5eTk8Pe//73R+I1NQRw3bhxbt25l0qRJJCUlsW3bNh5//HGcTmfkmeYWOtWmaTAYUBTVfiClxO/3N3im1vBsMDQciTabT05T0uv1eDwe2utSTafTsWbNGhYsWMAjjzzCgw8+CLTcmv7mm2/48ssv+dvf/obT6cTv9xMTE8Njjz3WYp7Dhw9nwoQJrFmzJhLWUo8hPz+fQ4cOMXz4cADcbjfDhw/n0KFDTeZT+31ddtllAGzatIm1a9dGFGaP7TEIIa4HVjflKE8IkQkMlFL+t3NE6/l8V7t+YUhDwzPQfhuDsxyqj8L0ZaclX3fhyc1FZ7ViHjmyu0XpscybN4/HH3+cF154gQkTJvCzn/2MqVOnIoRg9+7djB49Gr1enecxZ84cVq1axU033cRrr73GOec0dFF27733csUVVzBr1qyIcnG73U3m31SamZmZbNmyhWuvvZZ3332XQCAQiZOTk0N+fj5Jcf159/23uHP5j1pV1oSEBGw2G99++y2zZs1i1apVkXuFhYXcfPPNTQ75WK1WPvjgA+bOnUtqaiq33XZbi63pujO5XnrpJTZv3hxRCjfffDN33XUXM2bMaDL+Pffcw7XXntwBsaUew9ixYykpKYl8jomJiSiFt99+m5ycHB599NF6cX784x8zc+ZMLrzwwkhPqu731WN7DMAgIFcIkQNsAcoBCzAcyAJqgPs6W8CeTE5+JSaDjolp9Z2I5dnzSI5KJtbUTlfERWqLm7Rppylh9+DO3UbU5EkIvTaBrSnmzp3L73//e2bPnk10dDQWiyXSKl23bh0XXXRR5NmnnnqKH/zgB/zpT38iJSWFF198sUF6EyZMYOXKldx88804HA6SkpJIT0/nN7/5TaP5N5XmsmXLuOyyy5gxYwbz58+v18uYPXs2999/P9tzdzB75hyuuupKXC5Xq8r7/PPPs2zZMqKjo8nKyiIuTv2bKS4uxmBo3gFDYmIiH3/8MfPmzSM5OTnSym4PO3bsaNEOMGbMGKZMmcLW8My60+Hw4cPExjasB/r378/q1au57777KCwspF+/fiQnJ/OrX/2q1WnPnTuXffv24XQ6SUtL4/nnn+fCCy88bZkBtbvY1IGqOL4HPIw6I+lp4MfAkObiddUxdepU2V42btzY7ri1LPrrl/Laf/y3QfgNH94gb/341vYn/Pnvpfx1vJQ+Z5uidUSZTpegwyn3jBkry1Y+ddppdVZ59uzZ0ynptoaampoWn1mwYIEsKirqAmlaz8aNG+Ull1wiFUWR5UdrpL3cLaVsXXmklNLhcESuH330UblixQoppZR//etf5bvvvtvxAjeC3W6XV199dYvPtbZMrWHJkiWyrKysw9JrD1u3bm0QBmyWzdStzapqKWVQCPGNVFc+a9TB6Quyu6iGO7PqT6GTUpJfnc/FQy9uf+KFW6DfWDC13flVd+PdsR0UhagpvdM+0hP49NNPu1uEJlFCEkWRGMxt6w1++OGHPProowSDQTIyMnjppZcAWrX4rKOIjY3ljTfe6LL8AF599dUuza+jaI0TvS3h4aQXpZTrO1ug3sLWI1WEFNnAvnDCcwJHwNF+w7OUqmIY0wHO97oB99ZcEIKoSRO7WxSNDiQrK4usrCx8btXmYDS1zXHeddddV29WlEbPpjXf7gjgX8AyIcRBIcRvhRCtX2nSR8nJr0SvE0xJr+/o+rS386zKB09VL17Ylot55Ej0fcBtsUZDAr7wjCSjZj/qy7SoGKSUipRynVQ9qy4DbgO2CSE2CCGaNu33cXLyKxk/MJZoc/1OV2RGUnsVQ2HY4NULFYMMhfBs20ZUD9t/QaPjCPoVDEY9Qqd5p+3LtKgYhBDxQogfCyE2AfcDPwUSgf/llIVvZwreQIhtx6sbDCOBqhiijdH0s7ZzYVrhFjBEQcqYlp/tYfgOHUJxuXrF+gWNtiOlJOAPYWjjMJJG76M1NobvgH8D10opj9QJ/za8L/QZx47jdvxBhRmn+EeCsI+kuKHt9/dfuAUGTgZ9q/dQ6jF4cnMBNMNzH0UJSqQiMbbR8KzR+2iN6h8lpXzoFKUAgJTykfZkGu6FrBVC7BNC7BVCzBZCJAohPg3bMT4VQvSkXSrrkZNfAcD0zIYi5lfnt9/wHAqozvN64TASqB5V9cnJGNN6zr7TvZme5nY7ULvi2VRfMWhut1UHiFOmTGHy5Mmcc845za56rqU3u90G+EgIEfGpK4RIEEJ8eJr5rgQ+llKOBiYBe1GHqTZIKUegbgTUOk9b3cCm/EpGpdqIt5rqhTv8Dso8Ze1XDKW7Iejtxa4wtmE96yxtd7QOoqe53Y64wjDWrzY0t9vwox/9iNdee41t27Zxww038PDDDzf7fK91u12H/lLKSHNASlkFDGxvhkKIWGAe6oI5pJT+cPqXoe79QPjcev+zXUgwpLD1SFWj9oXTnpEU8aja+3oMwfJyAseOafsvtILe6nZ74cULuPWOJYwbP4477rgj4lNJc7ut+pGqqVG99NrtdgYObL6K7M1ut2sJCSHSpJTHAYQQrVfZjTMU1b3Gi0KISajuNv4HSJVSFgNIKYuFEI1ab4UQtwO3A6Smprbbo6HT6WxX3Hx7CJc/RIynhOzsE/XubXJuAuDE/hNk57U97VH7PiTJGMt/t+WDKGhz/PaWqSMw5+YSD+xDEuggGTqrPHFxcTgcDgDMGx9CV7a7Q9NX+o3Dd17jrihCoRBTp07l6aef5tZbb2XTpk34fD4qKyvZsGED06dPx+Fw8P7773POOefgcDi44447uOaaa1iyZAmvvPIKd955J6+//nq9dHfu3MmPfvSjSLkay7esrCzipO3aa69tNM1AIIDH46mXjsPhwOVysTV3M19/sYlhozK48soree2111i0aBFSSpxOJ06nk4MHD/Lcc8/x5z//maVLl/Lqq69y/fXXs3TpUp566ilmzpzJQw89hKIoTcpai8/nY/HixXz00UcMGjQo8vzq1asjirUuQ4cO5ZVXXkFRFH7yk5/wzDPP8MUXX+D3+1vMKxAIkJKSwsyZM/n3v//NJZdcgpQSh8OBw+Go56KkLs8//zyjR4/mqaee4nvf+x5RUVHYbDY2bNjQbJ7bt2/nhhtuaPKZlspYi9vtJhgMNpuXlLLNf0etUQy/Ar4Ou+AGOA9onfespvOcAiyXUm4SQqykDcNGUspngGcApk2bJrOystolRHZ2Nu2Je+jLPGAvt1xyDqmx9Xdny92Si7HKyJXzr8Sga4fxePf9kDmbrPPOa3tc2l+mjqD0201UmUzMvvFGdCZTyxFaQWeVZ+/evSdbWkZTxxv6jSZMTbTkHA4H8+bNY9ky1UGi1Wpl+vTp7N+/n5ycHJ566ilsNhtffPEFt956Kzabje+++4733nsPo9HIsmXL+NWvftWgpajT6YiOjsZmszXqdluv13PTTTdF4jWVptFojFRutdhsNizmKM6aNJWxY8cQZTNx4403smXLFi6//HKEEJGexZAhQzj7bHUfkpkzZ1JaWkooFMLlcrFgwQJA7ZWsX7++xdau0Wjk7LPPZtWqVfXcbv/whz/khz/8YZPxnn76aRYtWsSYMWPYtGkTJpOpVXlFRUXx0EMPcemll3LNNdcghMBms2Gz2dixY0ez8f/5z3+ybt06Zs6cyZ/+9CceeughnnvuuSafNxgM9d7zqW63WypjLVarFYPB0Gz5hBBt/jtq8S9CSvlheL3CbEAA90kpy9qUS32OA8ellJvCn9eiKoZSIcSAcG9hAHA6eXQaOfmVZCRZGygFUGckZcRmtE8p+BxQvg/GXdEBUnY9ntxcLOPHd5hS6DK+17Ir5o6mN7rdDgUUBKKe4bkxOc5Et9spKSls376dmeGNqa677romexi19Fq326fgBY4S9q4qhBgu2+luW0pZIoQ4JoQYJaXcD8wH9oSPpcBj4fO77Um/M1EUyXcFlSwYk9ro/Xx7PiMT2ulqumgbIHulfUHx+fDs2UPS0pu7W5ReQ29zux0KKuRu38Kx40fIHJLJ6tWruf3221tV1r7udjsYDGK32zlw4AAjR47k008/jXwHfdHtNgBCiB8AP0d1w70TmA58i+p6u70sB14TQpiAPOBWVEP4GiHEbahK6Jpm4ncLh8qdVLkDjRqe/SE/xxzHuDCznW5ve/FWnt5duyAQ0AzPbaC3ud0OBhSmTZvBAw8+wM6dOyP7P2tut9Ve1rPPPstVV12FTqcjISEhMrupT7rdDncBdwJRwLbw53HA6y3F64qjq91u/+ubAplx3wey4ERDd9gHKg/I8S+Nlx8c/qB9Aq26Ucq/TGpf3DDd4XZbURRZ9ItfyD2jRstARUWHpq253e4ZKIoi3179obzwgosa3NPcbjdPn3S7HcYrpfQIIRBCmKSUu4UQo1uO1vf4Lr+S1Fgz6YnWBvc6xEdS+qzTEa9bKF+5kuo31pJ4yy0YEhv2pDTaTk9zux0KKkhFnpZ/JM3tdu+iNYqhOLzA7X3gEyFEJVDauWL1PKSU5ORXMmNIUqNGtyM16sLwzLjMtifuKIGa473OvnDiH/+k4h//JP6aa+h3X8ubyWv0ToJ+hbNnz+XSK5o3qDaH5na7d9GaWUmLw5e/FELMB+KA01353Os4VumhpMbbqH0BoMhZRJIliShDVNsT74UeVStffpnyv/yF2EWL6P/rh7TVzn2YoD8EAs153hlEs4pBCKEHtkopJwFIKRufOnAGsCnsH2lGZtOKYWBMOxeEF24BoYcBvWNzm6o1ayh99DFsCxcy8NFHtL2d+zgBf0h1ta0p/zOGZpsAUsoQsEcIMaiL5OmxfFdQSbzVyIh+MY3eL3KdpmJIHQfGdvQ2uhj7e+9R8tCviT53HoMe/xOihRklGr0bKSVBn4LRpCn/M4nW/FUnA3uFEN8AkblpUsorO02qHkhOfiXTMxPRNWKAU6RCsbOY8wef3/aEFUUdShrf819nzSfrKbr/AawzZ5K2ciWity1m02gzoaCizlIxa8NIZxKt+bYfA64A/gj8X53jjKGsxktBhbvJYaQKTwV+xd++HkPlYfDZIW3aaUrZuTi/+ILCu+8matIkBv/f0+gsDVd+a3QsTbnd7mz32nUJ+tVV0AaTvtPcazdFVlYWmzdvBqCgoIARI0bwySeftCmNxYsXt8ph30svvYROp6vn+mL8+PGNeqdtjrVr1yKEiMjdHKWlpdxwww0MHTqUqVOnMnv2bN5+++1W5eN2u7nkkksYPXo048aN4/77O9YZdWu29tzQ2NGhUvRwcgoqAZo2PLuKANqnGHqBR1XXN99wfPkKLCNHMviZf6JrxrWCRsfRmNvtrnCvXZeALwRCYDDqOt29dlMcP36cCy+8kCeeeKJNC7jeeuutens6tERaWhq///3v2yMioLrOqHUU2BJSSi6//HLmzZtHXl4eW7ZsYdWqVRw/frzV+d19993s27eP3Nxcvv76a9atW9du2U+lNVt7OoQQNeHDLYTwCSFqOkyCXkBOfiVWk55xAxuuYATV8AwwMLqdisEUA8ntdKXRybi3buXYnT/GlJHB4Oee++OB2AAAIABJREFURd9Jbn7PJNrqdrsuXeFeu3ZV89ixY1nxkx+j06t+kRpzrz1jxowOca/dFCUlJSxcuJCHH36YxYsXtxwhjNPp5M9//jO/+MUvWh3n0ksvZffu3Rw8eLA9ovLLX/6Se++9F0sretOff/45JpOJO+64IxKWkZHB8uXLW5WX1WrlvLCzTZPJxJQpU9qkVFqiNdNVIzWBEEIHXIm6uc4ZQ05+JVMzEjDoG9ejEcXQ3h7DwLNA1znGPRkKESguxp+Xhy8vD39ePsGKCvQx0ehssehsMehtsehjbehibOrZFoveFkOwvJxjP7oTY2oq6S++gCGhx26q127+kPMH9lXu69A0RyeO5r4Z9zV5f968eTzxxBOsWLGCzZs34/P5CAQCfPXVVxG3GJ999hnz589vEHf37t3cfffdzeZfXV3NF198AcCiRYu4+eabWbp0KS+88AIrVqzgnXfeaTZ+Tk4Oe/bsIT09nfnnLWTd+ve56dYb6j1z8P+3d+bxbVVXHv9eyYu8b7EdkwUSZ2PLTkqSQgO0NEMoW1lKWDOZplDWQhsCXYYp0E9pgTC0UKAwQNmStlCgtMM0MHFKhjRNvGUPSXBWOd4SW5YtWZZ0548nO1YsybIledP5fj6KrKf37js3z37n3Xvu+Z09e3j77bd56qmnWLp0Ke+88w433ngjS5Ys4cUXX2TevHlRmd64+eabefTRR7nmmhMKObt37w6aE1FSUkJ2djY//vGPuf/++0lN7Z6MGgyTycTy5ct54okneOutt/y+u+6669i9e3e3Y+677z5uvvlmysvLOXToEJdeeilPPPFEj+favn07M0OUwA2njx00Njby5z//mXvuuafH84ZLr5aUaK29wB+VUt8Hfhw1KwYxja0udh1tZtHZwfVVrHYr2cnZpCaG/0sIgLsNjm6FcyNRMTfwOp0kHDxE04d/MZxAleEEXPv3o33lCgHMWVkkFBTQ1tqKp7kZb3MzhFC/TBw9mrGvvkLCiBER2ygYzJo1i9LSUpqbm0lOTmbmzJls3ryZTz/9tHMk8dFHH7FkyZKQ7QSS1wb8bigbNmzg3XffBeCmm25i+fKeExHnzJnD+PHjcbd7uPKyb7Jx84ZujmHcuHFMnz6d5uZmZs2axf79+2lsbKS5ublTFG7x4sWdaqF95atf/Sqvv/46t956a+dNfvLkySFF7SoqKti7dy8rV67sdYxg8eLFPPLII1RVVfltX716ddBjvF4v3/ve9zqzufvCHXfcwfr160lKSmLTpk099rEDt9vN9ddfz91338348X1UXQhAOCJ6XcdvJmA2hvx2XLB5/3EgeHwBjBhDUVpoYa6A1GwDjyvi+IL2eNj39YXk1dRgBTCZSBw9muRx40ibN4+k8eNIHj+epPHjuz31a68Xb2srXput01F4bM14m214HQ7SL7iQxMKANZOGBaGe7GNFJLLbsZbX7rqPu8343hxgpBxNee1QLF++nDfeeINrrrmG999/n4SEhB6fpjds2EBpaSmnnXYabreb2tpaFixYEFaxmoSEBO666y4ef/xxv+2hRgyXX34527Zt66x5cPToUS677DI++OADZs8OvKjkzDPP5J133un8/Oyzz1JfX9+5f7gjhmXLljFx4kTuvffeHvvWG8IZMXRVOXUD+zHKcMYF/9x/jCSziWljggfdrHZr3+o8RynjuW3vXtw1NbR8/eucfecdJJ56ath1EZTJhDk9HXN6OokRWSH0ht7Ibncl1vLaYDilqqoqcjMKef/Dd7njrvBGtJHIa4di5cqVLF68mKVLl/Lqq6/2+DR9++23c/vths379+/n0ksv7XQKv/71r4HQGk033HADX/rSl/yqooUaMQDU15+o5rhgwQKeeOIJZs+eHbTfF154IQ899BC/+c1vOm3teh3DGTH86Ec/oqmpKWRBoL4Szqqkm7q8lmit/0NrfTTqlgxSNlYdY/qYbCyJgWMAWmuqW6r7Hl9IL4TMyPIHHRWVxvv8eSRPnDj0iuXEIeeddx7V1dXMnTuXwsLCkLLbXekqrz1lyhTmz5/Pzp07Wbx4ccD9n3nmGV555RWmTp3K66+/3lkJ7dvf/jbr1q1jzpw5bNy40W+UMXfuXFasWMGceTM57dTTuOqq8HNsXn75ZZYtW8bcuXPRWvdKXjsYSilee+01qqurw5oKC8WuXbvIy8sLuU9SUhJ33303tbWR1woL1m+lFO+99x7r1q1j3LhxzJkzh1tuuaXbSCUYhw8f5rHHHmPHjh3MnDmT6dOnR9dBhJJe9Q0NXwayu3zOAX7b03H98Yq17Lbb49UTHvqL/tlfgss0Nzga9FmvnqVf3/5674341Wyt3/pW7487iSMPPqR3f+lcvfZ//zfitgYTIrvd/6xdu1YvWrRIe71eXXvApm0NjpD7n9yfwSCvHYpFixbptra2kPtEU3Z7MPQ7VrLbM7XWnVktWuvjSqnBu+g+itQ1t9Hu0YwJILPdQbW9GujDiiRnE9R/DlOv7XnfHnBUVpIybRqIls2wYDDIbnvafRnPvRTOGwzy2qGINBjeWwZLv3tLOI7BpJTK0lo3ASilciA+pqOPNBprs0dlB9cwOmI/AvTBMVjLjfcI4wsemw3Xvn1kXbooonYEAYz58QULFuC0G8Ho3mokibz28CAcx/A0sEEptRrQwLcw5DGGPVafYzglhGOobunjiKEj4/mUyEp5OrZsBTBGDF1WlghCJLS7vCilMCeKRlI8Ek7w+RUMZ9AENAPXaa1fjbFdg4ITjiF4JqPVbiU9MZ3MpMBZ0UE5UgZ5EyElMokBR2UFKIXl7LMjakcQuuJ2eUhIMonUdpwSTh7DOcBOrfUW3+cMpdRsrXXPKlFDHGujgwxLAhmW4DNnfarDoDUc3gzjF0RkHxjxheQJxSJVIUQNrTVulxdLelzMGAsBCGec+CLQdaF0C/BCbMwZXFibnCHjC+Crw9BbjSSbFexHI09s0xpH5RYs0+JKoUSIMR2B50Sp2Ba3hHPlTdqQwgA6ZTHi4lHC2ugIGV+APo4YoqSo6tq/H29TkxFfEIYdAyW73e7yAIbUdldEdjs4Bw8e5IILLmDGjBlMnTqVv/71rz0eM6Rlt4EqpdTtSimzUsqklLoDI/t52GM4huDxBZvLhr3d3jfHYEqEkZEpTzoqjcQ2cQzDk4GS3XYHCTyL7HZwHn30Ua699lrKy8tZtWoV3/3ud0Pur4e67DbwHeAioMb3+grw7ahZMEhpdbk53toecsTQZ1XVI6Uw8mxISO553xA4KisxpaWRXFwcUTtC/zLYZbe/fslFLPnOYs4880xuu+22Tk0lkd0OjlIKm82oRtDU1MQpp4S+JwwH2e0a4OqonXGIYG10AqFzGPpUh6HdaeQwTLs+IvvAcAyWqWejAmjqCOFx9Gc/o21ndGW3k0+fwsiHHgr6/WCW3dZaU1ZRyj8/K+fMaZNYuHAh7777Lldf7X8LENltgw7Z7YcffpiLL76YX/3qV7S0tPDxxx+HPNeQl91WSiUDtwJnAp3zKlrrZVGzYhDSsVS1KKtnx1CU3gtl1c//G1x2mHJJRPZ5W1tp2/05ed/+t4jaEfqfwSy77fVoZkybxcRJxZjNZq6//nrWr1/fzTGI7LY/b7/9Nrfeeiv3338/GzZs4KabbmLbtm2YTOEF8Iec7DbwO+AL4FLgMWAxsD1qFgxSwsphaLGSkpBCTnIvCthUvA0Zp8C4r0Rkn3P7dvB4JL4QIaGe7GPFYJbd9no0CoU5wdTtmK6I7LZBx4jh5Zdf5qOPPgIMEUKn00l9fT0FBYEl6we77HY47myS1vpBwK61fhlYCEQ2cTgEsDY6MCkozAyd3FaUVhR+EpC9FvZ+DNOui7himwSehzYdstvnn38+5513Hs8//zzTp08PS3b7scceY+fOnZ3bwpHdBgLKbgN+sttej5fyylIOHjqA1+tl9erVncf0RFfZbaCb7HagqbFwWLlyJZmZmSxduhStdefTdKBXdnY2t99+O1arlf3797N+/XomTZrkJ7vdIb0djBtuuIGPP/6Yurq6zm2rV68OeL6bb74ZgLFjx3ZKa+/cuROn00l+fn7Qfl944YU4nU4/5x9IdjtYH+GE7PbTTz/dp//XUITjGDqE2huVUqcDGcCpUbdkkHGk0UlhpoXEIOU8oQ9LVbf8HrQHpgWWSO4NjspKEseOJSE3eAEhYfAyWGW3vR7NrJnn8MMfPcRZZ53FuHHjuPLKK8PuV7zKbj/55JP89re/Zdq0aVx//fW8+uqrKKWGtez2dzCkti8ADgL1wHd7Oq4/XrGU3f7WCxv0Vc/9X8h95r89X//0s5+Gf9Ln5mn94gXh7x8Er9erd3/5y/rw93/gtz1WMtUDhchu9z9//tNH+msXfT3s/UV2OzSDod8xkd3WWndkOa8FxkbPJQ1urE0Opo4Ovma7tb2Vpram8EcM1VuMUp6X9FwovCfc1dV46uplGmmYMpCy216PNyJ9JJHd9mew9Lu39G1sN8zxejXVjU4WnhU6vgC9yGGofBvMSXDWNyO2T+ILQqyYO+fLnbWL+4LIbg8PRAwlAA0tLlweb+gchpZeOAZPuxFfmLQQUiOPCTgqKlHJyVgmT4q4LUHowOvVaK/2W5EkxCc9/gYopbqNKgJtG050LlUNI4chrOS2PWugtR6mRx50Bl9i25lnoqS2sxBFPG5j+ao4BiGc34B/hrlt2BBOgR6r3UqSKYm8lNArHACofAvS8mHCVyO2zety4dyxQ6aRhKjjFccg+Aj65K+UKgCKgBSl1NlAR0QqEwg/zzx4+2ZgM3BEa32pUmocsArIBcqAm7TWA1KSLJySntYWK0XpRZhUD39Ercdg90cwZxmYIxelbdu1C+1yiWMQos6JEYMU54l3Qt3VFgG/BkYDz3Z5PQT8OArnvgfY2eXz48BKrfVE4DiwNArn6BPWRidpSWYyU4LPmFntYdZh2PYOeNtheuTaSGDEFwBSpotjGO70t+y2x61RSqFMgR2DyG4Hp79lt7sSbh97Q1DHoLV+RWt9HrBUa32+1vo83+sSrfUfIjmpUmo0huN5yfdZARcCHXKPrwFXRHKOSLA2OijKTgm5bC/s5LaKt6DwbENNNQo4KitJKCwkceTIqLQnDF76W3bb4/ZiTgxezlNkt4MzELLb0Ps+hks4QeQCpVSm1tqmlHoemAk8qLX+JILzPg0sx8iiBsgDGrXWHb/Jh4FRgQ5USi0DlgEUFhaGpX8SCLvdHvTY3YccpCWpoN+7vC4anA04a50hz5/acog51jL2Fv8rh/to58nkbfwH7jFjA543VJ+GIrHqT1ZWFs3NzVFvNxw8Hg+PPPIIycnJ3H777axYsYJt27bx4YcfUlJSwhtvvMFLL72EzWbD4XBgsVj8bH300Ue57777GD16dOf2Dvnl5uZmLrnkEr70pS/xj3/8g0suuYTLL7+cO+64g/r6ekaMGMFzzz3HmDFjuO2221i4cCFXXGE8fxUVFVG1y8pn//iUX6z8Gbm5uezZs4f58+fz1FNPYTKZOOuss1i3bh12u51vfvObzJ07l40bN1JUVMSqVatISUmhtLSUO++8k9TUVObOncuaNWvYuHFjn/+vvvjiC2688UZ++MMfcsEFF4R93ex2O7/85S955plnuOWWW3o8zul0cvHFF/PZZ5+xa9cupkyZgtfrxW63h3VOt9tNXV0dzc3NWK1WCgsLQx5XUlKC2Wzmhhtu6NwvNzeXW2+9Nep91Fr3+u8oHMewTGv9a6XUxRjTSrdjlPvsU/kxpdSlQK3WulQptaBjc4BdAypyaa1f9J2f2bNn676uuS4pKQm6Xvv769dw7vhCFiyYGvD7qqYqOATzz57PguIQ51/z76DMTLjyQSakBxbT6g3u+nr21DdwypJ/JS+A7aH6NBSJVX927txJhq9G9qe//5z6Q/YejugdI8akc961gZcSNzc387WvfY0nn3yS5cuXs2XLFtra2rBYLJSVlXHhhReSkZHBmjVruPjiizvt7ODzzz/nwQcf7La9A7PZTGtrK+vXrwcM2e0lS5Z0ym4/9NBDvPfeeyQmJpKSkuLXjvYqkpISKS0tZceOHZx66qksXLiQNWvWcPXVV6OU6nw63bdvH6tXr6a4uJilS5fyt7/9jRtvvJE777zTT3bbZDIFtbUnzGYzt912G48++minJhGEJzD3k5/8hOXLl5Ofnx+WDRaLBYvFwooVK1i5ciVvvfUWJpOJ9PR0MjIyehTRe+yxx7j44ot58cUXO2W3Q52zqqqKc845J+g+0eyjUqrXf0fhOIaOG/S/AK/4buiRLFuYD1ymlLoEQ8Y7E2MEka2USvCNGkYD1gjO0Wec7R7q7a7wlqqGmkryemDLapj4NYiCUwBw+OY/Jb4wtBm0sttaoxIUc+bM6ZRwFtltg8Emux1JH8MhHMdQqZT6KzAJ+KFSKp0gT/PhoA2l1gcBfCOG72utb1BK/QGjINAq4Bbg/b6eIxKqm4wCPSGXqvqS20alB5ztMviiBJqrYeHPo2abo6ISEhKwnHFG1NqMd4I92ceSwSy7bTJ3jzGI7Pbgk92OpI/hEI47WwI8DMzRWrdiPOXHYsXQA8B9Sqm9GDGHl2Nwjh4JN4chQSWQn5IfdB8q3wZLNkz+l6jZ5qisxDJ5MqaU4LYJQ4PBKrttNhtOqaqqSmS3B7Hsdqg+RoMeHYPW2gOMx4gtAKSEc1w4aK1LtNaX+n7+Qms9R2s9QWt9jda6LRrn6C3WcHIY7FYK0woxB6up4LTBzg8NXaQI6zp3oD0eHFu3Sv7CMGGwym6bTCbmzp3LihUrRHZ7EMtux5xQ0qu+oeGvgReAnb7PucCmno7rj1csZLefXvO5Pm3Fh9rZ7g567I1/uVEv+WhJ8MZLX9P63zO1PrSpz/adjGPXLr1j8hTdGELCV2S3w0Nkt7vTVNeq6w4167Vr1+pFixaFfZzIbodmMPQ7JrLbwDyt9UylVLnPkRxTSg1bkR5ro4P89GSSE4JXWLO2WJlbNDd4IxVvQ95EGNWnhVsB6UxskxHDsGcgZLc9bm9UMp5FdtufwdLv3hKOY2j3rULSAEqpPMAbU6sGEGuTI2R8od3TTl1rXfAVSceq4OBncNFPIAJd+5NxVFZizs4mcWzclMQQ+hGPW5NkMbNgwQKR3RaCxwq6KKg+C7wD5Cul/gNYjyFfMSw50ugIGV842nIUjQ7uGCpXAQqmfiuqdjkqK0mZNi2iIirCCXQMVtAMVbRX4/V4RTxvGNLX3/NQvwn/9DX8O+BHwBMYGkbXaK1XhThuyKK1NuQwskIU6OmowxBIJ8nrNVYjjf8KZIVYytpLPDYbrn37JH8hSlgsFhoaGsQ5+PB4RDxvOKK1pqGhAY/H0+tjQ00ldf6WaK23A9v7YNuQ4nhrO852b49LVSFIctvBDdB4AC74YVTtcmzZCkh8IVqMHj2aw4cP+y1H7C+cTicWS/AHj4HA3e7BYWsntSkJc2LvRg2DsT+RMpz6ZLFYaGlp6fVxoRxDvlLqvmBfaq2f6vXZBjlh5TC0WDEpE4Vphd2/rHgLktLh9EujapejsgKUwnJ2dIT44p3ExETGjRs3IOcuKSlhxowZA3LuYGwtOcymVZ9z68/nk5bdu+XVg7E/kTLc+nTgwIFeHxPKMZiBdALrGA1LwqrDYLeSn5JPoumk2gquFtjxHpxxBSQFzzztC47KSpInFGPuo+aMIITCVu/AnGgiNXPYLjYUekkox1Cttf5pv1kyCDgxYggRY7BbA0th7PoruOxRq7vQgdYaR+UWMr4WefU3QQiErcFJZp4laB0GIf4INaEYd78l1kYHyQkmctOCPzlZ7Ubltm7s/xQsWTB2XlRtcu3fj7epSeILQsyw1TvIHCEyK8IJQjmGvgmbDGGsTU5GhSjQ4/a6qWmtCbwiqboCTpkBYaophoujUhLbhNihtcZWJ45B8CdUBbdj/WnIYMDaGDq5rba1Fo/2dF+R1O6Emh2GY4gyjspKTGlpJBcXR71tQWhrdeNyesgcMTxW4QjRQTJaumA4htDxBQiwVLV2u1HXuWh61G1yVFZimXo2KoDSpiBEiq3eiKvJiEHoijgGHy63l9rmtrDqMHSbSrKWG+9RHjF4W1tp2/25TCMJMaOpThyD0B1xDD5qbE607rkOA9A9+Gwth5RcyI6ujpFz+3bweMQxCDGjucEoTCVTSUJXxDH4CCeHobqlmhEpI0g2n5QEZK00RgtR1jFq9ZX2E8cgxIqmegeW9ESSLH2rlSAMT8Qx+OjIYQilk3TEfiRA4NkBtTEKPJeVk3TaaSTk5ka9bUEAZEWSEBBxDD7CkcOotld3jy8c3QbaA6dEN/CsvV4c5eWkzJwZ1XYFoSu2BqdMIwndEMfg40ijk7y0JCyJgVf/eLWX6pbq7iOGGAWeXVVVeBobSZ0ljkGIDV6PF3uDU0YMQjfEMfjoKYeh3lFPu7c98IqktHzIjJ7MNkBrWRkAKTPEMQixwX68Da9XkyWOQTgJcQw+ws1h6LYiqSPjOcqBZ0dpGeacHJLGnRbVdgWhA5tvRVKGTCUJJyGOgRMFesJZquonoOdqgbpdMQk8t5aXkTJzplRsE2JGR3KbjBiEkxHHANicblpcntBy277ktqK0LiOGo1tBe6Oe8eyur6f9wEFSJfAsxBBbvQNlUqTn9K4GgzD8EcdAmAV67FZyknNITUztsjE2gefO+MLM4VMsRBh82OqdZOQmYzLLbUDwR34jCN8xdM94roD0kZAZQIY7AhylZajkZCxnnhnVdgWhKyK3LQRDHANhFuhpCVCgx1oeo/hCOZazz8KUJBW1hNhhq3eQmSeBZ6E74hgwchiSzCZGpAWea9VaU22v9o8vtDVD/efRF85zOHDu2EHqzFlRbVcQuuJyunE0t5OZLyMGoTviGDBGDEXZFkxBShsecx7D6XH6J7dVbwF01B2DY8tWcLslviDElBPieeIYhO6IY8DnGEJoJHXWYeia3NYZeI7uiiRHWSkAqTPEMQixo7MOQ544BqE74hjoOeu5sw6D34ihwsh2Ti+Iqi2tZeUkT5yAOSsrqu0KQlds9b4RQ77EGITuxL1jcHu8HLU5Q8tt26uBkxxDDALP2uPxCedJfEGILU31DhItZixpiQNtijAIiXvHUNPchreHAj1H7EfISMogIynD2OBsgoa9UZ9Gatu7F6/dLsJ5QsxprneQmZcimfVCQOLeMVSHI7fdcpLcdnWl8R7txLZSI74gUttCrGmqF7ltIThx7xhOVG4LXaDHL7mtI/BcFOUVSWXlJBQUkDgqukqtgtAVrbUxYpClqkIQ4t4xWBuNIFxRVuA/Eq011S3V/slt1grIGgtpeVG1pbWsVITzhJjTanPhbvfKiiQhKP3uGJRSY5RSa5VSO5VS25VS9/i25yql1iil9vjec/rDHmujg+zURNKSA9e8tblstLS3+Ce3WcujHl9or67Gba0W4Twh5nSuSJKpJCEIAzFicAP3a61PB84F7lBKnQGsAD7RWk8EPvF9jjnWRgenBBktQAC5bcdxOF4VQ+E8cQxCbOmU25apJCEI/e4YtNbVWusy38/NwE5gFHA58Jpvt9eAK/rDniNh1mHojDFYK4z3aGc8l5WjUlOxTJkc1XYF4WQ6HEOG6CQJQQg8f9JPKKVOA2YAG4FCrXU1GM5DKRUwc0wptQxYBlBYWEhJSUmfzm232ykpKeFgfQujkhxB2/m77e8A7K/YT525jjEH36UYWP+FHfehvp07ELl/X4d37BjWrV/f5zY6+jRcGG79gcHRpyPbvCSkwPr/+zTitgZDf6LNcOtTn/qjtR6QF5AOlAJX+T43nvT98Z7amDVrlu4ra9eu1TaHS5/6wIf6+ZK9Qff7+caf63PeOEd7vV5jw+qbtH56ap/PGwh3c7PecfoZuvaZX0XUztq1a6Nj0CBhuPVH68HRp3efKNXv/GJzVNoaDP2JNsOtT4H6A2zWIe6tA7IqSSmVCLwDvKm1fte3uUYpVeT7vgiojbUd1U2+FUk9TCWNSh91YqVQDDKeHRWV4PWKcJ7QL0gdBqEnBmJVkgJeBnZqrZ/q8tUHwC2+n28B3o+1LeHkMFhbrCdWJLU0QOPBgI7BuWMHtU8+iXa7e22Ho6wMTCZSpkV3pZMgnIyn3Yu9sU1WJAkhGYgYw3zgJmCrUsoXyeUh4OfA75VSS4GDwDWxNqSnym1NbU0caj7EtPxpxobqwKU8tceD9YEVtO3ZQ+KYMeRce22v7GgtKyN5ymTM6Wm964Ag9JLmY07QIrcthKbfHYPWej0QLIProv60pbrRidmkKMjo/vTU7mnn3rX34vK4+EbxN4yNHSuSiqb57dv4zju07dmDecQI6v7zGTIvWRT2TV63t+PYsoXsq66KqC+CEA6dctviGIQQxHXms7XRwchMC+aTCvRorXl4w8NsrtnMI/MfOTFisJZDbjFYTkhie+wt1D3zK1JmzGDMc8/iaWig4aXfhm2Dc9dudGurCOcJ/YI4BiEc4toxHGl0BJTbfmHLC3yw7wPumH4Hi8YvOvGFtaLbNFLDyy/hqa+ncMUDpEydSuall3LslVdpr64OywZHuSS2Cf2Hrd6JOcFEWpbUExeCE7eOIantONYmB6ecFHj+8IsPebbiWS4rvozvTP3OiS/stWA77OcY2qurOfbKq2Recgkp04xRRcH37gWtqXv66bDsaC0tI3HUKBILCyPvlCD0gK3eQUaeBRWkjK0gQLw6hvVPM3vTXSQ0HfQLPJfWlPKT//sJ54w8h4fnPuwvZhcg47nu6afB6yX/vvs6tyWOGkXuLbfQ9P4HOLZuC2mG1rpTOE8Q+oMmWaoqhEF8OobTvwHay7PmpxiTrgE4YDvAPWvvYVT6KFYuWEmi+aTKVtUVgIKiqQA4tm2n6f0ZdsidAAAOtklEQVQPyL3lZpJG+8tk531nGebcXGoff7wjWS8g7YcP46mrl/iC0G80N0gdBqFn4tMx5BXzyan3MVkd5MLPH6HRcZzvfvxdTJh47qLnyEoOUG/ZWg4jJkJyBlprah9/HHNODnnLlnXb1ZyeTv7dd9G6eTP2Tz4JakZnYZ4Z4hiE2ONsaaet1S0jBqFH4tMxAFsTp/ML97fIOfgX7vngGo62HOWZC59hTOaYwAd0yXi2f/IJrZs2MeKuOzFnZATcPfvqq0maUEzNL3+JdrkC7uMoK8eUkUHyxAlR6ZMghKJTVVUcg9ADcesYGpyaFzyL+FHxNMqcNTw24VtMLwiSeWyrhuZqOGUG2uWi9pdPkFRcHDKRTSUkUPiDH9B+4CDHV60KuE9rWSkpM6ajTHF7GYR+pKMOQ4ZMJQk9ELd3pAaHl4yRa/lv73HucSWzcN2voGFf4J2rTwSej69ajevAAQp+8H1UQuj8wLTzzydt3jzqn30OT1OT33eexkZce/eROnNWNLojCD0iOQxCuMStY6jyboKcNVw54UqWfvMPoEyw6gZoa+6+s7UClAlP6qnUP/ssqXPPJf0rX+nxHEopCh5Yjsdmo/43z/t911puyGuIcJ7QX9jqHSSnJZCcMqBq+8IQIC4dw6ajm6hN/yMZ+nR+PPfHqNxxcM2rUL8b/nQbeL3+B1jLYcRk6l/6HR6bjcIHHgi7LrNl8mSyvnkVx958E9eBA53bHWXlkJhIytlnR7FnghAcW4NT4gtCWMSlY6hrrQNXAV/OvI9Ek29Z6vgFcPGjsOtD+PTJEztrDdZyXMlTOPbmm2RddSWWKVN6db78u+9GJSZS++QJMdnWsjIsZ5yOKUX+UIX+wVYnOQxCeMSlY7hg9Ndp/uJOTs0Z4f/Fud+FqdfB2sdg938b22xWaKmldt0xVEIC+Xff0+vzJRYUkLf0X2n+299oLS3F63Lh3Lq1x/jC8aMtVG2px+XsvZS3IHTF69U0H5McBiE84nKy0drkAMzddZKUgm/8J9TtgneXwb99Ag17aK1LonnTHkbcdSeJhQErjvZI3pIlNP7+D9Q8/gsKVzyAdrkCJrY11rayd3Mte0traDjSAoA5wcSYM3KZMDOf06aOIDk1sdtxghCKlsY2vB4tIwYhLOLTMYSqw5CYAte9CS8ugFWL0eMuoKYii4SCfPKWLOnzOU2pqeTfey/VDz5IzeOPA5Aywwg82+od7C2tZW9pLXUHjeB3UXEW5103kZyRaezfWs8X5XXs31KPyawYPSWX4pn5jJ+WjyVdnEQscLd78Lp1p0C8UgqlAAXK+IfON5MKO+YUDbRXY29s4/jRFppqHSSnJpBVkEp2QUrQhwZbnaxIEsInzh1DkGF19hi49jX43eXYyg7jbMih6Gffw5SaGtF5sy6/jGOv/w5n5Ra8xWexrbyFvaVV1FTZACg4LZP5V0+geGYBGbknbBtzei5fvnoiNftt7CuvY19ZLWtfb6Dkzd2MmpRN8cwCxk/Pj8i2eEJrjaO5neZjTuzHnDT7XvZjbcb7cSeO5vaw2zOZFKlZSaRmJZOWlURadjJpWcmkZSf53o3PyWm9+3Nzuzw01rZy/Kjxaqxp5fjRFhprWnG7vAGPsaQlklWQQnZBKlkFKZ0/1x+xA8hUkhAWcekY9O/XseJ4Lh/d/z+YPW2Yva4u7y7M3jYSPC7MnmWYXK3kFdcw6bLLIj5vu8uL7cr72Ja0g6bsCfDHveSPzWDulcVMmFUQ8mlOmRQjx2cxcnwW864qpv6Qnb1ltewrq2XdW7tZ9/ZuTAmw94O/o0wKk0kZT7Im48allPHZZDaebLVXo7Ux96w9RgFwr1ejvR3vxj6xQCnfU7bvZQpiZ0uLF2vJP8Kys6c2TWZju7vdS8vxNjxu/xtrQrKZjFwLGbkWCk7NID3HgjnRBBq08Y/x/6aN/zfjs/Gz2+Wl1dZGS5OLpjoH1r2NtLV0jwuZEhQozd4//92vr512drlu7U4PzcednedFQWaehezCNEZNyiFnZCo5I1PJyk/F2dpOU62DploHjXWtNNU6OPL5cXZvPOr//25SpOeKYxB6Ji4dw+x5xVT+bSspadm4tRm3Nx23zsTtNePSJtxek2+7CY2JL4DtD22geFYBk84ppHBcZthTB9qrse5tZNdn1ewtq8Xt8pJ5yjhmzcphylcnk13Y+1GIUor8sRnkj83g3MvHc8zaQlVlHXt2VTHqlJHGTVMbN8+Tb6AdDkGZOMmBKEwKlNlkvPu2Ba21FwleTtzcO+z0dPx8ws72uhZyC9LCs1P33KbXqzGZFRnT80nPtZCRm0xGnoX0HAvJqQlRnQ5yt3tobXLR0uSipbGNlqY2Wpva2F91sPs1CmCnOcHE6SOLyC5MJWdkGtkFKSQkmQOeKy07mbxT0rvb4PLQVO9zGLWtpGUmYTbH5XoToZfEpWOYcOU8Due4WLBgQY/7trd5OLCtgb2ba9jxqZWtaw+TkWth4jkFTJhdyIjR6QFvKM3HnOzaUM2uDdXY6p0kWsxMmjOS0+cV9cqx9IRSirxR6eSNSseeeoDzF0yKSruDgZKSEhYsGJp5HgmJZjJHpHQbBbaVHO63a5SQZCbvlPSATkMQQhGXjqE3JCabmTCrgAmzCnA53FRV1vH5ploq1hyi7H8OkjMylQmzC5k424gLfFFRx87Pqjm8+zhoGDU5hznfGM/4GfkkBnniEwRBGEyIY+gFSSkJTD63iMnnFuGwu9hXVsfezTVs+ksVmz6swpxowtPuJSPPwjmLxjHl3JGyCkQQhCGHOIY+kpKexFnnj+Ks80dhP97GvrJaGmtbKZ6Rz6hJOVI6URCEIYs4hiiQnpPMtIuC1HEQBEEYYsgSBUEQBMEPcQyCIAiCH+IYBEEQBD/EMQiCIAh+iGMQBEEQ/BDHIAiCIPghjkEQBEHwQxyDIAiC4IfSOjbSyv2BUqoOONDHw0cA9VE0ZzAw3Po03PoDw69Pw60/MPz6FKg/p2qtgxZxGdKOIRKUUpu11rMH2o5oMtz6NNz6A8OvT8OtPzD8+tSX/shUkiAIguCHOAZBEATBj3h2DC8OtAExYLj1abj1B4Zfn4Zbf2D49anX/YnbGIMgCIIQmHgeMQiCIAgBEMcgCIIg+BGXjkEptVAptVsptVcptWKg7YkUpdR+pdRWpVSFUmrzQNvTF5RS/6WUqlVKbeuyLVcptUYptcf3njOQNvaGIP15WCl1xHedKpRSlwykjb1FKTVGKbVWKbVTKbVdKXWPb/uQvE4h+jNkr5NSyqKU+qdSqtLXp//wbR+nlNrou0arlVJJIduJtxiDUsoMfA58DTgMbAKu11rvGFDDIkAptR+YrbUeskk5SqnzATvwO631Wb5tvwCOaa1/7nPgOVrrBwbSznAJ0p+HAbvW+omBtK2vKKWKgCKtdZlSKgMoBa4AbmUIXqcQ/bmWIXqdlFIKSNNa25VSicB64B7gPuBdrfUqpdTzQKXW+jfB2onHEcMcYK/W+guttQtYBVw+wDbFPVrrvwPHTtp8OfCa7+fXMP5ohwRB+jOk0VpXa63LfD83AzuBUQzR6xSiP0MWbWD3fUz0vTRwIfBH3/Yer1E8OoZRwKEunw8zxH8ZMC7835RSpUqpZQNtTBQp1FpXg/FHDBQMsD3R4E6l1BbfVNOQmHIJhFLqNGAGsJFhcJ1O6g8M4euklDIrpSqAWmANsA9o1Fq7fbv0eM+LR8egAmwb6vNp87XWM4F/Ae7wTWMIg4/fAMXAdKAaeHJgzekbSql04B3gXq21baDtiZQA/RnS10lr7dFaTwdGY8yQnB5ot1BtxKNjOAyM6fJ5NGAdIFuigtba6nuvBf6E8cswHKjxzQN3zAfXDrA9EaG1rvH90XqB3zIEr5Nv3vod4E2t9bu+zUP2OgXqz3C4TgBa60agBDgXyFZKJfi+6vGeF4+OYRMw0RelTwK+BXwwwDb1GaVUmi9whlIqDbgY2Bb6qCHDB8Atvp9vAd4fQFsipuPm6eNKhth18gU2XwZ2aq2f6vLVkLxOwfozlK+TUipfKZXt+zkF+CpG7GQtcLVvtx6vUdytSgLwLT97GjAD/6W1fmyATeozSqnxGKMEgATgraHYH6XU28ACDIngGuDfgfeA3wNjgYPANVrrIRHQDdKfBRjTExrYD3ynY25+KKCU+jLwKbAV8Po2P4QxLz/krlOI/lzPEL1OSqmpGMFlM8aD/++11j/13SdWAblAOXCj1rotaDvx6BgEQRCE4MTjVJIgCIIQAnEMgiAIgh/iGARBEAQ/xDEIgiAIfohjEARBEPwQxyAI/YhSaoFS6sOBtkMQQiGOQRAEQfBDHIMgBEApdaNP175CKfWCT5jMrpR6UilVppT6RCmV79t3ulLqHz7RtT91iK4ppSYopT72aeOXKaWKfc2nK6X+qJTapZR605eBi1Lq50qpHb52hpzkszB8EMcgCCehlDoduA5DnHA64AFuANKAMp9g4TqMbGaA3wEPaK2nYmTRdmx/E3hWaz0NmIchyAaGiue9wBnAeGC+UioXQ37hTF87j8a2l4IQHHEMgtCdi4BZwCaffPFFGDdwL7Dat88bwJeVUllAttZ6nW/7a8D5Pv2qUVrrPwForZ1a61bfPv/UWh/2ibRVAKcBNsAJvKSUugro2FcQ+h1xDILQHQW8prWe7ntN1lo/HGC/UHoygeTdO+iqUeMBEnxa+XMwlD6vAD7qpc2CEDXEMQhCdz4BrlZKFUBnTeNTMf5eOhQqFwPrtdZNwHGl1Hm+7TcB63y6/oeVUlf42khWSqUGO6GvJkCW1vqvGNNM02PRMUEIh4SedxGE+EJrvUMp9SOMqngmoB24A2gBzlRKlQJNGHEIMGSMn/fd+L8Alvi23wS8oJT6qa+Na0KcNgN4XyllwRhtfC/K3RKEsBF1VUEIE6WUXWudPtB2CEKskakkQRAEwQ8ZMQiCIAh+yIhBEARB8EMcgyAIguCHOAZBEATBD3EMgiAIgh/iGARBEAQ//h/zGXpztnl+0AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "B_sel = 0\n",
    "\n",
    "# plt.plot(acc_test_arr_v1[0,B_sel,0,0:30],label='N=2, B='+str(B_array[B_sel]))\n",
    "plt.plot(acc_test_arr_K4_G1[2,B_sel,0,0:30],label='w/o Grouping, K=4, N=4, G=1' )\n",
    "plt.plot(acc_test_arr_K4_G1[3,B_sel,0,0:30],label='w/o Grouping, K=4, N=8, G=1' )\n",
    "# plt.plot(acc_test_arr_v1[2,B_sel,0,0:30],label='N=6, B='+str(B_array[B_sel]))\n",
    "# plt.plot(acc_test_arr_v1[3,B_sel,0,0:30],label='N=8, B='+str(B_array[B_sel]))\n",
    "plt.plot(acc_test_arr_K4_G2_N4[0,0:30],label='w/ Grouping,   K=4, N=4, G=2')\n",
    "plt.plot(acc_test_arr_K4_G2_N8[0,0:30],label='w/ Grouping,   K=4, N=8, G=2')\n",
    "plt.plot(acc_test_arr_K4_G4_N8[0,0:30],label='w/ Grouping,   K=4, N=8, G=4')\n",
    "# plt.plot(acc_test_arr_G3[0,0:30],label='Grouping, N=6, G=3, B='+str(B_array[B_sel]))\n",
    "\n",
    "# plt.plot(acc_test_arr_v1[4,B_sel,0,0:30],label='N=7, B='+str(B_array[B_sel]))\n",
    "# plt.plot(acc_test_arr_v1[5,B_sel,0,0:30],label='N=8, B='+str(B_array[B_sel]))\n",
    "\n",
    "# plt.plot(plot_acc_v2[2,sigma_sel,:],label='T=10')\n",
    "# plt.plot(plot_acc_v2[3,sigma_sel,:],label='T=11')\n",
    "\n",
    "# plt.plot(acc_test_arr_uncoded,'r',label='uncoded')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "# plt.title(title_name)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Test accuracy(%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Without model encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z_array: [-0.94  -0.73  -0.534 -0.125  0.125  0.534  0.73   0.94 ]\n",
      "0.4486236179368535\n",
      "0.49983046821646326\n",
      "0.48963480280841937\n",
      "0.4857187736123886\n",
      "0.48571877361238697\n",
      "0.4896348028084205\n",
      "0.4998304682164621\n",
      "0.4486236179368556\n",
      "@BACC_Enc: N,K,T, m_i= 8 4 5 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 8 4 5 15000 \n",
      "\n",
      "(T, sigma)= 5 1 )  0 -th Trial!!\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   1, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 2.2877 \n",
      "Accuracy: 3944/10000 (39.44%)\n",
      "\n",
      "Round   2, Average loss 2.288 Test accuracy 39.440\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 2.0699 \n",
      "Accuracy: 7961/10000 (79.61%)\n",
      "\n",
      "Round   3, Average loss 2.070 Test accuracy 79.610\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.7928 \n",
      "Accuracy: 9264/10000 (92.64%)\n",
      "\n",
      "Round   4, Average loss 1.793 Test accuracy 92.640\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.7944 \n",
      "Accuracy: 9352/10000 (93.52%)\n",
      "\n",
      "Round   5, Average loss 1.794 Test accuracy 93.520\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.7905 \n",
      "Accuracy: 9338/10000 (93.38%)\n",
      "\n",
      "Round   6, Average loss 1.791 Test accuracy 93.380\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.7860 \n",
      "Accuracy: 9381/10000 (93.81%)\n",
      "\n",
      "Round   7, Average loss 1.786 Test accuracy 93.810\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.7889 \n",
      "Accuracy: 9403/10000 (94.03%)\n",
      "\n",
      "Round   8, Average loss 1.789 Test accuracy 94.030\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.7936 \n",
      "Accuracy: 9412/10000 (94.12%)\n",
      "\n",
      "Round   9, Average loss 1.794 Test accuracy 94.120\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.8002 \n",
      "Accuracy: 9412/10000 (94.12%)\n",
      "\n",
      "Round  10, Average loss 1.800 Test accuracy 94.120\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.7667 \n",
      "Accuracy: 9398/10000 (93.98%)\n",
      "\n",
      "Round  11, Average loss 1.767 Test accuracy 93.980\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.8284 \n",
      "Accuracy: 9358/10000 (93.58%)\n",
      "\n",
      "Round  12, Average loss 1.828 Test accuracy 93.580\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.8112 \n",
      "Accuracy: 9403/10000 (94.03%)\n",
      "\n",
      "Round  13, Average loss 1.811 Test accuracy 94.030\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.7871 \n",
      "Accuracy: 9352/10000 (93.52%)\n",
      "\n",
      "Round  14, Average loss 1.787 Test accuracy 93.520\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.9026 \n",
      "Accuracy: 9219/10000 (92.19%)\n",
      "\n",
      "Round  15, Average loss 1.903 Test accuracy 92.190\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.8916 \n",
      "Accuracy: 9286/10000 (92.86%)\n",
      "\n",
      "Round  16, Average loss 1.892 Test accuracy 92.860\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.7630 \n",
      "Accuracy: 9417/10000 (94.17%)\n",
      "\n",
      "Round  17, Average loss 1.763 Test accuracy 94.170\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.7902 \n",
      "Accuracy: 9397/10000 (93.97%)\n",
      "\n",
      "Round  18, Average loss 1.790 Test accuracy 93.970\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.7999 \n",
      "Accuracy: 9450/10000 (94.50%)\n",
      "\n",
      "Round  19, Average loss 1.800 Test accuracy 94.500\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.7676 \n",
      "Accuracy: 9429/10000 (94.29%)\n",
      "\n",
      "Round  20, Average loss 1.768 Test accuracy 94.290\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.7842 \n",
      "Accuracy: 9435/10000 (94.35%)\n",
      "\n",
      "Round  21, Average loss 1.784 Test accuracy 94.350\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.7863 \n",
      "Accuracy: 9451/10000 (94.51%)\n",
      "\n",
      "Round  22, Average loss 1.786 Test accuracy 94.510\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.8083 \n",
      "Accuracy: 9404/10000 (94.04%)\n",
      "\n",
      "Round  23, Average loss 1.808 Test accuracy 94.040\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.8002 \n",
      "Accuracy: 9399/10000 (93.99%)\n",
      "\n",
      "Round  24, Average loss 1.800 Test accuracy 93.990\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.8325 \n",
      "Accuracy: 9422/10000 (94.22%)\n",
      "\n",
      "Round  25, Average loss 1.832 Test accuracy 94.220\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.7755 \n",
      "Accuracy: 9407/10000 (94.07%)\n",
      "\n",
      "Round  26, Average loss 1.776 Test accuracy 94.070\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.7625 \n",
      "Accuracy: 9430/10000 (94.30%)\n",
      "\n",
      "Round  27, Average loss 1.763 Test accuracy 94.300\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.7824 \n",
      "Accuracy: 9419/10000 (94.19%)\n",
      "\n",
      "Round  28, Average loss 1.782 Test accuracy 94.190\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.7719 \n",
      "Accuracy: 9339/10000 (93.39%)\n",
      "\n",
      "Round  29, Average loss 1.772 Test accuracy 93.390\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "K = 4\n",
    "T = 5\n",
    "sigma = 1\n",
    "Noise_Alloc = [0,2,4,6,8]\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "\n",
    "N_array = [8]\n",
    "B_array = [0.5]\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 30\n",
    "\n",
    "\n",
    "\n",
    "loss_test_arr_K4_G1_v2 = np.zeros((len(N_array),len(B_array),N_trials,N_epochs))\n",
    "acc_test_arr_K4_G1_v2  = np.zeros((len(N_array),len(B_array),N_trials,N_epochs))\n",
    "\n",
    "for N_idx in range(len(N_array)):\n",
    "    \n",
    "    N = N_array[N_idx]\n",
    "    \n",
    "    \n",
    "           \n",
    "        \n",
    "    # print(\"alpha_array: \",alpha_array,'\\n')\n",
    "    \n",
    "    \n",
    "    for B_idx in range(len(B_array)):\n",
    "        \n",
    "        B = B_array[B_idx]\n",
    "        z_array = []\n",
    "#         while(len(z_array)<N):\n",
    "#             z_tmp = np.random.uniform(-1,1,1)\n",
    "#             MIS_tmp = MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_tmp], 1,sigma)\n",
    "#             if MIS_tmp < B and MIS_tmp > 0.1:\n",
    "#                 z_array.append(z_tmp[0])\n",
    "#         \n",
    "#         z_array = np.sort(z_array)\n",
    "\n",
    "        if N==2:\n",
    "            z_array = np.array([-0.81, 0.81])\n",
    "        elif N ==4:\n",
    "            z_array = np.array([-0.81, -0.22, 0.22, 0.81])\n",
    "        elif N ==5:\n",
    "            z_array = np.array([-0.81, -0.22, 0, 0.22, 0.81])\n",
    "        elif N ==6:\n",
    "            z_array = np.array([-0.9, -0.81, -0.22, 0.22, 0.81, 0.9])\n",
    "        elif N ==7:\n",
    "            z_array = np.array([-0.9, -0.82, -0.21, 0, 0.21, 0.82, 0.9])\n",
    "        else:\n",
    "            z_array = np.array([-0.94, -0.73, -0.534, -0.125, 0.125, 0.534, 0.73, 0.94])\n",
    "#             z_array = np.array([-0.9, -0.81, -0.22, -0.20, 0.20, 0.22, 0.81, 0.9])\n",
    "            \n",
    "        print('z_array:',z_array)\n",
    "        for j in range(len(z_array)):\n",
    "            print(MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_array[j]], 1,sigma))\n",
    "        \n",
    "        \n",
    "        _Noise_label = np.ones((15000*T,10)) * 0.1\n",
    "\n",
    "        X_tilde,a,b = BACC_Enc_Data_v3(encoding_input_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde,a,b = BACC_Enc_Data_v3(encoding_label_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "\n",
    "        m = N # m is the number of received result @ master\n",
    "    #     print('number of results:',m)\n",
    "\n",
    "        for trial_idx in range(N_trials):\n",
    "            print('(T, sigma)=',T,sigma,') ',trial_idx,'-th Trial!!')\n",
    "\n",
    "            net_glob = CNNMnist2(args=args)\n",
    "            net_glob.cuda()\n",
    "            net_glob.train()\n",
    "\n",
    "            # copy weights\n",
    "            w_glob = net_glob.state_dict()\n",
    "\n",
    "            for iter in range(N_epochs): #args.epochs\n",
    "                w_locals, loss_locals = [], []\n",
    "                idxs_users = np.random.choice(range(N), m, replace=False)\n",
    "                idxs_users = np.sort(idxs_users)\n",
    "                print('selected users:',idxs_users)\n",
    "                \n",
    "                coded_net = BACC_Enc_Model_withNoise_v3(net_glob.cuda(), N, K, T, 0.01, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "                dec_z_array = []\n",
    "                for idx in idxs_users: #for idx in range(N):\n",
    "            #         print(idx)\n",
    "                    local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[idx,:,:], label=y_tilde[idx,:,:])\n",
    "#                     w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "                    w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                    w_locals.append(copy.deepcopy(w))\n",
    "                    loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                    dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "                # update global weights\n",
    "                #w_glob = FedAvg(w_locals)\n",
    "                w_glob = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "\n",
    "                # copy weight to net_glob\n",
    "                net_glob.load_state_dict(w_glob)\n",
    "\n",
    "                # print loss\n",
    "            #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "            #     loss_train_arr.append(loss_train)\n",
    "\n",
    "                acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "                acc_test_arr_K4_G1_v2[N_idx][B_idx][trial_idx][iter] = acc_test\n",
    "                loss_test_arr_K4_G1_v2[N_idx][B_idx][trial_idx][iter] = loss_test\n",
    "                if iter % 1 ==0:\n",
    "                    print('Round {:3d}, Average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_test, acc_test))\n",
    "                #print(loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2 3\n",
      "##########################################\n",
      "###### 0 -th Trial!! ###########\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.2202 \n",
      "Accuracy: 4077/10000 (40.77%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.7585 \n",
      "Accuracy: 8011/10000 (80.11%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2700 \n",
      "Accuracy: 9568/10000 (95.68%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1997 \n",
      "Accuracy: 9657/10000 (96.57%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2082 \n",
      "Accuracy: 9679/10000 (96.79%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1947 \n",
      "Accuracy: 9698/10000 (96.98%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2030 \n",
      "Accuracy: 9723/10000 (97.23%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1999 \n",
      "Accuracy: 9734/10000 (97.34%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2276 \n",
      "Accuracy: 9734/10000 (97.34%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2207 \n",
      "Accuracy: 9753/10000 (97.53%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2127 \n",
      "Accuracy: 9746/10000 (97.46%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2390 \n",
      "Accuracy: 9750/10000 (97.50%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2313 \n",
      "Accuracy: 9753/10000 (97.53%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2331 \n",
      "Accuracy: 9752/10000 (97.52%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2413 \n",
      "Accuracy: 9751/10000 (97.51%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2361 \n",
      "Accuracy: 9747/10000 (97.47%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2493 \n",
      "Accuracy: 9745/10000 (97.45%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2544 \n",
      "Accuracy: 9746/10000 (97.46%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2613 \n",
      "Accuracy: 9750/10000 (97.50%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2671 \n",
      "Accuracy: 9745/10000 (97.45%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2708 \n",
      "Accuracy: 9740/10000 (97.40%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2621 \n",
      "Accuracy: 9755/10000 (97.55%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2776 \n",
      "Accuracy: 9752/10000 (97.52%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2759 \n",
      "Accuracy: 9750/10000 (97.50%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2884 \n",
      "Accuracy: 9743/10000 (97.43%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2711 \n",
      "Accuracy: 9742/10000 (97.42%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2798 \n",
      "Accuracy: 9745/10000 (97.45%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2802 \n",
      "Accuracy: 9742/10000 (97.42%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2836 \n",
      "Accuracy: 9732/10000 (97.32%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2929 \n",
      "Accuracy: 9737/10000 (97.37%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "G = 2\n",
    "N = 4 # N should be divisible by G\n",
    "K = 4 # N should be divisible by G\n",
    "\n",
    "size_per_group = int(60000/G)\n",
    "\n",
    "X_group = np.reshape(encoding_input_array_np, (G,size_per_group,28*28))\n",
    "y_group = np.reshape(encoding_label_array_np, (G,size_per_group,args.num_classes)) \n",
    "\n",
    "N_i = int(N/G) # = 2\n",
    "K_i = int(K/G) # = 2\n",
    "T = 3\n",
    "sigma = 1\n",
    "Noise_Alloc = [0,2,4]\n",
    "m = N_i # number of selected workers (if there is no straggler, m=N_i)\n",
    "\n",
    "print(N_i,K_i,T)\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K_i+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K_i+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K_i+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "z_array = np.array([-0.81, 0.81])\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 30\n",
    "\n",
    "loss_test_arr_G2_N4_v2 = np.zeros((N_trials,N_epochs))\n",
    "acc_test_arr_G2_N4_v2  = np.zeros((N_trials,N_epochs))\n",
    "\n",
    "for trial_idx in range(N_trials):\n",
    "    \n",
    "    print('##########################################')\n",
    "    print('######',trial_idx,'-th Trial!! ###########')\n",
    "    \n",
    "    net_glob = CNNMnist2(args=args)\n",
    "    net_glob.cuda()\n",
    "    net_glob.train()\n",
    "    \n",
    "    # copy weights\n",
    "    w_glob = net_glob.state_dict()\n",
    "    \n",
    "    X_tilde = np.empty((N,Size_submatrices,28*28))\n",
    "    y_tilde = np.empty((N,Size_submatrices,10))\n",
    "    \n",
    "    for G_idx in range(G):\n",
    "        \n",
    "        _Noise_label = np.ones((size_per_group*T,10)) * 0.1\n",
    "        \n",
    "        X_tilde_tmp,a,b = BACC_Enc_Data_v3(X_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde_tmp,a,b = BACC_Enc_Data_v3(y_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "        \n",
    "        stt_pos = G_idx * N_i\n",
    "        end_pos = (G_idx+1) * N_i\n",
    "        \n",
    "        X_tilde[stt_pos:end_pos,:,:] = X_tilde_tmp\n",
    "        y_tilde[stt_pos:end_pos,:,:] = y_tilde_tmp\n",
    "        \n",
    "   \n",
    "\n",
    "    for iter in range(N_epochs): #args.epochs\n",
    "        \n",
    "        w_group_array = []\n",
    "        for G_idx in range(G):\n",
    "            w_locals, loss_locals = [], []\n",
    "            idxs_users = np.random.choice(range(N_i), m, replace=False)\n",
    "            idxs_users = np.sort(idxs_users)\n",
    "            print('selected users:',idxs_users)\n",
    "\n",
    "#             coded_net = BACC_Enc_Model_withNoise_v3(net_glob.cuda(), N_i, K_i, T, 0.01, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "            dec_z_array = []\n",
    "            for idx in idxs_users: #for idx in range(N):\n",
    "        #         print(idx)\n",
    "                local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[G_idx*N_i+idx,:,:], label=y_tilde[G_idx*N_i+idx,:,:])\n",
    "#                 w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "                w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                w_locals.append(copy.deepcopy(w))\n",
    "                loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "            # update global weights\n",
    "            #w_glob = FedAvg(w_locals)\n",
    "            w_group = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "            \n",
    "            w_group_array.append(copy.deepcopy(w_group))\n",
    "        \n",
    "        w_glob = copy.deepcopy(w_group_array[0])\n",
    "        for k in w_glob.keys():\n",
    "            for G_idx in range(1,G):\n",
    "                w_glob[k] += w_group_array[G_idx][k]\n",
    "            w_glob[k] = torch.div(w_glob[k], len(w_group_array))\n",
    "        \n",
    "        # copy weight to net_glob\n",
    "        net_glob.load_state_dict(w_glob)\n",
    "\n",
    "        # print loss\n",
    "    #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "    #     loss_train_arr.append(loss_train)\n",
    "\n",
    "        acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "        acc_test_arr_G2_N4_v2[trial_idx][iter] = acc_test\n",
    "        loss_test_arr_G2_N4_v2[trial_idx][iter] = loss_test\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2 3\n",
      "##########################################\n",
      "###### 0 -th Trial!! ###########\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 15000 \n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.3025 \n",
      "Accuracy: 974/10000 (9.74%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.2943 \n",
      "Accuracy: 2544/10000 (25.44%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.0631 \n",
      "Accuracy: 6656/10000 (66.56%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9568 \n",
      "Accuracy: 8542/10000 (85.42%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8062 \n",
      "Accuracy: 9612/10000 (96.12%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.7800 \n",
      "Accuracy: 9607/10000 (96.07%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8027 \n",
      "Accuracy: 9622/10000 (96.22%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.7711 \n",
      "Accuracy: 9628/10000 (96.28%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.7664 \n",
      "Accuracy: 9609/10000 (96.09%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.7680 \n",
      "Accuracy: 9612/10000 (96.12%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.7728 \n",
      "Accuracy: 9601/10000 (96.01%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8523 \n",
      "Accuracy: 9533/10000 (95.33%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.7927 \n",
      "Accuracy: 9604/10000 (96.04%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8505 \n",
      "Accuracy: 9528/10000 (95.28%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8085 \n",
      "Accuracy: 9595/10000 (95.95%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8481 \n",
      "Accuracy: 9562/10000 (95.62%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8205 \n",
      "Accuracy: 9600/10000 (96.00%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8440 \n",
      "Accuracy: 9582/10000 (95.82%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8253 \n",
      "Accuracy: 9595/10000 (95.95%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8130 \n",
      "Accuracy: 9581/10000 (95.81%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8652 \n",
      "Accuracy: 9552/10000 (95.52%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8122 \n",
      "Accuracy: 9614/10000 (96.14%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8160 \n",
      "Accuracy: 9585/10000 (95.85%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8972 \n",
      "Accuracy: 9468/10000 (94.68%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8030 \n",
      "Accuracy: 9609/10000 (96.09%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8643 \n",
      "Accuracy: 9546/10000 (95.46%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8635 \n",
      "Accuracy: 9497/10000 (94.97%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8276 \n",
      "Accuracy: 9598/10000 (95.98%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8942 \n",
      "Accuracy: 9453/10000 (94.53%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "N = 8\n",
    "K = 4\n",
    "G = 2\n",
    "\n",
    "N_i = int(N/G) # = 2\n",
    "K_i = int(K/G) # = 2\n",
    "T = 3\n",
    "sigma = 1\n",
    "Noise_Alloc = [0,2,4]\n",
    "m = N_i # number of selected workers (if there is no straggler, m=N_i)\n",
    "\n",
    "print(N_i,K_i,T)\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K_i+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K_i+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K_i+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "z_array = np.array([-0.81, -0.22, 0.22, 0.81])\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 30\n",
    "\n",
    "loss_test_arr_K4_G2_N8_v2 = np.zeros((N_trials,N_epochs))\n",
    "acc_test_arr_K4_G2_N8_v2  = np.zeros((N_trials,N_epochs))\n",
    "\n",
    "for trial_idx in range(N_trials):\n",
    "    \n",
    "    print('##########################################')\n",
    "    print('######',trial_idx,'-th Trial!! ###########')\n",
    "    \n",
    "    net_glob = CNNMnist2(args=args)\n",
    "    net_glob.cuda()\n",
    "    net_glob.train()\n",
    "    \n",
    "    # copy weights\n",
    "    w_glob = net_glob.state_dict()\n",
    "    \n",
    "    X_tilde = np.empty((N,Size_submatrices,28*28))\n",
    "    y_tilde = np.empty((N,Size_submatrices,10))\n",
    "    \n",
    "    for G_idx in range(G):\n",
    "        \n",
    "        _Noise_label = np.ones((size_per_group*T,10)) * 0.1\n",
    "        \n",
    "        X_tilde_tmp,a,b = BACC_Enc_Data_v3(X_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde_tmp,a,b = BACC_Enc_Data_v3(y_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "        \n",
    "        stt_pos = G_idx * N_i\n",
    "        end_pos = (G_idx+1) * N_i\n",
    "        \n",
    "        X_tilde[stt_pos:end_pos,:,:] = X_tilde_tmp\n",
    "        y_tilde[stt_pos:end_pos,:,:] = y_tilde_tmp\n",
    "        \n",
    "   \n",
    "\n",
    "    for iter in range(N_epochs): #args.epochs\n",
    "        \n",
    "        w_group_array = []\n",
    "        for G_idx in range(G):\n",
    "            w_locals, loss_locals = [], []\n",
    "            idxs_users = np.random.choice(range(N_i), m, replace=False)\n",
    "            idxs_users = np.sort(idxs_users)\n",
    "            print('selected users:',idxs_users)\n",
    "\n",
    "#             coded_net = BACC_Enc_Model_withNoise_v3(net_glob.cuda(), N_i, K_i, T, 0.01, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "            dec_z_array = []\n",
    "            for idx in idxs_users: #for idx in range(N):\n",
    "        #         print(idx)\n",
    "                local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[G_idx*N_i+idx,:,:], label=y_tilde[G_idx*N_i+idx,:,:])\n",
    "#                 w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "                w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                w_locals.append(copy.deepcopy(w))\n",
    "                loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "            # update global weights\n",
    "            #w_glob = FedAvg(w_locals)\n",
    "            w_group = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "            \n",
    "            w_group_array.append(copy.deepcopy(w_group))\n",
    "        \n",
    "        w_glob = copy.deepcopy(w_group_array[0])\n",
    "        for k in w_glob.keys():\n",
    "            for G_idx in range(1,G):\n",
    "                w_glob[k] += w_group_array[G_idx][k]\n",
    "            w_glob[k] = torch.div(w_glob[k], len(w_group_array))\n",
    "        \n",
    "        # copy weight to net_glob\n",
    "        net_glob.load_state_dict(w_glob)\n",
    "\n",
    "        # print loss\n",
    "    #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "    #     loss_train_arr.append(loss_train)\n",
    "\n",
    "        acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "        acc_test_arr_K4_G2_N8_v2[trial_idx][iter] = acc_test\n",
    "        loss_test_arr_K4_G2_N8_v2[trial_idx][iter] = loss_test\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1 2\n",
      "##########################################\n",
      "###### 0 -th Trial!! ###########\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.2967 \n",
      "Accuracy: 4766/10000 (47.66%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.9995 \n",
      "Accuracy: 7548/10000 (75.48%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.8282 \n",
      "Accuracy: 9493/10000 (94.93%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7811 \n",
      "Accuracy: 9636/10000 (96.36%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7788 \n",
      "Accuracy: 9639/10000 (96.39%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7531 \n",
      "Accuracy: 9632/10000 (96.32%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7798 \n",
      "Accuracy: 9646/10000 (96.46%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.8117 \n",
      "Accuracy: 9660/10000 (96.60%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7924 \n",
      "Accuracy: 9684/10000 (96.84%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7672 \n",
      "Accuracy: 9678/10000 (96.78%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7768 \n",
      "Accuracy: 9657/10000 (96.57%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.8813 \n",
      "Accuracy: 9592/10000 (95.92%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.8124 \n",
      "Accuracy: 9658/10000 (96.58%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7945 \n",
      "Accuracy: 9658/10000 (96.58%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.8279 \n",
      "Accuracy: 9681/10000 (96.81%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.8005 \n",
      "Accuracy: 9667/10000 (96.67%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7784 \n",
      "Accuracy: 9642/10000 (96.42%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.8515 \n",
      "Accuracy: 9594/10000 (95.94%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7832 \n",
      "Accuracy: 9662/10000 (96.62%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7906 \n",
      "Accuracy: 9662/10000 (96.62%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.8648 \n",
      "Accuracy: 9641/10000 (96.41%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7804 \n",
      "Accuracy: 9661/10000 (96.61%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.8232 \n",
      "Accuracy: 9613/10000 (96.13%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7817 \n",
      "Accuracy: 9664/10000 (96.64%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.8109 \n",
      "Accuracy: 9643/10000 (96.43%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7931 \n",
      "Accuracy: 9662/10000 (96.62%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.8087 \n",
      "Accuracy: 9677/10000 (96.77%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.8037 \n",
      "Accuracy: 9701/10000 (97.01%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.8532 \n",
      "Accuracy: 9631/10000 (96.31%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "N = 8\n",
    "K = 4\n",
    "G = 4\n",
    "\n",
    "size_per_group = int(60000/G)\n",
    "\n",
    "X_group = np.reshape(encoding_input_array_np, (G,size_per_group,28*28))\n",
    "y_group = np.reshape(encoding_label_array_np, (G,size_per_group,args.num_classes)) \n",
    "\n",
    "\n",
    "N_i = int(N/G) # = 2\n",
    "K_i = int(K/G) # = 2\n",
    "T = 2\n",
    "sigma = 1\n",
    "Noise_Alloc = [0,2]\n",
    "m = N_i # number of selected workers (if there is no straggler, m=N_i)\n",
    "\n",
    "print(N_i,K_i,T)\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K_i+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K_i+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K_i+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "z_array = np.array([-0.521, 0.521])\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 30\n",
    "\n",
    "loss_test_arr_K4_G4_N8_v2 = np.zeros((N_trials,N_epochs))\n",
    "acc_test_arr_K4_G4_N8_v2  = np.zeros((N_trials,N_epochs))\n",
    "\n",
    "for trial_idx in range(N_trials):\n",
    "    \n",
    "    print('##########################################')\n",
    "    print('######',trial_idx,'-th Trial!! ###########')\n",
    "    \n",
    "    net_glob = CNNMnist2(args=args)\n",
    "    net_glob.cuda()\n",
    "    net_glob.train()\n",
    "    \n",
    "    # copy weights\n",
    "    w_glob = net_glob.state_dict()\n",
    "    \n",
    "    X_tilde = np.empty((N,Size_submatrices,28*28))\n",
    "    y_tilde = np.empty((N,Size_submatrices,10))\n",
    "    \n",
    "    for G_idx in range(G):\n",
    "        \n",
    "        _Noise_label = np.ones((size_per_group*T,10)) * 0.1\n",
    "        \n",
    "        X_tilde_tmp,a,b = BACC_Enc_Data_v3(X_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde_tmp,a,b = BACC_Enc_Data_v3(y_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "        \n",
    "        stt_pos = G_idx * N_i\n",
    "        end_pos = (G_idx+1) * N_i\n",
    "        \n",
    "        X_tilde[stt_pos:end_pos,:,:] = X_tilde_tmp\n",
    "        y_tilde[stt_pos:end_pos,:,:] = y_tilde_tmp\n",
    "        \n",
    "   \n",
    "\n",
    "    for iter in range(N_epochs): #args.epochs\n",
    "        \n",
    "        w_group_array = []\n",
    "        for G_idx in range(G):\n",
    "            w_locals, loss_locals = [], []\n",
    "            idxs_users = np.random.choice(range(N_i), m, replace=False)\n",
    "            idxs_users = np.sort(idxs_users)\n",
    "            print('selected users:',idxs_users)\n",
    "\n",
    "            coded_net = BACC_Enc_Model_withNoise_v3(net_glob.cuda(), N_i, K_i, T, 0.01, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "            dec_z_array = []\n",
    "            for idx in idxs_users: #for idx in range(N):\n",
    "        #         print(idx)\n",
    "                local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[G_idx*N_i+idx,:,:], label=y_tilde[G_idx*N_i+idx,:,:])\n",
    "#                 w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "                w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                w_locals.append(copy.deepcopy(w))\n",
    "                loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "            # update global weights\n",
    "            #w_glob = FedAvg(w_locals)\n",
    "            w_group = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "            \n",
    "            w_group_array.append(copy.deepcopy(w_group))\n",
    "        \n",
    "        w_glob = copy.deepcopy(w_group_array[0])\n",
    "        for k in w_glob.keys():\n",
    "            for G_idx in range(1,G):\n",
    "                w_glob[k] += w_group_array[G_idx][k]\n",
    "            w_glob[k] = torch.div(w_glob[k], len(w_group_array))\n",
    "        \n",
    "        # copy weight to net_glob\n",
    "        net_glob.load_state_dict(w_glob)\n",
    "\n",
    "        # print loss\n",
    "    #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "    #     loss_train_arr.append(loss_train)\n",
    "\n",
    "        acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "        acc_test_arr_K4_G4_N8_v2[trial_idx][iter] = acc_test\n",
    "        loss_test_arr_K4_G4_N8_v2[trial_idx][iter] = loss_test\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "B_sel = 0\n",
    "\n",
    "# plt.plot(acc_test_arr_v1[0,B_sel,0,0:30],label='N=2, B='+str(B_array[B_sel]))\n",
    "plt.plot(acc_test_arr_K4_G1_v2[0,B_sel,0,0:30],label='w/o Grouping, K=4, N=8, G=1' )\n",
    "# plt.plot(acc_test_arr_v1[2,B_sel,0,0:30],label='N=6, B='+str(B_array[B_sel]))\n",
    "# plt.plot(acc_test_arr_v1[3,B_sel,0,0:30],label='N=8, B='+str(B_array[B_sel]))\n",
    "plt.plot(acc_test_arr_G2_N4_v2[0,0:30],label='w/ Grouping,   K=4, N=4, G=2')\n",
    "plt.plot(acc_test_arr_K4_G2_N8_v2[0,0:30],label='w/ Grouping,   K=4, N=8, G=2')\n",
    "plt.plot(acc_test_arr_K4_G4_N8_v2[0,0:30],label='w/ Grouping,   K=4, N=8, G=4')\n",
    "# plt.plot(acc_test_arr_G3[0,0:30],label='Grouping, N=6, G=3, B='+str(B_array[B_sel]))\n",
    "\n",
    "# plt.plot(acc_test_arr_v1[4,B_sel,0,0:30],label='N=7, B='+str(B_array[B_sel]))\n",
    "# plt.plot(acc_test_arr_v1[5,B_sel,0,0:30],label='N=8, B='+str(B_array[B_sel]))\n",
    "\n",
    "# plt.plot(plot_acc_v2[2,sigma_sel,:],label='T=10')\n",
    "# plt.plot(plot_acc_v2[3,sigma_sel,:],label='T=11')\n",
    "\n",
    "# plt.plot(acc_test_arr_uncoded,'r',label='uncoded')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "# plt.title(title_name)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Test accuracy(%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Fix the model encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 !!!\n",
      "z_array: [-0.94  -0.534  0.534  0.94 ]\n",
      "0.4486236179368535\n",
      "0.48963480280841937\n",
      "0.4896348028084205\n",
      "0.4486236179368556\n",
      "@BACC_Enc: N,K,T, m_i= 4 4 5 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 4 5 15000 \n",
      "\n",
      "(T, sigma)= 5 1 )  0 -th Trial!!\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.012858290672302247\n",
      "conv1.bias 0.015318877063691616\n",
      "conv2.weight 0.0004168525710701942\n",
      "conv2.bias 0.00041445204988121986\n",
      "fc1.weight 0.0003280941862612963\n",
      "fc1.bias 0.0004276319872587919\n",
      "\n",
      "Test set: Average loss: 2.2418 \n",
      "Accuracy: 5172/10000 (51.72%)\n",
      "\n",
      "Round   0, Average loss 2.242 Test accuracy 51.720\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0006372705101966858\n",
      "conv1.bias 0.008826728910207748\n",
      "conv2.weight 0.0009889981895685196\n",
      "conv2.bias 0.0022288253530859947\n",
      "fc1.weight 0.0006293328013271093\n",
      "fc1.bias 0.0011550872586667538\n",
      "\n",
      "Test set: Average loss: 1.6220 \n",
      "Accuracy: 5660/10000 (56.60%)\n",
      "\n",
      "Round   1, Average loss 1.622 Test accuracy 56.600\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0007359202206134796\n",
      "conv1.bias 0.014967684634029865\n",
      "conv2.weight 0.0003107859566807747\n",
      "conv2.bias 0.002844051457941532\n",
      "fc1.weight 0.0007100942544639111\n",
      "fc1.bias 0.001087057963013649\n",
      "\n",
      "Test set: Average loss: 0.8046 \n",
      "Accuracy: 7892/10000 (78.92%)\n",
      "\n",
      "Round   2, Average loss 0.805 Test accuracy 78.920\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0009673633426427841\n",
      "conv1.bias 0.025020290166139603\n",
      "conv2.weight 0.00029406601563096045\n",
      "conv2.bias 0.0032652460504323244\n",
      "fc1.weight 0.0009181177243590355\n",
      "fc1.bias 0.002338675782084465\n",
      "\n",
      "Test set: Average loss: 0.4323 \n",
      "Accuracy: 9065/10000 (90.65%)\n",
      "\n",
      "Round   3, Average loss 0.432 Test accuracy 90.650\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0008137691020965577\n",
      "conv1.bias 0.02279841899871826\n",
      "conv2.weight 0.0003407572209835053\n",
      "conv2.bias 0.0031184498220682144\n",
      "fc1.weight 0.0011064625345170499\n",
      "fc1.bias 0.006042740866541862\n",
      "\n",
      "Test set: Average loss: 0.4614 \n",
      "Accuracy: 9176/10000 (91.76%)\n",
      "\n",
      "Round   4, Average loss 0.461 Test accuracy 91.760\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0008544601500034332\n",
      "conv1.bias 0.017656369134783745\n",
      "conv2.weight 0.0005607713013887405\n",
      "conv2.bias 0.003173875156790018\n",
      "fc1.weight 0.0009951620362699033\n",
      "fc1.bias 0.004555537179112434\n",
      "\n",
      "Test set: Average loss: 0.4978 \n",
      "Accuracy: 9149/10000 (91.49%)\n",
      "\n",
      "Round   5, Average loss 0.498 Test accuracy 91.490\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0008408243954181671\n",
      "conv1.bias 0.01626782864332199\n",
      "conv2.weight 0.0007574604451656341\n",
      "conv2.bias 0.0034547233954072\n",
      "fc1.weight 0.0010746994987130166\n",
      "fc1.bias 0.004795691370964051\n",
      "\n",
      "Test set: Average loss: 0.4402 \n",
      "Accuracy: 9119/10000 (91.19%)\n",
      "\n",
      "Round   6, Average loss 0.440 Test accuracy 91.190\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0010771815478801726\n",
      "conv1.bias 0.018332211300730705\n",
      "conv2.weight 0.0005150944739580154\n",
      "conv2.bias 0.002985149621963501\n",
      "fc1.weight 0.0011076577939093113\n",
      "fc1.bias 0.004431231692433357\n",
      "\n",
      "Test set: Average loss: 0.4832 \n",
      "Accuracy: 9044/10000 (90.44%)\n",
      "\n",
      "Round   7, Average loss 0.483 Test accuracy 90.440\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0010731541365385055\n",
      "conv1.bias 0.018827367573976517\n",
      "conv2.weight 0.0004606245085597038\n",
      "conv2.bias 0.0029375122394412756\n",
      "fc1.weight 0.0011463904753327369\n",
      "fc1.bias 0.0027765406295657156\n",
      "\n",
      "Test set: Average loss: 0.4751 \n",
      "Accuracy: 9106/10000 (91.06%)\n",
      "\n",
      "Round   8, Average loss 0.475 Test accuracy 91.060\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0015227867662906648\n",
      "conv1.bias 0.02207975462079048\n",
      "conv2.weight 0.00040820442140102384\n",
      "conv2.bias 0.002880119951441884\n",
      "fc1.weight 0.000853126123547554\n",
      "fc1.bias 0.0023345792666077613\n",
      "\n",
      "Test set: Average loss: 0.5530 \n",
      "Accuracy: 8535/10000 (85.35%)\n",
      "\n",
      "Round   9, Average loss 0.553 Test accuracy 85.350\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0008052057772874832\n",
      "conv1.bias 0.024601012468338013\n",
      "conv2.weight 0.0004169144108891487\n",
      "conv2.bias 0.003294286783784628\n",
      "fc1.weight 0.0013752662576735019\n",
      "fc1.bias 0.002215949259698391\n",
      "\n",
      "Test set: Average loss: 0.4381 \n",
      "Accuracy: 9217/10000 (92.17%)\n",
      "\n",
      "Round  10, Average loss 0.438 Test accuracy 92.170\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0009555377066135407\n",
      "conv1.bias 0.02361413463950157\n",
      "conv2.weight 0.00058232631534338\n",
      "conv2.bias 0.003452187404036522\n",
      "fc1.weight 0.001012993324548006\n",
      "fc1.bias 0.004080909863114357\n",
      "\n",
      "Test set: Average loss: 0.4931 \n",
      "Accuracy: 8936/10000 (89.36%)\n",
      "\n",
      "Round  11, Average loss 0.493 Test accuracy 89.360\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0008990642428398132\n",
      "conv1.bias 0.01622222363948822\n",
      "conv2.weight 0.0006964562833309173\n",
      "conv2.bias 0.0034305250737816095\n",
      "fc1.weight 0.0010389570146799088\n",
      "fc1.bias 0.0024493973702192306\n",
      "\n",
      "Test set: Average loss: 0.5265 \n",
      "Accuracy: 8666/10000 (86.66%)\n",
      "\n",
      "Round  12, Average loss 0.527 Test accuracy 86.660\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0009465830028057098\n",
      "conv1.bias 0.02237185835838318\n",
      "conv2.weight 0.0006227980926632881\n",
      "conv2.bias 0.0034350655041635036\n",
      "fc1.weight 0.0009389439597725868\n",
      "fc1.bias 0.002468537725508213\n",
      "\n",
      "Test set: Average loss: 0.4244 \n",
      "Accuracy: 9127/10000 (91.27%)\n",
      "\n",
      "Round  13, Average loss 0.424 Test accuracy 91.270\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0012847618758678437\n",
      "conv1.bias 0.023513415828347206\n",
      "conv2.weight 0.0005747056752443314\n",
      "conv2.bias 0.003615284338593483\n",
      "fc1.weight 0.0008736655116081237\n",
      "fc1.bias 0.0037969361990690233\n",
      "\n",
      "Test set: Average loss: 0.4216 \n",
      "Accuracy: 9178/10000 (91.78%)\n",
      "\n",
      "Round  14, Average loss 0.422 Test accuracy 91.780\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0009463539719581604\n",
      "conv1.bias 0.016104698181152344\n",
      "conv2.weight 0.0007604130357503891\n",
      "conv2.bias 0.0035801338963210583\n",
      "fc1.weight 0.0011012540198862553\n",
      "fc1.bias 0.0029583564028143883\n",
      "\n",
      "Test set: Average loss: 0.4493 \n",
      "Accuracy: 9110/10000 (91.10%)\n",
      "\n",
      "Round  15, Average loss 0.449 Test accuracy 91.100\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0011922595649957657\n",
      "conv1.bias 0.016669515520334244\n",
      "conv2.weight 0.0006072564795613289\n",
      "conv2.bias 0.0030778516083955765\n",
      "fc1.weight 0.0010841567069292068\n",
      "fc1.bias 0.003397832065820694\n",
      "\n",
      "Test set: Average loss: 0.4623 \n",
      "Accuracy: 9163/10000 (91.63%)\n",
      "\n",
      "Round  16, Average loss 0.462 Test accuracy 91.630\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.000982251763343811\n",
      "conv1.bias 0.016699275001883507\n",
      "conv2.weight 0.0006426682323217392\n",
      "conv2.bias 0.0031827776692807674\n",
      "fc1.weight 0.0009515273384749889\n",
      "fc1.bias 0.003972950577735901\n",
      "\n",
      "Test set: Average loss: 0.4824 \n",
      "Accuracy: 8857/10000 (88.57%)\n",
      "\n",
      "Round  17, Average loss 0.482 Test accuracy 88.570\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0009277258813381195\n",
      "conv1.bias 0.014102811925113201\n",
      "conv2.weight 0.001016448140144348\n",
      "conv2.bias 0.004132211208343506\n",
      "fc1.weight 0.0009258652105927467\n",
      "fc1.bias 0.0019820321351289747\n",
      "\n",
      "Test set: Average loss: 0.4678 \n",
      "Accuracy: 9163/10000 (91.63%)\n",
      "\n",
      "Round  18, Average loss 0.468 Test accuracy 91.630\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0014994125068187713\n",
      "conv1.bias 0.01723206415772438\n",
      "conv2.weight 0.0006667047739028931\n",
      "conv2.bias 0.003247372340410948\n",
      "fc1.weight 0.0010774891823530197\n",
      "fc1.bias 0.004083807766437531\n",
      "\n",
      "Test set: Average loss: 0.4446 \n",
      "Accuracy: 9075/10000 (90.75%)\n",
      "\n",
      "Round  19, Average loss 0.445 Test accuracy 90.750\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.001395980268716812\n",
      "conv1.bias 0.019522130489349365\n",
      "conv2.weight 0.0005200818181037903\n",
      "conv2.bias 0.0028091934509575367\n",
      "fc1.weight 0.0009628860279917717\n",
      "fc1.bias 0.003322822228074074\n",
      "\n",
      "Test set: Average loss: 0.4425 \n",
      "Accuracy: 9115/10000 (91.15%)\n",
      "\n",
      "Round  20, Average loss 0.443 Test accuracy 91.150\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0012472989410161972\n",
      "conv1.bias 0.01950153335928917\n",
      "conv2.weight 0.0004565514251589775\n",
      "conv2.bias 0.0028217246290296316\n",
      "fc1.weight 0.0008915572427213192\n",
      "fc1.bias 0.0034170795232057573\n",
      "\n",
      "Test set: Average loss: 0.4985 \n",
      "Accuracy: 8664/10000 (86.64%)\n",
      "\n",
      "Round  21, Average loss 0.498 Test accuracy 86.640\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.001102021485567093\n",
      "conv1.bias 0.019143279641866684\n",
      "conv2.weight 0.0003839781880378723\n",
      "conv2.bias 0.0029357755556702614\n",
      "fc1.weight 0.001006550621241331\n",
      "fc1.bias 0.0032227028161287306\n",
      "\n",
      "Test set: Average loss: 0.4521 \n",
      "Accuracy: 9036/10000 (90.36%)\n",
      "\n",
      "Round  22, Average loss 0.452 Test accuracy 90.360\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0010400594770908357\n",
      "conv1.bias 0.020040541887283325\n",
      "conv2.weight 0.0005091172084212303\n",
      "conv2.bias 0.003497619414702058\n",
      "fc1.weight 0.0008856014348566533\n",
      "fc1.bias 0.004671280831098556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.4298 \n",
      "Accuracy: 9127/10000 (91.27%)\n",
      "\n",
      "Round  23, Average loss 0.430 Test accuracy 91.270\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0011326855421066285\n",
      "conv1.bias 0.020428882911801338\n",
      "conv2.weight 0.0005401876196265221\n",
      "conv2.bias 0.0034400380682200193\n",
      "fc1.weight 0.0009613695554435253\n",
      "fc1.bias 0.0050285980105400085\n",
      "\n",
      "Test set: Average loss: 0.4482 \n",
      "Accuracy: 9109/10000 (91.09%)\n",
      "\n",
      "Round  24, Average loss 0.448 Test accuracy 91.090\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0009945099055767058\n",
      "conv1.bias 0.016359535977244377\n",
      "conv2.weight 0.000573878027498722\n",
      "conv2.bias 0.003497322089970112\n",
      "fc1.weight 0.0009720459580421448\n",
      "fc1.bias 0.003074977919459343\n",
      "\n",
      "Test set: Average loss: 0.4101 \n",
      "Accuracy: 9125/10000 (91.25%)\n",
      "\n",
      "Round  25, Average loss 0.410 Test accuracy 91.250\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.000986441671848297\n",
      "conv1.bias 0.01547280140221119\n",
      "conv2.weight 0.0005771603807806969\n",
      "conv2.bias 0.0036638746969401836\n",
      "fc1.weight 0.001145122479647398\n",
      "fc1.bias 0.004723327606916428\n",
      "\n",
      "Test set: Average loss: 0.4914 \n",
      "Accuracy: 9044/10000 (90.44%)\n",
      "\n",
      "Round  26, Average loss 0.491 Test accuracy 90.440\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.000980876088142395\n",
      "conv1.bias 0.014156101271510124\n",
      "conv2.weight 0.0006693227589130401\n",
      "conv2.bias 0.0032694931142032146\n",
      "fc1.weight 0.0010546255856752396\n",
      "fc1.bias 0.0034640278667211533\n",
      "\n",
      "Test set: Average loss: 0.4583 \n",
      "Accuracy: 9069/10000 (90.69%)\n",
      "\n",
      "Round  27, Average loss 0.458 Test accuracy 90.690\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.000777866616845131\n",
      "conv1.bias 0.012497860938310623\n",
      "conv2.weight 0.0007351788133382797\n",
      "conv2.bias 0.003919053822755814\n",
      "fc1.weight 0.0009870365262031554\n",
      "fc1.bias 0.002052403800189495\n",
      "\n",
      "Test set: Average loss: 0.4953 \n",
      "Accuracy: 8936/10000 (89.36%)\n",
      "\n",
      "Round  28, Average loss 0.495 Test accuracy 89.360\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0007865707576274872\n",
      "conv1.bias 0.015421128831803799\n",
      "conv2.weight 0.0007107339054346084\n",
      "conv2.bias 0.0036988225765526295\n",
      "fc1.weight 0.0009514881297945976\n",
      "fc1.bias 0.002412373572587967\n",
      "\n",
      "Test set: Average loss: 0.4496 \n",
      "Accuracy: 9078/10000 (90.78%)\n",
      "\n",
      "Round  29, Average loss 0.450 Test accuracy 90.780\n",
      "1 !!!\n",
      "z_array: [-0.94 -0.73  0.73  0.94]\n",
      "0.4486236179368535\n",
      "0.49983046821646326\n",
      "0.4998304682164621\n",
      "0.4486236179368556\n",
      "@BACC_Enc: N,K,T, m_i= 4 4 5 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 4 5 15000 \n",
      "\n",
      "(T, sigma)= 5 1 )  0 -th Trial!!\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.013265790939331055\n",
      "conv1.bias 0.013975989073514938\n",
      "conv2.weight 0.00041684485971927644\n",
      "conv2.bias 0.00037007633363828063\n",
      "fc1.weight 0.00032004802487790586\n",
      "fc1.bias 0.0001724717440083623\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.003024858236312866\n",
      "conv1.bias 0.010047690942883492\n",
      "conv2.weight 0.0013843172788619996\n",
      "conv2.bias 0.002580230589956045\n",
      "fc1.weight 0.001182383205741644\n",
      "fc1.bias 0.006025481969118118\n",
      "\n",
      "Test set: Average loss: 5.6156 \n",
      "Accuracy: 1120/10000 (11.20%)\n",
      "\n",
      "Round   1, Average loss 5.616 Test accuracy 11.200\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0015099579095840454\n",
      "conv1.bias 0.016476020216941833\n",
      "conv2.weight 0.0006508561968803406\n",
      "conv2.bias 0.004886542446911335\n",
      "fc1.weight 0.001229814626276493\n",
      "fc1.bias 0.0031505204737186433\n",
      "\n",
      "Test set: Average loss: 1.8242 \n",
      "Accuracy: 4497/10000 (44.97%)\n",
      "\n",
      "Round   2, Average loss 1.824 Test accuracy 44.970\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.005458663702011108\n",
      "conv1.bias 0.0015634219162166119\n",
      "conv2.weight 0.0015719224512577056\n",
      "conv2.bias 0.004551366437226534\n",
      "fc1.weight 0.0022501640021800997\n",
      "fc1.bias 0.00474507063627243\n",
      "\n",
      "Test set: Average loss: 21.4806 \n",
      "Accuracy: 1407/10000 (14.07%)\n",
      "\n",
      "Round   3, Average loss 21.481 Test accuracy 14.070\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0039024585485458374\n",
      "conv1.bias 0.014572441577911377\n",
      "conv2.weight 0.0009692952036857605\n",
      "conv2.bias 0.003305890131741762\n",
      "fc1.weight 0.0023191032931208612\n",
      "fc1.bias 0.008672017604112625\n",
      "\n",
      "Test set: Average loss: 18.8111 \n",
      "Accuracy: 2122/10000 (21.22%)\n",
      "\n",
      "Round   4, Average loss 18.811 Test accuracy 21.220\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.004382235407829285\n",
      "conv1.bias 0.02836957946419716\n",
      "conv2.weight 0.0013329485058784485\n",
      "conv2.bias 0.005741334054619074\n",
      "fc1.weight 0.002788756787776947\n",
      "fc1.bias 0.005674824863672256\n",
      "\n",
      "Test set: Average loss: 19.8998 \n",
      "Accuracy: 1178/10000 (11.78%)\n",
      "\n",
      "Round   5, Average loss 19.900 Test accuracy 11.780\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0038491615653038023\n",
      "conv1.bias 0.01574196293950081\n",
      "conv2.weight 0.0017013032734394073\n",
      "conv2.bias 0.004207129590213299\n",
      "fc1.weight 0.0026215743273496628\n",
      "fc1.bias 0.00542767271399498\n",
      "\n",
      "Test set: Average loss: 3.1306 \n",
      "Accuracy: 3707/10000 (37.07%)\n",
      "\n",
      "Round   6, Average loss 3.131 Test accuracy 37.070\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.003941234350204468\n",
      "conv1.bias 0.015499932691454887\n",
      "conv2.weight 0.0011569659411907196\n",
      "conv2.bias 0.005083375610411167\n",
      "fc1.weight 0.00207978505641222\n",
      "fc1.bias 0.0021940542384982107\n",
      "\n",
      "Test set: Average loss: 2.4650 \n",
      "Accuracy: 911/10000 (9.11%)\n",
      "\n",
      "Round   7, Average loss 2.465 Test accuracy 9.110\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0017762181162834167\n",
      "conv1.bias 0.011340616270899773\n",
      "conv2.weight 0.0008606967329978942\n",
      "conv2.bias 0.00339592806994915\n",
      "fc1.weight 0.002354513853788376\n",
      "fc1.bias 0.005317309126257897\n",
      "\n",
      "Test set: Average loss: 1.9075 \n",
      "Accuracy: 4413/10000 (44.13%)\n",
      "\n",
      "Round   8, Average loss 1.907 Test accuracy 44.130\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0040906393527984615\n",
      "conv1.bias 0.003543501254171133\n",
      "conv2.weight 0.0016551679372787476\n",
      "conv2.bias 0.00392354279756546\n",
      "fc1.weight 0.0020020313560962675\n",
      "fc1.bias 0.006403372436761856\n",
      "\n",
      "Test set: Average loss: 9.9654 \n",
      "Accuracy: 1932/10000 (19.32%)\n",
      "\n",
      "Round   9, Average loss 9.965 Test accuracy 19.320\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.003432561159133911\n",
      "conv1.bias 0.023843005299568176\n",
      "conv2.weight 0.0010561942309141158\n",
      "conv2.bias 0.004230939783155918\n",
      "fc1.weight 0.004519351571798324\n",
      "fc1.bias 0.009065253287553787\n",
      "\n",
      "Test set: Average loss: 2.3066 \n",
      "Accuracy: 937/10000 (9.37%)\n",
      "\n",
      "Round  10, Average loss 2.307 Test accuracy 9.370\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0038153716921806337\n",
      "conv1.bias 0.007957777939736843\n",
      "conv2.weight 0.0011469669640064239\n",
      "conv2.bias 0.0032698125578463078\n",
      "fc1.weight 0.0022827289998531342\n",
      "fc1.bias 0.0014411119744181633\n",
      "\n",
      "Test set: Average loss: 6.1438 \n",
      "Accuracy: 2125/10000 (21.25%)\n",
      "\n",
      "Round  11, Average loss 6.144 Test accuracy 21.250\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0026871103048324587\n",
      "conv1.bias 0.039015185087919235\n",
      "conv2.weight 0.0011721399426460266\n",
      "conv2.bias 0.006716471631079912\n",
      "fc1.weight 0.0018302865326404571\n",
      "fc1.bias 0.003360641002655029\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  12, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.006785602569580078\n",
      "conv1.bias 0.007328042760491371\n",
      "conv2.weight 0.0016869068145751952\n",
      "conv2.bias 0.003646369092166424\n",
      "fc1.weight 0.0034011103212833404\n",
      "fc1.bias 0.003223970904946327\n",
      "\n",
      "Test set: Average loss: 1.5892 \n",
      "Accuracy: 4348/10000 (43.48%)\n",
      "\n",
      "Round  13, Average loss 1.589 Test accuracy 43.480\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.001369607001543045\n",
      "conv1.bias 0.011186007410287857\n",
      "conv2.weight 0.000798698142170906\n",
      "conv2.bias 0.007541511207818985\n",
      "fc1.weight 0.001257532276213169\n",
      "fc1.bias 0.0013300372287631036\n",
      "\n",
      "Test set: Average loss: 2.1638 \n",
      "Accuracy: 1034/10000 (10.34%)\n",
      "\n",
      "Round  14, Average loss 2.164 Test accuracy 10.340\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.004871429204940796\n",
      "conv1.bias 0.0057290345430374146\n",
      "conv2.weight 0.0012133649736642837\n",
      "conv2.bias 0.004683311562985182\n",
      "fc1.weight 0.0024518918246030806\n",
      "fc1.bias 0.0033404774963855743\n",
      "\n",
      "Test set: Average loss: 29.7804 \n",
      "Accuracy: 1204/10000 (12.04%)\n",
      "\n",
      "Round  15, Average loss 29.780 Test accuracy 12.040\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.006072588562965393\n",
      "conv1.bias 0.02677147276699543\n",
      "conv2.weight 0.001337936371564865\n",
      "conv2.bias 0.0063507575541734695\n",
      "fc1.weight 0.0026300836354494093\n",
      "fc1.bias 0.007742884010076523\n",
      "\n",
      "Test set: Average loss: 34.7291 \n",
      "Accuracy: 1956/10000 (19.56%)\n",
      "\n",
      "Round  16, Average loss 34.729 Test accuracy 19.560\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.002753714621067047\n",
      "conv1.bias 0.0206197090446949\n",
      "conv2.weight 0.0016850334405899047\n",
      "conv2.bias 0.0031849199440330267\n",
      "fc1.weight 0.002508986368775368\n",
      "fc1.bias 0.002012903057038784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 58.6623 \n",
      "Accuracy: 974/10000 (9.74%)\n",
      "\n",
      "Round  17, Average loss 58.662 Test accuracy 9.740\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0034457188844680786\n",
      "conv1.bias 0.025090564042329788\n",
      "conv2.weight 0.0012654034793376922\n",
      "conv2.bias 0.003640583949163556\n",
      "fc1.weight 0.003008909709751606\n",
      "fc1.bias 0.0016027364879846572\n",
      "\n",
      "Test set: Average loss: 29.5323 \n",
      "Accuracy: 3474/10000 (34.74%)\n",
      "\n",
      "Round  18, Average loss 29.532 Test accuracy 34.740\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.005907734632492065\n",
      "conv1.bias 0.04078202694654465\n",
      "conv2.weight 0.0016320143640041352\n",
      "conv2.bias 0.007898061536252499\n",
      "fc1.weight 0.003443567454814911\n",
      "fc1.bias 0.005083336681127548\n",
      "\n",
      "Test set: Average loss: 19.7878 \n",
      "Accuracy: 1005/10000 (10.05%)\n",
      "\n",
      "Round  19, Average loss 19.788 Test accuracy 10.050\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.003910978436470032\n",
      "conv1.bias 0.01596067100763321\n",
      "conv2.weight 0.0013189630210399628\n",
      "conv2.bias 0.003738993778824806\n",
      "fc1.weight 0.0023956380784511564\n",
      "fc1.bias 0.001346097607165575\n",
      "\n",
      "Test set: Average loss: 2.2858 \n",
      "Accuracy: 1239/10000 (12.39%)\n",
      "\n",
      "Round  20, Average loss 2.286 Test accuracy 12.390\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.003607574701309204\n",
      "conv1.bias 0.002644589636474848\n",
      "conv2.weight 0.0013552673161029815\n",
      "conv2.bias 0.003710891818627715\n",
      "fc1.weight 0.0015453469008207322\n",
      "fc1.bias 0.001860031858086586\n",
      "\n",
      "Test set: Average loss: 2.3028 \n",
      "Accuracy: 941/10000 (9.41%)\n",
      "\n",
      "Round  21, Average loss 2.303 Test accuracy 9.410\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0009219244122505188\n",
      "conv1.bias 0.007149959914386272\n",
      "conv2.weight 0.0007511726766824722\n",
      "conv2.bias 0.002919947961345315\n",
      "fc1.weight 0.000933688785880804\n",
      "fc1.bias 0.00195551123470068\n",
      "\n",
      "Test set: Average loss: 17.5183 \n",
      "Accuracy: 1028/10000 (10.28%)\n",
      "\n",
      "Round  22, Average loss 17.518 Test accuracy 10.280\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0020195046067237855\n",
      "conv1.bias 0.013341594487428665\n",
      "conv2.weight 0.0006305783987045288\n",
      "conv2.bias 0.004833717830479145\n",
      "fc1.weight 0.0014556793496012688\n",
      "fc1.bias 0.0020499199628829954\n",
      "\n",
      "Test set: Average loss: 4.1979 \n",
      "Accuracy: 4237/10000 (42.37%)\n",
      "\n",
      "Round  23, Average loss 4.198 Test accuracy 42.370\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0032816362380981446\n",
      "conv1.bias 0.012075974605977535\n",
      "conv2.weight 0.0017483830451965332\n",
      "conv2.bias 0.00580323301255703\n",
      "fc1.weight 0.003090251237154007\n",
      "fc1.bias 0.005945297703146934\n",
      "\n",
      "Test set: Average loss: 4.9597 \n",
      "Accuracy: 1275/10000 (12.75%)\n",
      "\n",
      "Round  24, Average loss 4.960 Test accuracy 12.750\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0017768266797065736\n",
      "conv1.bias 0.021796133369207382\n",
      "conv2.weight 0.001112285852432251\n",
      "conv2.bias 0.002911136019974947\n",
      "fc1.weight 0.0023293454200029374\n",
      "fc1.bias 0.004872884973883629\n",
      "\n",
      "Test set: Average loss: 4.1959 \n",
      "Accuracy: 1041/10000 (10.41%)\n",
      "\n",
      "Round  25, Average loss 4.196 Test accuracy 10.410\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0033278316259384155\n",
      "conv1.bias 0.01414122711867094\n",
      "conv2.weight 0.0010508520901203156\n",
      "conv2.bias 0.0033456035889685154\n",
      "fc1.weight 0.0029910163953900336\n",
      "fc1.bias 0.002236615866422653\n",
      "\n",
      "Test set: Average loss: 3.1078 \n",
      "Accuracy: 2235/10000 (22.35%)\n",
      "\n",
      "Round  26, Average loss 3.108 Test accuracy 22.350\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.003526494801044464\n",
      "conv1.bias 0.00998697429895401\n",
      "conv2.weight 0.0017150335013866424\n",
      "conv2.bias 0.00629082415252924\n",
      "fc1.weight 0.002767669782042503\n",
      "fc1.bias 0.004377007111907005\n",
      "\n",
      "Test set: Average loss: 3.0261 \n",
      "Accuracy: 1093/10000 (10.93%)\n",
      "\n",
      "Round  27, Average loss 3.026 Test accuracy 10.930\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0019063520431518554\n",
      "conv1.bias 0.01850401610136032\n",
      "conv2.weight 0.0007107423990964889\n",
      "conv2.bias 0.002832940313965082\n",
      "fc1.weight 0.0023284753784537315\n",
      "fc1.bias 0.002267440594732761\n",
      "\n",
      "Test set: Average loss: 2.6871 \n",
      "Accuracy: 2043/10000 (20.43%)\n",
      "\n",
      "Round  28, Average loss 2.687 Test accuracy 20.430\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0033176887035369874\n",
      "conv1.bias 0.014782401733100414\n",
      "conv2.weight 0.0008944085240364074\n",
      "conv2.bias 0.0045944759622216225\n",
      "fc1.weight 0.0012549926526844502\n",
      "fc1.bias 0.0023320602253079415\n",
      "\n",
      "Test set: Average loss: 2.3996 \n",
      "Accuracy: 689/10000 (6.89%)\n",
      "\n",
      "Round  29, Average loss 2.400 Test accuracy 6.890\n",
      "2 !!!\n",
      "z_array: [-0.94  -0.125  0.125  0.94 ]\n",
      "0.4486236179368535\n",
      "0.4857187736123886\n",
      "0.48571877361238697\n",
      "0.4486236179368556\n",
      "@BACC_Enc: N,K,T, m_i= 4 4 5 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 4 5 15000 \n",
      "\n",
      "(T, sigma)= 5 1 )  0 -th Trial!!\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.013858480453491211\n",
      "conv1.bias 0.012573538348078728\n",
      "conv2.weight 0.0004160062223672867\n",
      "conv2.bias 0.00044482017983682454\n",
      "fc1.weight 0.00032244722824543716\n",
      "fc1.bias 0.0004337082151323557\n",
      "\n",
      "Test set: Average loss: 2.3013 \n",
      "Accuracy: 1032/10000 (10.32%)\n",
      "\n",
      "Round   0, Average loss 2.301 Test accuracy 10.320\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.001027267947793007\n",
      "conv1.bias 0.005073073320090771\n",
      "conv2.weight 0.0010481970012187957\n",
      "conv2.bias 0.001966263400390744\n",
      "fc1.weight 0.000296829198487103\n",
      "fc1.bias 0.0014136976562440396\n",
      "\n",
      "Test set: Average loss: 2.2960 \n",
      "Accuracy: 1817/10000 (18.17%)\n",
      "\n",
      "Round   1, Average loss 2.296 Test accuracy 18.170\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00047297149896621707\n",
      "conv1.bias 0.010678798891603947\n",
      "conv2.weight 8.536145091056824e-05\n",
      "conv2.bias 0.0026852728333324194\n",
      "fc1.weight 0.0001404508831910789\n",
      "fc1.bias 0.001109425351023674\n",
      "\n",
      "Test set: Average loss: 2.3010 \n",
      "Accuracy: 1204/10000 (12.04%)\n",
      "\n",
      "Round   2, Average loss 2.301 Test accuracy 12.040\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004944900423288345\n",
      "conv1.bias 0.014128385111689568\n",
      "conv2.weight 0.00025966096669435503\n",
      "conv2.bias 0.005838585551828146\n",
      "fc1.weight 0.00030349560547620057\n",
      "fc1.bias 0.0010601080022752285\n",
      "\n",
      "Test set: Average loss: 2.0833 \n",
      "Accuracy: 6618/10000 (66.18%)\n",
      "\n",
      "Round   3, Average loss 2.083 Test accuracy 66.180\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0007283878326416015\n",
      "conv1.bias 0.019267510622739792\n",
      "conv2.weight 0.0002503792382776737\n",
      "conv2.bias 0.004265598487108946\n",
      "fc1.weight 0.00048636980354785917\n",
      "fc1.bias 0.0012807244434952735\n",
      "\n",
      "Test set: Average loss: 1.4580 \n",
      "Accuracy: 7633/10000 (76.33%)\n",
      "\n",
      "Round   4, Average loss 1.458 Test accuracy 76.330\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0009471336007118225\n",
      "conv1.bias 0.02142968773841858\n",
      "conv2.weight 0.00031341709196567533\n",
      "conv2.bias 0.004409231711179018\n",
      "fc1.weight 0.0008780233561992645\n",
      "fc1.bias 0.0033601321280002593\n",
      "\n",
      "Test set: Average loss: 0.5469 \n",
      "Accuracy: 8604/10000 (86.04%)\n",
      "\n",
      "Round   5, Average loss 0.547 Test accuracy 86.040\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0011220303177833556\n",
      "conv1.bias 0.026597067713737488\n",
      "conv2.weight 0.0004557505249977112\n",
      "conv2.bias 0.004418077878654003\n",
      "fc1.weight 0.0009542200714349747\n",
      "fc1.bias 0.005865253135561943\n",
      "\n",
      "Test set: Average loss: 0.3595 \n",
      "Accuracy: 9382/10000 (93.82%)\n",
      "\n",
      "Round   6, Average loss 0.359 Test accuracy 93.820\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0012257057428359986\n",
      "conv1.bias 0.024884678423404694\n",
      "conv2.weight 0.0005024998262524604\n",
      "conv2.bias 0.003978308290243149\n",
      "fc1.weight 0.000994410552084446\n",
      "fc1.bias 0.005259010568261146\n",
      "\n",
      "Test set: Average loss: 0.2729 \n",
      "Accuracy: 9521/10000 (95.21%)\n",
      "\n",
      "Round   7, Average loss 0.273 Test accuracy 95.210\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0011853287369012833\n",
      "conv1.bias 0.02337529882788658\n",
      "conv2.weight 0.0006457614153623581\n",
      "conv2.bias 0.004578011576086283\n",
      "fc1.weight 0.0008965669199824333\n",
      "fc1.bias 0.007211381942033768\n",
      "\n",
      "Test set: Average loss: 0.2524 \n",
      "Accuracy: 9527/10000 (95.27%)\n",
      "\n",
      "Round   8, Average loss 0.252 Test accuracy 95.270\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0016954278945922852\n",
      "conv1.bias 0.022095398977398872\n",
      "conv2.weight 0.000436478853225708\n",
      "conv2.bias 0.00349231343716383\n",
      "fc1.weight 0.0009093294851481914\n",
      "fc1.bias 0.005692675709724426\n",
      "\n",
      "Test set: Average loss: 0.3138 \n",
      "Accuracy: 9526/10000 (95.26%)\n",
      "\n",
      "Round   9, Average loss 0.314 Test accuracy 95.260\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0014276304841041565\n",
      "conv1.bias 0.021781641989946365\n",
      "conv2.weight 0.000566449947655201\n",
      "conv2.bias 0.0038680415600538254\n",
      "fc1.weight 0.0010094117373228074\n",
      "fc1.bias 0.00499451495707035\n",
      "\n",
      "Test set: Average loss: 0.3001 \n",
      "Accuracy: 9572/10000 (95.72%)\n",
      "\n",
      "Round  10, Average loss 0.300 Test accuracy 95.720\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0014664044976234437\n",
      "conv1.bias 0.021186044439673424\n",
      "conv2.weight 0.0005791011825203896\n",
      "conv2.bias 0.003788457252085209\n",
      "fc1.weight 0.0009502956643700599\n",
      "fc1.bias 0.004299384355545044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.2267 \n",
      "Accuracy: 9574/10000 (95.74%)\n",
      "\n",
      "Round  11, Average loss 0.227 Test accuracy 95.740\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0016348937153816223\n",
      "conv1.bias 0.019785067066550255\n",
      "conv2.weight 0.0005435574054718017\n",
      "conv2.bias 0.00346518587321043\n",
      "fc1.weight 0.0009837357327342033\n",
      "fc1.bias 0.005536499246954918\n",
      "\n",
      "Test set: Average loss: 0.2983 \n",
      "Accuracy: 9517/10000 (95.17%)\n",
      "\n",
      "Round  12, Average loss 0.298 Test accuracy 95.170\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0014174069464206697\n",
      "conv1.bias 0.021441340446472168\n",
      "conv2.weight 0.0005622993409633636\n",
      "conv2.bias 0.003557561431080103\n",
      "fc1.weight 0.0010406550019979477\n",
      "fc1.bias 0.005531302466988563\n",
      "\n",
      "Test set: Average loss: 0.2739 \n",
      "Accuracy: 9529/10000 (95.29%)\n",
      "\n",
      "Round  13, Average loss 0.274 Test accuracy 95.290\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0012950021028518676\n",
      "conv1.bias 0.02111590839922428\n",
      "conv2.weight 0.0006455644965171814\n",
      "conv2.bias 0.0036865274887531996\n",
      "fc1.weight 0.0009174076840281487\n",
      "fc1.bias 0.004364854097366333\n",
      "\n",
      "Test set: Average loss: 0.2088 \n",
      "Accuracy: 9570/10000 (95.70%)\n",
      "\n",
      "Round  14, Average loss 0.209 Test accuracy 95.700\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0015667280554771423\n",
      "conv1.bias 0.02464626170694828\n",
      "conv2.weight 0.0004223936051130295\n",
      "conv2.bias 0.0032088402658700943\n",
      "fc1.weight 0.0009115847758948803\n",
      "fc1.bias 0.004532490670681\n",
      "\n",
      "Test set: Average loss: 0.2509 \n",
      "Accuracy: 9579/10000 (95.79%)\n",
      "\n",
      "Round  15, Average loss 0.251 Test accuracy 95.790\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0014579275250434876\n",
      "conv1.bias 0.022809434682130814\n",
      "conv2.weight 0.00045447777956724166\n",
      "conv2.bias 0.0033524285536259413\n",
      "fc1.weight 0.00092243617400527\n",
      "fc1.bias 0.004379658401012421\n",
      "\n",
      "Test set: Average loss: 0.2756 \n",
      "Accuracy: 9489/10000 (94.89%)\n",
      "\n",
      "Round  16, Average loss 0.276 Test accuracy 94.890\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0014493852853775025\n",
      "conv1.bias 0.02316785231232643\n",
      "conv2.weight 0.0004918861389160156\n",
      "conv2.bias 0.0033107823692262173\n",
      "fc1.weight 0.00089021110907197\n",
      "fc1.bias 0.004620468616485596\n",
      "\n",
      "Test set: Average loss: 0.2335 \n",
      "Accuracy: 9580/10000 (95.80%)\n",
      "\n",
      "Round  17, Average loss 0.233 Test accuracy 95.800\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0013977165520191192\n",
      "conv1.bias 0.021824896335601807\n",
      "conv2.weight 0.0005213327333331108\n",
      "conv2.bias 0.003451314987614751\n",
      "fc1.weight 0.0009598844684660435\n",
      "fc1.bias 0.0057906053960323335\n",
      "\n",
      "Test set: Average loss: 0.1870 \n",
      "Accuracy: 9612/10000 (96.12%)\n",
      "\n",
      "Round  18, Average loss 0.187 Test accuracy 96.120\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0018376556038856506\n",
      "conv1.bias 0.02401295304298401\n",
      "conv2.weight 0.00041160468012094495\n",
      "conv2.bias 0.003247357439249754\n",
      "fc1.weight 0.0009146744385361671\n",
      "fc1.bias 0.006057366356253624\n",
      "\n",
      "Test set: Average loss: 0.2506 \n",
      "Accuracy: 9590/10000 (95.90%)\n",
      "\n",
      "Round  19, Average loss 0.251 Test accuracy 95.900\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0014089930057525635\n",
      "conv1.bias 0.02432209625840187\n",
      "conv2.weight 0.0004866572469472885\n",
      "conv2.bias 0.003465237095952034\n",
      "fc1.weight 0.0008831152692437172\n",
      "fc1.bias 0.005002997443079948\n",
      "\n",
      "Test set: Average loss: 0.2003 \n",
      "Accuracy: 9618/10000 (96.18%)\n",
      "\n",
      "Round  20, Average loss 0.200 Test accuracy 96.180\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0015539586544036864\n",
      "conv1.bias 0.025942983105778694\n",
      "conv2.weight 0.0004726046696305275\n",
      "conv2.bias 0.003516857512295246\n",
      "fc1.weight 0.0010283101350069047\n",
      "fc1.bias 0.005485390871763229\n",
      "\n",
      "Test set: Average loss: 0.1864 \n",
      "Accuracy: 9602/10000 (96.02%)\n",
      "\n",
      "Round  21, Average loss 0.186 Test accuracy 96.020\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0017158883810043336\n",
      "conv1.bias 0.024120718240737915\n",
      "conv2.weight 0.00044130809605121615\n",
      "conv2.bias 0.0033124019391834736\n",
      "fc1.weight 0.0010929732583463193\n",
      "fc1.bias 0.005764010921120643\n",
      "\n",
      "Test set: Average loss: 0.3407 \n",
      "Accuracy: 9577/10000 (95.77%)\n",
      "\n",
      "Round  22, Average loss 0.341 Test accuracy 95.770\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0014951348304748536\n",
      "conv1.bias 0.025758104398846626\n",
      "conv2.weight 0.000502999722957611\n",
      "conv2.bias 0.0034284263383597136\n",
      "fc1.weight 0.000778371375054121\n",
      "fc1.bias 0.006391300261020661\n",
      "\n",
      "Test set: Average loss: 0.2212 \n",
      "Accuracy: 9600/10000 (96.00%)\n",
      "\n",
      "Round  23, Average loss 0.221 Test accuracy 96.000\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0017372724413871766\n",
      "conv1.bias 0.023240545764565468\n",
      "conv2.weight 0.00042633257806301117\n",
      "conv2.bias 0.0032306616194546223\n",
      "fc1.weight 0.0008138497360050678\n",
      "fc1.bias 0.0067166976630687715\n",
      "\n",
      "Test set: Average loss: 0.2599 \n",
      "Accuracy: 9603/10000 (96.03%)\n",
      "\n",
      "Round  24, Average loss 0.260 Test accuracy 96.030\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0016512909531593322\n",
      "conv1.bias 0.023644516244530678\n",
      "conv2.weight 0.0004798872023820877\n",
      "conv2.bias 0.0033703232184052467\n",
      "fc1.weight 0.0009831381961703301\n",
      "fc1.bias 0.005927694216370583\n",
      "\n",
      "Test set: Average loss: 0.3660 \n",
      "Accuracy: 9582/10000 (95.82%)\n",
      "\n",
      "Round  25, Average loss 0.366 Test accuracy 95.820\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.001484971046447754\n",
      "conv1.bias 0.021238036453723907\n",
      "conv2.weight 0.0005840148031711578\n",
      "conv2.bias 0.003493474330753088\n",
      "fc1.weight 0.0008252271451056004\n",
      "fc1.bias 0.006187994033098221\n",
      "\n",
      "Test set: Average loss: 0.2499 \n",
      "Accuracy: 9583/10000 (95.83%)\n",
      "\n",
      "Round  26, Average loss 0.250 Test accuracy 95.830\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0016061438620090485\n",
      "conv1.bias 0.019888753071427345\n",
      "conv2.weight 0.0005183541402220726\n",
      "conv2.bias 0.0034413987305015326\n",
      "fc1.weight 0.0008504831232130528\n",
      "fc1.bias 0.006663608551025391\n",
      "\n",
      "Test set: Average loss: 0.3037 \n",
      "Accuracy: 9517/10000 (95.17%)\n",
      "\n",
      "Round  27, Average loss 0.304 Test accuracy 95.170\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0018173079192638397\n",
      "conv1.bias 0.022325662896037102\n",
      "conv2.weight 0.0004000290855765343\n",
      "conv2.bias 0.003027130849659443\n",
      "fc1.weight 0.0008792783133685589\n",
      "fc1.bias 0.006049492210149765\n",
      "\n",
      "Test set: Average loss: 0.2465 \n",
      "Accuracy: 9624/10000 (96.24%)\n",
      "\n",
      "Round  28, Average loss 0.246 Test accuracy 96.240\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.001572582870721817\n",
      "conv1.bias 0.022488204762339592\n",
      "conv2.weight 0.00048799075186252596\n",
      "conv2.bias 0.0032202613074332476\n",
      "fc1.weight 0.0009180549532175064\n",
      "fc1.bias 0.007253345102071762\n",
      "\n",
      "Test set: Average loss: 0.2272 \n",
      "Accuracy: 9593/10000 (95.93%)\n",
      "\n",
      "Round  29, Average loss 0.227 Test accuracy 95.930\n",
      "3 !!!\n",
      "z_array: [-0.94  -0.73  -0.534 -0.125  0.125  0.534  0.73   0.94 ]\n",
      "0.4486236179368535\n",
      "0.49983046821646326\n",
      "0.48963480280841937\n",
      "0.4857187736123886\n",
      "0.48571877361238697\n",
      "0.4896348028084205\n",
      "0.4998304682164621\n",
      "0.4486236179368556\n",
      "@BACC_Enc: N,K,T, m_i= 8 4 5 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 8 4 5 15000 \n",
      "\n",
      "(T, sigma)= 5 1 )  0 -th Trial!!\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.013575670719146728\n",
      "conv1.bias 0.012905303388834\n",
      "conv2.weight 0.00041601780802011487\n",
      "conv2.bias 0.0003667752316687256\n",
      "fc1.weight 0.0003263113554567099\n",
      "fc1.bias 0.00043729059398174287\n",
      "\n",
      "Test set: Average loss: 2.3025 \n",
      "Accuracy: 1594/10000 (15.94%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 15.940\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0001819874346256256\n",
      "conv1.bias 0.0029500527307391167\n",
      "conv2.weight 0.0002811575680971146\n",
      "conv2.bias 0.0006384659791365266\n",
      "fc1.weight 7.413432467728853e-05\n",
      "fc1.bias 0.0003202957566827536\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1010/10000 (10.10%)\n",
      "\n",
      "Round   1, Average loss 2.303 Test accuracy 10.100\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00010744940489530563\n",
      "conv1.bias 0.0021973121911287308\n",
      "conv2.weight 7.244293112307787e-05\n",
      "conv2.bias 0.001071697217412293\n",
      "fc1.weight 6.522315088659525e-05\n",
      "fc1.bias 0.0004149568732827902\n",
      "\n",
      "Test set: Average loss: 2.2987 \n",
      "Accuracy: 1519/10000 (15.19%)\n",
      "\n",
      "Round   2, Average loss 2.299 Test accuracy 15.190\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0004477892816066742\n",
      "conv1.bias 0.0005828466382808983\n",
      "conv2.weight 8.500395342707634e-05\n",
      "conv2.bias 0.0009833364747464657\n",
      "fc1.weight 0.00020580242853611707\n",
      "fc1.bias 0.00041719842702150347\n",
      "\n",
      "Test set: Average loss: 2.2888 \n",
      "Accuracy: 3412/10000 (34.12%)\n",
      "\n",
      "Round   3, Average loss 2.289 Test accuracy 34.120\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0001811636798083782\n",
      "conv1.bias 0.0019020388135686517\n",
      "conv2.weight 4.764717537909746e-05\n",
      "conv2.bias 0.0011324826627969742\n",
      "fc1.weight 9.866140317171812e-05\n",
      "fc1.bias 0.00047269766218960285\n",
      "\n",
      "Test set: Average loss: 2.3016 \n",
      "Accuracy: 2205/10000 (22.05%)\n",
      "\n",
      "Round   4, Average loss 2.302 Test accuracy 22.050\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0003402380645275116\n",
      "conv1.bias 0.0011634344700723886\n",
      "conv2.weight 7.236283738166094e-05\n",
      "conv2.bias 0.0007849104003980756\n",
      "fc1.weight 0.00018011074280366302\n",
      "fc1.bias 0.0004645814187824726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.2952 \n",
      "Accuracy: 2061/10000 (20.61%)\n",
      "\n",
      "Round   5, Average loss 2.295 Test accuracy 20.610\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00036832012236118314\n",
      "conv1.bias 0.0012810869375243783\n",
      "conv2.weight 8.617975749075413e-05\n",
      "conv2.bias 0.0009723107796162367\n",
      "fc1.weight 0.00023001062218099832\n",
      "fc1.bias 0.0005224475637078286\n",
      "\n",
      "Test set: Average loss: 2.3010 \n",
      "Accuracy: 1853/10000 (18.53%)\n",
      "\n",
      "Round   6, Average loss 2.301 Test accuracy 18.530\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00027801811695098876\n",
      "conv1.bias 0.001669212244451046\n",
      "conv2.weight 3.4159000497311356e-05\n",
      "conv2.bias 0.0007447409443557262\n",
      "fc1.weight 0.00011149424826726318\n",
      "fc1.bias 0.0004980222787708044\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 974/10000 (9.74%)\n",
      "\n",
      "Round   7, Average loss 2.303 Test accuracy 9.740\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0001716284640133381\n",
      "conv1.bias 0.002495931228622794\n",
      "conv2.weight 0.00010263183154165744\n",
      "conv2.bias 0.001184726832434535\n",
      "fc1.weight 0.00022518020123243331\n",
      "fc1.bias 0.0007929818704724312\n",
      "\n",
      "Test set: Average loss: 2.3004 \n",
      "Accuracy: 1828/10000 (18.28%)\n",
      "\n",
      "Round   8, Average loss 2.300 Test accuracy 18.280\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0002028469368815422\n",
      "conv1.bias 0.0014477497898042202\n",
      "conv2.weight 6.469348445534707e-05\n",
      "conv2.bias 0.0012524017365649343\n",
      "fc1.weight 0.0001105002244003117\n",
      "fc1.bias 0.00024758081417530775\n",
      "\n",
      "Test set: Average loss: 2.2941 \n",
      "Accuracy: 2460/10000 (24.60%)\n",
      "\n",
      "Round   9, Average loss 2.294 Test accuracy 24.600\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0003118100017309189\n",
      "conv1.bias 0.0014008840080350637\n",
      "conv2.weight 5.975328851491213e-05\n",
      "conv2.bias 0.0009568382520228624\n",
      "fc1.weight 0.00026281196624040605\n",
      "fc1.bias 0.0006355054676532746\n",
      "\n",
      "Test set: Average loss: 2.2937 \n",
      "Accuracy: 3068/10000 (30.68%)\n",
      "\n",
      "Round  10, Average loss 2.294 Test accuracy 30.680\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0003641755506396294\n",
      "conv1.bias 0.001443220884539187\n",
      "conv2.weight 5.639171227812767e-05\n",
      "conv2.bias 0.0006922500906512141\n",
      "fc1.weight 0.0001414290629327297\n",
      "fc1.bias 0.0003484091255813837\n",
      "\n",
      "Test set: Average loss: 2.1645 \n",
      "Accuracy: 6299/10000 (62.99%)\n",
      "\n",
      "Round  11, Average loss 2.164 Test accuracy 62.990\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00024906668812036515\n",
      "conv1.bias 0.0019981120713055134\n",
      "conv2.weight 4.839726258069277e-05\n",
      "conv2.bias 0.0008329721167683601\n",
      "fc1.weight 0.00017567370086908342\n",
      "fc1.bias 0.0005675703752785922\n",
      "\n",
      "Test set: Average loss: 2.2460 \n",
      "Accuracy: 5644/10000 (56.44%)\n",
      "\n",
      "Round  12, Average loss 2.246 Test accuracy 56.440\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0002884545549750328\n",
      "conv1.bias 0.0014862880343571305\n",
      "conv2.weight 6.854908540844918e-05\n",
      "conv2.bias 0.0007026821840554476\n",
      "fc1.weight 0.00020072751212865114\n",
      "fc1.bias 0.00033014435321092605\n",
      "\n",
      "Test set: Average loss: 2.2535 \n",
      "Accuracy: 5740/10000 (57.40%)\n",
      "\n",
      "Round  13, Average loss 2.254 Test accuracy 57.400\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0001761678047478199\n",
      "conv1.bias 0.0022651818580925465\n",
      "conv2.weight 6.452493369579315e-05\n",
      "conv2.bias 0.0008351808646693826\n",
      "fc1.weight 0.00015119370073080063\n",
      "fc1.bias 0.00019530953140929342\n",
      "\n",
      "Test set: Average loss: 2.2783 \n",
      "Accuracy: 4982/10000 (49.82%)\n",
      "\n",
      "Round  14, Average loss 2.278 Test accuracy 49.820\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0002557102963328362\n",
      "conv1.bias 0.0013038991019129753\n",
      "conv2.weight 4.622858017683029e-05\n",
      "conv2.bias 0.0007534291944466531\n",
      "fc1.weight 0.00015829703770577907\n",
      "fc1.bias 0.00019500483758747578\n",
      "\n",
      "Test set: Average loss: 2.2213 \n",
      "Accuracy: 5612/10000 (56.12%)\n",
      "\n",
      "Round  15, Average loss 2.221 Test accuracy 56.120\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00019886549562215806\n",
      "conv1.bias 0.0026101560797542334\n",
      "conv2.weight 4.4666612520813945e-05\n",
      "conv2.bias 0.0006903081084601581\n",
      "fc1.weight 0.00023033376783132553\n",
      "fc1.bias 0.0007478646002709866\n",
      "\n",
      "Test set: Average loss: 2.1773 \n",
      "Accuracy: 4715/10000 (47.15%)\n",
      "\n",
      "Round  16, Average loss 2.177 Test accuracy 47.150\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00046233482658863067\n",
      "conv1.bias 0.0014037882210686803\n",
      "conv2.weight 0.00014267685823142527\n",
      "conv2.bias 0.000683173886500299\n",
      "fc1.weight 0.00015112458495423198\n",
      "fc1.bias 0.0005492524709552526\n",
      "\n",
      "Test set: Average loss: 2.2017 \n",
      "Accuracy: 5924/10000 (59.24%)\n",
      "\n",
      "Round  17, Average loss 2.202 Test accuracy 59.240\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00015207309275865554\n",
      "conv1.bias 0.0016530368011444807\n",
      "conv2.weight 7.59323826059699e-05\n",
      "conv2.bias 0.0008141479920595884\n",
      "fc1.weight 0.00013397850561887027\n",
      "fc1.bias 0.0007181958761066198\n",
      "\n",
      "Test set: Average loss: 2.1918 \n",
      "Accuracy: 4370/10000 (43.70%)\n",
      "\n",
      "Round  18, Average loss 2.192 Test accuracy 43.700\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0003146689385175705\n",
      "conv1.bias 0.001819405471906066\n",
      "conv2.weight 6.154757924377919e-05\n",
      "conv2.bias 0.0006513592088595033\n",
      "fc1.weight 0.00017696866998448968\n",
      "fc1.bias 0.00032991752959787846\n",
      "\n",
      "Test set: Average loss: 2.0928 \n",
      "Accuracy: 6858/10000 (68.58%)\n",
      "\n",
      "Round  19, Average loss 2.093 Test accuracy 68.580\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0005485139787197113\n",
      "conv1.bias 0.001355698681436479\n",
      "conv2.weight 0.0001362152025103569\n",
      "conv2.bias 0.0006741436664015055\n",
      "fc1.weight 0.00026518611703068017\n",
      "fc1.bias 0.0010115542449057103\n",
      "\n",
      "Test set: Average loss: 2.1085 \n",
      "Accuracy: 5990/10000 (59.90%)\n",
      "\n",
      "Round  20, Average loss 2.108 Test accuracy 59.900\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00024449465796351434\n",
      "conv1.bias 0.0024168251547962427\n",
      "conv2.weight 5.12404553592205e-05\n",
      "conv2.bias 0.0007073708111420274\n",
      "fc1.weight 0.0001586180762387812\n",
      "fc1.bias 0.0009695949032902717\n",
      "\n",
      "Test set: Average loss: 2.1640 \n",
      "Accuracy: 5771/10000 (57.71%)\n",
      "\n",
      "Round  21, Average loss 2.164 Test accuracy 57.710\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0004608115926384926\n",
      "conv1.bias 0.0016425022622570395\n",
      "conv2.weight 6.814303807914257e-05\n",
      "conv2.bias 0.0006450476357713342\n",
      "fc1.weight 0.00016794721595942975\n",
      "fc1.bias 0.0009024075232446193\n",
      "\n",
      "Test set: Average loss: 2.2105 \n",
      "Accuracy: 6147/10000 (61.47%)\n",
      "\n",
      "Round  22, Average loss 2.211 Test accuracy 61.470\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0004312049224972725\n",
      "conv1.bias 0.0013091256842017174\n",
      "conv2.weight 0.0001564224623143673\n",
      "conv2.bias 0.0006903324974700809\n",
      "fc1.weight 0.0002912651048973203\n",
      "fc1.bias 0.00046747219748795034\n",
      "\n",
      "Test set: Average loss: 2.0671 \n",
      "Accuracy: 6896/10000 (68.96%)\n",
      "\n",
      "Round  23, Average loss 2.067 Test accuracy 68.960\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00036230295896530154\n",
      "conv1.bias 0.0018655462190508842\n",
      "conv2.weight 0.00011208368465304375\n",
      "conv2.bias 0.0009503829060122371\n",
      "fc1.weight 0.00018742054235190153\n",
      "fc1.bias 0.0012949260883033276\n",
      "\n",
      "Test set: Average loss: 2.2214 \n",
      "Accuracy: 6818/10000 (68.18%)\n",
      "\n",
      "Round  24, Average loss 2.221 Test accuracy 68.180\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0001982470415532589\n",
      "conv1.bias 0.001610061153769493\n",
      "conv2.weight 6.956416182219982e-05\n",
      "conv2.bias 0.0007887992542237043\n",
      "fc1.weight 0.00025524424854665994\n",
      "fc1.bias 0.00024104069452732802\n",
      "\n",
      "Test set: Average loss: 2.0991 \n",
      "Accuracy: 6383/10000 (63.83%)\n",
      "\n",
      "Round  25, Average loss 2.099 Test accuracy 63.830\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00029021408408880234\n",
      "conv1.bias 0.0023246491327881813\n",
      "conv2.weight 7.617342285811901e-05\n",
      "conv2.bias 0.0008973149233497679\n",
      "fc1.weight 0.00020800121128559113\n",
      "fc1.bias 0.0005293357186019421\n",
      "\n",
      "Test set: Average loss: 2.2647 \n",
      "Accuracy: 5365/10000 (53.65%)\n",
      "\n",
      "Round  26, Average loss 2.265 Test accuracy 53.650\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0002153586596250534\n",
      "conv1.bias 0.002463276032358408\n",
      "conv2.weight 4.01858938857913e-05\n",
      "conv2.bias 0.0007345845806412399\n",
      "fc1.weight 0.00020541788544505834\n",
      "fc1.bias 0.00017986600287258624\n",
      "\n",
      "Test set: Average loss: 2.2050 \n",
      "Accuracy: 6654/10000 (66.54%)\n",
      "\n",
      "Round  27, Average loss 2.205 Test accuracy 66.540\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00030003661289811133\n",
      "conv1.bias 0.0024506011977791786\n",
      "conv2.weight 6.000235676765442e-05\n",
      "conv2.bias 0.0007977450150065124\n",
      "fc1.weight 0.00015651895664632321\n",
      "fc1.bias 0.0005757900886237622\n",
      "\n",
      "Test set: Average loss: 2.1910 \n",
      "Accuracy: 6647/10000 (66.47%)\n",
      "\n",
      "Round  28, Average loss 2.191 Test accuracy 66.470\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00024812377989292145\n",
      "conv1.bias 0.0028797739651054144\n",
      "conv2.weight 5.6375819258391854e-05\n",
      "conv2.bias 0.0008708829409442842\n",
      "fc1.weight 0.00016718071419745683\n",
      "fc1.bias 0.0004325095098465681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.1309 \n",
      "Accuracy: 6367/10000 (63.67%)\n",
      "\n",
      "Round  29, Average loss 2.131 Test accuracy 63.670\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "K = 4\n",
    "T = 5\n",
    "sigma = 1\n",
    "Noise_Alloc = [0,2,4,6,8]\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "\n",
    "N_array = [4, 4, 4, 8]\n",
    "B_array = [0.5]\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 30\n",
    "\n",
    "\n",
    "\n",
    "loss_test_arr_K4_G1_v3 = np.zeros((len(N_array),len(B_array),N_trials,N_epochs))\n",
    "acc_test_arr_K4_G1_v3  = np.zeros((len(N_array),len(B_array),N_trials,N_epochs))\n",
    "\n",
    "for N_idx in range(len(N_array)):\n",
    "    \n",
    "    N = N_array[N_idx]\n",
    "    \n",
    "    \n",
    "           \n",
    "        \n",
    "    # print(\"alpha_array: \",alpha_array,'\\n')\n",
    "    \n",
    "    \n",
    "    for B_idx in range(len(B_array)):\n",
    "        \n",
    "        B = B_array[B_idx]\n",
    "        z_array = []\n",
    "#         while(len(z_array)<N):\n",
    "#             z_tmp = np.random.uniform(-1,1,1)\n",
    "#             MIS_tmp = MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_tmp], 1,sigma)\n",
    "#             if MIS_tmp < B and MIS_tmp > 0.1:\n",
    "#                 z_array.append(z_tmp[0])\n",
    "#         \n",
    "#         z_array = np.sort(z_array)\n",
    "        print(N_idx,'!!!')\n",
    "        if N_idx==0:\n",
    "            z_array = np.array([-0.94,-0.534,0.534, 0.94])\n",
    "        elif N_idx==1:\n",
    "            z_array = np.array([-0.94, -0.73, 0.73, 0.94])\n",
    "        elif N_idx==2:\n",
    "            z_array = np.array([-0.94, -0.125, 0.125, 0.94])\n",
    "        else:\n",
    "            z_array = np.array([-0.94, -0.73, -0.534, -0.125, 0.125, 0.534, 0.73, 0.94])\n",
    "#             z_array = np.array([-0.9, -0.81, -0.22, -0.20, 0.20, 0.22, 0.81, 0.9])\n",
    "            \n",
    "        print('z_array:',z_array)\n",
    "        for j in range(len(z_array)):\n",
    "            print(MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_array[j]], 1,sigma))\n",
    "        \n",
    "        \n",
    "        _Noise_label = np.ones((15000*T,10)) * 0.1\n",
    "\n",
    "        X_tilde,a,b = BACC_Enc_Data_v3(encoding_input_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde,a,b = BACC_Enc_Data_v3(encoding_label_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "\n",
    "        m = N # m is the number of received result @ master\n",
    "    #     print('number of results:',m)\n",
    "\n",
    "        for trial_idx in range(N_trials):\n",
    "            print('(T, sigma)=',T,sigma,') ',trial_idx,'-th Trial!!')\n",
    "\n",
    "            net_glob = CNNMnist2(args=args)\n",
    "            net_glob.cuda()\n",
    "            net_glob.train()\n",
    "\n",
    "            # copy weights\n",
    "            w_glob = net_glob.state_dict()\n",
    "\n",
    "            for iter in range(N_epochs): #args.epochs\n",
    "                w_locals, loss_locals = [], []\n",
    "                idxs_users = np.random.choice(range(N), m, replace=False)\n",
    "                idxs_users = np.sort(idxs_users)\n",
    "                print('selected users:',idxs_users)\n",
    "                \n",
    "                coded_net = BACC_Enc_Model_withNoise_v4(net_glob.cuda(), N, K, T, 1, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "                dec_z_array = []\n",
    "                for idx in idxs_users: #for idx in range(N):\n",
    "            #         print(idx)\n",
    "                    local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[idx,:,:], label=y_tilde[idx,:,:])\n",
    "                    w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "#                     w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                    w_locals.append(copy.deepcopy(w))\n",
    "                    loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                    dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "                # update global weights\n",
    "                #w_glob = FedAvg(w_locals)\n",
    "                w_glob = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "\n",
    "                # copy weight to net_glob\n",
    "                net_glob.load_state_dict(w_glob)\n",
    "\n",
    "                # print loss\n",
    "            #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "            #     loss_train_arr.append(loss_train)\n",
    "\n",
    "                acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "                acc_test_arr_K4_G1_v3[N_idx][B_idx][trial_idx][iter] = acc_test\n",
    "                loss_test_arr_K4_G1_v3[N_idx][B_idx][trial_idx][iter] = loss_test\n",
    "                if iter % 1 ==0:\n",
    "                    print('Round {:3d}, Average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_test, acc_test))\n",
    "                #print(loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2 3\n",
      "##########################################\n",
      "###### 0 -th Trial!! ###########\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.013846602439880371\n",
      "conv1.bias 0.01534747239202261\n",
      "conv2.weight 0.0004163539037108421\n",
      "conv2.bias 0.000362216989742592\n",
      "fc1.weight 0.000324129150249064\n",
      "fc1.bias 0.0003244827501475811\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.013846602439880371\n",
      "conv1.bias 0.01534747239202261\n",
      "conv2.weight 0.0004163539037108421\n",
      "conv2.bias 0.000362216989742592\n",
      "fc1.weight 0.000324129150249064\n",
      "fc1.bias 0.0003244827501475811\n",
      "\n",
      "Test set: Average loss: 2.3025 \n",
      "Accuracy: 1022/10000 (10.22%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0028013825416564942\n",
      "conv1.bias 0.006639664061367512\n",
      "conv2.weight 0.001011577770113945\n",
      "conv2.bias 0.0016109333373606205\n",
      "fc1.weight 0.00031267027370631697\n",
      "fc1.bias 0.0013865872286260128\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0028013825416564942\n",
      "conv1.bias 0.006639664061367512\n",
      "conv2.weight 0.001011577770113945\n",
      "conv2.bias 0.0016109333373606205\n",
      "fc1.weight 0.00031267027370631697\n",
      "fc1.bias 0.0013865872286260128\n",
      "\n",
      "Test set: Average loss: 2.2032 \n",
      "Accuracy: 3401/10000 (34.01%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00039269931614398956\n",
      "conv1.bias 0.017618214711546898\n",
      "conv2.weight 0.00021530190482735634\n",
      "conv2.bias 0.003927913028746843\n",
      "fc1.weight 0.00032709466759115455\n",
      "fc1.bias 0.0026986891403794288\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00039269931614398956\n",
      "conv1.bias 0.017618214711546898\n",
      "conv2.weight 0.00021530190482735634\n",
      "conv2.bias 0.003927913028746843\n",
      "fc1.weight 0.00032709466759115455\n",
      "fc1.bias 0.0026986891403794288\n",
      "\n",
      "Test set: Average loss: 1.4432 \n",
      "Accuracy: 5934/10000 (59.34%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0007650426030158996\n",
      "conv1.bias 0.017743177711963654\n",
      "conv2.weight 0.00038039524108171466\n",
      "conv2.bias 0.004534865729510784\n",
      "fc1.weight 0.0008136188611388206\n",
      "fc1.bias 0.0023599125444889067\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0007650426030158996\n",
      "conv1.bias 0.017743177711963654\n",
      "conv2.weight 0.00038039524108171466\n",
      "conv2.bias 0.004534865729510784\n",
      "fc1.weight 0.0008136188611388206\n",
      "fc1.bias 0.0023599125444889067\n",
      "\n",
      "Test set: Average loss: 0.4655 \n",
      "Accuracy: 9213/10000 (92.13%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0015075656771659852\n",
      "conv1.bias 0.02671993151307106\n",
      "conv2.weight 0.000344356968998909\n",
      "conv2.bias 0.003638477763161063\n",
      "fc1.weight 0.0010540272109210492\n",
      "fc1.bias 0.0030244726687669752\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0015075656771659852\n",
      "conv1.bias 0.02671993151307106\n",
      "conv2.weight 0.000344356968998909\n",
      "conv2.bias 0.003638477763161063\n",
      "fc1.weight 0.0010540272109210492\n",
      "fc1.bias 0.0030244726687669752\n",
      "\n",
      "Test set: Average loss: 0.2633 \n",
      "Accuracy: 9582/10000 (95.82%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001682557910680771\n",
      "conv1.bias 0.02532571740448475\n",
      "conv2.weight 0.0004475625604391098\n",
      "conv2.bias 0.003681638976559043\n",
      "fc1.weight 0.0010330367833375931\n",
      "fc1.bias 0.004244617000222206\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001682557910680771\n",
      "conv1.bias 0.02532571740448475\n",
      "conv2.weight 0.0004475625604391098\n",
      "conv2.bias 0.003681638976559043\n",
      "fc1.weight 0.0010330367833375931\n",
      "fc1.bias 0.004244617000222206\n",
      "\n",
      "Test set: Average loss: 0.3184 \n",
      "Accuracy: 9580/10000 (95.80%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001646498143672943\n",
      "conv1.bias 0.02554057352244854\n",
      "conv2.weight 0.0006072990596294403\n",
      "conv2.bias 0.0038978825323283672\n",
      "fc1.weight 0.0011285742744803428\n",
      "fc1.bias 0.005069185793399811\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001646498143672943\n",
      "conv1.bias 0.02554057352244854\n",
      "conv2.weight 0.0006072990596294403\n",
      "conv2.bias 0.0038978825323283672\n",
      "fc1.weight 0.0011285742744803428\n",
      "fc1.bias 0.005069185793399811\n",
      "\n",
      "Test set: Average loss: 0.2605 \n",
      "Accuracy: 9627/10000 (96.27%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0017947253584861755\n",
      "conv1.bias 0.022792212665081024\n",
      "conv2.weight 0.0005538163706660271\n",
      "conv2.bias 0.003448846749961376\n",
      "fc1.weight 0.0008743705227971077\n",
      "fc1.bias 0.0057027354836463925\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0017947253584861755\n",
      "conv1.bias 0.022792212665081024\n",
      "conv2.weight 0.0005538163706660271\n",
      "conv2.bias 0.003448846749961376\n",
      "fc1.weight 0.0008743705227971077\n",
      "fc1.bias 0.0057027354836463925\n",
      "\n",
      "Test set: Average loss: 0.3055 \n",
      "Accuracy: 9619/10000 (96.19%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001654500514268875\n",
      "conv1.bias 0.024632520973682404\n",
      "conv2.weight 0.000643085166811943\n",
      "conv2.bias 0.0037871787790209055\n",
      "fc1.weight 0.0010451686568558217\n",
      "fc1.bias 0.005250418931245804\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001654500514268875\n",
      "conv1.bias 0.024632520973682404\n",
      "conv2.weight 0.000643085166811943\n",
      "conv2.bias 0.0037871787790209055\n",
      "fc1.weight 0.0010451686568558217\n",
      "fc1.bias 0.005250418931245804\n",
      "\n",
      "Test set: Average loss: 0.2687 \n",
      "Accuracy: 9610/10000 (96.10%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0017231380939483642\n",
      "conv1.bias 0.024419691413640976\n",
      "conv2.weight 0.0005311636254191398\n",
      "conv2.bias 0.0033546851482242346\n",
      "fc1.weight 0.001230197213590145\n",
      "fc1.bias 0.005289020016789436\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0017231380939483642\n",
      "conv1.bias 0.024419691413640976\n",
      "conv2.weight 0.0005311636254191398\n",
      "conv2.bias 0.0033546851482242346\n",
      "fc1.weight 0.001230197213590145\n",
      "fc1.bias 0.005289020016789436\n",
      "\n",
      "Test set: Average loss: 0.2509 \n",
      "Accuracy: 9644/10000 (96.44%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00177312970161438\n",
      "conv1.bias 0.024919545277953148\n",
      "conv2.weight 0.0006292282789945602\n",
      "conv2.bias 0.0035485485568642616\n",
      "fc1.weight 0.001186804659664631\n",
      "fc1.bias 0.005671864002943039\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00177312970161438\n",
      "conv1.bias 0.024919545277953148\n",
      "conv2.weight 0.0006292282789945602\n",
      "conv2.bias 0.0035485485568642616\n",
      "fc1.weight 0.001186804659664631\n",
      "fc1.bias 0.005671864002943039\n",
      "\n",
      "Test set: Average loss: 0.2855 \n",
      "Accuracy: 9635/10000 (96.35%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019083766639232636\n",
      "conv1.bias 0.022080838680267334\n",
      "conv2.weight 0.0006543938815593719\n",
      "conv2.bias 0.003248341614380479\n",
      "fc1.weight 0.0012552499771118163\n",
      "fc1.bias 0.006261911988258362\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019083766639232636\n",
      "conv1.bias 0.022080838680267334\n",
      "conv2.weight 0.0006543938815593719\n",
      "conv2.bias 0.003248341614380479\n",
      "fc1.weight 0.0012552499771118163\n",
      "fc1.bias 0.006261911988258362\n",
      "\n",
      "Test set: Average loss: 0.2241 \n",
      "Accuracy: 9643/10000 (96.43%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001879166215658188\n",
      "conv1.bias 0.021471770480275154\n",
      "conv2.weight 0.0006438188999891281\n",
      "conv2.bias 0.0036032169591635466\n",
      "fc1.weight 0.0011701577343046666\n",
      "fc1.bias 0.005347852408885956\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001879166215658188\n",
      "conv1.bias 0.021471770480275154\n",
      "conv2.weight 0.0006438188999891281\n",
      "conv2.bias 0.0036032169591635466\n",
      "fc1.weight 0.0011701577343046666\n",
      "fc1.bias 0.005347852408885956\n",
      "\n",
      "Test set: Average loss: 0.2320 \n",
      "Accuracy: 9634/10000 (96.34%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0017890074849128722\n",
      "conv1.bias 0.02458171546459198\n",
      "conv2.weight 0.0006137474998831749\n",
      "conv2.bias 0.00331497797742486\n",
      "fc1.weight 0.0011547839269042016\n",
      "fc1.bias 0.004213361069560051\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0017890074849128722\n",
      "conv1.bias 0.02458171546459198\n",
      "conv2.weight 0.0006137474998831749\n",
      "conv2.bias 0.00331497797742486\n",
      "fc1.weight 0.0011547839269042016\n",
      "fc1.bias 0.004213361069560051\n",
      "\n",
      "Test set: Average loss: 0.2286 \n",
      "Accuracy: 9637/10000 (96.37%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019208726286888123\n",
      "conv1.bias 0.024848811328411102\n",
      "conv2.weight 0.0006148033589124679\n",
      "conv2.bias 0.00345378159545362\n",
      "fc1.weight 0.0010720452293753624\n",
      "fc1.bias 0.004235822707414627\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019208726286888123\n",
      "conv1.bias 0.024848811328411102\n",
      "conv2.weight 0.0006148033589124679\n",
      "conv2.bias 0.00345378159545362\n",
      "fc1.weight 0.0010720452293753624\n",
      "fc1.bias 0.004235822707414627\n",
      "\n",
      "Test set: Average loss: 0.2384 \n",
      "Accuracy: 9637/10000 (96.37%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0017468039691448211\n",
      "conv1.bias 0.023503847420215607\n",
      "conv2.weight 0.0007071304321289063\n",
      "conv2.bias 0.003680350724607706\n",
      "fc1.weight 0.0010233703069388866\n",
      "fc1.bias 0.005778663232922554\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0017468039691448211\n",
      "conv1.bias 0.023503847420215607\n",
      "conv2.weight 0.0007071304321289063\n",
      "conv2.bias 0.003680350724607706\n",
      "fc1.weight 0.0010233703069388866\n",
      "fc1.bias 0.005778663232922554\n",
      "\n",
      "Test set: Average loss: 0.2583 \n",
      "Accuracy: 9626/10000 (96.26%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018546444177627564\n",
      "conv1.bias 0.02410038933157921\n",
      "conv2.weight 0.0006891255080699921\n",
      "conv2.bias 0.003520965576171875\n",
      "fc1.weight 0.001382106263190508\n",
      "fc1.bias 0.006421586126089096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected users: [0 1]\n",
      "conv1.weight 0.0018546444177627564\n",
      "conv1.bias 0.02410038933157921\n",
      "conv2.weight 0.0006891255080699921\n",
      "conv2.bias 0.003520965576171875\n",
      "fc1.weight 0.001382106263190508\n",
      "fc1.bias 0.006421586126089096\n",
      "\n",
      "Test set: Average loss: 0.2391 \n",
      "Accuracy: 9630/10000 (96.30%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019441361725330353\n",
      "conv1.bias 0.025488246232271194\n",
      "conv2.weight 0.0006017382815480233\n",
      "conv2.bias 0.0034130322746932507\n",
      "fc1.weight 0.001223507523536682\n",
      "fc1.bias 0.006876526772975922\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019441361725330353\n",
      "conv1.bias 0.025488246232271194\n",
      "conv2.weight 0.0006017382815480233\n",
      "conv2.bias 0.0034130322746932507\n",
      "fc1.weight 0.001223507523536682\n",
      "fc1.bias 0.006876526772975922\n",
      "\n",
      "Test set: Average loss: 0.2185 \n",
      "Accuracy: 9661/10000 (96.61%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002030629515647888\n",
      "conv1.bias 0.02344357781112194\n",
      "conv2.weight 0.0004928788542747498\n",
      "conv2.bias 0.0030704722739756107\n",
      "fc1.weight 0.0009367094375193119\n",
      "fc1.bias 0.0056505478918552395\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002030629515647888\n",
      "conv1.bias 0.02344357781112194\n",
      "conv2.weight 0.0004928788542747498\n",
      "conv2.bias 0.0030704722739756107\n",
      "fc1.weight 0.0009367094375193119\n",
      "fc1.bias 0.0056505478918552395\n",
      "\n",
      "Test set: Average loss: 0.2134 \n",
      "Accuracy: 9645/10000 (96.45%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001934327930212021\n",
      "conv1.bias 0.02430254966020584\n",
      "conv2.weight 0.0005889856442809105\n",
      "conv2.bias 0.0033655010629445314\n",
      "fc1.weight 0.00097779156640172\n",
      "fc1.bias 0.005882130935788155\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001934327930212021\n",
      "conv1.bias 0.02430254966020584\n",
      "conv2.weight 0.0005889856442809105\n",
      "conv2.bias 0.0033655010629445314\n",
      "fc1.weight 0.00097779156640172\n",
      "fc1.bias 0.005882130935788155\n",
      "\n",
      "Test set: Average loss: 0.2570 \n",
      "Accuracy: 9638/10000 (96.38%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019179049134254455\n",
      "conv1.bias 0.023630116134881973\n",
      "conv2.weight 0.0005873883515596389\n",
      "conv2.bias 0.0033048829063773155\n",
      "fc1.weight 0.0008891508914530277\n",
      "fc1.bias 0.005005907267332077\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019179049134254455\n",
      "conv1.bias 0.023630116134881973\n",
      "conv2.weight 0.0005873883515596389\n",
      "conv2.bias 0.0033048829063773155\n",
      "fc1.weight 0.0008891508914530277\n",
      "fc1.bias 0.005005907267332077\n",
      "\n",
      "Test set: Average loss: 0.2494 \n",
      "Accuracy: 9619/10000 (96.19%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021093110740184784\n",
      "conv1.bias 0.02701818197965622\n",
      "conv2.weight 0.00062333595007658\n",
      "conv2.bias 0.003382615279406309\n",
      "fc1.weight 0.00111912302672863\n",
      "fc1.bias 0.0047159653156995775\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021093110740184784\n",
      "conv1.bias 0.02701818197965622\n",
      "conv2.weight 0.00062333595007658\n",
      "conv2.bias 0.003382615279406309\n",
      "fc1.weight 0.00111912302672863\n",
      "fc1.bias 0.0047159653156995775\n",
      "\n",
      "Test set: Average loss: 0.2479 \n",
      "Accuracy: 9650/10000 (96.50%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002040301412343979\n",
      "conv1.bias 0.024602331221103668\n",
      "conv2.weight 0.0005993127077817917\n",
      "conv2.bias 0.0031656399369239807\n",
      "fc1.weight 0.0011612329632043838\n",
      "fc1.bias 0.005364274978637696\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002040301412343979\n",
      "conv1.bias 0.024602331221103668\n",
      "conv2.weight 0.0005993127077817917\n",
      "conv2.bias 0.0031656399369239807\n",
      "fc1.weight 0.0011612329632043838\n",
      "fc1.bias 0.005364274978637696\n",
      "\n",
      "Test set: Average loss: 0.2747 \n",
      "Accuracy: 9633/10000 (96.33%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019210700690746308\n",
      "conv1.bias 0.02496698684990406\n",
      "conv2.weight 0.0006644032150506973\n",
      "conv2.bias 0.00343182566575706\n",
      "fc1.weight 0.0008658980950713158\n",
      "fc1.bias 0.005114149302244186\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019210700690746308\n",
      "conv1.bias 0.02496698684990406\n",
      "conv2.weight 0.0006644032150506973\n",
      "conv2.bias 0.00343182566575706\n",
      "fc1.weight 0.0008658980950713158\n",
      "fc1.bias 0.005114149302244186\n",
      "\n",
      "Test set: Average loss: 0.2544 \n",
      "Accuracy: 9623/10000 (96.23%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018740186095237731\n",
      "conv1.bias 0.023235397413372993\n",
      "conv2.weight 0.0006638172268867492\n",
      "conv2.bias 0.003347791265696287\n",
      "fc1.weight 0.0009114651009440422\n",
      "fc1.bias 0.005235223844647407\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018740186095237731\n",
      "conv1.bias 0.023235397413372993\n",
      "conv2.weight 0.0006638172268867492\n",
      "conv2.bias 0.003347791265696287\n",
      "fc1.weight 0.0009114651009440422\n",
      "fc1.bias 0.005235223844647407\n",
      "\n",
      "Test set: Average loss: 0.2567 \n",
      "Accuracy: 9616/10000 (96.16%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018924450874328613\n",
      "conv1.bias 0.024511704221367836\n",
      "conv2.weight 0.0006046704947948456\n",
      "conv2.bias 0.0033667709212750196\n",
      "fc1.weight 0.0009506555274128914\n",
      "fc1.bias 0.005217891931533813\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018924450874328613\n",
      "conv1.bias 0.024511704221367836\n",
      "conv2.weight 0.0006046704947948456\n",
      "conv2.bias 0.0033667709212750196\n",
      "fc1.weight 0.0009506555274128914\n",
      "fc1.bias 0.005217891931533813\n",
      "\n",
      "Test set: Average loss: 0.2704 \n",
      "Accuracy: 9650/10000 (96.50%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021071302890777587\n",
      "conv1.bias 0.024374231696128845\n",
      "conv2.weight 0.0005577811971306801\n",
      "conv2.bias 0.003126278519630432\n",
      "fc1.weight 0.0009336905553936958\n",
      "fc1.bias 0.005237878859043121\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021071302890777587\n",
      "conv1.bias 0.024374231696128845\n",
      "conv2.weight 0.0005577811971306801\n",
      "conv2.bias 0.003126278519630432\n",
      "fc1.weight 0.0009336905553936958\n",
      "fc1.bias 0.005237878859043121\n",
      "\n",
      "Test set: Average loss: 0.2570 \n",
      "Accuracy: 9639/10000 (96.39%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021634113788604734\n",
      "conv1.bias 0.022952305153012276\n",
      "conv2.weight 0.0006425368785858154\n",
      "conv2.bias 0.00325667392462492\n",
      "fc1.weight 0.0010215234011411666\n",
      "fc1.bias 0.005327563360333442\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021634113788604734\n",
      "conv1.bias 0.022952305153012276\n",
      "conv2.weight 0.0006425368785858154\n",
      "conv2.bias 0.00325667392462492\n",
      "fc1.weight 0.0010215234011411666\n",
      "fc1.bias 0.005327563360333442\n",
      "\n",
      "Test set: Average loss: 0.2432 \n",
      "Accuracy: 9647/10000 (96.47%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0022188551723957063\n",
      "conv1.bias 0.023036787286400795\n",
      "conv2.weight 0.0006302541494369507\n",
      "conv2.bias 0.003228414338082075\n",
      "fc1.weight 0.0011357300914824008\n",
      "fc1.bias 0.004959239810705185\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0022188551723957063\n",
      "conv1.bias 0.023036787286400795\n",
      "conv2.weight 0.0006302541494369507\n",
      "conv2.bias 0.003228414338082075\n",
      "fc1.weight 0.0011357300914824008\n",
      "fc1.bias 0.004959239810705185\n",
      "\n",
      "Test set: Average loss: 0.2516 \n",
      "Accuracy: 9647/10000 (96.47%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00198604479432106\n",
      "conv1.bias 0.023269804194569588\n",
      "conv2.weight 0.0006691012531518936\n",
      "conv2.bias 0.003226121189072728\n",
      "fc1.weight 0.0013855775818228722\n",
      "fc1.bias 0.004936210811138153\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00198604479432106\n",
      "conv1.bias 0.023269804194569588\n",
      "conv2.weight 0.0006691012531518936\n",
      "conv2.bias 0.003226121189072728\n",
      "fc1.weight 0.0013855775818228722\n",
      "fc1.bias 0.004936210811138153\n",
      "\n",
      "Test set: Average loss: 0.2424 \n",
      "Accuracy: 9648/10000 (96.48%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "G = 2\n",
    "N = 4 # N should be divisible by G\n",
    "K = 4 # N should be divisible by G\n",
    "\n",
    "size_per_group = int(60000/G)\n",
    "\n",
    "X_group = np.reshape(encoding_input_array_np, (G,size_per_group,28*28))\n",
    "y_group = np.reshape(encoding_label_array_np, (G,size_per_group,args.num_classes)) \n",
    "\n",
    "N_i = int(N/G) # = 2\n",
    "K_i = int(K/G) # = 2\n",
    "T = 3\n",
    "sigma = 1\n",
    "Noise_Alloc = [0,2,4]\n",
    "m = N_i # number of selected workers (if there is no straggler, m=N_i)\n",
    "\n",
    "print(N_i,K_i,T)\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K_i+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K_i+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K_i+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "z_array = np.array([-0.81, 0.81])\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 30\n",
    "\n",
    "loss_test_arr_G2_N4_v3 = np.zeros((N_trials,N_epochs))\n",
    "acc_test_arr_G2_N4_v3  = np.zeros((N_trials,N_epochs))\n",
    "\n",
    "for trial_idx in range(N_trials):\n",
    "    \n",
    "    print('##########################################')\n",
    "    print('######',trial_idx,'-th Trial!! ###########')\n",
    "    \n",
    "    net_glob = CNNMnist2(args=args)\n",
    "    net_glob.cuda()\n",
    "    net_glob.train()\n",
    "    \n",
    "    # copy weights\n",
    "    w_glob = net_glob.state_dict()\n",
    "    \n",
    "    X_tilde = np.empty((N,Size_submatrices,28*28))\n",
    "    y_tilde = np.empty((N,Size_submatrices,10))\n",
    "    \n",
    "    for G_idx in range(G):\n",
    "        \n",
    "        _Noise_label = np.ones((size_per_group*T,10)) * 0.1\n",
    "        \n",
    "        X_tilde_tmp,a,b = BACC_Enc_Data_v3(X_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde_tmp,a,b = BACC_Enc_Data_v3(y_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "        \n",
    "        stt_pos = G_idx * N_i\n",
    "        end_pos = (G_idx+1) * N_i\n",
    "        \n",
    "        X_tilde[stt_pos:end_pos,:,:] = X_tilde_tmp\n",
    "        y_tilde[stt_pos:end_pos,:,:] = y_tilde_tmp\n",
    "        \n",
    "   \n",
    "\n",
    "    for iter in range(N_epochs): #args.epochs\n",
    "        \n",
    "        w_group_array = []\n",
    "        for G_idx in range(G):\n",
    "            w_locals, loss_locals = [], []\n",
    "            idxs_users = np.random.choice(range(N_i), m, replace=False)\n",
    "            idxs_users = np.sort(idxs_users)\n",
    "            print('selected users:',idxs_users)\n",
    "\n",
    "            coded_net = BACC_Enc_Model_withNoise_v4(net_glob.cuda(), N_i, K_i, T, 1, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "            dec_z_array = []\n",
    "            for idx in idxs_users: #for idx in range(N):\n",
    "        #         print(idx)\n",
    "                local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[G_idx*N_i+idx,:,:], label=y_tilde[G_idx*N_i+idx,:,:])\n",
    "                w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "#                 w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                w_locals.append(copy.deepcopy(w))\n",
    "                loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "            # update global weights\n",
    "            #w_glob = FedAvg(w_locals)\n",
    "            w_group = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "            \n",
    "            w_group_array.append(copy.deepcopy(w_group))\n",
    "        \n",
    "        w_glob = copy.deepcopy(w_group_array[0])\n",
    "        for k in w_glob.keys():\n",
    "            for G_idx in range(1,G):\n",
    "                w_glob[k] += w_group_array[G_idx][k]\n",
    "            w_glob[k] = torch.div(w_glob[k], len(w_group_array))\n",
    "        \n",
    "        # copy weight to net_glob\n",
    "        net_glob.load_state_dict(w_glob)\n",
    "\n",
    "        # print loss\n",
    "    #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "    #     loss_train_arr.append(loss_train)\n",
    "\n",
    "        acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "        acc_test_arr_G2_N4_v3[trial_idx][iter] = acc_test\n",
    "        loss_test_arr_G2_N4_v3[trial_idx][iter] = loss_test\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2 3\n",
      "##########################################\n",
      "###### 0 -th Trial!! ###########\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 15000 \n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.012238214015960694\n",
      "conv1.bias 0.012197405099868774\n",
      "conv2.weight 0.00041676975786685943\n",
      "conv2.bias 0.0004572461766656488\n",
      "fc1.weight 0.0003275977214798331\n",
      "fc1.bias 0.00024432113859802486\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.012238214015960694\n",
      "conv1.bias 0.012197405099868774\n",
      "conv2.weight 0.00041676975786685943\n",
      "conv2.bias 0.0004572461766656488\n",
      "fc1.weight 0.0003275977214798331\n",
      "fc1.bias 0.00024432113859802486\n",
      "\n",
      "Test set: Average loss: 2.3025 \n",
      "Accuracy: 1595/10000 (15.95%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00032409843057394026\n",
      "conv1.bias 0.0013604930136352777\n",
      "conv2.weight 0.00028322940692305566\n",
      "conv2.bias 0.0005288161337375641\n",
      "fc1.weight 6.303526461124421e-05\n",
      "fc1.bias 0.00033167744986712934\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00032409843057394026\n",
      "conv1.bias 0.0013604930136352777\n",
      "conv2.weight 0.00028322940692305566\n",
      "conv2.bias 0.0005288161337375641\n",
      "fc1.weight 6.303526461124421e-05\n",
      "fc1.bias 0.00033167744986712934\n",
      "\n",
      "Test set: Average loss: 2.3020 \n",
      "Accuracy: 1939/10000 (19.39%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00012632465921342374\n",
      "conv1.bias 0.002227664925158024\n",
      "conv2.weight 8.447748608887195e-05\n",
      "conv2.bias 0.000817574211396277\n",
      "fc1.weight 8.046133443713188e-05\n",
      "fc1.bias 0.00030036643147468567\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00012632465921342374\n",
      "conv1.bias 0.002227664925158024\n",
      "conv2.weight 8.447748608887195e-05\n",
      "conv2.bias 0.000817574211396277\n",
      "fc1.weight 8.046133443713188e-05\n",
      "fc1.bias 0.00030036643147468567\n",
      "\n",
      "Test set: Average loss: 2.2537 \n",
      "Accuracy: 4499/10000 (44.99%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0003543279320001602\n",
      "conv1.bias 0.0014963981229811907\n",
      "conv2.weight 5.145753733813763e-05\n",
      "conv2.bias 0.0009247111156582832\n",
      "fc1.weight 0.00013917067553848028\n",
      "fc1.bias 0.0011611338704824448\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0003543279320001602\n",
      "conv1.bias 0.0014963981229811907\n",
      "conv2.weight 5.145753733813763e-05\n",
      "conv2.bias 0.0009247111156582832\n",
      "fc1.weight 0.00013917067553848028\n",
      "fc1.bias 0.0011611338704824448\n",
      "\n",
      "Test set: Average loss: 2.1877 \n",
      "Accuracy: 6014/10000 (60.14%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00026002053171396254\n",
      "conv1.bias 0.0010987990535795689\n",
      "conv2.weight 0.00011063654907047748\n",
      "conv2.bias 0.0008152778027579188\n",
      "fc1.weight 0.00018959415610879659\n",
      "fc1.bias 0.0011879747733473777\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00026002053171396254\n",
      "conv1.bias 0.0010987990535795689\n",
      "conv2.weight 0.00011063654907047748\n",
      "conv2.bias 0.0008152778027579188\n",
      "fc1.weight 0.00018959415610879659\n",
      "fc1.bias 0.0011879747733473777\n",
      "\n",
      "Test set: Average loss: 2.3006 \n",
      "Accuracy: 1051/10000 (10.51%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0005406852439045906\n",
      "conv1.bias 0.0014294799184426665\n",
      "conv2.weight 0.00024870313704013826\n",
      "conv2.bias 0.000934129930101335\n",
      "fc1.weight 0.00010942546650767326\n",
      "fc1.bias 0.0003271390683948994\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0005406852439045906\n",
      "conv1.bias 0.0014294799184426665\n",
      "conv2.weight 0.00024870313704013826\n",
      "conv2.bias 0.000934129930101335\n",
      "fc1.weight 0.00010942546650767326\n",
      "fc1.bias 0.0003271390683948994\n",
      "\n",
      "Test set: Average loss: 2.3024 \n",
      "Accuracy: 958/10000 (9.58%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00024091765284538268\n",
      "conv1.bias 0.0011944530997425318\n",
      "conv2.weight 4.600471816956997e-05\n",
      "conv2.bias 0.0007271879585459828\n",
      "fc1.weight 7.76805158238858e-05\n",
      "fc1.bias 0.0001642424613237381\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00024091765284538268\n",
      "conv1.bias 0.0011944530997425318\n",
      "conv2.weight 4.600471816956997e-05\n",
      "conv2.bias 0.0007271879585459828\n",
      "fc1.weight 7.76805158238858e-05\n",
      "fc1.bias 0.0001642424613237381\n",
      "\n",
      "Test set: Average loss: 2.2819 \n",
      "Accuracy: 3673/10000 (36.73%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00029405776411294937\n",
      "conv1.bias 0.0008406575652770698\n",
      "conv2.weight 0.00018544413149356842\n",
      "conv2.bias 0.0015138806775212288\n",
      "fc1.weight 0.00011581910075619817\n",
      "fc1.bias 0.0005465793423354625\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00029405776411294937\n",
      "conv1.bias 0.0008406575652770698\n",
      "conv2.weight 0.00018544413149356842\n",
      "conv2.bias 0.0015138806775212288\n",
      "fc1.weight 0.00011581910075619817\n",
      "fc1.bias 0.0005465793423354625\n",
      "\n",
      "Test set: Average loss: 2.2761 \n",
      "Accuracy: 3924/10000 (39.24%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0002469045296311378\n",
      "conv1.bias 0.001910212216898799\n",
      "conv2.weight 7.984833791851997e-05\n",
      "conv2.bias 0.0009626801474951208\n",
      "fc1.weight 0.00013306264299899339\n",
      "fc1.bias 0.0004935250617563725\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0002469045296311378\n",
      "conv1.bias 0.001910212216898799\n",
      "conv2.weight 7.984833791851997e-05\n",
      "conv2.bias 0.0009626801474951208\n",
      "fc1.weight 0.00013306264299899339\n",
      "fc1.bias 0.0004935250617563725\n",
      "\n",
      "Test set: Average loss: 2.2380 \n",
      "Accuracy: 5801/10000 (58.01%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004236386716365814\n",
      "conv1.bias 0.000873232027515769\n",
      "conv2.weight 0.00011711448431015015\n",
      "conv2.bias 0.0009446431649848819\n",
      "fc1.weight 0.00024941384326666595\n",
      "fc1.bias 0.0007467037532478571\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004236386716365814\n",
      "conv1.bias 0.000873232027515769\n",
      "conv2.weight 0.00011711448431015015\n",
      "conv2.bias 0.0009446431649848819\n",
      "fc1.weight 0.00024941384326666595\n",
      "fc1.bias 0.0007467037532478571\n",
      "\n",
      "Test set: Average loss: 2.0854 \n",
      "Accuracy: 7478/10000 (74.78%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0002992149628698826\n",
      "conv1.bias 0.0029762922786176205\n",
      "conv2.weight 8.929801173508167e-05\n",
      "conv2.bias 0.00142100325319916\n",
      "fc1.weight 0.00015891126822680234\n",
      "fc1.bias 0.0007367669139057397\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0002992149628698826\n",
      "conv1.bias 0.0029762922786176205\n",
      "conv2.weight 8.929801173508167e-05\n",
      "conv2.bias 0.00142100325319916\n",
      "fc1.weight 0.00015891126822680234\n",
      "fc1.bias 0.0007367669139057397\n",
      "\n",
      "Test set: Average loss: 2.0804 \n",
      "Accuracy: 8070/10000 (80.70%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00029652398079633714\n",
      "conv1.bias 0.0027771107852458954\n",
      "conv2.weight 6.284372881054878e-05\n",
      "conv2.bias 0.0009871851652860641\n",
      "fc1.weight 0.0001799940480850637\n",
      "fc1.bias 0.00080221276730299\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00029652398079633714\n",
      "conv1.bias 0.0027771107852458954\n",
      "conv2.weight 6.284372881054878e-05\n",
      "conv2.bias 0.0009871851652860641\n",
      "fc1.weight 0.0001799940480850637\n",
      "fc1.bias 0.00080221276730299\n",
      "\n",
      "Test set: Average loss: 2.1178 \n",
      "Accuracy: 7771/10000 (77.71%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00034073900431394574\n",
      "conv1.bias 0.003768517868593335\n",
      "conv2.weight 5.977261811494827e-05\n",
      "conv2.bias 0.0009133227868005633\n",
      "fc1.weight 0.000176446873228997\n",
      "fc1.bias 0.0007011857815086842\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00034073900431394574\n",
      "conv1.bias 0.003768517868593335\n",
      "conv2.weight 5.977261811494827e-05\n",
      "conv2.bias 0.0009133227868005633\n",
      "fc1.weight 0.000176446873228997\n",
      "fc1.bias 0.0007011857815086842\n",
      "\n",
      "Test set: Average loss: 2.1481 \n",
      "Accuracy: 7532/10000 (75.32%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004662966728210449\n",
      "conv1.bias 0.002142732497304678\n",
      "conv2.weight 6.903477013111115e-05\n",
      "conv2.bias 0.000789778889156878\n",
      "fc1.weight 0.00018303351243957876\n",
      "fc1.bias 0.0007621862925589085\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004662966728210449\n",
      "conv1.bias 0.002142732497304678\n",
      "conv2.weight 6.903477013111115e-05\n",
      "conv2.bias 0.000789778889156878\n",
      "fc1.weight 0.00018303351243957876\n",
      "fc1.bias 0.0007621862925589085\n",
      "\n",
      "Test set: Average loss: 2.0761 \n",
      "Accuracy: 8577/10000 (85.77%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004857601597905159\n",
      "conv1.bias 0.0011329988483339548\n",
      "conv2.weight 5.9414226561784745e-05\n",
      "conv2.bias 0.0007254766533151269\n",
      "fc1.weight 0.0002566825831308961\n",
      "fc1.bias 0.0011475862935185432\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004857601597905159\n",
      "conv1.bias 0.0011329988483339548\n",
      "conv2.weight 5.9414226561784745e-05\n",
      "conv2.bias 0.0007254766533151269\n",
      "fc1.weight 0.0002566825831308961\n",
      "fc1.bias 0.0011475862935185432\n",
      "\n",
      "Test set: Average loss: 2.1418 \n",
      "Accuracy: 7512/10000 (75.12%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004403708875179291\n",
      "conv1.bias 0.0010100736981257796\n",
      "conv2.weight 0.00016050683334469795\n",
      "conv2.bias 0.0007882289355620742\n",
      "fc1.weight 0.00020547243766486645\n",
      "fc1.bias 0.0006706926971673966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004403708875179291\n",
      "conv1.bias 0.0010100736981257796\n",
      "conv2.weight 0.00016050683334469795\n",
      "conv2.bias 0.0007882289355620742\n",
      "fc1.weight 0.00020547243766486645\n",
      "fc1.bias 0.0006706926971673966\n",
      "\n",
      "Test set: Average loss: 2.2047 \n",
      "Accuracy: 6005/10000 (60.05%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0003312908858060837\n",
      "conv1.bias 0.0010048537515103817\n",
      "conv2.weight 0.00017637999728322028\n",
      "conv2.bias 0.0008865775307640433\n",
      "fc1.weight 9.666537516750395e-05\n",
      "fc1.bias 0.000818482879549265\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0003312908858060837\n",
      "conv1.bias 0.0010048537515103817\n",
      "conv2.weight 0.00017637999728322028\n",
      "conv2.bias 0.0008865775307640433\n",
      "fc1.weight 9.666537516750395e-05\n",
      "fc1.bias 0.000818482879549265\n",
      "\n",
      "Test set: Average loss: 2.2191 \n",
      "Accuracy: 3909/10000 (39.09%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00034947231411933897\n",
      "conv1.bias 0.0019296954851597548\n",
      "conv2.weight 3.93584044650197e-05\n",
      "conv2.bias 0.0006756547372788191\n",
      "fc1.weight 0.0002297444734722376\n",
      "fc1.bias 0.0005597282666713\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00034947231411933897\n",
      "conv1.bias 0.0019296954851597548\n",
      "conv2.weight 3.93584044650197e-05\n",
      "conv2.bias 0.0006756547372788191\n",
      "fc1.weight 0.0002297444734722376\n",
      "fc1.bias 0.0005597282666713\n",
      "\n",
      "Test set: Average loss: 2.1884 \n",
      "Accuracy: 6471/10000 (64.71%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0003902480751276016\n",
      "conv1.bias 0.0011002495884895325\n",
      "conv2.weight 4.019440151751042e-05\n",
      "conv2.bias 0.0006182012148201466\n",
      "fc1.weight 0.00023457258939743041\n",
      "fc1.bias 0.0005255976691842079\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0003902480751276016\n",
      "conv1.bias 0.0011002495884895325\n",
      "conv2.weight 4.019440151751042e-05\n",
      "conv2.bias 0.0006182012148201466\n",
      "fc1.weight 0.00023457258939743041\n",
      "fc1.bias 0.0005255976691842079\n",
      "\n",
      "Test set: Average loss: 2.0791 \n",
      "Accuracy: 7483/10000 (74.83%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.000428064651787281\n",
      "conv1.bias 0.0011259911116212606\n",
      "conv2.weight 0.00016956297680735587\n",
      "conv2.bias 0.000753542990423739\n",
      "fc1.weight 0.0002500317757949233\n",
      "fc1.bias 0.0005286134779453278\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.000428064651787281\n",
      "conv1.bias 0.0011259911116212606\n",
      "conv2.weight 0.00016956297680735587\n",
      "conv2.bias 0.000753542990423739\n",
      "fc1.weight 0.0002500317757949233\n",
      "fc1.bias 0.0005286134779453278\n",
      "\n",
      "Test set: Average loss: 2.1476 \n",
      "Accuracy: 7451/10000 (74.51%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004134809970855713\n",
      "conv1.bias 0.0010070878779515624\n",
      "conv2.weight 0.00011188249103724957\n",
      "conv2.bias 0.0007216766243800521\n",
      "fc1.weight 0.00022850334644317627\n",
      "fc1.bias 0.0008201885037124157\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004134809970855713\n",
      "conv1.bias 0.0010070878779515624\n",
      "conv2.weight 0.00011188249103724957\n",
      "conv2.bias 0.0007216766243800521\n",
      "fc1.weight 0.00022850334644317627\n",
      "fc1.bias 0.0008201885037124157\n",
      "\n",
      "Test set: Average loss: 2.1519 \n",
      "Accuracy: 7400/10000 (74.00%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0005622725561261178\n",
      "conv1.bias 0.0014971812488511205\n",
      "conv2.weight 4.7476068139076234e-05\n",
      "conv2.bias 0.0006731998873874545\n",
      "fc1.weight 0.00021044411696493625\n",
      "fc1.bias 0.00030260724015533923\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0005622725561261178\n",
      "conv1.bias 0.0014971812488511205\n",
      "conv2.weight 4.7476068139076234e-05\n",
      "conv2.bias 0.0006731998873874545\n",
      "fc1.weight 0.00021044411696493625\n",
      "fc1.bias 0.00030260724015533923\n",
      "\n",
      "Test set: Average loss: 2.1689 \n",
      "Accuracy: 8045/10000 (80.45%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0003941396996378899\n",
      "conv1.bias 0.0011060817632824183\n",
      "conv2.weight 7.927875965833664e-05\n",
      "conv2.bias 0.000648144050501287\n",
      "fc1.weight 0.0001426650327630341\n",
      "fc1.bias 0.00079368706792593\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0003941396996378899\n",
      "conv1.bias 0.0011060817632824183\n",
      "conv2.weight 7.927875965833664e-05\n",
      "conv2.bias 0.000648144050501287\n",
      "fc1.weight 0.0001426650327630341\n",
      "fc1.bias 0.00079368706792593\n",
      "\n",
      "Test set: Average loss: 2.1173 \n",
      "Accuracy: 7873/10000 (78.73%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00037938326597213744\n",
      "conv1.bias 0.002348514273762703\n",
      "conv2.weight 5.411738995462656e-05\n",
      "conv2.bias 0.0007538808858953416\n",
      "fc1.weight 0.00016853873385116457\n",
      "fc1.bias 0.0005132935475558043\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00037938326597213744\n",
      "conv1.bias 0.002348514273762703\n",
      "conv2.weight 5.411738995462656e-05\n",
      "conv2.bias 0.0007538808858953416\n",
      "fc1.weight 0.00016853873385116457\n",
      "fc1.bias 0.0005132935475558043\n",
      "\n",
      "Test set: Average loss: 2.1086 \n",
      "Accuracy: 7339/10000 (73.39%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.000515025220811367\n",
      "conv1.bias 0.0013434677384793758\n",
      "conv2.weight 0.00024815727025270464\n",
      "conv2.bias 0.0008134583476930857\n",
      "fc1.weight 0.00036100954748690127\n",
      "fc1.bias 0.00109981382265687\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.000515025220811367\n",
      "conv1.bias 0.0013434677384793758\n",
      "conv2.weight 0.00024815727025270464\n",
      "conv2.bias 0.0008134583476930857\n",
      "fc1.weight 0.00036100954748690127\n",
      "fc1.bias 0.00109981382265687\n",
      "\n",
      "Test set: Average loss: 2.1264 \n",
      "Accuracy: 7848/10000 (78.48%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00047024019062519073\n",
      "conv1.bias 0.0034824777394533157\n",
      "conv2.weight 4.2103030718863014e-05\n",
      "conv2.bias 0.000625379616394639\n",
      "fc1.weight 0.0001223504077643156\n",
      "fc1.bias 0.0007468017749488354\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00047024019062519073\n",
      "conv1.bias 0.0034824777394533157\n",
      "conv2.weight 4.2103030718863014e-05\n",
      "conv2.bias 0.000625379616394639\n",
      "fc1.weight 0.0001223504077643156\n",
      "fc1.bias 0.0007468017749488354\n",
      "\n",
      "Test set: Average loss: 2.0696 \n",
      "Accuracy: 7517/10000 (75.17%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0005603860318660736\n",
      "conv1.bias 0.0014627565396949649\n",
      "conv2.weight 0.0001518127415329218\n",
      "conv2.bias 0.0007567958091385663\n",
      "fc1.weight 0.00019851319957524537\n",
      "fc1.bias 0.0013001642189919948\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0005603860318660736\n",
      "conv1.bias 0.0014627565396949649\n",
      "conv2.weight 0.0001518127415329218\n",
      "conv2.bias 0.0007567958091385663\n",
      "fc1.weight 0.00019851319957524537\n",
      "fc1.bias 0.0013001642189919948\n",
      "\n",
      "Test set: Average loss: 2.1512 \n",
      "Accuracy: 7391/10000 (73.91%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004545583575963974\n",
      "conv1.bias 0.001197524950839579\n",
      "conv2.weight 0.00011240163818001747\n",
      "conv2.bias 0.0007458134205080569\n",
      "fc1.weight 0.00029147665482014417\n",
      "fc1.bias 0.0006787162739783526\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004545583575963974\n",
      "conv1.bias 0.001197524950839579\n",
      "conv2.weight 0.00011240163818001747\n",
      "conv2.bias 0.0007458134205080569\n",
      "fc1.weight 0.00029147665482014417\n",
      "fc1.bias 0.0006787162739783526\n",
      "\n",
      "Test set: Average loss: 2.1819 \n",
      "Accuracy: 6865/10000 (68.65%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00037117652595043183\n",
      "conv1.bias 0.0015188297256827354\n",
      "conv2.weight 0.00019275248050689697\n",
      "conv2.bias 0.000766487093642354\n",
      "fc1.weight 0.0002674023155122995\n",
      "fc1.bias 0.00022751358337700368\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00037117652595043183\n",
      "conv1.bias 0.0015188297256827354\n",
      "conv2.weight 0.00019275248050689697\n",
      "conv2.bias 0.000766487093642354\n",
      "fc1.weight 0.0002674023155122995\n",
      "fc1.bias 0.00022751358337700368\n",
      "\n",
      "Test set: Average loss: 2.1512 \n",
      "Accuracy: 6291/10000 (62.91%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00027412701398134233\n",
      "conv1.bias 0.0015478695277124643\n",
      "conv2.weight 0.00011455400846898555\n",
      "conv2.bias 0.0008496578084304929\n",
      "fc1.weight 0.00013718598056584598\n",
      "fc1.bias 0.0005919083952903748\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00027412701398134233\n",
      "conv1.bias 0.0015478695277124643\n",
      "conv2.weight 0.00011455400846898555\n",
      "conv2.bias 0.0008496578084304929\n",
      "fc1.weight 0.00013718598056584598\n",
      "fc1.bias 0.0005919083952903748\n",
      "\n",
      "Test set: Average loss: 2.1858 \n",
      "Accuracy: 5877/10000 (58.77%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "N = 8\n",
    "K = 4\n",
    "G = 2\n",
    "\n",
    "N_i = int(N/G) # = 2\n",
    "K_i = int(K/G) # = 2\n",
    "T = 3\n",
    "sigma = 1\n",
    "Noise_Alloc = [0,2,4]\n",
    "m = N_i # number of selected workers (if there is no straggler, m=N_i)\n",
    "\n",
    "print(N_i,K_i,T)\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K_i+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K_i+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K_i+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "z_array = np.array([-0.81, -0.22, 0.22, 0.81])\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 30\n",
    "\n",
    "loss_test_arr_K4_G2_N8_v3 = np.zeros((N_trials,N_epochs))\n",
    "acc_test_arr_K4_G2_N8_v3  = np.zeros((N_trials,N_epochs))\n",
    "\n",
    "for trial_idx in range(N_trials):\n",
    "    \n",
    "    print('##########################################')\n",
    "    print('######',trial_idx,'-th Trial!! ###########')\n",
    "    \n",
    "    net_glob = CNNMnist2(args=args)\n",
    "    net_glob.cuda()\n",
    "    net_glob.train()\n",
    "    \n",
    "    # copy weights\n",
    "    w_glob = net_glob.state_dict()\n",
    "    \n",
    "    X_tilde = np.empty((N,Size_submatrices,28*28))\n",
    "    y_tilde = np.empty((N,Size_submatrices,10))\n",
    "    \n",
    "    for G_idx in range(G):\n",
    "        \n",
    "        _Noise_label = np.ones((size_per_group*T,10)) * 0.1\n",
    "        \n",
    "        X_tilde_tmp,a,b = BACC_Enc_Data_v3(X_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde_tmp,a,b = BACC_Enc_Data_v3(y_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "        \n",
    "        stt_pos = G_idx * N_i\n",
    "        end_pos = (G_idx+1) * N_i\n",
    "        \n",
    "        X_tilde[stt_pos:end_pos,:,:] = X_tilde_tmp\n",
    "        y_tilde[stt_pos:end_pos,:,:] = y_tilde_tmp\n",
    "        \n",
    "   \n",
    "\n",
    "    for iter in range(N_epochs): #args.epochs\n",
    "        \n",
    "        w_group_array = []\n",
    "        for G_idx in range(G):\n",
    "            w_locals, loss_locals = [], []\n",
    "            idxs_users = np.random.choice(range(N_i), m, replace=False)\n",
    "            idxs_users = np.sort(idxs_users)\n",
    "            print('selected users:',idxs_users)\n",
    "\n",
    "            coded_net = BACC_Enc_Model_withNoise_v4(net_glob.cuda(), N_i, K_i, T, 1, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "            dec_z_array = []\n",
    "            for idx in idxs_users: #for idx in range(N):\n",
    "        #         print(idx)\n",
    "                local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[G_idx*N_i+idx,:,:], label=y_tilde[G_idx*N_i+idx,:,:])\n",
    "                w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "#                 w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                w_locals.append(copy.deepcopy(w))\n",
    "                loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "            # update global weights\n",
    "            #w_glob = FedAvg(w_locals)\n",
    "            w_group = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "            \n",
    "            w_group_array.append(copy.deepcopy(w_group))\n",
    "        \n",
    "        w_glob = copy.deepcopy(w_group_array[0])\n",
    "        for k in w_glob.keys():\n",
    "            for G_idx in range(1,G):\n",
    "                w_glob[k] += w_group_array[G_idx][k]\n",
    "            w_glob[k] = torch.div(w_glob[k], len(w_group_array))\n",
    "        \n",
    "        # copy weight to net_glob\n",
    "        net_glob.load_state_dict(w_glob)\n",
    "\n",
    "        # print loss\n",
    "    #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "    #     loss_train_arr.append(loss_train)\n",
    "\n",
    "        acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "        acc_test_arr_K4_G2_N8_v3[trial_idx][iter] = acc_test\n",
    "        loss_test_arr_K4_G2_N8_v3[trial_idx][iter] = loss_test\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1 2\n",
      "##########################################\n",
      "###### 0 -th Trial!! ###########\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.013706564903259277\n",
      "conv1.bias 0.011211898177862167\n",
      "conv2.weight 0.0004167964681982994\n",
      "conv2.bias 0.0004621360276360065\n",
      "fc1.weight 0.0003252577967941761\n",
      "fc1.bias 0.00027899821288883685\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.013706564903259277\n",
      "conv1.bias 0.011211898177862167\n",
      "conv2.weight 0.0004167964681982994\n",
      "conv2.bias 0.0004621360276360065\n",
      "fc1.weight 0.0003252577967941761\n",
      "fc1.bias 0.00027899821288883685\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.013706564903259277\n",
      "conv1.bias 0.011211898177862167\n",
      "conv2.weight 0.0004167964681982994\n",
      "conv2.bias 0.0004621360276360065\n",
      "fc1.weight 0.0003252577967941761\n",
      "fc1.bias 0.00027899821288883685\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.013706564903259277\n",
      "conv1.bias 0.011211898177862167\n",
      "conv2.weight 0.0004167964681982994\n",
      "conv2.bias 0.0004621360276360065\n",
      "fc1.weight 0.0003252577967941761\n",
      "fc1.bias 0.00027899821288883685\n",
      "\n",
      "Test set: Average loss: 2.3025 \n",
      "Accuracy: 1662/10000 (16.62%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00029880218207836153\n",
      "conv1.bias 0.0018205440137535334\n",
      "conv2.weight 0.000312846302986145\n",
      "conv2.bias 0.0005309251137077808\n",
      "fc1.weight 8.225440979003906e-05\n",
      "fc1.bias 0.0002622508443892002\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00029880218207836153\n",
      "conv1.bias 0.0018205440137535334\n",
      "conv2.weight 0.000312846302986145\n",
      "conv2.bias 0.0005309251137077808\n",
      "fc1.weight 8.225440979003906e-05\n",
      "fc1.bias 0.0002622508443892002\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00029880218207836153\n",
      "conv1.bias 0.0018205440137535334\n",
      "conv2.weight 0.000312846302986145\n",
      "conv2.bias 0.0005309251137077808\n",
      "fc1.weight 8.225440979003906e-05\n",
      "fc1.bias 0.0002622508443892002\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00029880218207836153\n",
      "conv1.bias 0.0018205440137535334\n",
      "conv2.weight 0.000312846302986145\n",
      "conv2.bias 0.0005309251137077808\n",
      "fc1.weight 8.225440979003906e-05\n",
      "fc1.bias 0.0002622508443892002\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1010/10000 (10.10%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0001648205891251564\n",
      "conv1.bias 0.0017252296674996614\n",
      "conv2.weight 7.313523907214404e-05\n",
      "conv2.bias 0.0008658032165840268\n",
      "fc1.weight 4.6148954425007105e-05\n",
      "fc1.bias 0.00024625230580568316\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0001648205891251564\n",
      "conv1.bias 0.0017252296674996614\n",
      "conv2.weight 7.313523907214404e-05\n",
      "conv2.bias 0.0008658032165840268\n",
      "fc1.weight 4.6148954425007105e-05\n",
      "fc1.bias 0.00024625230580568316\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0001648205891251564\n",
      "conv1.bias 0.0017252296674996614\n",
      "conv2.weight 7.313523907214404e-05\n",
      "conv2.bias 0.0008658032165840268\n",
      "fc1.weight 4.6148954425007105e-05\n",
      "fc1.bias 0.00024625230580568316\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0001648205891251564\n",
      "conv1.bias 0.0017252296674996614\n",
      "conv2.weight 7.313523907214404e-05\n",
      "conv2.bias 0.0008658032165840268\n",
      "fc1.weight 4.6148954425007105e-05\n",
      "fc1.bias 0.00024625230580568316\n",
      "\n",
      "Test set: Average loss: 2.3000 \n",
      "Accuracy: 2335/10000 (23.35%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006034981459379196\n",
      "conv1.bias 0.0005924117285758257\n",
      "conv2.weight 3.9238082244992256e-05\n",
      "conv2.bias 0.0008603668538853526\n",
      "fc1.weight 0.000167428539134562\n",
      "fc1.bias 0.00028208943549543617\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006034981459379196\n",
      "conv1.bias 0.0005924117285758257\n",
      "conv2.weight 3.9238082244992256e-05\n",
      "conv2.bias 0.0008603668538853526\n",
      "fc1.weight 0.000167428539134562\n",
      "fc1.bias 0.00028208943549543617\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006034981459379196\n",
      "conv1.bias 0.0005924117285758257\n",
      "conv2.weight 3.9238082244992256e-05\n",
      "conv2.bias 0.0008603668538853526\n",
      "fc1.weight 0.000167428539134562\n",
      "fc1.bias 0.00028208943549543617\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006034981459379196\n",
      "conv1.bias 0.0005924117285758257\n",
      "conv2.weight 3.9238082244992256e-05\n",
      "conv2.bias 0.0008603668538853526\n",
      "fc1.weight 0.000167428539134562\n",
      "fc1.bias 0.00028208943549543617\n",
      "\n",
      "Test set: Average loss: 2.3022 \n",
      "Accuracy: 1761/10000 (17.61%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00012012304738163948\n",
      "conv1.bias 0.00216109910979867\n",
      "conv2.weight 2.953995019197464e-05\n",
      "conv2.bias 0.000854434329085052\n",
      "fc1.weight 0.00011905314167961478\n",
      "fc1.bias 0.0003359281923621893\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00012012304738163948\n",
      "conv1.bias 0.00216109910979867\n",
      "conv2.weight 2.953995019197464e-05\n",
      "conv2.bias 0.000854434329085052\n",
      "fc1.weight 0.00011905314167961478\n",
      "fc1.bias 0.0003359281923621893\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00012012304738163948\n",
      "conv1.bias 0.00216109910979867\n",
      "conv2.weight 2.953995019197464e-05\n",
      "conv2.bias 0.000854434329085052\n",
      "fc1.weight 0.00011905314167961478\n",
      "fc1.bias 0.0003359281923621893\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00012012304738163948\n",
      "conv1.bias 0.00216109910979867\n",
      "conv2.weight 2.953995019197464e-05\n",
      "conv2.bias 0.000854434329085052\n",
      "fc1.weight 0.00011905314167961478\n",
      "fc1.bias 0.0003359281923621893\n",
      "\n",
      "Test set: Average loss: 2.2885 \n",
      "Accuracy: 2320/10000 (23.20%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005748530849814415\n",
      "conv1.bias 0.0009928655344992876\n",
      "conv2.weight 7.153609301894903e-05\n",
      "conv2.bias 0.0009149281540885568\n",
      "fc1.weight 0.00034404995385557414\n",
      "fc1.bias 0.000601599458605051\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005748530849814415\n",
      "conv1.bias 0.0009928655344992876\n",
      "conv2.weight 7.153609301894903e-05\n",
      "conv2.bias 0.0009149281540885568\n",
      "fc1.weight 0.00034404995385557414\n",
      "fc1.bias 0.000601599458605051\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005748530849814415\n",
      "conv1.bias 0.0009928655344992876\n",
      "conv2.weight 7.153609301894903e-05\n",
      "conv2.bias 0.0009149281540885568\n",
      "fc1.weight 0.00034404995385557414\n",
      "fc1.bias 0.000601599458605051\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005748530849814415\n",
      "conv1.bias 0.0009928655344992876\n",
      "conv2.weight 7.153609301894903e-05\n",
      "conv2.bias 0.0009149281540885568\n",
      "fc1.weight 0.00034404995385557414\n",
      "fc1.bias 0.000601599458605051\n",
      "\n",
      "Test set: Average loss: 2.2584 \n",
      "Accuracy: 4803/10000 (48.03%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00019927728921175002\n",
      "conv1.bias 0.0037486874498426914\n",
      "conv2.weight 6.70102657750249e-05\n",
      "conv2.bias 0.0010332015808671713\n",
      "fc1.weight 0.0001010046573355794\n",
      "fc1.bias 0.00043536843731999396\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00019927728921175002\n",
      "conv1.bias 0.0037486874498426914\n",
      "conv2.weight 6.70102657750249e-05\n",
      "conv2.bias 0.0010332015808671713\n",
      "fc1.weight 0.0001010046573355794\n",
      "fc1.bias 0.00043536843731999396\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00019927728921175002\n",
      "conv1.bias 0.0037486874498426914\n",
      "conv2.weight 6.70102657750249e-05\n",
      "conv2.bias 0.0010332015808671713\n",
      "fc1.weight 0.0001010046573355794\n",
      "fc1.bias 0.00043536843731999396\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00019927728921175002\n",
      "conv1.bias 0.0037486874498426914\n",
      "conv2.weight 6.70102657750249e-05\n",
      "conv2.bias 0.0010332015808671713\n",
      "fc1.weight 0.0001010046573355794\n",
      "fc1.bias 0.00043536843731999396\n",
      "\n",
      "Test set: Average loss: 2.1298 \n",
      "Accuracy: 6051/10000 (60.51%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00038100473582744596\n",
      "conv1.bias 0.0017142724245786667\n",
      "conv2.weight 5.0292899832129475e-05\n",
      "conv2.bias 0.000782617018558085\n",
      "fc1.weight 0.0002524277660995722\n",
      "fc1.bias 0.0005534810479730368\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00038100473582744596\n",
      "conv1.bias 0.0017142724245786667\n",
      "conv2.weight 5.0292899832129475e-05\n",
      "conv2.bias 0.000782617018558085\n",
      "fc1.weight 0.0002524277660995722\n",
      "fc1.bias 0.0005534810479730368\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00038100473582744596\n",
      "conv1.bias 0.0017142724245786667\n",
      "conv2.weight 5.0292899832129475e-05\n",
      "conv2.bias 0.000782617018558085\n",
      "fc1.weight 0.0002524277660995722\n",
      "fc1.bias 0.0005534810479730368\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00038100473582744596\n",
      "conv1.bias 0.0017142724245786667\n",
      "conv2.weight 5.0292899832129475e-05\n",
      "conv2.bias 0.000782617018558085\n",
      "fc1.weight 0.0002524277660995722\n",
      "fc1.bias 0.0005534810479730368\n",
      "\n",
      "Test set: Average loss: 2.0914 \n",
      "Accuracy: 6711/10000 (67.11%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00040762938559055326\n",
      "conv1.bias 0.0028022630140185356\n",
      "conv2.weight 5.303455051034689e-05\n",
      "conv2.bias 0.0007048744009807706\n",
      "fc1.weight 0.00016356507549062372\n",
      "fc1.bias 0.0006015048362314701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected users: [0 1]\n",
      "conv1.weight 0.00040762938559055326\n",
      "conv1.bias 0.0028022630140185356\n",
      "conv2.weight 5.303455051034689e-05\n",
      "conv2.bias 0.0007048744009807706\n",
      "fc1.weight 0.00016356507549062372\n",
      "fc1.bias 0.0006015048362314701\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00040762938559055326\n",
      "conv1.bias 0.0028022630140185356\n",
      "conv2.weight 5.303455051034689e-05\n",
      "conv2.bias 0.0007048744009807706\n",
      "fc1.weight 0.00016356507549062372\n",
      "fc1.bias 0.0006015048362314701\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00040762938559055326\n",
      "conv1.bias 0.0028022630140185356\n",
      "conv2.weight 5.303455051034689e-05\n",
      "conv2.bias 0.0007048744009807706\n",
      "fc1.weight 0.00016356507549062372\n",
      "fc1.bias 0.0006015048362314701\n",
      "\n",
      "Test set: Average loss: 2.0002 \n",
      "Accuracy: 8161/10000 (81.61%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00033715613186359407\n",
      "conv1.bias 0.004703369922935963\n",
      "conv2.weight 5.9067700058221816e-05\n",
      "conv2.bias 0.000669245608150959\n",
      "fc1.weight 0.00020826351828873158\n",
      "fc1.bias 0.0011112980544567108\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00033715613186359407\n",
      "conv1.bias 0.004703369922935963\n",
      "conv2.weight 5.9067700058221816e-05\n",
      "conv2.bias 0.000669245608150959\n",
      "fc1.weight 0.00020826351828873158\n",
      "fc1.bias 0.0011112980544567108\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00033715613186359407\n",
      "conv1.bias 0.004703369922935963\n",
      "conv2.weight 5.9067700058221816e-05\n",
      "conv2.bias 0.000669245608150959\n",
      "fc1.weight 0.00020826351828873158\n",
      "fc1.bias 0.0011112980544567108\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00033715613186359407\n",
      "conv1.bias 0.004703369922935963\n",
      "conv2.weight 5.9067700058221816e-05\n",
      "conv2.bias 0.000669245608150959\n",
      "fc1.weight 0.00020826351828873158\n",
      "fc1.bias 0.0011112980544567108\n",
      "\n",
      "Test set: Average loss: 1.9530 \n",
      "Accuracy: 8408/10000 (84.08%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006019122153520584\n",
      "conv1.bias 0.00378876063041389\n",
      "conv2.weight 0.00011823505163192749\n",
      "conv2.bias 0.0004560720990411937\n",
      "fc1.weight 0.0002218919340521097\n",
      "fc1.bias 0.0008398315869271756\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006019122153520584\n",
      "conv1.bias 0.00378876063041389\n",
      "conv2.weight 0.00011823505163192749\n",
      "conv2.bias 0.0004560720990411937\n",
      "fc1.weight 0.0002218919340521097\n",
      "fc1.bias 0.0008398315869271756\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006019122153520584\n",
      "conv1.bias 0.00378876063041389\n",
      "conv2.weight 0.00011823505163192749\n",
      "conv2.bias 0.0004560720990411937\n",
      "fc1.weight 0.0002218919340521097\n",
      "fc1.bias 0.0008398315869271756\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006019122153520584\n",
      "conv1.bias 0.00378876063041389\n",
      "conv2.weight 0.00011823505163192749\n",
      "conv2.bias 0.0004560720990411937\n",
      "fc1.weight 0.0002218919340521097\n",
      "fc1.bias 0.0008398315869271756\n",
      "\n",
      "Test set: Average loss: 1.9615 \n",
      "Accuracy: 7475/10000 (74.75%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005998449772596359\n",
      "conv1.bias 0.0027518263086676598\n",
      "conv2.weight 0.00010749182663857937\n",
      "conv2.bias 0.0006272591999731958\n",
      "fc1.weight 0.0002458338625729084\n",
      "fc1.bias 0.0010992874391376971\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005998449772596359\n",
      "conv1.bias 0.0027518263086676598\n",
      "conv2.weight 0.00010749182663857937\n",
      "conv2.bias 0.0006272591999731958\n",
      "fc1.weight 0.0002458338625729084\n",
      "fc1.bias 0.0010992874391376971\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005998449772596359\n",
      "conv1.bias 0.0027518263086676598\n",
      "conv2.weight 0.00010749182663857937\n",
      "conv2.bias 0.0006272591999731958\n",
      "fc1.weight 0.0002458338625729084\n",
      "fc1.bias 0.0010992874391376971\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005998449772596359\n",
      "conv1.bias 0.0027518263086676598\n",
      "conv2.weight 0.00010749182663857937\n",
      "conv2.bias 0.0006272591999731958\n",
      "fc1.weight 0.0002458338625729084\n",
      "fc1.bias 0.0010992874391376971\n",
      "\n",
      "Test set: Average loss: 1.8365 \n",
      "Accuracy: 9481/10000 (94.81%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004076918214559555\n",
      "conv1.bias 0.0024581653997302055\n",
      "conv2.weight 0.00019980557262897492\n",
      "conv2.bias 0.000934704439714551\n",
      "fc1.weight 0.0001825631014071405\n",
      "fc1.bias 0.0019192390143871307\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004076918214559555\n",
      "conv1.bias 0.0024581653997302055\n",
      "conv2.weight 0.00019980557262897492\n",
      "conv2.bias 0.000934704439714551\n",
      "fc1.weight 0.0001825631014071405\n",
      "fc1.bias 0.0019192390143871307\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004076918214559555\n",
      "conv1.bias 0.0024581653997302055\n",
      "conv2.weight 0.00019980557262897492\n",
      "conv2.bias 0.000934704439714551\n",
      "fc1.weight 0.0001825631014071405\n",
      "fc1.bias 0.0019192390143871307\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004076918214559555\n",
      "conv1.bias 0.0024581653997302055\n",
      "conv2.weight 0.00019980557262897492\n",
      "conv2.bias 0.000934704439714551\n",
      "fc1.weight 0.0001825631014071405\n",
      "fc1.bias 0.0019192390143871307\n",
      "\n",
      "Test set: Average loss: 1.9734 \n",
      "Accuracy: 6876/10000 (68.76%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006785190105438232\n",
      "conv1.bias 0.002739862073212862\n",
      "conv2.weight 8.630047552287578e-05\n",
      "conv2.bias 0.0007091446314007044\n",
      "fc1.weight 0.00027409824542701244\n",
      "fc1.bias 0.0008574935607612133\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006785190105438232\n",
      "conv1.bias 0.002739862073212862\n",
      "conv2.weight 8.630047552287578e-05\n",
      "conv2.bias 0.0007091446314007044\n",
      "fc1.weight 0.00027409824542701244\n",
      "fc1.bias 0.0008574935607612133\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006785190105438232\n",
      "conv1.bias 0.002739862073212862\n",
      "conv2.weight 8.630047552287578e-05\n",
      "conv2.bias 0.0007091446314007044\n",
      "fc1.weight 0.00027409824542701244\n",
      "fc1.bias 0.0008574935607612133\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006785190105438232\n",
      "conv1.bias 0.002739862073212862\n",
      "conv2.weight 8.630047552287578e-05\n",
      "conv2.bias 0.0007091446314007044\n",
      "fc1.weight 0.00027409824542701244\n",
      "fc1.bias 0.0008574935607612133\n",
      "\n",
      "Test set: Average loss: 1.8976 \n",
      "Accuracy: 9308/10000 (93.08%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004568749293684959\n",
      "conv1.bias 0.0020876452326774597\n",
      "conv2.weight 0.0001166410930454731\n",
      "conv2.bias 0.0006959328893572092\n",
      "fc1.weight 0.0003134922357276082\n",
      "fc1.bias 0.0011943913996219635\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004568749293684959\n",
      "conv1.bias 0.0020876452326774597\n",
      "conv2.weight 0.0001166410930454731\n",
      "conv2.bias 0.0006959328893572092\n",
      "fc1.weight 0.0003134922357276082\n",
      "fc1.bias 0.0011943913996219635\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004568749293684959\n",
      "conv1.bias 0.0020876452326774597\n",
      "conv2.weight 0.0001166410930454731\n",
      "conv2.bias 0.0006959328893572092\n",
      "fc1.weight 0.0003134922357276082\n",
      "fc1.bias 0.0011943913996219635\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004568749293684959\n",
      "conv1.bias 0.0020876452326774597\n",
      "conv2.weight 0.0001166410930454731\n",
      "conv2.bias 0.0006959328893572092\n",
      "fc1.weight 0.0003134922357276082\n",
      "fc1.bias 0.0011943913996219635\n",
      "\n",
      "Test set: Average loss: 1.9896 \n",
      "Accuracy: 9338/10000 (93.38%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00036828894168138503\n",
      "conv1.bias 0.001648658188059926\n",
      "conv2.weight 0.0003137638419866562\n",
      "conv2.bias 0.0009536314755678177\n",
      "fc1.weight 0.00018737821374088527\n",
      "fc1.bias 0.0008041128516197204\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00036828894168138503\n",
      "conv1.bias 0.001648658188059926\n",
      "conv2.weight 0.0003137638419866562\n",
      "conv2.bias 0.0009536314755678177\n",
      "fc1.weight 0.00018737821374088527\n",
      "fc1.bias 0.0008041128516197204\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00036828894168138503\n",
      "conv1.bias 0.001648658188059926\n",
      "conv2.weight 0.0003137638419866562\n",
      "conv2.bias 0.0009536314755678177\n",
      "fc1.weight 0.00018737821374088527\n",
      "fc1.bias 0.0008041128516197204\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00036828894168138503\n",
      "conv1.bias 0.001648658188059926\n",
      "conv2.weight 0.0003137638419866562\n",
      "conv2.bias 0.0009536314755678177\n",
      "fc1.weight 0.00018737821374088527\n",
      "fc1.bias 0.0008041128516197204\n",
      "\n",
      "Test set: Average loss: 1.8035 \n",
      "Accuracy: 9537/10000 (95.37%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00048611320555210115\n",
      "conv1.bias 0.003044361714273691\n",
      "conv2.weight 9.528588503599167e-05\n",
      "conv2.bias 0.0009009326458908617\n",
      "fc1.weight 0.00016560319345444442\n",
      "fc1.bias 0.0013114918954670429\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00048611320555210115\n",
      "conv1.bias 0.003044361714273691\n",
      "conv2.weight 9.528588503599167e-05\n",
      "conv2.bias 0.0009009326458908617\n",
      "fc1.weight 0.00016560319345444442\n",
      "fc1.bias 0.0013114918954670429\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00048611320555210115\n",
      "conv1.bias 0.003044361714273691\n",
      "conv2.weight 9.528588503599167e-05\n",
      "conv2.bias 0.0009009326458908617\n",
      "fc1.weight 0.00016560319345444442\n",
      "fc1.bias 0.0013114918954670429\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00048611320555210115\n",
      "conv1.bias 0.003044361714273691\n",
      "conv2.weight 9.528588503599167e-05\n",
      "conv2.bias 0.0009009326458908617\n",
      "fc1.weight 0.00016560319345444442\n",
      "fc1.bias 0.0013114918954670429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.9422 \n",
      "Accuracy: 8970/10000 (89.70%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004333682730793953\n",
      "conv1.bias 0.0034522288478910923\n",
      "conv2.weight 8.975024335086345e-05\n",
      "conv2.bias 0.0007489094277843833\n",
      "fc1.weight 0.0002597528975456953\n",
      "fc1.bias 0.000991189107298851\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004333682730793953\n",
      "conv1.bias 0.0034522288478910923\n",
      "conv2.weight 8.975024335086345e-05\n",
      "conv2.bias 0.0007489094277843833\n",
      "fc1.weight 0.0002597528975456953\n",
      "fc1.bias 0.000991189107298851\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004333682730793953\n",
      "conv1.bias 0.0034522288478910923\n",
      "conv2.weight 8.975024335086345e-05\n",
      "conv2.bias 0.0007489094277843833\n",
      "fc1.weight 0.0002597528975456953\n",
      "fc1.bias 0.000991189107298851\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004333682730793953\n",
      "conv1.bias 0.0034522288478910923\n",
      "conv2.weight 8.975024335086345e-05\n",
      "conv2.bias 0.0007489094277843833\n",
      "fc1.weight 0.0002597528975456953\n",
      "fc1.bias 0.000991189107298851\n",
      "\n",
      "Test set: Average loss: 1.8916 \n",
      "Accuracy: 8631/10000 (86.31%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00043255895376205446\n",
      "conv1.bias 0.0034947108943015337\n",
      "conv2.weight 0.00010945071466267109\n",
      "conv2.bias 0.0009185754461213946\n",
      "fc1.weight 0.00020325568038970232\n",
      "fc1.bias 0.0015417839400470256\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00043255895376205446\n",
      "conv1.bias 0.0034947108943015337\n",
      "conv2.weight 0.00010945071466267109\n",
      "conv2.bias 0.0009185754461213946\n",
      "fc1.weight 0.00020325568038970232\n",
      "fc1.bias 0.0015417839400470256\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00043255895376205446\n",
      "conv1.bias 0.0034947108943015337\n",
      "conv2.weight 0.00010945071466267109\n",
      "conv2.bias 0.0009185754461213946\n",
      "fc1.weight 0.00020325568038970232\n",
      "fc1.bias 0.0015417839400470256\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00043255895376205446\n",
      "conv1.bias 0.0034947108943015337\n",
      "conv2.weight 0.00010945071466267109\n",
      "conv2.bias 0.0009185754461213946\n",
      "fc1.weight 0.00020325568038970232\n",
      "fc1.bias 0.0015417839400470256\n",
      "\n",
      "Test set: Average loss: 2.1611 \n",
      "Accuracy: 7510/10000 (75.10%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003474568948149681\n",
      "conv1.bias 0.0017791050486266613\n",
      "conv2.weight 0.00020363008603453637\n",
      "conv2.bias 0.0010774782858788967\n",
      "fc1.weight 0.00025532685685902834\n",
      "fc1.bias 0.00048082112334668636\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003474568948149681\n",
      "conv1.bias 0.0017791050486266613\n",
      "conv2.weight 0.00020363008603453637\n",
      "conv2.bias 0.0010774782858788967\n",
      "fc1.weight 0.00025532685685902834\n",
      "fc1.bias 0.00048082112334668636\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003474568948149681\n",
      "conv1.bias 0.0017791050486266613\n",
      "conv2.weight 0.00020363008603453637\n",
      "conv2.bias 0.0010774782858788967\n",
      "fc1.weight 0.00025532685685902834\n",
      "fc1.bias 0.00048082112334668636\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003474568948149681\n",
      "conv1.bias 0.0017791050486266613\n",
      "conv2.weight 0.00020363008603453637\n",
      "conv2.bias 0.0010774782858788967\n",
      "fc1.weight 0.00025532685685902834\n",
      "fc1.bias 0.00048082112334668636\n",
      "\n",
      "Test set: Average loss: 1.9282 \n",
      "Accuracy: 7612/10000 (76.12%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004933107271790504\n",
      "conv1.bias 0.0041814520955085754\n",
      "conv2.weight 9.854853153228759e-05\n",
      "conv2.bias 0.0010893179569393396\n",
      "fc1.weight 0.00017060150858014823\n",
      "fc1.bias 0.000779875461012125\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004933107271790504\n",
      "conv1.bias 0.0041814520955085754\n",
      "conv2.weight 9.854853153228759e-05\n",
      "conv2.bias 0.0010893179569393396\n",
      "fc1.weight 0.00017060150858014823\n",
      "fc1.bias 0.000779875461012125\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004933107271790504\n",
      "conv1.bias 0.0041814520955085754\n",
      "conv2.weight 9.854853153228759e-05\n",
      "conv2.bias 0.0010893179569393396\n",
      "fc1.weight 0.00017060150858014823\n",
      "fc1.bias 0.000779875461012125\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004933107271790504\n",
      "conv1.bias 0.0041814520955085754\n",
      "conv2.weight 9.854853153228759e-05\n",
      "conv2.bias 0.0010893179569393396\n",
      "fc1.weight 0.00017060150858014823\n",
      "fc1.bias 0.000779875461012125\n",
      "\n",
      "Test set: Average loss: 2.0236 \n",
      "Accuracy: 7467/10000 (74.67%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00042505130171775817\n",
      "conv1.bias 0.004027931950986385\n",
      "conv2.weight 7.033997215330601e-05\n",
      "conv2.bias 0.0007435501320287585\n",
      "fc1.weight 0.0002759733470156789\n",
      "fc1.bias 0.0009011850692331791\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00042505130171775817\n",
      "conv1.bias 0.004027931950986385\n",
      "conv2.weight 7.033997215330601e-05\n",
      "conv2.bias 0.0007435501320287585\n",
      "fc1.weight 0.0002759733470156789\n",
      "fc1.bias 0.0009011850692331791\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00042505130171775817\n",
      "conv1.bias 0.004027931950986385\n",
      "conv2.weight 7.033997215330601e-05\n",
      "conv2.bias 0.0007435501320287585\n",
      "fc1.weight 0.0002759733470156789\n",
      "fc1.bias 0.0009011850692331791\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00042505130171775817\n",
      "conv1.bias 0.004027931950986385\n",
      "conv2.weight 7.033997215330601e-05\n",
      "conv2.bias 0.0007435501320287585\n",
      "fc1.weight 0.0002759733470156789\n",
      "fc1.bias 0.0009011850692331791\n",
      "\n",
      "Test set: Average loss: 1.9509 \n",
      "Accuracy: 7767/10000 (77.67%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004600038006901741\n",
      "conv1.bias 0.0023741957265883684\n",
      "conv2.weight 8.0850375816226e-05\n",
      "conv2.bias 0.0006625513778999448\n",
      "fc1.weight 0.0003563020611181855\n",
      "fc1.bias 0.0011185988783836364\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004600038006901741\n",
      "conv1.bias 0.0023741957265883684\n",
      "conv2.weight 8.0850375816226e-05\n",
      "conv2.bias 0.0006625513778999448\n",
      "fc1.weight 0.0003563020611181855\n",
      "fc1.bias 0.0011185988783836364\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004600038006901741\n",
      "conv1.bias 0.0023741957265883684\n",
      "conv2.weight 8.0850375816226e-05\n",
      "conv2.bias 0.0006625513778999448\n",
      "fc1.weight 0.0003563020611181855\n",
      "fc1.bias 0.0011185988783836364\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004600038006901741\n",
      "conv1.bias 0.0023741957265883684\n",
      "conv2.weight 8.0850375816226e-05\n",
      "conv2.bias 0.0006625513778999448\n",
      "fc1.weight 0.0003563020611181855\n",
      "fc1.bias 0.0011185988783836364\n",
      "\n",
      "Test set: Average loss: 1.8108 \n",
      "Accuracy: 9538/10000 (95.38%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004947742074728012\n",
      "conv1.bias 0.004279829561710358\n",
      "conv2.weight 0.00018383480608463287\n",
      "conv2.bias 0.0010939670028164983\n",
      "fc1.weight 0.00017235019477084278\n",
      "fc1.bias 0.0011085712350904942\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004947742074728012\n",
      "conv1.bias 0.004279829561710358\n",
      "conv2.weight 0.00018383480608463287\n",
      "conv2.bias 0.0010939670028164983\n",
      "fc1.weight 0.00017235019477084278\n",
      "fc1.bias 0.0011085712350904942\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004947742074728012\n",
      "conv1.bias 0.004279829561710358\n",
      "conv2.weight 0.00018383480608463287\n",
      "conv2.bias 0.0010939670028164983\n",
      "fc1.weight 0.00017235019477084278\n",
      "fc1.bias 0.0011085712350904942\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004947742074728012\n",
      "conv1.bias 0.004279829561710358\n",
      "conv2.weight 0.00018383480608463287\n",
      "conv2.bias 0.0010939670028164983\n",
      "fc1.weight 0.00017235019477084278\n",
      "fc1.bias 0.0011085712350904942\n",
      "\n",
      "Test set: Average loss: 2.1219 \n",
      "Accuracy: 7796/10000 (77.96%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003942105919122696\n",
      "conv1.bias 0.0018162616761401296\n",
      "conv2.weight 0.00025326885282993315\n",
      "conv2.bias 0.0009364373981952667\n",
      "fc1.weight 0.00036806738935410974\n",
      "fc1.bias 0.0005363862030208111\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003942105919122696\n",
      "conv1.bias 0.0018162616761401296\n",
      "conv2.weight 0.00025326885282993315\n",
      "conv2.bias 0.0009364373981952667\n",
      "fc1.weight 0.00036806738935410974\n",
      "fc1.bias 0.0005363862030208111\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003942105919122696\n",
      "conv1.bias 0.0018162616761401296\n",
      "conv2.weight 0.00025326885282993315\n",
      "conv2.bias 0.0009364373981952667\n",
      "fc1.weight 0.00036806738935410974\n",
      "fc1.bias 0.0005363862030208111\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003942105919122696\n",
      "conv1.bias 0.0018162616761401296\n",
      "conv2.weight 0.00025326885282993315\n",
      "conv2.bias 0.0009364373981952667\n",
      "fc1.weight 0.00036806738935410974\n",
      "fc1.bias 0.0005363862030208111\n",
      "\n",
      "Test set: Average loss: 1.9920 \n",
      "Accuracy: 8900/10000 (89.00%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00029945600777864456\n",
      "conv1.bias 0.003218692960217595\n",
      "conv2.weight 0.00012535521760582925\n",
      "conv2.bias 0.0011503624264150858\n",
      "fc1.weight 0.00019295464735478163\n",
      "fc1.bias 0.0007758977822959423\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00029945600777864456\n",
      "conv1.bias 0.003218692960217595\n",
      "conv2.weight 0.00012535521760582925\n",
      "conv2.bias 0.0011503624264150858\n",
      "fc1.weight 0.00019295464735478163\n",
      "fc1.bias 0.0007758977822959423\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00029945600777864456\n",
      "conv1.bias 0.003218692960217595\n",
      "conv2.weight 0.00012535521760582925\n",
      "conv2.bias 0.0011503624264150858\n",
      "fc1.weight 0.00019295464735478163\n",
      "fc1.bias 0.0007758977822959423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected users: [0 1]\n",
      "conv1.weight 0.00029945600777864456\n",
      "conv1.bias 0.003218692960217595\n",
      "conv2.weight 0.00012535521760582925\n",
      "conv2.bias 0.0011503624264150858\n",
      "fc1.weight 0.00019295464735478163\n",
      "fc1.bias 0.0007758977822959423\n",
      "\n",
      "Test set: Average loss: 1.8970 \n",
      "Accuracy: 8594/10000 (85.94%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004805023968219757\n",
      "conv1.bias 0.004676062613725662\n",
      "conv2.weight 8.430168963968754e-05\n",
      "conv2.bias 0.0009683126118034124\n",
      "fc1.weight 0.00020473995245993138\n",
      "fc1.bias 0.0012194647453725338\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004805023968219757\n",
      "conv1.bias 0.004676062613725662\n",
      "conv2.weight 8.430168963968754e-05\n",
      "conv2.bias 0.0009683126118034124\n",
      "fc1.weight 0.00020473995245993138\n",
      "fc1.bias 0.0012194647453725338\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004805023968219757\n",
      "conv1.bias 0.004676062613725662\n",
      "conv2.weight 8.430168963968754e-05\n",
      "conv2.bias 0.0009683126118034124\n",
      "fc1.weight 0.00020473995245993138\n",
      "fc1.bias 0.0012194647453725338\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004805023968219757\n",
      "conv1.bias 0.004676062613725662\n",
      "conv2.weight 8.430168963968754e-05\n",
      "conv2.bias 0.0009683126118034124\n",
      "fc1.weight 0.00020473995245993138\n",
      "fc1.bias 0.0012194647453725338\n",
      "\n",
      "Test set: Average loss: 1.9234 \n",
      "Accuracy: 9170/10000 (91.70%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00032113056629896165\n",
      "conv1.bias 0.004377583973109722\n",
      "conv2.weight 0.00012517612427473068\n",
      "conv2.bias 0.0010145563865080476\n",
      "fc1.weight 0.00025055916048586366\n",
      "fc1.bias 0.001707865670323372\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00032113056629896165\n",
      "conv1.bias 0.004377583973109722\n",
      "conv2.weight 0.00012517612427473068\n",
      "conv2.bias 0.0010145563865080476\n",
      "fc1.weight 0.00025055916048586366\n",
      "fc1.bias 0.001707865670323372\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00032113056629896165\n",
      "conv1.bias 0.004377583973109722\n",
      "conv2.weight 0.00012517612427473068\n",
      "conv2.bias 0.0010145563865080476\n",
      "fc1.weight 0.00025055916048586366\n",
      "fc1.bias 0.001707865670323372\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00032113056629896165\n",
      "conv1.bias 0.004377583973109722\n",
      "conv2.weight 0.00012517612427473068\n",
      "conv2.bias 0.0010145563865080476\n",
      "fc1.weight 0.00025055916048586366\n",
      "fc1.bias 0.001707865670323372\n",
      "\n",
      "Test set: Average loss: 1.8742 \n",
      "Accuracy: 8721/10000 (87.21%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005152181908488273\n",
      "conv1.bias 0.004653789568692446\n",
      "conv2.weight 8.652186952531337e-05\n",
      "conv2.bias 0.0008826683042570949\n",
      "fc1.weight 0.00019420116441324354\n",
      "fc1.bias 0.002067183144390583\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005152181908488273\n",
      "conv1.bias 0.004653789568692446\n",
      "conv2.weight 8.652186952531337e-05\n",
      "conv2.bias 0.0008826683042570949\n",
      "fc1.weight 0.00019420116441324354\n",
      "fc1.bias 0.002067183144390583\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005152181908488273\n",
      "conv1.bias 0.004653789568692446\n",
      "conv2.weight 8.652186952531337e-05\n",
      "conv2.bias 0.0008826683042570949\n",
      "fc1.weight 0.00019420116441324354\n",
      "fc1.bias 0.002067183144390583\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005152181908488273\n",
      "conv1.bias 0.004653789568692446\n",
      "conv2.weight 8.652186952531337e-05\n",
      "conv2.bias 0.0008826683042570949\n",
      "fc1.weight 0.00019420116441324354\n",
      "fc1.bias 0.002067183144390583\n",
      "\n",
      "Test set: Average loss: 2.0006 \n",
      "Accuracy: 8502/10000 (85.02%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000305098295211792\n",
      "conv1.bias 0.003776762168854475\n",
      "conv2.weight 0.00010866315104067325\n",
      "conv2.bias 0.0008320084889419377\n",
      "fc1.weight 0.0003145331516861916\n",
      "fc1.bias 0.0011655843816697597\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000305098295211792\n",
      "conv1.bias 0.003776762168854475\n",
      "conv2.weight 0.00010866315104067325\n",
      "conv2.bias 0.0008320084889419377\n",
      "fc1.weight 0.0003145331516861916\n",
      "fc1.bias 0.0011655843816697597\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000305098295211792\n",
      "conv1.bias 0.003776762168854475\n",
      "conv2.weight 0.00010866315104067325\n",
      "conv2.bias 0.0008320084889419377\n",
      "fc1.weight 0.0003145331516861916\n",
      "fc1.bias 0.0011655843816697597\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000305098295211792\n",
      "conv1.bias 0.003776762168854475\n",
      "conv2.weight 0.00010866315104067325\n",
      "conv2.bias 0.0008320084889419377\n",
      "fc1.weight 0.0003145331516861916\n",
      "fc1.bias 0.0011655843816697597\n",
      "\n",
      "Test set: Average loss: 1.9423 \n",
      "Accuracy: 8669/10000 (86.69%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "N = 8\n",
    "K = 4\n",
    "G = 4\n",
    "\n",
    "size_per_group = int(60000/G)\n",
    "\n",
    "X_group = np.reshape(encoding_input_array_np, (G,size_per_group,28*28))\n",
    "y_group = np.reshape(encoding_label_array_np, (G,size_per_group,args.num_classes)) \n",
    "\n",
    "\n",
    "N_i = int(N/G) # = 2\n",
    "K_i = int(K/G) # = 2\n",
    "T = 2\n",
    "sigma = 1\n",
    "Noise_Alloc = [0,2]\n",
    "m = N_i # number of selected workers (if there is no straggler, m=N_i)\n",
    "\n",
    "print(N_i,K_i,T)\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K_i+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K_i+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K_i+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "z_array = np.array([-0.521, 0.521])\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 30\n",
    "\n",
    "loss_test_arr_K4_G4_N8_v3 = np.zeros((N_trials,N_epochs))\n",
    "acc_test_arr_K4_G4_N8_v3  = np.zeros((N_trials,N_epochs))\n",
    "\n",
    "for trial_idx in range(N_trials):\n",
    "    \n",
    "    print('##########################################')\n",
    "    print('######',trial_idx,'-th Trial!! ###########')\n",
    "    \n",
    "    net_glob = CNNMnist2(args=args)\n",
    "    net_glob.cuda()\n",
    "    net_glob.train()\n",
    "    \n",
    "    # copy weights\n",
    "    w_glob = net_glob.state_dict()\n",
    "    \n",
    "    X_tilde = np.empty((N,Size_submatrices,28*28))\n",
    "    y_tilde = np.empty((N,Size_submatrices,10))\n",
    "    \n",
    "    for G_idx in range(G):\n",
    "        \n",
    "        _Noise_label = np.ones((size_per_group*T,10)) * 0.1\n",
    "        \n",
    "        X_tilde_tmp,a,b = BACC_Enc_Data_v3(X_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde_tmp,a,b = BACC_Enc_Data_v3(y_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "        \n",
    "        stt_pos = G_idx * N_i\n",
    "        end_pos = (G_idx+1) * N_i\n",
    "        \n",
    "        X_tilde[stt_pos:end_pos,:,:] = X_tilde_tmp\n",
    "        y_tilde[stt_pos:end_pos,:,:] = y_tilde_tmp\n",
    "        \n",
    "   \n",
    "\n",
    "    for iter in range(N_epochs): #args.epochs\n",
    "        \n",
    "        w_group_array = []\n",
    "        for G_idx in range(G):\n",
    "            w_locals, loss_locals = [], []\n",
    "            idxs_users = np.random.choice(range(N_i), m, replace=False)\n",
    "            idxs_users = np.sort(idxs_users)\n",
    "            print('selected users:',idxs_users)\n",
    "\n",
    "            coded_net = BACC_Enc_Model_withNoise_v4(net_glob.cuda(), N_i, K_i, T, 1, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "            dec_z_array = []\n",
    "            for idx in idxs_users: #for idx in range(N):\n",
    "        #         print(idx)\n",
    "                local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[G_idx*N_i+idx,:,:], label=y_tilde[G_idx*N_i+idx,:,:])\n",
    "                w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "#                 w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                w_locals.append(copy.deepcopy(w))\n",
    "                loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "            # update global weights\n",
    "            #w_glob = FedAvg(w_locals)\n",
    "            w_group = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "            \n",
    "            w_group_array.append(copy.deepcopy(w_group))\n",
    "        \n",
    "        w_glob = copy.deepcopy(w_group_array[0])\n",
    "        for k in w_glob.keys():\n",
    "            for G_idx in range(1,G):\n",
    "                w_glob[k] += w_group_array[G_idx][k]\n",
    "            w_glob[k] = torch.div(w_glob[k], len(w_group_array))\n",
    "        \n",
    "        # copy weight to net_glob\n",
    "        net_glob.load_state_dict(w_glob)\n",
    "\n",
    "        # print loss\n",
    "    #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "    #     loss_train_arr.append(loss_train)\n",
    "\n",
    "        acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "        acc_test_arr_K4_G4_N8_v3[trial_idx][iter] = acc_test\n",
    "        loss_test_arr_K4_G4_N8_v3[trial_idx][iter] = loss_test\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "B_sel = 0\n",
    "\n",
    "# plt.plot(acc_test_arr_v1[0,B_sel,0,0:30],label='N=2, B='+str(B_array[B_sel]))\n",
    "plt.plot(acc_test_arr_K4_G1_v3[2,B_sel,0,0:30],label='w/o Grouping, K=4, N=4, G=1' )\n",
    "plt.plot(acc_test_arr_K4_G1_v3[3,B_sel,0,0:30],label='w/o Grouping, K=4, N=8, G=1' )\n",
    "# plt.plot(acc_test_arr_v1[2,B_sel,0,0:30],label='N=6, B='+str(B_array[B_sel]))\n",
    "# plt.plot(acc_test_arr_v1[3,B_sel,0,0:30],label='N=8, B='+str(B_array[B_sel]))\n",
    "plt.plot(acc_test_arr_G2_N4_v3[0,0:30],label='w/ Grouping,   K=4, N=4, G=2')\n",
    "plt.plot(acc_test_arr_K4_G2_N8_v3[0,0:30],label='w/ Grouping,   K=4, N=8, G=2')\n",
    "plt.plot(acc_test_arr_K4_G4_N8_v3[0,0:30],label='w/ Grouping,   K=4, N=8, G=4')\n",
    "# plt.plot(acc_test_arr_G3[0,0:30],label='Grouping, N=6, G=3, B='+str(B_array[B_sel]))\n",
    "\n",
    "# plt.plot(acc_test_arr_v1[4,B_sel,0,0:30],label='N=7, B='+str(B_array[B_sel]))\n",
    "# plt.plot(acc_test_arr_v1[5,B_sel,0,0:30],label='N=8, B='+str(B_array[B_sel]))\n",
    "\n",
    "# plt.plot(plot_acc_v2[2,sigma_sel,:],label='T=10')\n",
    "# plt.plot(plot_acc_v2[3,sigma_sel,:],label='T=11')\n",
    "\n",
    "# plt.plot(acc_test_arr_uncoded,'r',label='uncoded')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "# plt.title(title_name)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Test accuracy(%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
