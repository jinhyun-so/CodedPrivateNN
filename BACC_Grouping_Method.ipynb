{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "\n",
    "import math\n",
    "\n",
    "from utils.sampling import mnist_iid, mnist_noniid, cifar_iid\n",
    "from utils.options import args_parser\n",
    "from models.Update import LocalUpdate\n",
    "from models.Nets import MLP, CNNMnist, CNNCifar, LeNet, CNNMnist2\n",
    "from models.Fed import FedAvg\n",
    "from models.Fed import FedQAvg, Quantization, Quantization_Finite, my_score, my_score_Finite\n",
    "from models.test import test_img\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "\n",
    "from utils.sampling import mnist_iid, mnist_noniid, cifar_iid\n",
    "from utils.options import args_parser\n",
    "from models.Update import LocalUpdate\n",
    "from models.Nets import MLP, CNNMnist, CNNCifar, LeNet, CNNMnist2, CNNMnist3\n",
    "from models.Fed import FedAvg\n",
    "from models.Fed import FedQAvg, Quantization, Quantization_Finite, my_score, my_score_Finite\n",
    "from models.test import test_img\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from utils.functions import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "class DatasetSplit(Dataset):\n",
    "    def __init__(self, dataset, idxs):\n",
    "        self.dataset = dataset\n",
    "        self.idxs = list(idxs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image, label = self.dataset[self.idxs[item]]\n",
    "        return image, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. G=2, N=4 (N_1=2, N_2=2), K=4 (K_1=2, K_2=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of X: (60000, 784)\n",
      "size of Y: (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "class my_argument:    \n",
    "    epochs = 200    #\"rounds of training\"\n",
    "    num_users = 15  # \"number of users: N\"\n",
    "    num_partition = 4 # \"number of users: K\"\n",
    "    frac = 0.5 #\"the fraction of clients: C\"\n",
    "    local_ep = 1 #\"the number of local epochs: E\"\n",
    "    local_bs = 200 #\"local batch size: B\"\n",
    "    bs=200 #\"test batch size\"\n",
    "    lr=0.01 #\"learning rate\"\n",
    "    momentum=0.5 # \"SGD momentum (default: 0.5)\"\n",
    "    split='user' # \"train-test split type, user or sample\"\n",
    "    opt='ADAM'\n",
    "    loss='Custom' # 'Custom' or 'Default'\n",
    "\n",
    "    # model arguments\n",
    "    model = 'cnn'\n",
    "    kernel_num=9 #, help='number of each kind of kernel')\n",
    "    kernel_sizes='3,4,5' #  help='comma-separated kernel size to use for convolution')\n",
    "    norm='None' #, help=\"batch_norm, layer_norm, or None\")\n",
    "    num_filters=32 #, help=\"number of filters for conv nets\")\n",
    "    max_pool='True' #help=\"Whether use max pooling rather than strided convolutions\")\n",
    "\n",
    "    # other arguments\n",
    "    dataset='mnist' #, help=\"name of dataset\")\n",
    "    iid=1\n",
    "    num_classes=10#, help=\"number of classes\")\n",
    "    num_channels=1#, help=\"number of channels of imges\")\n",
    "    gpu=1#, help=\"GPU ID, -1 for CPU\")\n",
    "    stopping_rounds=10#, help='rounds of early stopping')\n",
    "    verbose='False'#, help='verbose print')\n",
    "    seed=1#, help='random seed (default: 1)')\n",
    "    \n",
    "args = my_argument()\n",
    "\n",
    "args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "# load dataset and split users\n",
    "trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "dataset_train = datasets.MNIST('../data/mnist/', train=True, download=True, transform=trans_mnist)\n",
    "dataset_test = datasets.MNIST('../data/mnist/', train=False, download=True, transform=trans_mnist)\n",
    "\n",
    "dict_users = mnist_iid(dataset_train, args.num_partition)\n",
    "\n",
    "encoding_input_array_np = np.empty((len(dataset_train),28*28))\n",
    "encoding_label_array_np = np.empty((len(dataset_train),args.num_classes))\n",
    "print(\"size of X:\" ,encoding_input_array_np.shape)\n",
    "print(\"size of Y:\" ,encoding_label_array_np.shape)\n",
    "\n",
    "Size_submatrices = int(60000/args.num_partition)\n",
    "\n",
    "for i in range(args.num_partition):\n",
    "    \n",
    "    stt_pos = i*Size_submatrices\n",
    "    end_pos = (i+1)*Size_submatrices\n",
    "#     print(i,stt_pos,end_pos)\n",
    "    Temp_train = DataLoader(DatasetSplit(dataset_train, dict_users[i]), batch_size=Size_submatrices, shuffle=True)\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(Temp_train):\n",
    "        \n",
    "        images_np = images.detach().cpu().numpy()\n",
    "        encoding_input_array_np[stt_pos:end_pos,:] = np.reshape(images_np, (Size_submatrices,28*28))\n",
    "#         print(encoding_input_array_np[stt_pos:end_pos,:].shape)\n",
    "\n",
    "        onehot_labels = torch.nn.functional.one_hot(labels,num_classes=args.num_classes)\n",
    "        labels_np = onehot_labels.detach().cpu().numpy()\n",
    "#         print(labels_np.shape)\n",
    "        encoding_label_array_np[stt_pos:end_pos,:] = labels_np\n",
    "\n",
    "\n",
    "# print(labels_np[0:10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = 2\n",
    "N = 4 # N should be divisible by G\n",
    "K = 4 # N should be divisible by G\n",
    "\n",
    "size_per_group = int(60000/G)\n",
    "\n",
    "X_group = np.reshape(encoding_input_array_np, (G,size_per_group,28*28))\n",
    "y_group = np.reshape(encoding_label_array_np, (G,size_per_group,args.num_classes)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2 3\n",
      "##########################################\n",
      "###### 0 -th Trial!! ###########\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.2973 \n",
      "Accuracy: 2163/10000 (21.63%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.2949 \n",
      "Accuracy: 1899/10000 (18.99%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.8496 \n",
      "Accuracy: 7254/10000 (72.54%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3448 \n",
      "Accuracy: 9530/10000 (95.30%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3019 \n",
      "Accuracy: 9539/10000 (95.39%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2349 \n",
      "Accuracy: 9617/10000 (96.17%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2454 \n",
      "Accuracy: 9615/10000 (96.15%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2358 \n",
      "Accuracy: 9608/10000 (96.08%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2291 \n",
      "Accuracy: 9604/10000 (96.04%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2499 \n",
      "Accuracy: 9606/10000 (96.06%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2615 \n",
      "Accuracy: 9591/10000 (95.91%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2418 \n",
      "Accuracy: 9637/10000 (96.37%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2138 \n",
      "Accuracy: 9633/10000 (96.33%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2905 \n",
      "Accuracy: 9616/10000 (96.16%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2591 \n",
      "Accuracy: 9635/10000 (96.35%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2987 \n",
      "Accuracy: 9593/10000 (95.93%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2687 \n",
      "Accuracy: 9647/10000 (96.47%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2265 \n",
      "Accuracy: 9650/10000 (96.50%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2389 \n",
      "Accuracy: 9631/10000 (96.31%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2505 \n",
      "Accuracy: 9635/10000 (96.35%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2267 \n",
      "Accuracy: 9640/10000 (96.40%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2649 \n",
      "Accuracy: 9642/10000 (96.42%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2526 \n",
      "Accuracy: 9605/10000 (96.05%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2283 \n",
      "Accuracy: 9663/10000 (96.63%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2171 \n",
      "Accuracy: 9668/10000 (96.68%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2659 \n",
      "Accuracy: 9640/10000 (96.40%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2047 \n",
      "Accuracy: 9671/10000 (96.71%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.4531 \n",
      "Accuracy: 9568/10000 (95.68%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2314 \n",
      "Accuracy: 9604/10000 (96.04%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "N_i = int(N/G) # = 2\n",
    "K_i = int(K/G) # = 2\n",
    "T = 3\n",
    "sigma = 1\n",
    "Noise_Alloc = [0,2,4]\n",
    "m = N_i # number of selected workers (if there is no straggler, m=N_i)\n",
    "\n",
    "print(N_i,K_i,T)\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K_i+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K_i+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K_i+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "z_array = np.array([-0.81, 0.81])\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 30\n",
    "\n",
    "loss_test_arr_G2 = np.zeros((N_trials,N_epochs))\n",
    "acc_test_arr_G2  = np.zeros((N_trials,N_epochs))\n",
    "\n",
    "for trial_idx in range(N_trials):\n",
    "    \n",
    "    print('##########################################')\n",
    "    print('######',trial_idx,'-th Trial!! ###########')\n",
    "    \n",
    "    net_glob = CNNMnist2(args=args)\n",
    "    net_glob.cuda()\n",
    "    net_glob.train()\n",
    "    \n",
    "    # copy weights\n",
    "    w_glob = net_glob.state_dict()\n",
    "    \n",
    "    X_tilde = np.empty((N,Size_submatrices,28*28))\n",
    "    y_tilde = np.empty((N,Size_submatrices,10))\n",
    "    \n",
    "    for G_idx in range(G):\n",
    "        \n",
    "        _Noise_label = np.ones((size_per_group*T,10)) * 0.1\n",
    "        \n",
    "        X_tilde_tmp,a,b = BACC_Enc_Data_v3(X_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde_tmp,a,b = BACC_Enc_Data_v3(y_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "        \n",
    "        stt_pos = G_idx * N_i\n",
    "        end_pos = (G_idx+1) * N_i\n",
    "        \n",
    "        X_tilde[stt_pos:end_pos,:,:] = X_tilde_tmp\n",
    "        y_tilde[stt_pos:end_pos,:,:] = y_tilde_tmp\n",
    "        \n",
    "   \n",
    "\n",
    "    for iter in range(N_epochs): #args.epochs\n",
    "        \n",
    "        w_group_array = []\n",
    "        for G_idx in range(G):\n",
    "            w_locals, loss_locals = [], []\n",
    "            idxs_users = np.random.choice(range(N_i), m, replace=False)\n",
    "            idxs_users = np.sort(idxs_users)\n",
    "            print('selected users:',idxs_users)\n",
    "\n",
    "            coded_net = BACC_Enc_Model_withNoise_v3(net_glob.cuda(), N_i, K_i, T, 0.01, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "            dec_z_array = []\n",
    "            for idx in idxs_users: #for idx in range(N):\n",
    "        #         print(idx)\n",
    "                local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[G_idx*N_i+idx,:,:], label=y_tilde[G_idx*N_i+idx,:,:])\n",
    "                w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "#                 w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                w_locals.append(copy.deepcopy(w))\n",
    "                loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "            # update global weights\n",
    "            #w_glob = FedAvg(w_locals)\n",
    "            w_group = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "            \n",
    "            w_group_array.append(copy.deepcopy(w_group))\n",
    "        \n",
    "        w_glob = copy.deepcopy(w_group_array[0])\n",
    "        for k in w_glob.keys():\n",
    "            for G_idx in range(1,G):\n",
    "                w_glob[k] += w_group_array[G_idx][k]\n",
    "            w_glob[k] = torch.div(w_glob[k], len(w_group_array))\n",
    "        \n",
    "        # copy weight to net_glob\n",
    "        net_glob.load_state_dict(w_glob)\n",
    "\n",
    "        # print loss\n",
    "    #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "    #     loss_train_arr.append(loss_train)\n",
    "\n",
    "        acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "        acc_test_arr_G2[trial_idx][iter] = acc_test\n",
    "        loss_test_arr_G2[trial_idx][iter] = loss_test\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_group[G_idx,:,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. G=3, K=6 (K_i=2), N=6 (N_i=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of X: (60000, 784)\n",
      "size of Y: (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "class my_argument:    \n",
    "    epochs = 200    #\"rounds of training\"\n",
    "    num_users = 15  # \"number of users: N\"\n",
    "    num_partition = 6 # \"number of users: K\"\n",
    "    frac = 0.5 #\"the fraction of clients: C\"\n",
    "    local_ep = 1 #\"the number of local epochs: E\"\n",
    "    local_bs = 200 #\"local batch size: B\"\n",
    "    bs=200 #\"test batch size\"\n",
    "    lr=0.01 #\"learning rate\"\n",
    "    momentum=0.5 # \"SGD momentum (default: 0.5)\"\n",
    "    split='user' # \"train-test split type, user or sample\"\n",
    "    opt='ADAM'\n",
    "    loss='Custom' # 'Custom' or 'Default'\n",
    "\n",
    "    # model arguments\n",
    "    model = 'cnn'\n",
    "    kernel_num=9 #, help='number of each kind of kernel')\n",
    "    kernel_sizes='3,4,5' #  help='comma-separated kernel size to use for convolution')\n",
    "    norm='None' #, help=\"batch_norm, layer_norm, or None\")\n",
    "    num_filters=32 #, help=\"number of filters for conv nets\")\n",
    "    max_pool='True' #help=\"Whether use max pooling rather than strided convolutions\")\n",
    "\n",
    "    # other arguments\n",
    "    dataset='mnist' #, help=\"name of dataset\")\n",
    "    iid=1\n",
    "    num_classes=10#, help=\"number of classes\")\n",
    "    num_channels=1#, help=\"number of channels of imges\")\n",
    "    gpu=1#, help=\"GPU ID, -1 for CPU\")\n",
    "    stopping_rounds=10#, help='rounds of early stopping')\n",
    "    verbose='False'#, help='verbose print')\n",
    "    seed=1#, help='random seed (default: 1)')\n",
    "    \n",
    "args = my_argument()\n",
    "\n",
    "args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "# load dataset and split users\n",
    "trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "dataset_train = datasets.MNIST('../data/mnist/', train=True, download=True, transform=trans_mnist)\n",
    "dataset_test = datasets.MNIST('../data/mnist/', train=False, download=True, transform=trans_mnist)\n",
    "\n",
    "dict_users = mnist_iid(dataset_train, args.num_partition)\n",
    "\n",
    "encoding_input_array_np = np.empty((len(dataset_train),28*28))\n",
    "encoding_label_array_np = np.empty((len(dataset_train),args.num_classes))\n",
    "print(\"size of X:\" ,encoding_input_array_np.shape)\n",
    "print(\"size of Y:\" ,encoding_label_array_np.shape)\n",
    "\n",
    "Size_submatrices = int(60000/args.num_partition)\n",
    "\n",
    "for i in range(args.num_partition):\n",
    "    \n",
    "    stt_pos = i*Size_submatrices\n",
    "    end_pos = (i+1)*Size_submatrices\n",
    "#     print(i,stt_pos,end_pos)\n",
    "    Temp_train = DataLoader(DatasetSplit(dataset_train, dict_users[i]), batch_size=Size_submatrices, shuffle=True)\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(Temp_train):\n",
    "        \n",
    "        images_np = images.detach().cpu().numpy()\n",
    "        encoding_input_array_np[stt_pos:end_pos,:] = np.reshape(images_np, (Size_submatrices,28*28))\n",
    "#         print(encoding_input_array_np[stt_pos:end_pos,:].shape)\n",
    "\n",
    "        onehot_labels = torch.nn.functional.one_hot(labels,num_classes=args.num_classes)\n",
    "        labels_np = onehot_labels.detach().cpu().numpy()\n",
    "#         print(labels_np.shape)\n",
    "        encoding_label_array_np[stt_pos:end_pos,:] = labels_np\n",
    "\n",
    "\n",
    "# print(labels_np[0:10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = 3\n",
    "N = 6 # N should be divisible by G\n",
    "K = 6 # N should be divisible by G\n",
    "\n",
    "size_per_group = int(60000/G)\n",
    "\n",
    "X_group = np.reshape(encoding_input_array_np, (G,size_per_group,28*28))\n",
    "y_group = np.reshape(encoding_label_array_np, (G,size_per_group,args.num_classes)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2 3\n",
      "##########################################\n",
      "###### 0 -th Trial!! ###########\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 10000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 10000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 10000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 10000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 10000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 10000 \n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1027/10000 (10.27%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.2964 \n",
      "Accuracy: 2574/10000 (25.74%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.2028 \n",
      "Accuracy: 5098/10000 (50.98%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4135 \n",
      "Accuracy: 6138/10000 (61.38%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.8341 \n",
      "Accuracy: 8016/10000 (80.16%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.4538 \n",
      "Accuracy: 8951/10000 (89.51%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.4353 \n",
      "Accuracy: 9057/10000 (90.57%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.4178 \n",
      "Accuracy: 9011/10000 (90.11%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3867 \n",
      "Accuracy: 9143/10000 (91.43%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3976 \n",
      "Accuracy: 9128/10000 (91.28%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3632 \n",
      "Accuracy: 9188/10000 (91.88%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3798 \n",
      "Accuracy: 9189/10000 (91.89%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3676 \n",
      "Accuracy: 9186/10000 (91.86%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3669 \n",
      "Accuracy: 9174/10000 (91.74%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3746 \n",
      "Accuracy: 9148/10000 (91.48%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3653 \n",
      "Accuracy: 9212/10000 (92.12%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3733 \n",
      "Accuracy: 9189/10000 (91.89%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3773 \n",
      "Accuracy: 9161/10000 (91.61%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3742 \n",
      "Accuracy: 9180/10000 (91.80%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3901 \n",
      "Accuracy: 9140/10000 (91.40%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3975 \n",
      "Accuracy: 9138/10000 (91.38%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3835 \n",
      "Accuracy: 9136/10000 (91.36%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3650 \n",
      "Accuracy: 9167/10000 (91.67%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3753 \n",
      "Accuracy: 9104/10000 (91.04%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3893 \n",
      "Accuracy: 9137/10000 (91.37%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3831 \n",
      "Accuracy: 9179/10000 (91.79%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3602 \n",
      "Accuracy: 9173/10000 (91.73%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3802 \n",
      "Accuracy: 9144/10000 (91.44%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3609 \n",
      "Accuracy: 9185/10000 (91.85%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3711 \n",
      "Accuracy: 9164/10000 (91.64%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "N_i = int(N/G)\n",
    "K_i = int(K/G)\n",
    "T = 3\n",
    "sigma = 1\n",
    "Noise_Alloc = [0,2,4]\n",
    "m = N_i # number of selected workers (if there is no straggler, m=N_i)\n",
    "\n",
    "print(N_i,K_i,T)\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K_i+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K_i+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K_i+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "z_array = np.array([-0.81, 0.81])\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 30\n",
    "\n",
    "loss_test_arr_G3 = np.zeros((N_trials,N_epochs))\n",
    "acc_test_arr_G3  = np.zeros((N_trials,N_epochs))\n",
    "\n",
    "for trial_idx in range(N_trials):\n",
    "    \n",
    "    print('##########################################')\n",
    "    print('######',trial_idx,'-th Trial!! ###########')\n",
    "    \n",
    "    net_glob = CNNMnist2(args=args)\n",
    "    net_glob.cuda()\n",
    "    net_glob.train()\n",
    "    \n",
    "    # copy weights\n",
    "    w_glob = net_glob.state_dict()\n",
    "    \n",
    "    X_tilde = np.empty((N,Size_submatrices,28*28))\n",
    "    y_tilde = np.empty((N,Size_submatrices,10))\n",
    "    \n",
    "    for G_idx in range(G):\n",
    "        \n",
    "        _Noise_label = np.ones((size_per_group*T,10)) * 0.1\n",
    "        \n",
    "        X_tilde_tmp,a,b = BACC_Enc_Data_v3(X_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde_tmp,a,b = BACC_Enc_Data_v3(y_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "        \n",
    "        stt_pos = G_idx * N_i\n",
    "        end_pos = (G_idx+1) * N_i\n",
    "        \n",
    "        X_tilde[stt_pos:end_pos,:,:] = X_tilde_tmp\n",
    "        y_tilde[stt_pos:end_pos,:,:] = y_tilde_tmp\n",
    "        \n",
    "   \n",
    "\n",
    "    for iter in range(N_epochs): #args.epochs\n",
    "        \n",
    "        w_group_array = []\n",
    "        for G_idx in range(G):\n",
    "            w_locals, loss_locals = [], []\n",
    "            idxs_users = np.random.choice(range(N_i), m, replace=False)\n",
    "            idxs_users = np.sort(idxs_users)\n",
    "            print('selected users:',idxs_users)\n",
    "\n",
    "            coded_net = BACC_Enc_Model_withNoise_v3(net_glob.cuda(), N_i, K_i, T, 0.01, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "            dec_z_array = []\n",
    "            for idx in idxs_users: #for idx in range(N):\n",
    "        #         print(idx)\n",
    "                local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[G_idx*N_i+idx,:,:], label=y_tilde[G_idx*N_i+idx,:,:])\n",
    "                w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "                w_locals.append(copy.deepcopy(w))\n",
    "                loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "            # update global weights\n",
    "            #w_glob = FedAvg(w_locals)\n",
    "            w_group = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "            \n",
    "            w_group_array.append(copy.deepcopy(w_group))\n",
    "        \n",
    "        w_glob = copy.deepcopy(w_group_array[0])\n",
    "        for k in w_glob.keys():\n",
    "            for G_idx in range(1,G):\n",
    "                w_glob[k] += w_group_array[G_idx][k]\n",
    "            w_glob[k] = torch.div(w_glob[k], len(w_group_array))\n",
    "        \n",
    "        # copy weight to net_glob\n",
    "        net_glob.load_state_dict(w_glob)\n",
    "\n",
    "        # print loss\n",
    "    #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "    #     loss_train_arr.append(loss_train)\n",
    "\n",
    "        acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "        acc_test_arr_G3[trial_idx][iter] = acc_test\n",
    "        loss_test_arr_G3[trial_idx][iter] = loss_test\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. K=2, Without Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of X: (60000, 784)\n",
      "size of Y: (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "class my_argument:    \n",
    "    epochs = 200    #\"rounds of training\"\n",
    "    num_users = 15  # \"number of users: N\"\n",
    "    num_partition = 2 # \"number of users: K\"\n",
    "    frac = 0.5 #\"the fraction of clients: C\"\n",
    "    local_ep = 1 #\"the number of local epochs: E\"\n",
    "    local_bs = 200 #\"local batch size: B\"\n",
    "    bs=200 #\"test batch size\"\n",
    "    lr=0.01 #\"learning rate\"\n",
    "    momentum=0.5 # \"SGD momentum (default: 0.5)\"\n",
    "    split='user' # \"train-test split type, user or sample\"\n",
    "    opt='ADAM'\n",
    "    loss='Custom' # 'Custom' or 'Default'\n",
    "\n",
    "    # model arguments\n",
    "    model = 'cnn'\n",
    "    kernel_num=9 #, help='number of each kind of kernel')\n",
    "    kernel_sizes='3,4,5' #  help='comma-separated kernel size to use for convolution')\n",
    "    norm='None' #, help=\"batch_norm, layer_norm, or None\")\n",
    "    num_filters=32 #, help=\"number of filters for conv nets\")\n",
    "    max_pool='True' #help=\"Whether use max pooling rather than strided convolutions\")\n",
    "\n",
    "    # other arguments\n",
    "    dataset='mnist' #, help=\"name of dataset\")\n",
    "    iid=1\n",
    "    num_classes=10#, help=\"number of classes\")\n",
    "    num_channels=1#, help=\"number of channels of imges\")\n",
    "    gpu=1#, help=\"GPU ID, -1 for CPU\")\n",
    "    stopping_rounds=10#, help='rounds of early stopping')\n",
    "    verbose='False'#, help='verbose print')\n",
    "    seed=1#, help='random seed (default: 1)')\n",
    "    \n",
    "args = my_argument()\n",
    "\n",
    "args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "# load dataset and split users\n",
    "trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "dataset_train = datasets.MNIST('../data/mnist/', train=True, download=True, transform=trans_mnist)\n",
    "dataset_test = datasets.MNIST('../data/mnist/', train=False, download=True, transform=trans_mnist)\n",
    "\n",
    "dict_users = mnist_iid(dataset_train, args.num_partition)\n",
    "\n",
    "encoding_input_array_np = np.empty((len(dataset_train),28*28))\n",
    "encoding_label_array_np = np.empty((len(dataset_train),args.num_classes))\n",
    "print(\"size of X:\" ,encoding_input_array_np.shape)\n",
    "print(\"size of Y:\" ,encoding_label_array_np.shape)\n",
    "\n",
    "Size_submatrices = int(60000/args.num_partition)\n",
    "\n",
    "for i in range(args.num_partition):\n",
    "    \n",
    "    stt_pos = i*Size_submatrices\n",
    "    end_pos = (i+1)*Size_submatrices\n",
    "#     print(i,stt_pos,end_pos)\n",
    "    Temp_train = DataLoader(DatasetSplit(dataset_train, dict_users[i]), batch_size=Size_submatrices, shuffle=True)\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(Temp_train):\n",
    "        \n",
    "        images_np = images.detach().cpu().numpy()\n",
    "        encoding_input_array_np[stt_pos:end_pos,:] = np.reshape(images_np, (Size_submatrices,28*28))\n",
    "#         print(encoding_input_array_np[stt_pos:end_pos,:].shape)\n",
    "\n",
    "        onehot_labels = torch.nn.functional.one_hot(labels,num_classes=args.num_classes)\n",
    "        labels_np = onehot_labels.detach().cpu().numpy()\n",
    "#         print(labels_np.shape)\n",
    "        encoding_label_array_np[stt_pos:end_pos,:] = labels_np\n",
    "\n",
    "\n",
    "# print(labels_np[0:10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z_array: [-0.81  0.81]\n",
      "0.4838626198316927\n",
      "0.48386261983169315\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 30000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 30000 \n",
      "\n",
      "(T, sigma)= 3 1 )  0 -th Trial!!\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.1504 \n",
      "Accuracy: 5550/10000 (55.50%)\n",
      "\n",
      "Round   0, Average loss 2.150 Test accuracy 55.500\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.7712 \n",
      "Accuracy: 8177/10000 (81.77%)\n",
      "\n",
      "Round   1, Average loss 0.771 Test accuracy 81.770\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2180 \n",
      "Accuracy: 9692/10000 (96.92%)\n",
      "\n",
      "Round   2, Average loss 0.218 Test accuracy 96.920\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1758 \n",
      "Accuracy: 9740/10000 (97.40%)\n",
      "\n",
      "Round   3, Average loss 0.176 Test accuracy 97.400\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1905 \n",
      "Accuracy: 9728/10000 (97.28%)\n",
      "\n",
      "Round   4, Average loss 0.191 Test accuracy 97.280\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1868 \n",
      "Accuracy: 9750/10000 (97.50%)\n",
      "\n",
      "Round   5, Average loss 0.187 Test accuracy 97.500\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1771 \n",
      "Accuracy: 9752/10000 (97.52%)\n",
      "\n",
      "Round   6, Average loss 0.177 Test accuracy 97.520\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1665 \n",
      "Accuracy: 9737/10000 (97.37%)\n",
      "\n",
      "Round   7, Average loss 0.166 Test accuracy 97.370\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1977 \n",
      "Accuracy: 9736/10000 (97.36%)\n",
      "\n",
      "Round   8, Average loss 0.198 Test accuracy 97.360\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1733 \n",
      "Accuracy: 9732/10000 (97.32%)\n",
      "\n",
      "Round   9, Average loss 0.173 Test accuracy 97.320\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1734 \n",
      "Accuracy: 9756/10000 (97.56%)\n",
      "\n",
      "Round  10, Average loss 0.173 Test accuracy 97.560\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1853 \n",
      "Accuracy: 9765/10000 (97.65%)\n",
      "\n",
      "Round  11, Average loss 0.185 Test accuracy 97.650\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3139 \n",
      "Accuracy: 9485/10000 (94.85%)\n",
      "\n",
      "Round  12, Average loss 0.314 Test accuracy 94.850\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1789 \n",
      "Accuracy: 9738/10000 (97.38%)\n",
      "\n",
      "Round  13, Average loss 0.179 Test accuracy 97.380\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2365 \n",
      "Accuracy: 9737/10000 (97.37%)\n",
      "\n",
      "Round  14, Average loss 0.236 Test accuracy 97.370\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2020 \n",
      "Accuracy: 9748/10000 (97.48%)\n",
      "\n",
      "Round  15, Average loss 0.202 Test accuracy 97.480\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1793 \n",
      "Accuracy: 9749/10000 (97.49%)\n",
      "\n",
      "Round  16, Average loss 0.179 Test accuracy 97.490\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1965 \n",
      "Accuracy: 9742/10000 (97.42%)\n",
      "\n",
      "Round  17, Average loss 0.196 Test accuracy 97.420\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1861 \n",
      "Accuracy: 9749/10000 (97.49%)\n",
      "\n",
      "Round  18, Average loss 0.186 Test accuracy 97.490\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3015 \n",
      "Accuracy: 9566/10000 (95.66%)\n",
      "\n",
      "Round  19, Average loss 0.302 Test accuracy 95.660\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1838 \n",
      "Accuracy: 9762/10000 (97.62%)\n",
      "\n",
      "Round  20, Average loss 0.184 Test accuracy 97.620\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1723 \n",
      "Accuracy: 9744/10000 (97.44%)\n",
      "\n",
      "Round  21, Average loss 0.172 Test accuracy 97.440\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2225 \n",
      "Accuracy: 9748/10000 (97.48%)\n",
      "\n",
      "Round  22, Average loss 0.223 Test accuracy 97.480\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1853 \n",
      "Accuracy: 9747/10000 (97.47%)\n",
      "\n",
      "Round  23, Average loss 0.185 Test accuracy 97.470\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1962 \n",
      "Accuracy: 9755/10000 (97.55%)\n",
      "\n",
      "Round  24, Average loss 0.196 Test accuracy 97.550\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2181 \n",
      "Accuracy: 9743/10000 (97.43%)\n",
      "\n",
      "Round  25, Average loss 0.218 Test accuracy 97.430\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2300 \n",
      "Accuracy: 9750/10000 (97.50%)\n",
      "\n",
      "Round  26, Average loss 0.230 Test accuracy 97.500\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1930 \n",
      "Accuracy: 9749/10000 (97.49%)\n",
      "\n",
      "Round  27, Average loss 0.193 Test accuracy 97.490\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1894 \n",
      "Accuracy: 9753/10000 (97.53%)\n",
      "\n",
      "Round  28, Average loss 0.189 Test accuracy 97.530\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2037 \n",
      "Accuracy: 9753/10000 (97.53%)\n",
      "\n",
      "Round  29, Average loss 0.204 Test accuracy 97.530\n",
      "z_array: [-0.81 -0.22  0.22  0.81]\n",
      "0.4838626198316927\n",
      "0.46852880899669475\n",
      "0.468528808996694\n",
      "0.48386261983169315\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 30000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 30000 \n",
      "\n",
      "(T, sigma)= 3 1 )  0 -th Trial!!\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.2978 \n",
      "Accuracy: 4081/10000 (40.81%)\n",
      "\n",
      "Round   0, Average loss 2.298 Test accuracy 40.810\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.2801 \n",
      "Accuracy: 3493/10000 (34.93%)\n",
      "\n",
      "Round   1, Average loss 2.280 Test accuracy 34.930\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.2261 \n",
      "Accuracy: 7954/10000 (79.54%)\n",
      "\n",
      "Round   2, Average loss 2.226 Test accuracy 79.540\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.1914 \n",
      "Accuracy: 7929/10000 (79.29%)\n",
      "\n",
      "Round   3, Average loss 2.191 Test accuracy 79.290\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.1369 \n",
      "Accuracy: 6894/10000 (68.94%)\n",
      "\n",
      "Round   4, Average loss 2.137 Test accuracy 68.940\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.0815 \n",
      "Accuracy: 8826/10000 (88.26%)\n",
      "\n",
      "Round   5, Average loss 2.081 Test accuracy 88.260\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.0964 \n",
      "Accuracy: 8591/10000 (85.91%)\n",
      "\n",
      "Round   6, Average loss 2.096 Test accuracy 85.910\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.1099 \n",
      "Accuracy: 9409/10000 (94.09%)\n",
      "\n",
      "Round   7, Average loss 2.110 Test accuracy 94.090\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.1391 \n",
      "Accuracy: 8650/10000 (86.50%)\n",
      "\n",
      "Round   8, Average loss 2.139 Test accuracy 86.500\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.0060 \n",
      "Accuracy: 9606/10000 (96.06%)\n",
      "\n",
      "Round   9, Average loss 2.006 Test accuracy 96.060\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9598 \n",
      "Accuracy: 9456/10000 (94.56%)\n",
      "\n",
      "Round  10, Average loss 1.960 Test accuracy 94.560\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9703 \n",
      "Accuracy: 9485/10000 (94.85%)\n",
      "\n",
      "Round  11, Average loss 1.970 Test accuracy 94.850\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9826 \n",
      "Accuracy: 9690/10000 (96.90%)\n",
      "\n",
      "Round  12, Average loss 1.983 Test accuracy 96.900\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.0128 \n",
      "Accuracy: 9711/10000 (97.11%)\n",
      "\n",
      "Round  13, Average loss 2.013 Test accuracy 97.110\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8374 \n",
      "Accuracy: 9722/10000 (97.22%)\n",
      "\n",
      "Round  14, Average loss 1.837 Test accuracy 97.220\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9075 \n",
      "Accuracy: 9455/10000 (94.55%)\n",
      "\n",
      "Round  15, Average loss 1.908 Test accuracy 94.550\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9255 \n",
      "Accuracy: 9717/10000 (97.17%)\n",
      "\n",
      "Round  16, Average loss 1.925 Test accuracy 97.170\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8705 \n",
      "Accuracy: 9569/10000 (95.69%)\n",
      "\n",
      "Round  17, Average loss 1.871 Test accuracy 95.690\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8759 \n",
      "Accuracy: 9661/10000 (96.61%)\n",
      "\n",
      "Round  18, Average loss 1.876 Test accuracy 96.610\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9110 \n",
      "Accuracy: 9700/10000 (97.00%)\n",
      "\n",
      "Round  19, Average loss 1.911 Test accuracy 97.000\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8551 \n",
      "Accuracy: 9721/10000 (97.21%)\n",
      "\n",
      "Round  20, Average loss 1.855 Test accuracy 97.210\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.0106 \n",
      "Accuracy: 9707/10000 (97.07%)\n",
      "\n",
      "Round  21, Average loss 2.011 Test accuracy 97.070\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.0318 \n",
      "Accuracy: 9667/10000 (96.67%)\n",
      "\n",
      "Round  22, Average loss 2.032 Test accuracy 96.670\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9992 \n",
      "Accuracy: 9299/10000 (92.99%)\n",
      "\n",
      "Round  23, Average loss 1.999 Test accuracy 92.990\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8857 \n",
      "Accuracy: 9527/10000 (95.27%)\n",
      "\n",
      "Round  24, Average loss 1.886 Test accuracy 95.270\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8113 \n",
      "Accuracy: 9700/10000 (97.00%)\n",
      "\n",
      "Round  25, Average loss 1.811 Test accuracy 97.000\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8391 \n",
      "Accuracy: 9736/10000 (97.36%)\n",
      "\n",
      "Round  26, Average loss 1.839 Test accuracy 97.360\n",
      "selected users: [0 1 2 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.9353 \n",
      "Accuracy: 9652/10000 (96.52%)\n",
      "\n",
      "Round  27, Average loss 1.935 Test accuracy 96.520\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.7971 \n",
      "Accuracy: 9755/10000 (97.55%)\n",
      "\n",
      "Round  28, Average loss 1.797 Test accuracy 97.550\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9200 \n",
      "Accuracy: 9682/10000 (96.82%)\n",
      "\n",
      "Round  29, Average loss 1.920 Test accuracy 96.820\n",
      "z_array: [-0.9  -0.81 -0.22  0.22  0.81  0.9 ]\n",
      "0.039572349708934106\n",
      "0.4838626198316927\n",
      "0.46852880899669475\n",
      "0.468528808996694\n",
      "0.48386261983169315\n",
      "0.039572349708934106\n",
      "@BACC_Enc: N,K,T, m_i= 6 2 3 30000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 6 2 3 30000 \n",
      "\n",
      "(T, sigma)= 3 1 )  0 -th Trial!!\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 2.2223 \n",
      "Accuracy: 6257/10000 (62.57%)\n",
      "\n",
      "Round   1, Average loss 2.222 Test accuracy 62.570\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 2.2161 \n",
      "Accuracy: 6335/10000 (63.35%)\n",
      "\n",
      "Round   2, Average loss 2.216 Test accuracy 63.350\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 1.6638 \n",
      "Accuracy: 8681/10000 (86.81%)\n",
      "\n",
      "Round   3, Average loss 1.664 Test accuracy 86.810\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 1.3783 \n",
      "Accuracy: 9617/10000 (96.17%)\n",
      "\n",
      "Round   4, Average loss 1.378 Test accuracy 96.170\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 1.2237 \n",
      "Accuracy: 9644/10000 (96.44%)\n",
      "\n",
      "Round   5, Average loss 1.224 Test accuracy 96.440\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.8223 \n",
      "Accuracy: 9638/10000 (96.38%)\n",
      "\n",
      "Round   6, Average loss 0.822 Test accuracy 96.380\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.7091 \n",
      "Accuracy: 9713/10000 (97.13%)\n",
      "\n",
      "Round   7, Average loss 0.709 Test accuracy 97.130\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.6716 \n",
      "Accuracy: 9713/10000 (97.13%)\n",
      "\n",
      "Round   8, Average loss 0.672 Test accuracy 97.130\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.6693 \n",
      "Accuracy: 9678/10000 (96.78%)\n",
      "\n",
      "Round   9, Average loss 0.669 Test accuracy 96.780\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.5860 \n",
      "Accuracy: 9650/10000 (96.50%)\n",
      "\n",
      "Round  10, Average loss 0.586 Test accuracy 96.500\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.5678 \n",
      "Accuracy: 9689/10000 (96.89%)\n",
      "\n",
      "Round  11, Average loss 0.568 Test accuracy 96.890\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.6501 \n",
      "Accuracy: 9699/10000 (96.99%)\n",
      "\n",
      "Round  12, Average loss 0.650 Test accuracy 96.990\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.5906 \n",
      "Accuracy: 9678/10000 (96.78%)\n",
      "\n",
      "Round  13, Average loss 0.591 Test accuracy 96.780\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.6501 \n",
      "Accuracy: 9738/10000 (97.38%)\n",
      "\n",
      "Round  14, Average loss 0.650 Test accuracy 97.380\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.5508 \n",
      "Accuracy: 9722/10000 (97.22%)\n",
      "\n",
      "Round  15, Average loss 0.551 Test accuracy 97.220\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.5475 \n",
      "Accuracy: 9593/10000 (95.93%)\n",
      "\n",
      "Round  16, Average loss 0.547 Test accuracy 95.930\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.5397 \n",
      "Accuracy: 9471/10000 (94.71%)\n",
      "\n",
      "Round  17, Average loss 0.540 Test accuracy 94.710\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.8024 \n",
      "Accuracy: 9654/10000 (96.54%)\n",
      "\n",
      "Round  18, Average loss 0.802 Test accuracy 96.540\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.6738 \n",
      "Accuracy: 9694/10000 (96.94%)\n",
      "\n",
      "Round  19, Average loss 0.674 Test accuracy 96.940\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.5364 \n",
      "Accuracy: 9693/10000 (96.93%)\n",
      "\n",
      "Round  20, Average loss 0.536 Test accuracy 96.930\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.6852 \n",
      "Accuracy: 9707/10000 (97.07%)\n",
      "\n",
      "Round  21, Average loss 0.685 Test accuracy 97.070\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.6254 \n",
      "Accuracy: 9715/10000 (97.15%)\n",
      "\n",
      "Round  22, Average loss 0.625 Test accuracy 97.150\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.5700 \n",
      "Accuracy: 9576/10000 (95.76%)\n",
      "\n",
      "Round  23, Average loss 0.570 Test accuracy 95.760\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.7501 \n",
      "Accuracy: 9319/10000 (93.19%)\n",
      "\n",
      "Round  24, Average loss 0.750 Test accuracy 93.190\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.8517 \n",
      "Accuracy: 9267/10000 (92.67%)\n",
      "\n",
      "Round  25, Average loss 0.852 Test accuracy 92.670\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.8802 \n",
      "Accuracy: 9491/10000 (94.91%)\n",
      "\n",
      "Round  26, Average loss 0.880 Test accuracy 94.910\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.8818 \n",
      "Accuracy: 8799/10000 (87.99%)\n",
      "\n",
      "Round  27, Average loss 0.882 Test accuracy 87.990\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.8674 \n",
      "Accuracy: 9460/10000 (94.60%)\n",
      "\n",
      "Round  28, Average loss 0.867 Test accuracy 94.600\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.6362 \n",
      "Accuracy: 9709/10000 (97.09%)\n",
      "\n",
      "Round  29, Average loss 0.636 Test accuracy 97.090\n",
      "z_array: [-0.9  -0.81 -0.22 -0.16  0.16  0.22  0.81  0.9 ]\n",
      "0.039572349708934106\n",
      "0.4838626198316927\n",
      "0.46852880899669475\n",
      "0.23252578774407598\n",
      "0.23252578774407545\n",
      "0.468528808996694\n",
      "0.48386261983169315\n",
      "0.039572349708934106\n",
      "@BACC_Enc: N,K,T, m_i= 8 2 3 30000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 8 2 3 30000 \n",
      "\n",
      "(T, sigma)= 3 1 )  0 -th Trial!!\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 2.2079 \n",
      "Accuracy: 4164/10000 (41.64%)\n",
      "\n",
      "Round   0, Average loss 2.208 Test accuracy 41.640\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.6987 \n",
      "Accuracy: 7686/10000 (76.86%)\n",
      "\n",
      "Round   1, Average loss 1.699 Test accuracy 76.860\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.5383 \n",
      "Accuracy: 9540/10000 (95.40%)\n",
      "\n",
      "Round   2, Average loss 0.538 Test accuracy 95.400\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.5397 \n",
      "Accuracy: 9462/10000 (94.62%)\n",
      "\n",
      "Round   3, Average loss 0.540 Test accuracy 94.620\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.6978 \n",
      "Accuracy: 9447/10000 (94.47%)\n",
      "\n",
      "Round   4, Average loss 0.698 Test accuracy 94.470\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.3977 \n",
      "Accuracy: 9543/10000 (95.43%)\n",
      "\n",
      "Round   5, Average loss 0.398 Test accuracy 95.430\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.8793 \n",
      "Accuracy: 9627/10000 (96.27%)\n",
      "\n",
      "Round   6, Average loss 0.879 Test accuracy 96.270\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.2739 \n",
      "Accuracy: 9606/10000 (96.06%)\n",
      "\n",
      "Round   7, Average loss 0.274 Test accuracy 96.060\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.2858 \n",
      "Accuracy: 9585/10000 (95.85%)\n",
      "\n",
      "Round   8, Average loss 0.286 Test accuracy 95.850\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.4189 \n",
      "Accuracy: 9596/10000 (95.96%)\n",
      "\n",
      "Round   9, Average loss 0.419 Test accuracy 95.960\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.4356 \n",
      "Accuracy: 9599/10000 (95.99%)\n",
      "\n",
      "Round  10, Average loss 0.436 Test accuracy 95.990\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.3830 \n",
      "Accuracy: 9478/10000 (94.78%)\n",
      "\n",
      "Round  11, Average loss 0.383 Test accuracy 94.780\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.3993 \n",
      "Accuracy: 9545/10000 (95.45%)\n",
      "\n",
      "Round  12, Average loss 0.399 Test accuracy 95.450\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.4343 \n",
      "Accuracy: 9518/10000 (95.18%)\n",
      "\n",
      "Round  13, Average loss 0.434 Test accuracy 95.180\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.2578 \n",
      "Accuracy: 9606/10000 (96.06%)\n",
      "\n",
      "Round  14, Average loss 0.258 Test accuracy 96.060\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.2952 \n",
      "Accuracy: 9710/10000 (97.10%)\n",
      "\n",
      "Round  15, Average loss 0.295 Test accuracy 97.100\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.3339 \n",
      "Accuracy: 9661/10000 (96.61%)\n",
      "\n",
      "Round  16, Average loss 0.334 Test accuracy 96.610\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.5458 \n",
      "Accuracy: 9503/10000 (95.03%)\n",
      "\n",
      "Round  17, Average loss 0.546 Test accuracy 95.030\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.4201 \n",
      "Accuracy: 9622/10000 (96.22%)\n",
      "\n",
      "Round  18, Average loss 0.420 Test accuracy 96.220\n",
      "selected users: [0 1 2 3 4 5 6 7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.5977 \n",
      "Accuracy: 9224/10000 (92.24%)\n",
      "\n",
      "Round  19, Average loss 0.598 Test accuracy 92.240\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.3295 \n",
      "Accuracy: 9644/10000 (96.44%)\n",
      "\n",
      "Round  20, Average loss 0.330 Test accuracy 96.440\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.3735 \n",
      "Accuracy: 9571/10000 (95.71%)\n",
      "\n",
      "Round  21, Average loss 0.373 Test accuracy 95.710\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.3915 \n",
      "Accuracy: 9354/10000 (93.54%)\n",
      "\n",
      "Round  22, Average loss 0.391 Test accuracy 93.540\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.4134 \n",
      "Accuracy: 9209/10000 (92.09%)\n",
      "\n",
      "Round  23, Average loss 0.413 Test accuracy 92.090\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.4039 \n",
      "Accuracy: 9634/10000 (96.34%)\n",
      "\n",
      "Round  24, Average loss 0.404 Test accuracy 96.340\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.4398 \n",
      "Accuracy: 9725/10000 (97.25%)\n",
      "\n",
      "Round  25, Average loss 0.440 Test accuracy 97.250\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.4958 \n",
      "Accuracy: 9551/10000 (95.51%)\n",
      "\n",
      "Round  26, Average loss 0.496 Test accuracy 95.510\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.4113 \n",
      "Accuracy: 9572/10000 (95.72%)\n",
      "\n",
      "Round  27, Average loss 0.411 Test accuracy 95.720\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.2503 \n",
      "Accuracy: 9627/10000 (96.27%)\n",
      "\n",
      "Round  28, Average loss 0.250 Test accuracy 96.270\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.3805 \n",
      "Accuracy: 9541/10000 (95.41%)\n",
      "\n",
      "Round  29, Average loss 0.381 Test accuracy 95.410\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "K = 2\n",
    "T = 3\n",
    "sigma = 1\n",
    "Noise_Alloc = [0,2,4]\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "\n",
    "N_array = [2,4,6,8]\n",
    "B_array = [0.5]\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 30\n",
    "\n",
    "\n",
    "\n",
    "loss_test_arr_v1 = np.zeros((len(N_array),len(B_array),N_trials,N_epochs))\n",
    "acc_test_arr_v1  = np.zeros((len(N_array),len(B_array),N_trials,N_epochs))\n",
    "\n",
    "for N_idx in range(len(N_array)):\n",
    "    \n",
    "    N = N_array[N_idx]\n",
    "    \n",
    "    \n",
    "           \n",
    "        \n",
    "    # print(\"alpha_array: \",alpha_array,'\\n')\n",
    "    \n",
    "    \n",
    "    for B_idx in range(len(B_array)):\n",
    "        \n",
    "        B = B_array[B_idx]\n",
    "        z_array = []\n",
    "#         while(len(z_array)<N):\n",
    "#             z_tmp = np.random.uniform(-1,1,1)\n",
    "#             MIS_tmp = MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_tmp], 1,sigma)\n",
    "#             if MIS_tmp < B and MIS_tmp > 0.1:\n",
    "#                 z_array.append(z_tmp[0])\n",
    "#         \n",
    "#         z_array = np.sort(z_array)\n",
    "\n",
    "        if N==2:\n",
    "            z_array = np.array([-0.81, 0.81])\n",
    "        elif N ==4:\n",
    "            z_array = np.array([-0.81, -0.22, 0.22, 0.81])\n",
    "        elif N ==5:\n",
    "            z_array = np.array([-0.81, -0.22, 0, 0.22, 0.81])\n",
    "        elif N ==6:\n",
    "            z_array = np.array([-0.9, -0.81, -0.22, 0.22, 0.81, 0.9])\n",
    "        elif N ==7:\n",
    "            z_array = np.array([-0.9, -0.82, -0.21, 0, 0.21, 0.82, 0.9])\n",
    "        else:\n",
    "            z_array = np.array([-0.9, -0.81, -0.22, -0.16, 0.16, 0.22, 0.81, 0.9])\n",
    "#             z_array = np.array([-0.9, -0.81, -0.22, -0.20, 0.20, 0.22, 0.81, 0.9])\n",
    "            \n",
    "        print('z_array:',z_array)\n",
    "        for j in range(len(z_array)):\n",
    "            print(MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_array[j]], 1,sigma))\n",
    "        \n",
    "        \n",
    "        _Noise_label = np.ones((30000*T,10)) * 0.1\n",
    "\n",
    "        X_tilde,a,b = BACC_Enc_Data_v3(encoding_input_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde,a,b = BACC_Enc_Data_v3(encoding_label_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "\n",
    "        m = N # m is the number of received result @ master\n",
    "    #     print('number of results:',m)\n",
    "\n",
    "        for trial_idx in range(N_trials):\n",
    "            print('(T, sigma)=',T,sigma,') ',trial_idx,'-th Trial!!')\n",
    "\n",
    "            net_glob = CNNMnist2(args=args)\n",
    "            net_glob.cuda()\n",
    "            net_glob.train()\n",
    "\n",
    "            # copy weights\n",
    "            w_glob = net_glob.state_dict()\n",
    "\n",
    "            for iter in range(N_epochs): #args.epochs\n",
    "                w_locals, loss_locals = [], []\n",
    "                idxs_users = np.random.choice(range(N), m, replace=False)\n",
    "                idxs_users = np.sort(idxs_users)\n",
    "                print('selected users:',idxs_users)\n",
    "                \n",
    "                coded_net = BACC_Enc_Model_withNoise_v3(net_glob.cuda(), N, K, T, 0.01, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "                dec_z_array = []\n",
    "                for idx in idxs_users: #for idx in range(N):\n",
    "            #         print(idx)\n",
    "                    local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[idx,:,:], label=y_tilde[idx,:,:])\n",
    "                    w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "                    w_locals.append(copy.deepcopy(w))\n",
    "                    loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                    dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "                # update global weights\n",
    "                #w_glob = FedAvg(w_locals)\n",
    "                w_glob = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "\n",
    "                # copy weight to net_glob\n",
    "                net_glob.load_state_dict(w_glob)\n",
    "\n",
    "                # print loss\n",
    "            #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "            #     loss_train_arr.append(loss_train)\n",
    "\n",
    "                acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "                acc_test_arr_v1[N_idx][B_idx][trial_idx][iter] = acc_test\n",
    "                loss_test_arr_v1[N_idx][B_idx][trial_idx][iter] = loss_test\n",
    "                if iter % 1 ==0:\n",
    "                    print('Round {:3d}, Average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_test, acc_test))\n",
    "                #print(loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "B_sel = 0\n",
    "\n",
    "# plt.plot(acc_test_arr_v1[0,B_sel,0,0:30],label='N=2, B='+str(B_array[B_sel]))\n",
    "plt.plot(acc_test_arr_v1[1,B_sel,0,0:30],label='N=4, B='+str(B_array[B_sel]))\n",
    "# plt.plot(acc_test_arr_v1[2,B_sel,0,0:30],label='N=6, B='+str(B_array[B_sel]))\n",
    "# plt.plot(acc_test_arr_v1[3,B_sel,0,0:30],label='N=8, B='+str(B_array[B_sel]))\n",
    "plt.plot(acc_test_arr_G2[0,0:30],label='Grouping, N=4, G=2, B='+str(B_array[B_sel]))\n",
    "# plt.plot(acc_test_arr_G3[0,0:30],label='Grouping, N=6, G=3, B='+str(B_array[B_sel]))\n",
    "\n",
    "# plt.plot(acc_test_arr_v1[4,B_sel,0,0:30],label='N=7, B='+str(B_array[B_sel]))\n",
    "# plt.plot(acc_test_arr_v1[5,B_sel,0,0:30],label='N=8, B='+str(B_array[B_sel]))\n",
    "\n",
    "# plt.plot(plot_acc_v2[2,sigma_sel,:],label='T=10')\n",
    "# plt.plot(plot_acc_v2[3,sigma_sel,:],label='T=11')\n",
    "\n",
    "# plt.plot(acc_test_arr_uncoded,'r',label='uncoded')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "# plt.title(title_name)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Test accuracy(%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. K=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of X: (60000, 784)\n",
      "size of Y: (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "class my_argument:    \n",
    "    epochs = 200    #\"rounds of training\"\n",
    "    num_users = 15  # \"number of users: N\"\n",
    "    num_partition = 4 # \"number of users: K\"\n",
    "    frac = 0.5 #\"the fraction of clients: C\"\n",
    "    local_ep = 1 #\"the number of local epochs: E\"\n",
    "    local_bs = 200 #\"local batch size: B\"\n",
    "    bs=200 #\"test batch size\"\n",
    "    lr=0.01 #\"learning rate\"\n",
    "    momentum=0.5 # \"SGD momentum (default: 0.5)\"\n",
    "    split='user' # \"train-test split type, user or sample\"\n",
    "    opt='ADAM'\n",
    "    loss='Custom' # 'Custom' or 'Default'\n",
    "\n",
    "    # model arguments\n",
    "    model = 'cnn'\n",
    "    kernel_num=9 #, help='number of each kind of kernel')\n",
    "    kernel_sizes='3,4,5' #  help='comma-separated kernel size to use for convolution')\n",
    "    norm='None' #, help=\"batch_norm, layer_norm, or None\")\n",
    "    num_filters=32 #, help=\"number of filters for conv nets\")\n",
    "    max_pool='True' #help=\"Whether use max pooling rather than strided convolutions\")\n",
    "\n",
    "    # other arguments\n",
    "    dataset='mnist' #, help=\"name of dataset\")\n",
    "    iid=1\n",
    "    num_classes=10#, help=\"number of classes\")\n",
    "    num_channels=1#, help=\"number of channels of imges\")\n",
    "    gpu=1#, help=\"GPU ID, -1 for CPU\")\n",
    "    stopping_rounds=10#, help='rounds of early stopping')\n",
    "    verbose='False'#, help='verbose print')\n",
    "    seed=1#, help='random seed (default: 1)')\n",
    "    \n",
    "args = my_argument()\n",
    "\n",
    "args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "# load dataset and split users\n",
    "trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "dataset_train = datasets.MNIST('../data/mnist/', train=True, download=True, transform=trans_mnist)\n",
    "dataset_test = datasets.MNIST('../data/mnist/', train=False, download=True, transform=trans_mnist)\n",
    "\n",
    "dict_users = mnist_iid(dataset_train, args.num_partition)\n",
    "\n",
    "encoding_input_array_np = np.empty((len(dataset_train),28*28))\n",
    "encoding_label_array_np = np.empty((len(dataset_train),args.num_classes))\n",
    "print(\"size of X:\" ,encoding_input_array_np.shape)\n",
    "print(\"size of Y:\" ,encoding_label_array_np.shape)\n",
    "\n",
    "Size_submatrices = int(60000/args.num_partition)\n",
    "\n",
    "for i in range(args.num_partition):\n",
    "    \n",
    "    stt_pos = i*Size_submatrices\n",
    "    end_pos = (i+1)*Size_submatrices\n",
    "#     print(i,stt_pos,end_pos)\n",
    "    Temp_train = DataLoader(DatasetSplit(dataset_train, dict_users[i]), batch_size=Size_submatrices, shuffle=True)\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(Temp_train):\n",
    "        \n",
    "        images_np = images.detach().cpu().numpy()\n",
    "        encoding_input_array_np[stt_pos:end_pos,:] = np.reshape(images_np, (Size_submatrices,28*28))\n",
    "#         print(encoding_input_array_np[stt_pos:end_pos,:].shape)\n",
    "\n",
    "        onehot_labels = torch.nn.functional.one_hot(labels,num_classes=args.num_classes)\n",
    "        labels_np = onehot_labels.detach().cpu().numpy()\n",
    "#         print(labels_np.shape)\n",
    "        encoding_label_array_np[stt_pos:end_pos,:] = labels_np\n",
    "\n",
    "\n",
    "# print(labels_np[0:10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "395 137 77 66 325\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.9967233991100619 0.012570500902590362\n",
      "-0.9948458393310273 0.009186154041335111\n",
      "-0.9903708272549521 0.0030278602507773783\n",
      "-0.9881497879818113 0.0011322378640799269\n",
      "-0.9856357664431257 7.237229030193549e-05\n",
      "-0.9819502816066774 0.0009151342222115455\n",
      "-0.9743801406025652 0.013807862953069145\n",
      "-0.9732429036029051 0.017308391808860374\n",
      "-0.9716499425996106 0.02300816009848999\n",
      "-0.9707810034884226 0.026529720146499475\n",
      "-0.9629358119852338 0.07356572119174355\n",
      "-0.9592848094889013 0.10648755223399524\n",
      "-0.9578307990518302 0.12188755258432364\n",
      "-0.9571486494917996 0.12959372414089768\n",
      "-0.9548728694739659 0.15764285382604115\n",
      "-0.9543836651467361 0.16416119397904505\n",
      "-0.9512302584551071 0.21060896428961837\n",
      "-0.9501286871582235 0.22873571263139653\n",
      "-0.9493079992209263 0.24291445045342466\n",
      "-0.9468712143720244 0.2885550929930391\n",
      "-0.9462045027048012 0.3020009958277021\n",
      "-0.9450226360856129 0.3268894006886918\n",
      "-0.942889051744954 0.37535344112475444\n",
      "-0.9420255449368415 0.3963084326299616\n",
      "-0.9399882819365448 0.44893946653047034\n",
      "-0.9393210692612008 0.4671793063599652\n",
      "-0.9361401480934441 0.5612547387172819\n",
      "-0.9334249487300454 0.6513177462798181\n",
      "-0.932257382371722 0.6929463184727821\n",
      "-0.9289758144235429 0.8197277601845057\n",
      "-0.927738182324308 0.8714218799219998\n",
      "-0.9271053605255939 0.8987001744079706\n",
      "-0.9235110987778754 1.0648437210320687\n",
      "-0.9234480005306531 1.0679348900683752\n",
      "-0.9233213357776953 1.0741587323924215\n",
      "-0.918849424826937 1.3102067934801664\n",
      "-0.9186182427332463 1.3232967588171969\n",
      "-0.9181858737213797 1.348019590780991\n",
      "-0.9168905756695997 1.4239895832414762\n",
      "-0.9163210916120146 1.4583072724489874\n",
      "-0.9150248839348085 1.5385496585068086\n",
      "-0.9135509049600627 1.6334782867338973\n",
      "-0.9123670637792722 1.712638254922767\n",
      "-0.9113186540835647 1.7849729240484424\n",
      "-0.9111252443304267 1.7985504623655058\n",
      "-0.9077342856705073 2.0488775378617987\n",
      "-0.9067488000118127 2.1261699951421535\n",
      "-0.9015728242039807 2.5692475464313973\n",
      "-0.9003597325562589 2.6829564783466524\n",
      "-0.8992964395654581 2.7860290679332906\n",
      "-0.8967889938292337 3.0426685631404755\n",
      "-0.8929303360009146 3.4801107236187185\n",
      "-0.8924259873123535 3.5416352736229997\n",
      "-0.8872975815773814 4.238337724143214\n",
      "-0.887075149848507 4.271965774429419\n",
      "-0.8869893920991923 4.285018736673706\n",
      "-0.8859689029392013 4.4442512946490895\n",
      "-0.8856471272842785 4.496022920882679\n",
      "-0.884814672454697 4.633690574454463\n",
      "-0.8834943859785533 4.864050930487142\n",
      "-0.8817329488836674 5.197882779004097\n",
      "-0.8813696754392899 5.271034956378789\n",
      "-0.881088643191996 5.328739739698576\n",
      "-0.8775609575033796 6.153928115797533\n",
      "-0.8760981634708074 6.568295483658911\n",
      "-0.8754728537973833 6.763118659869037\n",
      "-0.8733950435939555 7.5127198081558895\n",
      "-0.8718467400753278 8.217762885071172\n",
      "-0.8714329360779989 8.437082411993401\n",
      "-0.8713564142457995 8.479417893396516\n",
      "-0.8712801101631871 8.522224832496745\n",
      "-0.8712097850693972 8.562215870307737\n",
      "-0.8666359479299288 14.80640707622643\n",
      "-0.8602151865217669 8.405030664570349\n",
      "-0.8600162091893928 8.310905203003161\n",
      "-0.8580565555548707 7.526195737502597\n",
      "-0.8569598373455518 7.170603684365414\n",
      "-0.8555582684023841 6.7766703909722255\n",
      "-0.8554187656610377 6.740534281306398\n",
      "-0.8540222373097082 6.40420279824693\n",
      "-0.8505471285755672 5.72077682444265\n",
      "-0.848945743793434 5.459382023095871\n",
      "-0.8467526399397753 5.141357544923404\n",
      "-0.8429089171957409 4.668701539822457\n",
      "-0.8422189458196252 4.592981744328978\n",
      "-0.8418261566465528 4.550939645186029\n",
      "-0.8417852772103849 4.546607054711651\n",
      "-0.8367232578105628 4.0642245395150844\n",
      "-0.834410090789593 3.873466400053546\n",
      "-0.8343203722934223 3.866379642597188\n",
      "-0.8310231824514018 3.6202061676836648\n",
      "-0.8294629931577833 3.512459030807207\n",
      "-0.8294020994957441 3.5083565990694052\n",
      "-0.8292733419235208 3.4997067416723864\n",
      "-0.8261884715825596 3.3018595563598474\n",
      "-0.8260959127523366 3.296185379640544\n",
      "-0.8258111998665689 3.2788212977309\n",
      "-0.82540027336506 3.2539953691909878\n",
      "-0.8250963128562394 3.235807760371835\n",
      "-0.8248502784936174 3.2211939702272296\n",
      "-0.8196862915317098 2.934706144983515\n",
      "-0.819578855140563 2.929120554213317\n",
      "-0.8186380719997193 2.8807998822912695\n",
      "-0.815231582175499 2.7141329293319383\n",
      "-0.8151963431046056 2.712472512892968\n",
      "-0.8143493646763478 2.672932926152529\n",
      "-0.8124459782330016 2.5865638686058836\n",
      "-0.8110944291961184 2.5272120431152176\n",
      "-0.8109950273995756 2.522908883236551\n",
      "-0.8105533292123301 2.503887688958898\n",
      "-0.8095115288225925 2.4596585635288637\n",
      "-0.808791969177798 2.42961617858205\n",
      "-0.8071662407339542 2.363200193245918\n",
      "-0.8021155956991857 2.168699221820976\n",
      "-0.8015400712014591 2.147581174860692\n",
      "-0.8013005178213168 2.1388501584690376\n",
      "-0.7989153213248099 2.053747798058474\n",
      "-0.7975091540029895 2.0050729299582093\n",
      "-0.7913588869582644 1.8040366771494918\n",
      "-0.7886943895249638 1.7224090309312383\n",
      "-0.7871336949955006 1.676003377828127\n",
      "-0.783267205880174 1.5652476604607213\n",
      "-0.7830570752980781 1.5593935260597984\n",
      "-0.7829021036494994 1.5550866807477157\n",
      "-0.7815194669977168 1.517054876663595\n",
      "-0.7777251329885562 1.416199384521821\n",
      "-0.7759213435984083 1.3699862599288903\n",
      "-0.7744764671980322 1.3337425978377153\n",
      "-0.7737215575768588 1.315074393709174\n",
      "-0.7730350618723734 1.2982554611440067\n",
      "-0.7725850862483015 1.2873118177954865\n",
      "-0.7703319726417956 1.2334597223699109\n",
      "-0.7647822277315575 1.1072924532084405\n",
      "-0.7618916653743606 1.045074544119761\n",
      "-0.7585065060770475 0.9751280584900909\n",
      "-0.7563976312193976 0.9331019970079399\n",
      "-0.7561289395600914 0.9278314025810692\n",
      "-0.7550962992799191 0.9077502440770584\n",
      "-0.7549996807497663 0.9058855033848836\n",
      "-0.7536199576644498 0.8795190693028361\n",
      "-0.7516124249421976 0.8420227740366935\n",
      "-0.7445224665933863 0.717617184815701\n",
      "-0.7435870512292879 0.7021154665021572\n",
      "-0.7433009089907185 0.6974151850858306\n",
      "-0.7411016830869268 0.6619367490609414\n",
      "-0.7399306425846846 0.6435087323886064\n",
      "-0.7354891683059677 0.5764957813011693\n",
      "-0.7293167001632472 0.4907529018704423\n",
      "-0.7274519376681967 0.46649571953718183\n",
      "-0.726627761782827 0.4560136164016924\n",
      "-0.7245764823076315 0.43055428501280063\n",
      "-0.723467786783724 0.41716437297170067\n",
      "-0.721062520086799 0.3889985160910504\n",
      "-0.7203097425701162 0.3804293786795524\n",
      "-0.7195556876469855 0.37196218555747046\n",
      "-0.7175204468539162 0.34968583793166336\n",
      "-0.716525693067974 0.3391015919381231\n",
      "-0.7155282724817407 0.3286871846087321\n",
      "-0.7124722903169596 0.2979987633090837\n",
      "-0.7105865274255168 0.279965785348844\n",
      "-0.707637906451057 0.25312315903940424\n",
      "-0.705087772582756 0.23121340120606423\n",
      "-0.702776323158554 0.2123772915900445\n",
      "-0.6991041898082215 0.18440324852291753\n",
      "-0.6957852406367315 0.1611212717184458\n",
      "-0.6953277269784104 0.158057434486189\n",
      "-0.6948007031785346 0.15457121447150815\n",
      "-0.6917068192081144 0.1350236765822276\n",
      "-0.6887511671699826 0.11778312075528802\n",
      "-0.6874382580663505 0.11056294634804917\n",
      "-0.6866715044466281 0.1064687716213853\n",
      "-0.6844188136538432 0.09495479354327546\n",
      "-0.6739442791711638 0.05102410665193244\n",
      "-0.6727773851515093 0.047059908978024347\n",
      "-0.6652171827386859 0.025575688568246904\n",
      "-0.6631520792460948 0.020921449982884317\n",
      "-0.6622901569822615 0.01912694250626523\n",
      "-0.6617180665905069 0.01798336981783097\n",
      "-0.6593349101083004 0.013621631190405197\n",
      "-0.6531431635857339 0.00521897765877584\n",
      "-0.6525174448999624 0.0045973664838485145\n",
      "-0.6484888205967232 0.0015569859086773817\n",
      "-0.6479258162110044 0.0012622859980153933\n",
      "-0.6438280483255558 5.106797153315618e-05\n",
      "-0.6421145350287583 2.1255568856977764e-05\n",
      "-0.6415082450847731 7.66488825261708e-05\n",
      "-0.6389178445267303 0.0006956644510220618\n",
      "-0.6379725785996695 0.001073938353363978\n",
      "-0.6379527402896892 0.0010827409612590556\n",
      "-0.6364502780070158 0.0018518596990942122\n",
      "-0.6344714373770466 0.0031703679109744043\n",
      "-0.6287453322405203 0.00889470001635452\n",
      "-0.6286940052977006 0.008958592201225034\n",
      "-0.6280394144575618 0.009792737832132793\n",
      "-0.6277200973328232 0.010212606665850499\n",
      "-0.627415612552287 0.010620867097582825\n",
      "-0.6272424801552983 0.01085643806165269\n",
      "-0.6264699304532122 0.011937830942327386\n",
      "-0.6263718619390497 0.01207862939826423\n",
      "-0.618422634298947 0.026087069155648943\n",
      "-0.6153093658250433 0.03294413332116032\n",
      "-0.6100851692462712 0.04614540649902589\n",
      "-0.6078406290027223 0.05246278055953142\n",
      "-0.6066055501778782 0.05610342229623562\n",
      "-0.604545635512397 0.06243445228816902\n",
      "-0.6037394527551716 0.06500025915116753\n",
      "-0.6017944330303504 0.07139430112844833\n",
      "-0.5989062581182882 0.0814200888689244\n",
      "-0.5968145184413918 0.0890779143543549\n",
      "-0.5931188516938424 0.10342462564693505\n",
      "-0.5922797607991401 0.10682785054597058\n",
      "-0.5914530351857801 0.11023390020356422\n",
      "-0.588184472874028 0.12421719684875779\n",
      "-0.5872953403163661 0.1281644849098929\n",
      "-0.5859359035684486 0.13431901595991924\n",
      "-0.5790143140501938 0.16791910492029907\n",
      "-0.577173098046589 0.1775039578806022\n",
      "-0.5765191041231907 0.18097490510149647\n",
      "-0.5752881386070603 0.18760312001716922\n",
      "-0.5746699779796673 0.1909787143969019\n",
      "-0.5674900803661798 0.2325332794481938\n",
      "-0.5668364487036504 0.23653476387024377\n",
      "-0.5647786434909048 0.2493758964723925\n",
      "-0.5646114795662982 0.2504353687724136\n",
      "-0.5636368526893165 0.25666172602209664\n",
      "-0.5635156706739048 0.2574417859511214\n",
      "-0.5630198814626606 0.26064685768562706\n",
      "-0.5618968720837092 0.26798800158510794\n",
      "-0.5604746582561781 0.2774482740049406\n",
      "-0.5604734589915665 0.2774563287304097\n",
      "-0.5594892890069421 0.28411067343916374\n",
      "-0.5574320835027928 0.2983081438500432\n",
      "-0.556727131337279 0.3032637456977752\n",
      "-0.5559483231115714 0.3087927399800869\n",
      "-0.5553155593729171 0.3133270577570157\n",
      "-0.5552210518963121 0.31400754403624176\n",
      "-0.5517135856529489 0.33986792949612527\n",
      "-0.5487463163809616 0.3626831659506652\n",
      "-0.5482188255367728 0.3668307962901925\n",
      "-0.5451185120719841 0.3917790703681211\n",
      "-0.5429474153298361 0.4098406131187936\n",
      "-0.5416670180921854 0.4207249192192375\n",
      "-0.5376694754592712 0.4558425922245338\n",
      "-0.5341983649906659 0.4877690746831675\n",
      "-0.5328401462433701 0.5006343930936883\n",
      "-0.5324273015506786 0.5045871765098606\n",
      "-0.5320467153929183 0.5082486623958143\n",
      "-0.5275771146433694 0.5525312155152399\n",
      "-0.5268492352510659 0.5599704397111616\n",
      "-0.5266842936960068 0.5616652289004272\n",
      "-0.5237625541138808 0.5922455676443196\n",
      "-0.523136816425315 0.598934149296835\n",
      "-0.5225549934258247 0.6051980094765629\n",
      "-0.5156399227388082 0.6830319363353229\n",
      "-0.5152286859737194 0.6878627237403888\n",
      "-0.5088064961592806 0.7663847158815406\n",
      "-0.5087541104570823 0.7670495857182532\n",
      "-0.5081858901519469 0.7742872767500081\n",
      "-0.5075176920551989 0.7828594762908545\n",
      "-0.5072362092534513 0.7864904250422926\n",
      "-0.5022548684568737 0.8527336444599126\n",
      "-0.49724309191039207 0.9233152660349804\n",
      "-0.49386415288059626 0.973234366714796\n",
      "-0.4908332770103576 1.0196757955816567\n",
      "-0.4896886597724268 1.0376352074510848\n",
      "-0.48841825994605736 1.057843620169181\n",
      "-0.4864499267173197 1.089735695632759\n",
      "-0.48509828089886375 1.112052178784966\n",
      "-0.47798716246581385 1.235267373509185\n",
      "-0.47640013972383866 1.2641496745904428\n",
      "-0.47501414326444547 1.2898029006164609\n",
      "-0.47338448410949496 1.320487582126624\n",
      "-0.46842115544020024 1.4175277553766783\n",
      "-0.4677088254097046 1.431911307741003\n",
      "-0.4640696682671972 1.507252952855446\n",
      "-0.4628819273713862 1.5325319041709038\n",
      "-0.46075307870709614 1.5787153216697023\n",
      "-0.4601000956801953 1.5931102037923057\n",
      "-0.4598799142467562 1.5979886255779925\n",
      "-0.4586543516328454 1.6253712052438065\n",
      "-0.4575156086804133 1.6511653984895958\n",
      "-0.45720268275134424 1.658313653051073\n",
      "-0.45420177376951165 1.728205885658245\n",
      "-0.45387026657802565 1.7360784306869257\n",
      "-0.4506284086538601 1.814706276023717\n",
      "-0.44597614170289357 1.9329628220423474\n",
      "-0.445927366131305 1.934237888478609\n",
      "-0.44546994666963413 1.946232062393676\n",
      "-0.44457051292816474 1.9700105271812967\n",
      "-0.4425698484161493 2.0238418487327765\n",
      "-0.44158802717927004 2.0507437035697076\n",
      "-0.4378751241504053 2.155469822440791\n",
      "-0.43781405769279447 2.1572329185690675\n",
      "-0.43409659638002385 2.267152041089097\n",
      "-0.4320527578071214 2.329834043984122\n",
      "-0.4309483052854275 2.364399259810747\n",
      "-0.4303926483814069 2.381977478484739\n",
      "-0.4293802793682715 2.414333415186353\n",
      "-0.4281083174972491 2.4556008494383907\n",
      "-0.4265963165650084 2.5055696620344508\n",
      "-0.42405842755668965 2.591756062705796\n",
      "-0.42258063012252367 2.6433309340533704\n",
      "-0.4218834730046279 2.6680287490008423\n",
      "-0.41447599816508296 2.9460941144212436\n",
      "-0.4111859279774295 3.0796492106244955\n",
      "-0.40881815766723917 3.180033168148034\n",
      "-0.40498198336225477 3.3509391329294904\n",
      "-0.4042264302507641 3.3858873023581104\n",
      "-0.4039990470399246 3.396491858781821\n",
      "-0.40300449326680465 3.443357440824359\n",
      "-0.40142322003152575 3.519534045694932\n",
      "-0.4009803669391929 3.5412459370253373\n",
      "-0.3987532163600276 3.6530601120555843\n",
      "-0.3935178888633313 3.934754100334473\n",
      "-0.392672132981309 3.9829961965648666\n",
      "-0.39168536306781343 4.040329593659016\n",
      "-0.3911045637477306 4.074619364838942\n",
      "-0.390812729408456 4.092004864393874\n",
      "-0.38695280836607027 4.332414934303315\n",
      "-0.37788855104032715 4.990369029667964\n",
      "-0.3775599084890875 5.017255324053604\n",
      "-0.3737838966463589 5.345320703844731\n",
      "-0.3695826300457081 5.759493839935885\n",
      "-0.36833779108207865 5.894354227764473\n",
      "-0.3633806112358118 6.502885342282357\n",
      "-0.3613997520597134 6.786381059007433\n",
      "-0.3603216160793523 6.953037541928173\n",
      "-0.3592215607104656 7.13345383806893\n",
      "-0.3536968840806478 8.25937987306142\n",
      "-0.34961986085815133 9.504987405021756\n",
      "-0.3463469237460557 11.135605845722305\n",
      "-0.3447431506493912 12.474458710149976\n",
      "-0.3441482837436842 13.186673017094954\n",
      "-0.34349478819800594 14.24622854454702\n",
      "-0.3385145116512762 11.756233665757003\n",
      "-0.3384266379112326 11.684954021015294\n",
      "-0.33708204849987067 10.770216930730768\n",
      "-0.3369386750907646 10.687891294562107\n",
      "-0.33662271191416937 10.51440145351341\n",
      "-0.3344983560334225 9.560631575523201\n",
      "-0.3338948412729934 9.339039843272328\n",
      "-0.333663321362452 9.258397190957126\n",
      "-0.329770536563172 8.162194342180536\n",
      "-0.3259641090093597 7.388661232030878\n",
      "-0.3222268586994175 6.792120747775825\n",
      "-0.32147990822528616 6.686704042575233\n",
      "-0.31525668760893355 5.935512956758782\n",
      "-0.3063819934528411 5.127533605555235\n",
      "-0.3063259458859686 5.123117275096936\n",
      "-0.30419692283966016 4.960449738100935\n",
      "-0.30317087733357373 4.885399934068709\n",
      "-0.3018606791166387 4.792490846124613\n",
      "-0.30148717514448764 4.766578816267149\n",
      "-0.30110911067632085 4.740600996006331\n",
      "-0.2981000598585959 4.542308348175664\n",
      "-0.2955033303391843 4.382219672084914\n",
      "-0.2945662464547889 4.326723679430936\n",
      "-0.2943317244349821 4.313013615005469\n",
      "-0.2904935577576906 4.098137676777759\n",
      "-0.2824059235891967 3.6956715661554904\n",
      "-0.28166409063512754 3.6616814270771125\n",
      "-0.2697172623999189 3.1683951009070057\n",
      "-0.2675630015335777 3.0888102076565773\n",
      "-0.2666551252274023 3.0560129289985314\n",
      "-0.26420231159232865 2.969502438643957\n",
      "-0.2631813715677094 2.934363595856836\n",
      "-0.2590003138694845 2.79546657996625\n",
      "-0.2578029788213363 2.757098842491229\n",
      "-0.2577269198994818 2.754681876852246\n",
      "-0.25739187411494835 2.7440634416566514\n",
      "-0.2554122338486642 2.6822547110240764\n",
      "-0.25492856233464556 2.667390444094457\n",
      "-0.2533071214759133 2.618218826115725\n",
      "-0.25221476225083483 2.5856510117809157\n",
      "-0.2521160952711565 2.5827310453816184\n",
      "-0.24664759657196988 2.4262643772533266\n",
      "-0.24502045489197255 2.3816464407810867\n",
      "-0.23896757886590936 2.2228287800545283\n",
      "-0.23757752823333922 2.1878609898664645\n",
      "-0.23373409093749853 2.0939085752144737\n",
      "-0.232167302511344 2.0567153970262293\n",
      "-0.23070917765018106 2.0226555649687397\n",
      "-0.23055466918423528 2.0190771940306607\n",
      "-0.22506087714622036 1.8955271008575336\n",
      "-0.2182848443713279 1.7524006894524609\n",
      "-0.2151771905640909 1.6899282463096463\n",
      "-0.21414718248165454 1.6696396571521932\n",
      "-0.20435235202494195 1.4864706076318306\n",
      "-0.1947935243975425 1.3233773160006388\n",
      "-0.19234323798513064 1.2838570181366222\n",
      "-0.1923093625724268 1.2833169014866863\n",
      "-0.19160934125795626 1.2721933727513328\n",
      "-0.1902605826732664 1.2509628520782228\n",
      "-0.17382611904963263 1.0123422137919764\n",
      "-0.17323514358677206 1.0044135714395326\n",
      "-0.16832487398277607 0.9401757372438373\n",
      "-0.16472278358954173 0.8948663769781041\n",
      "-0.16459988744472143 0.8933469713093496\n",
      "-0.16441810726881512 0.8911027308752573\n",
      "-0.16404946193716974 0.8865630573799362\n",
      "-0.16070412247455756 0.8460692354001869\n",
      "-0.16068599526325333 0.845853226870656\n",
      "-0.16002845830809398 0.8380424628023702\n",
      "-0.15899954531334592 0.8259159193662394\n",
      "-0.15882027511338426 0.8238149732024225\n",
      "-0.15751995629946358 0.8086809510785641\n",
      "-0.15687040193639668 0.8011897305234104\n",
      "-0.15669340959696765 0.7991564015506707\n",
      "-0.15627627284437717 0.7943775791086931\n",
      "-0.1515414074764152 0.7414301397922214\n",
      "-0.14744875848973837 0.6975425654340138\n",
      "-0.1458101248269026 0.6804471755060603\n",
      "-0.14402383110809236 0.6621164625781965\n",
      "-0.14400815491040042 0.6619569937486922\n",
      "-0.14342853368985575 0.6560776864748508\n",
      "-0.14228947548105797 0.6446198347645276\n",
      "-0.14066928913572663 0.6285399775109662\n",
      "-0.14031328089926554 0.6250407316994595\n",
      "-0.13770017859005734 0.5997278236152631\n",
      "-0.13748694770237635 0.5976909411051644\n",
      "-0.1370917355104877 0.5939270475799174\n",
      "-0.13493917429631064 0.5736842466131212\n",
      "-0.13464437290986675 0.5709456080677111\n",
      "-0.13334577664471992 0.5589778135050038\n",
      "-0.13230253506304424 0.5494759407204453\n",
      "-0.1322690707601959 0.5491727993274489\n",
      "-0.12996243577212407 0.5285236496215138\n",
      "-0.12979087939731415 0.5270071317315963\n",
      "-0.12807103721540614 0.5119503070449642\n",
      "-0.1265563455365002 0.49890820425744253\n",
      "-0.12435875867259671 0.4803463560656056\n",
      "-0.12225588120702335 0.4629793284854724\n",
      "-0.11608425206435635 0.41419539640159286\n",
      "-0.11522608676777257 0.4076657969960914\n",
      "-0.11515207506195702 0.40710552204867445\n",
      "-0.11419605398391264 0.39990910596693074\n",
      "-0.11339495848664805 0.39393693578623556\n",
      "-0.11273755940725283 0.3890753903738322\n",
      "-0.11125758355540771 0.37825998154493357\n",
      "-0.10961108335696079 0.36643645748262915\n",
      "-0.10959828181151754 0.36634538637350994\n",
      "-0.10818220683256796 0.3563525024518876\n",
      "-0.10696639396010266 0.34790059355051106\n",
      "-0.10495785991882167 0.33419447271733915\n",
      "-0.10335528611011724 0.3234859024236075\n",
      "-0.10040264475091898 0.3042783932677299\n",
      "-0.10025882702975353 0.30335998828156163\n",
      "-0.09808491443367262 0.2896700160841939\n",
      "-0.09713743555236776 0.2838156961271181\n",
      "-0.09642270762372651 0.27944434105813176\n",
      "-0.0961724566526132 0.2779228630337643\n",
      "-0.09479449995707223 0.2696292289307203\n",
      "-0.091694157610126 0.2514852415479754\n",
      "-0.08637959210380663 0.22202029522193387\n",
      "-0.08592162377883228 0.21957666879339763\n",
      "-0.08490309280180108 0.21419571279234745\n",
      "-0.08440608632555824 0.21159682479232597\n",
      "-0.0762815176061209 0.17157150622321146\n",
      "-0.07069188534107429 0.1466699250446\n",
      "-0.07054238735856933 0.14603283803527287\n",
      "-0.06726805826435944 0.13245249605434814\n",
      "-0.0648643637417099 0.12293400623678798\n",
      "-0.06455385391741952 0.1217320292331845\n",
      "-0.06337602373754736 0.11722984359330556\n",
      "-0.06210545336192941 0.11247426233207981\n",
      "-0.06208317386674422 0.11239180622350818\n",
      "-0.05624019309530026 0.09186758523929046\n",
      "-0.05519587938854276 0.08842837069327045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.05391121245331343 0.08429198796341066\n",
      "-0.05116035452277945 0.07578293577457994\n",
      "-0.04994750389353486 0.07218121551212613\n",
      "-0.047126382974370884 0.06415583985005885\n",
      "-0.046510482267415165 0.06246897693064006\n",
      "-0.045224290722228 0.059021297802406635\n",
      "-0.04302562861680004 0.05336170018208458\n",
      "-0.04279176091843295 0.052777010177149704\n",
      "-0.042151974953842375 0.05119444792253114\n",
      "-0.0419389393921108 0.05067299418835479\n",
      "-0.037184159104619585 0.03974693693077096\n",
      "-0.03599721971014902 0.03723101295809522\n",
      "-0.034571804105439696 0.03432061510526045\n",
      "-0.032943207737284874 0.03114315714139796\n",
      "-0.02942909211734679 0.02482133992077939\n",
      "-0.02763593275015186 0.021875507639314232\n",
      "-0.0229133855453163 0.015016784374369168\n",
      "-0.02193911643328339 0.013763363491504177\n",
      "-0.020419263130643017 0.011917939772911697\n",
      "-0.01960321250414343 0.010982258618801446\n",
      "-0.018245328503861957 0.009510612438407\n",
      "-0.01776810438720222 0.009018683061269081\n",
      "-0.0156854533165125 0.007025478286782084\n",
      "-0.013339936301562716 0.005079418947050514\n",
      "-0.012433484066946665 0.004411964873108684\n",
      "-0.012078548019103907 0.004163450579494197\n",
      "-0.010949848546780494 0.0034211586919105853\n",
      "-0.009839345247192766 0.002762039623793556\n",
      "-0.008385491644375742 0.0020057937399096735\n",
      "-0.006302925707973639 0.0011330125374496849\n",
      "-0.0033408157747367717 0.0003182597496368227\n",
      "0.0026206227164513596 0.00019582790980456004\n",
      "0.003951404380300838 0.000445236849573699\n",
      "0.005937108257771939 0.0010052839017599647\n",
      "0.0071492633271668105 0.0014578145640921755\n",
      "0.011838489064855517 0.0039994633293697155\n",
      "0.01251225130836775 0.004468094392859734\n",
      "0.01264586540763979 0.004564121422331663\n",
      "0.013070954346615427 0.00487643934118596\n",
      "0.019658682287173823 0.011044640220268026\n",
      "0.02048258482147358 0.011992154907082027\n",
      "0.022177063362815774 0.01406440369511512\n",
      "0.023626132196444827 0.01596866978438043\n",
      "0.024723830779374234 0.01749246604787067\n",
      "0.02812912237378673 0.02266692422582319\n",
      "0.028619075071516065 0.023467266573199044\n",
      "0.029868035058375986 0.025571210913701917\n",
      "0.030215050366033447 0.026172058808321186\n",
      "0.03305273118623919 0.031351909316757756\n",
      "0.034282904413683646 0.03374547467036253\n",
      "0.035120380798741024 0.03542637569387634\n",
      "0.03564194767348283 0.03649430143290084\n",
      "0.03679461533545836 0.03891195388580108\n",
      "0.038912884218214705 0.043562077755897315\n",
      "0.04069484169380133 0.047682620341594054\n",
      "0.041292909168272995 0.0491084848827851\n",
      "0.041945496830263496 0.05068900402457782\n",
      "0.04684589677512241 0.06338473939549585\n",
      "0.04800957798645644 0.06661549333269745\n",
      "0.04938974434439958 0.07055552118216392\n",
      "0.05437770803453579 0.08578200051061538\n",
      "0.05716083138810757 0.09495678162503753\n",
      "0.058281322536268165 0.09878926085669265\n",
      "0.05925222857764423 0.10217486248987774\n",
      "0.06147863163038125 0.11016665115490032\n",
      "0.06170364888705193 0.11099211952103825\n",
      "0.062274990890096804 0.11310277052865059\n",
      "0.06557350850480326 0.12570273888094666\n",
      "0.06660102768290121 0.12977302404010574\n",
      "0.06800992237855419 0.13546706718279142\n",
      "0.06865974630178018 0.13813755712481618\n",
      "0.07311273888150538 0.15719523209624\n",
      "0.0763844399172866 0.17204993153159118\n",
      "0.07695880233917807 0.174733142292527\n",
      "0.07936906712760661 0.18624076448850824\n",
      "0.07983893452115942 0.18853095061795414\n",
      "0.08350106664325896 0.20690942356721714\n",
      "0.0842408470545104 0.21073666208401573\n",
      "0.08462353802260414 0.21273173709581802\n",
      "0.08496020591604303 0.2144954874037581\n",
      "0.08500396841882374 0.214725344618892\n",
      "0.08666613541543122 0.22355687366268834\n",
      "0.08809208945352642 0.23129126205611122\n",
      "0.08918549006692578 0.23732130943521376\n",
      "0.08959761258644039 0.23961664255647278\n",
      "0.08965322428449007 0.23992731970508163\n",
      "0.08974605558244719 0.24044642711421388\n",
      "0.09078341954327152 0.24629004429446197\n",
      "0.0911325850697875 0.24827463124678803\n",
      "0.09281880141297094 0.25798475946040966\n",
      "0.09418216751294484 0.26598923219274795\n",
      "0.09431116818382645 0.2667537528619966\n",
      "0.0969940095855506 0.28293540035880205\n",
      "0.09889986094758396 0.2947598655916852\n",
      "0.09911624365050065 0.2961197948445661\n",
      "0.10049076300725157 0.3048418908867959\n",
      "0.10549993989380901 0.3378622563356632\n",
      "0.10678895950019474 0.3466769536172976\n",
      "0.11262711736149078 0.38826213009186916\n",
      "0.11267333991144768 0.388602376952146\n",
      "0.11547178366838495 0.40952900137622783\n",
      "0.11593365694520408 0.41304511917173165\n",
      "0.11643624737087288 0.41689138293654615\n",
      "0.12144177731789552 0.4563585625464889\n",
      "0.12176139941124142 0.4589511158520638\n",
      "0.1229859957838455 0.4689656619504625\n",
      "0.12510714460164563 0.48661995725753143\n",
      "0.1304387611150042 0.5327481865515638\n",
      "0.1306541369643055 0.5346650961696962\n",
      "0.13079795700320251 0.5359474779964233\n",
      "0.13151299036010888 0.5423510065652053\n",
      "0.1329086561356425 0.5549843416273026\n",
      "0.1334544583798014 0.5599734449678545\n",
      "0.1337764993512034 0.5629300525540104\n",
      "0.13635038814752853 0.5869063517031461\n",
      "0.13779467167575432 0.6006318421676312\n",
      "0.14067936567254113 0.6286391987446588\n",
      "0.14278187469920778 0.6495573299991992\n",
      "0.14448902282994136 0.6668597461160974\n",
      "0.14531273748892026 0.6753112259961691\n",
      "0.15694887463394425 0.8020923239769199\n",
      "0.16058367892061165 0.8446346826833939\n",
      "0.16213939677859335 0.8632885388825667\n",
      "0.1648420325382225 0.8963423446868045\n",
      "0.16859591043001632 0.943646333292539\n",
      "0.1692124385007201 0.9515732771223747\n",
      "0.17020885150094656 0.9644800872917714\n",
      "0.1711326493020382 0.9765524843377844\n",
      "0.1719145456260558 0.9868509245265236\n",
      "0.17662150350389805 1.0504338865365264\n",
      "0.17800209982881832 1.0696100793463346\n",
      "0.1861330188719108 1.1876065418583077\n",
      "0.1863929768961643 1.1915262135086062\n",
      "0.18685850452316677 1.1985689208756158\n",
      "0.18819696450735912 1.218986448475961\n",
      "0.19004892306111798 1.2476550732904426\n",
      "0.1901001029234708 1.2484543115748268\n",
      "0.190702137719152 1.2578842109380242\n",
      "0.19101042552891734 1.2627333488176526\n",
      "0.1928076186628862 1.2912782774235658\n",
      "0.19345414129418037 1.3016635667071101\n",
      "0.19442798344246404 1.3174244913616369\n",
      "0.19627429399566165 1.3476992690754173\n",
      "0.19709811425683532 1.361376309680021\n",
      "0.19782908112186148 1.3735999487395705\n",
      "0.19783333191028385 1.3736712766189312\n",
      "0.19839906067078616 1.3831894361111732\n",
      "0.1996788132013827 1.4049071951679253\n",
      "0.20847297510698604 1.5614372443068099\n",
      "0.20857934372706532 1.5634116811439882\n",
      "0.20992736323788397 1.588609170968679\n",
      "0.2124592721011478 1.6368295809850768\n",
      "0.2126616481588297 1.640735071989899\n",
      "0.21865818354521438 1.7600358177531235\n",
      "0.2191550826226163 1.7702419222715535\n",
      "0.2196173302007729 1.7797817650286465\n",
      "0.2204122873314558 1.7962914062019408\n",
      "0.22134491507973086 1.8158285814156343\n",
      "0.22168206994928497 1.822936675213218\n",
      "0.22576938150140213 1.9110685401623273\n",
      "0.22707795233624628 1.9400724911627674\n",
      "0.22841562921557124 1.9701306211395138\n",
      "0.22851978873305834 1.9724887344164277\n",
      "0.22879561799643744 1.9787457558529444\n",
      "0.23366042800440812 2.092145888461686\n",
      "0.23380901738829651 2.095702931242435\n",
      "0.23497744885085692 2.12387339303028\n",
      "0.23821149161488497 2.2037419805574787\n",
      "0.24243001375079087 2.312333217579675\n",
      "0.24483830774152748 2.376704450374554\n",
      "0.24727755296817278 2.443769402097315\n",
      "0.24845348270102963 2.4767986778293114\n",
      "0.25325806697345543 2.6167467725131877\n",
      "0.25630080808982125 2.7098030979044188\n",
      "0.2605416732488801 2.8457621790449927\n",
      "0.261286588035756 2.8704440329200125\n",
      "0.26278228976297546 2.9207624521220525\n",
      "0.2647662631844032 2.989128266027264\n",
      "0.2649642707400961 2.996055990763148\n",
      "0.26586767238250464 3.0279106224311643\n",
      "0.2695064602668775 3.1604952282912766\n",
      "0.2715343369744163 3.2375366131966095\n",
      "0.272270937912406 3.2661128147689475\n",
      "0.2730873899945332 3.2981678438119832\n",
      "0.2741620008984129 3.3409846193533053\n",
      "0.27457094021168493 3.357469464950264\n",
      "0.27481946598609563 3.367540175255004\n",
      "0.27747525067118706 3.4777101252761446\n",
      "0.27932843309294286 3.5574873791677146\n",
      "0.28215910485320217 3.684313057493631\n",
      "0.28245363407904645 3.6978729272597435\n",
      "0.282858327449123 3.716620862677849\n",
      "0.2861181953210876 3.872770041579564\n",
      "0.28664108093530394 3.898708122489592\n",
      "0.29063707491009305 4.105866580155759\n",
      "0.29255472428867346 4.211363879459851\n",
      "0.2954734466821236 4.380431973945371\n",
      "0.2965429552212777 4.445168371090213\n",
      "0.2971744822172806 4.484142801014351\n",
      "0.2990114886029607 4.600843217512429\n",
      "0.2992347166736138 4.61537606606202\n",
      "0.30077938525719916 4.718146904973282\n",
      "0.30413716904941257 4.956021701133615\n",
      "0.30502205113908953 5.0223449369816535\n",
      "0.3146855489011675 5.875745147394209\n",
      "0.31560009570100744 5.972080022067387\n",
      "0.3168792237595046 6.112689877847305\n",
      "0.31850278869928994 6.30205878175313\n",
      "0.32123546234328537 6.653049411195009\n",
      "0.3281741287474875 7.811734736601667\n",
      "0.3323491892136019 8.839404678237512\n",
      "0.3343506486599541 9.504788373433286\n",
      "0.3355850862039138 10.008907455384032\n",
      "0.3356892666089035 10.055814782447435\n",
      "0.3369903394735638 10.717285514219235\n",
      "0.3389651343219675 12.15243491397265\n",
      "0.342494659614176 17.519606584308523\n",
      "0.3425232813892498 17.350564218870268\n",
      "0.34281224308513836 16.040615473857976\n",
      "0.3442738774297103 13.021013649571039\n",
      "0.3451608698107316 12.061965371440753\n",
      "0.3481915186166433 10.107992808823814\n",
      "0.3494658346449162 9.564313546632249\n",
      "0.3570661886078741 7.522849396196333\n",
      "0.35961684816671036 7.067337784983792\n",
      "0.36244476038164763 6.633413564386415\n",
      "0.3624734398741254 6.62932596859903\n",
      "0.36599783546799136 6.165962588537028\n",
      "0.36948054898930227 5.770322494056096\n",
      "0.36952149687242386 5.765973982537143\n",
      "0.37061622874926714 5.652044879206219\n",
      "0.37494987943473945 5.240040007149541\n",
      "0.37525031216324356 5.213515153578784\n",
      "0.3772553129506726 5.0423970868597845\n",
      "0.37868485139958086 4.926231419478706\n",
      "0.3940845355756015 3.9028823825742305\n",
      "0.39461686181765443 3.8732620956069295\n",
      "0.3953585057830873 3.832501676840589\n",
      "0.395529728948961 3.8231737392420895\n",
      "0.39554790636595527 3.822185260445919\n",
      "0.3959891629221355 3.7982948562229426\n",
      "0.39699060240375506 3.7448100238768562\n",
      "0.39732615012704287 3.727112585448833\n",
      "0.40065300507690926 3.557404542886796\n",
      "0.4101397551283468 3.1235451077167764\n",
      "0.4104803362650702 3.109176467881906\n",
      "0.4119321615960603 3.0487688160213455\n",
      "0.4137422318064876 2.975307935675125\n",
      "0.41382806959015195 2.971873846093307\n",
      "0.41432312701012597 2.952154082235906\n",
      "0.41600596654526534 2.886194879593756\n",
      "0.41623230462840355 2.877447619884907\n",
      "0.4170970294625522 2.8442935688995905\n",
      "0.4193262752001021 2.760708560190755\n",
      "0.4198860693421689 2.740133896928122\n",
      "0.42020653498428784 2.7284286373653375\n",
      "0.42300425187401425 2.628439362260425\n",
      "0.4259196579137834 2.5282610540532633\n",
      "0.4274894193039025 2.4759327772204185\n",
      "0.4294947914702185 2.410651984461252\n",
      "0.4296008774262636 2.4072463760412104\n",
      "0.43252012137919693 2.3153551955176277\n",
      "0.4362451178983866 2.202996354434115\n",
      "0.4419052176172449 2.0420173230690972\n",
      "0.44333667732644044 2.0030540262103345\n",
      "0.44533483905531956 1.949787449937105\n",
      "0.4537931503586565 1.737914161882596\n",
      "0.4557849936727787 1.6910268586028374\n",
      "0.4575759350829247 1.649790335718938\n",
      "0.45934650561755497 1.6098587147928447\n",
      "0.4595863363017789 1.6045126216077843\n",
      "0.46083334775894413 1.5769532946386215\n",
      "0.46565951300121466 1.4739513774788247\n",
      "0.4673709838777407 1.4387741573100015\n",
      "0.4691234529533648 1.403460935286817\n",
      "0.4698782516524307 1.3884677652520414\n",
      "0.4728932407109485 1.3298492727392663\n",
      "0.48969306340642094 1.0375656643448572\n",
      "0.49782947808124733 0.9148465421578392\n",
      "0.499234135015048 0.8947894882068718\n",
      "0.5011945462614062 0.8673303385270582\n",
      "0.505924694275836 0.803564502454691\n",
      "0.5075651443504325 0.7822485336395003\n",
      "0.5103642660856129 0.7467969439291351\n",
      "0.5103849132431446 0.7465396883470584\n",
      "0.5105668486096253 0.7442755048756067\n",
      "0.5133796535119459 0.7098721446287078\n",
      "0.5239389792915141 0.5903686996519396\n",
      "0.5287351410041683 0.5408285910364513\n",
      "0.5289449921542715 0.5387252538391866\n",
      "0.5297225083433292 0.5309782899248338\n",
      "0.5305067451463605 0.5232374612762866\n",
      "0.5313210580277254 0.5152768690020098\n",
      "0.5327119827957181 0.5018593771282844\n",
      "0.5329098678946096 0.49996879424351565\n",
      "0.5371824376703473 0.4602407808480188\n",
      "0.5409283126975386 0.42708393875122186\n",
      "0.5425060496181802 0.413572900592166\n",
      "0.5428202213770112 0.41091408157812687\n",
      "0.5461642255330741 0.3832543570132613\n",
      "0.5475431787125902 0.37218432355585035\n",
      "0.5484082771477461 0.36533793064869635\n",
      "0.5519659926470484 0.33796720668587216\n",
      "0.5588211464570838 0.288678857963198\n",
      "0.5593909383335796 0.28478053407517673\n",
      "0.5597295103426627 0.28247827103985584\n",
      "0.5602369850537705 0.27904714050254126\n",
      "0.5613400652129248 0.27166994685016516\n",
      "0.5637123038958631 0.25617669955978317\n",
      "0.5658043632318135 0.2429288295035961\n",
      "0.568603947023774 0.2257994033875916\n",
      "0.5734222426138285 0.19788866576851794\n",
      "0.5737146494627516 0.19625771222642235\n",
      "0.5743073691093799 0.19297352084205016\n",
      "0.5749281900281478 0.18956485531570122\n",
      "0.5753688565482482 0.18716467085371188\n",
      "0.5763027670259779 0.1821307755146501\n",
      "0.5764620401723608 0.1812794205552469\n",
      "0.5776967338688388 0.17475002545888127\n",
      "0.5780316072209801 0.17300054286440003\n",
      "0.5837106335706304 0.14470650308275795\n",
      "0.5840011684450592 0.1433281399396617\n",
      "0.586833816857121 0.13023772066922842\n",
      "0.5891341972538755 0.12006885043629713\n",
      "0.5896349966019632 0.11790957522768966\n",
      "0.5927669484473883 0.10484530316900584\n",
      "0.5944817847658965 0.09801197437310233\n",
      "0.5955476881634694 0.09387808899428911\n",
      "0.5959896128905247 0.09218965751329097\n",
      "0.5991500517958912 0.08054926132724402\n",
      "0.6000570444909008 0.07734923832530369\n",
      "0.6067815078106868 0.05557763352707068\n",
      "0.6075529434031137 0.05330037676938917\n",
      "0.6075931311193703 0.05318299003750771\n",
      "0.6089348193331896 0.04933489541513746\n",
      "0.6153236428943321 0.0329109557067011\n",
      "0.6160200880359814 0.03131184571957372\n",
      "0.6184630536170794 0.02600305511869127\n",
      "0.6185044299799554 0.02591718543696677\n",
      "0.6190759990773953 0.024744836294575828\n",
      "0.6192472997269358 0.02439851571983268\n",
      "0.6193574187581639 0.024177114570213407\n",
      "0.6195679833162988 0.02375643880941311\n",
      "0.622328220270536 0.018568657045695453\n",
      "0.6265921797303435 0.011763426225690467\n",
      "0.626812561312482 0.01145213887580022\n",
      "0.6350134991676617 0.0027749315981174544\n",
      "0.6385986281048479 0.000814374521545946\n",
      "0.6399183415583001 0.00038362657848637076\n",
      "0.6407556480525971 0.00019289716669685966\n",
      "0.6423305572248796 9.807875478158869e-06\n",
      "0.6450812361169733 0.0002491827992312181\n",
      "0.6476443306986648 0.0011267161874287611\n",
      "0.6513966378161065 0.003585359208787746\n",
      "0.6525741889740784 0.0046520579059692166\n",
      "0.6581904101745633 0.011753885493639355\n",
      "0.66462830901782 0.02419704162020164\n",
      "0.6685080396566192 0.03405162569532099\n",
      "0.6696031401589044 0.03716796174102168\n",
      "0.6721608537579935 0.04503725547731151\n",
      "0.6724880742477601 0.046104615972841406\n",
      "0.6726899074136137 0.04676991147233452\n",
      "0.6743350406372173 0.05239170765294506\n",
      "0.6753739209237939 0.05612642653217372\n",
      "0.6759007906834122 0.05807578348180802\n",
      "0.6772248047613891 0.0631405194864422\n",
      "0.6799455898052249 0.07430684640408257\n",
      "0.6858487364204897 0.1021749356236247\n",
      "0.6925142768816344 0.13997537863533074\n",
      "0.6925282396405272 0.1400619289350094\n",
      "0.7049018368375224 0.2296625109470454\n",
      "0.7069626571814871 0.24720503349612086\n",
      "0.7075237153397189 0.25211640347072867\n",
      "0.7077569882764958 0.2541756160776253\n",
      "0.7080236380283991 0.2565418703465377\n",
      "0.7089817563386598 0.2651539044071107\n",
      "0.7098622658458418 0.27322054743627044\n",
      "0.7156607207911847 0.33005874689177306\n",
      "0.7169699616290861 0.3438041558495179\n",
      "0.7184622158177234 0.35988956343829154\n",
      "0.7207069062959053 0.3849359177828705\n",
      "0.7226183810719835 0.40708031806046086\n",
      "0.7226224462036788 0.40712822019540057\n",
      "0.7247040306672146 0.43211131750043846\n",
      "0.7261912595260027 0.4505209860280233\n",
      "0.7271570146601551 0.4627280545924879\n",
      "0.7295400177785933 0.49370844819263365\n",
      "0.7357410503412496 0.5801756008277614\n",
      "0.7362106716373187 0.587074855683452\n",
      "0.7363645031612869 0.5893457042493527\n",
      "0.7386058136919913 0.6230451372531528\n",
      "0.7392146840283853 0.632399388902408\n",
      "0.7403348337248965 0.6498330919424783\n",
      "0.7408864730629412 0.6585260736483748\n",
      "0.7414482443601729 0.6674519152895879\n",
      "0.742222228766168 0.6798710240642192\n",
      "0.7426954928243206 0.6875344867439047\n",
      "0.7440945344642316 0.7104995513176611\n",
      "0.7441324697715823 0.7111287460107186\n",
      "0.7451194267325849 0.7276195082451711\n",
      "0.7467489110389285 0.7553594815259633\n",
      "0.7488728484653411 0.792487171263842\n",
      "0.7492398647651048 0.7990151543841878\n",
      "0.7537945587112451 0.8828286880708557\n",
      "0.7551063594450635 0.9079445443169013\n",
      "0.7556990797647427 0.9194385133770994\n",
      "0.7584419675124316 0.9738244675531313\n",
      "0.7588918141011676 0.9829338546734917\n",
      "0.7628771098948945 1.0660241293586634\n",
      "0.7630414949682123 1.0695449743850796\n",
      "0.7642861543135528 1.0964483696012857\n",
      "0.7652049306785609 1.116587710086584\n",
      "0.7667041175900975 1.1499669463071511\n",
      "0.7720503406085564 1.2743888710311138\n",
      "0.7731203359168279 1.3003365452426365\n",
      "0.7744973859337396 1.33426249996924\n",
      "0.7753767488095034 1.3562456534747223\n",
      "0.7761879372157812 1.3767481893662943\n",
      "0.7770077045410513 1.3976885532324015\n",
      "0.7780341593693263 1.4242264281394004\n",
      "0.7789547231096596 1.4483319228367793\n",
      "0.7799247081792975 1.4740492475547553\n",
      "0.7812123775064954 1.5087027055846933\n",
      "0.7885609501863153 1.7184016123583044\n",
      "0.7897869450888602 1.7555056916510476\n",
      "0.7904117231158996 1.7746639361097267\n",
      "0.7904409326355748 1.775563806483641\n",
      "0.7918558737824015 1.819610109858545\n",
      "0.7960215129687449 1.9547242067134758\n",
      "0.7962383708224972 1.9619921831160887\n",
      "0.7964918570506034 1.9705183942713032\n",
      "0.7974577123266349 2.003312507966239\n",
      "0.7990163259754932 2.057285843903567\n",
      "0.8013716316826123 2.141438455611601\n",
      "0.8017343420606513 2.154687161656672\n",
      "0.8023614735922877 2.1777831382269524\n",
      "0.8027057986479473 2.1905671187778615\n",
      "0.8065431401711987 2.3382620645404413\n",
      "0.8148903085222436 2.6981045350173263\n",
      "0.8154059232611741 2.7223659480757214\n",
      "0.8160973797604512 2.755322858829493\n",
      "0.8168100852716713 2.7898120909505697\n",
      "0.8200590988398573 2.954197863908956\n",
      "0.8270199159662139 3.3534863754191218\n",
      "0.8276228462587256 3.3916837627012297\n",
      "0.8293038004616591 3.501749921894906\n",
      "0.8293652347938587 3.5058766447188616\n",
      "0.8319621942652997 3.6876291506379895\n",
      "0.8340576027302526 3.845749958056787\n",
      "0.8363839059744493 4.035223482560703\n",
      "0.8392288284687781 4.290478098968781\n",
      "0.8438178194952126 4.772302849202502\n",
      "0.8438614665452029 4.7773938034403\n",
      "0.8486306291142391 5.411057795549757\n",
      "0.8520429651323951 5.992580371307452\n",
      "0.856233775963394 6.9591325750561825\n",
      "0.857201596931999 7.2449772123192195\n",
      "0.8587028686829035 7.7604982894179075\n",
      "0.859351377493115 8.018244881581099\n",
      "0.8656411747420587 16.15825000193857\n",
      "0.8659888074985806 22.937204549609486\n",
      "0.8669430280462134 13.625970106148985\n",
      "0.8697781424503921 9.517336504817084\n",
      "0.8700413384214754 9.317589431147166\n",
      "0.8748367786605682 6.974312177031384\n",
      "0.8783314193747447 5.955236694660734\n",
      "0.8785674381715123 5.8967086841745635\n",
      "0.8795608928403085 5.66116369596308\n",
      "0.8799310106416811 5.577530239380281\n",
      "0.8799998751274642 5.562199195977348\n",
      "0.8801604538904846 5.5267224236027905\n",
      "0.8816775869185682 5.208928765539675\n",
      "0.8835046594111335 4.8621970973661925\n",
      "0.8838950574704998 4.79248694181836\n",
      "0.8878324491417531 4.158786584896114\n",
      "0.8880488229765939 4.127116875805464\n",
      "0.8894572739221553 3.92774471169818\n",
      "0.8911570405208478 3.7014476571592962\n",
      "0.8930443065045035 3.4663581233876166\n",
      "0.8965376411599628 3.069515044907278\n",
      "0.8985535088816465 2.860022799187699\n",
      "0.8987154582526631 2.8437509268306873\n",
      "0.8993340626139186 2.7823258638421935\n",
      "0.9020989369916335 2.5211684333521838\n",
      "0.9039155217064578 2.3606370213629075\n",
      "0.9044566899064126 2.3143969194295417\n",
      "0.9055302613286971 2.224728364109934\n",
      "0.9083203788547769 2.0039055306929363\n",
      "0.9100926237744587 1.8722941572623994\n",
      "0.9103262486823254 1.8554237811327088\n",
      "0.9132115910807461 1.6558980047322676\n",
      "0.9183031940925435 1.3412800308410058\n",
      "0.9232445012730006 1.077946193227693\n",
      "0.9258162038128452 0.9560743744873579\n",
      "0.9300635619294226 0.7760713652337551\n",
      "0.9312605797660702 0.7299110730544554\n",
      "0.9314865682577889 0.7214142314785215\n",
      "0.9355108005551793 0.5813083521553027\n",
      "0.9368642376436371 0.5387814047824355\n",
      "0.9414562203411698 0.41055978010222305\n",
      "0.9417611790670963 0.40288261783495555\n",
      "0.9434720292721548 0.3616487124972864\n",
      "0.9461114260919699 0.3039118672816688\n",
      "0.9496387581010401 0.2371296989071135\n",
      "0.9579959403858966 0.12006887879259644\n",
      "0.9584281105991843 0.1153947198256853\n",
      "0.9591237053712096 0.10812666404325144\n",
      "0.959836496854304 0.10099842918826729\n",
      "0.9609315416888069 0.09065889202266351\n",
      "0.9618242863108875 0.08275983037376387\n",
      "0.9651540859887777 0.05723170506832543\n",
      "0.9665148307658236 0.04846264339061356\n",
      "0.9685842739926298 0.03681872007074643\n",
      "0.969604924448507 0.03178404404831495\n",
      "0.9696825399977858 0.03141950490200762\n",
      "0.9703130386755512 0.02855193011513684\n",
      "0.9708956723675464 0.026047766715865384\n",
      "0.9730470665381177 0.017958047968366288\n",
      "0.9730490783542025 0.017951302781695228\n",
      "0.9761660384939395 0.009206212250337431\n",
      "0.9776349628455692 0.006190148988716695\n",
      "0.9793104344567629 0.0035369407821729142\n",
      "0.9800417430011463 0.0026267191031790474\n",
      "0.9816309918637716 0.0011369926581744105\n",
      "0.9831071153519164 0.00031808695468072985\n",
      "0.9831427672942374 0.0003047130491231175\n",
      "0.9834466122701795 0.00020264102580844673\n",
      "0.9856681831621861 7.8109018143563e-05\n",
      "0.9877582318879015 0.0008880338495862145\n",
      "0.9899061899504238 0.0025620877740545286\n",
      "0.9969925500351093 0.013089983201700068\n",
      "0.9974367451796156 0.013965256868339949\n"
     ]
    }
   ],
   "source": [
    "K = 4\n",
    "T = 5\n",
    "Noise_Alloc = [0,2,4,6,8]\n",
    "sigma = 1\n",
    "\n",
    "N = 10\n",
    "\n",
    "j_array = np.array(range(K+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K+T)))\n",
    "\n",
    "i_array = np.array(range(N))\n",
    "beta_array = np.cos(i_array*2*math.pi/(N-1)/2) # np.cos(i_array*2*math.pi/N/2)\n",
    "# print(\"z_array: \",z_array,'\\n')\n",
    "\n",
    "N = 1000\n",
    "z_array = np.random.uniform(-1,1,N) #np.cos(i_array[1:]*2*math.pi/(K+T)/2) # np.cos(i_array*2*math.pi/N/2)\n",
    "z_array = np.sort(z_array)\n",
    "MIS_array = np.zeros((N))\n",
    "MIS_LCC_array = np.zeros((N))\n",
    "# print(z_array)\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "B = [0.5, 1, 1.5, 2]\n",
    "\n",
    "z_array_0 = []\n",
    "z_array_1 = []\n",
    "z_array_2 = []\n",
    "z_array_3 = []\n",
    "z_array_4 = []\n",
    "\n",
    "for j in range(len(z_array)):\n",
    "    MIS_array[j] = MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_array[j]], 1,sigma)\n",
    "    MIS_LCC_array[j] = MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_array[j]], 1,sigma, _is_LCC=True)\n",
    "    \n",
    "    if MIS_array[j] < B[0]:\n",
    "        z_array_0.append(z_array[j])\n",
    "    elif MIS_array[j] < B[1]:\n",
    "        z_array_1.append(z_array[j])\n",
    "    elif MIS_array[j] < B[2]:\n",
    "        z_array_2.append(z_array[j])\n",
    "    elif MIS_array[j] < B[3]:\n",
    "        z_array_3.append(z_array[j])\n",
    "    else:\n",
    "        z_array_4.append(z_array[j])\n",
    "#     print('(beta index, MIS) = ',j,',',MIS_array[j])\n",
    "#     print()\n",
    "\n",
    "\n",
    "\n",
    "print(len(z_array_0),len(z_array_1),len(z_array_2),len(z_array_3),len(z_array_4))\n",
    "\n",
    "\n",
    "plt.plot(z_array, MIS_array, label='Mutual Information Security, BACC')\n",
    "plt.plot(z_array, MIS_LCC_array, label='Mutual Information Security, LCC')\n",
    "plt.plot(alpha_array[Signal_Alloc],0*np.ones(len(Signal_Alloc)),'g*',label='alpha_i, for X')\n",
    "plt.plot(alpha_array[Noise_Alloc],0*np.ones(len(Noise_Alloc)),'r*',label='alpha_i, for N')\n",
    "# plt.plot(beta_array,0*np.ones(len(beta_array)),'b.',label='beta_i')\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('MIS')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "    \n",
    "for i in range(len(z_array)):\n",
    "    print(z_array[i],MIS_array[i])\n",
    "    \n",
    "# print(alpha_array[Signal_Alloc])\n",
    "# print(alpha_array[Noise_Alloc])\n",
    "# print(alpha_array)\n",
    "\n",
    "# plt.plot((2*j_array[Signal_Alloc]+1)/(K+T),alpha_array[Signal_Alloc],'g*',label='alpha_i, for X')\n",
    "# plt.plot((2*j_array[Noise_Alloc]+1)/(K+T),alpha_array[Noise_Alloc],'r*',label='alpha_i, for N')\n",
    "# plt.plot(2*i_array[1:]/(K+T), z_array,'b.',label='beta_i')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.44862362 0.49983047 0.4896348  0.48571877 0.48571877 0.4896348\n",
      " 0.49983047 0.44862362]\n"
     ]
    }
   ],
   "source": [
    "z_array_ = np.array([-0.94, -0.73, -0.534, -0.125, 0.125, 0.534, 0.73, 0.94])\n",
    "MIS_array_ = np.zeros(len(z_array_))\n",
    "for j in range(len(z_array_)):\n",
    "    MIS_array_[j] = MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_array_[j]], 1,sigma)\n",
    "\n",
    "print(MIS_array_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 !!!\n",
      "z_array: [-0.94  -0.534  0.534  0.94 ]\n",
      "0.4486236179368535\n",
      "0.48963480280841937\n",
      "0.4896348028084205\n",
      "0.4486236179368556\n",
      "@BACC_Enc: N,K,T, m_i= 4 4 5 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 4 5 15000 \n",
      "\n",
      "(T, sigma)= 5 1 )  0 -th Trial!!\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.1096 \n",
      "Accuracy: 5721/10000 (57.21%)\n",
      "\n",
      "Round   0, Average loss 2.110 Test accuracy 57.210\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.1266 \n",
      "Accuracy: 7100/10000 (71.00%)\n",
      "\n",
      "Round   1, Average loss 1.127 Test accuracy 71.000\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.5521 \n",
      "Accuracy: 8830/10000 (88.30%)\n",
      "\n",
      "Round   2, Average loss 0.552 Test accuracy 88.300\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.5766 \n",
      "Accuracy: 8792/10000 (87.92%)\n",
      "\n",
      "Round   3, Average loss 0.577 Test accuracy 87.920\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.5347 \n",
      "Accuracy: 8754/10000 (87.54%)\n",
      "\n",
      "Round   4, Average loss 0.535 Test accuracy 87.540\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.4962 \n",
      "Accuracy: 8952/10000 (89.52%)\n",
      "\n",
      "Round   5, Average loss 0.496 Test accuracy 89.520\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.5815 \n",
      "Accuracy: 8530/10000 (85.30%)\n",
      "\n",
      "Round   6, Average loss 0.581 Test accuracy 85.300\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.5218 \n",
      "Accuracy: 8721/10000 (87.21%)\n",
      "\n",
      "Round   7, Average loss 0.522 Test accuracy 87.210\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.4822 \n",
      "Accuracy: 8970/10000 (89.70%)\n",
      "\n",
      "Round   8, Average loss 0.482 Test accuracy 89.700\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.4971 \n",
      "Accuracy: 8965/10000 (89.65%)\n",
      "\n",
      "Round   9, Average loss 0.497 Test accuracy 89.650\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.4843 \n",
      "Accuracy: 8931/10000 (89.31%)\n",
      "\n",
      "Round  10, Average loss 0.484 Test accuracy 89.310\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.4700 \n",
      "Accuracy: 9055/10000 (90.55%)\n",
      "\n",
      "Round  11, Average loss 0.470 Test accuracy 90.550\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.5940 \n",
      "Accuracy: 8268/10000 (82.68%)\n",
      "\n",
      "Round  12, Average loss 0.594 Test accuracy 82.680\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.5906 \n",
      "Accuracy: 8421/10000 (84.21%)\n",
      "\n",
      "Round  13, Average loss 0.591 Test accuracy 84.210\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.4872 \n",
      "Accuracy: 8947/10000 (89.47%)\n",
      "\n",
      "Round  14, Average loss 0.487 Test accuracy 89.470\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.4743 \n",
      "Accuracy: 9090/10000 (90.90%)\n",
      "\n",
      "Round  15, Average loss 0.474 Test accuracy 90.900\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.4518 \n",
      "Accuracy: 9072/10000 (90.72%)\n",
      "\n",
      "Round  16, Average loss 0.452 Test accuracy 90.720\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.6107 \n",
      "Accuracy: 8392/10000 (83.92%)\n",
      "\n",
      "Round  17, Average loss 0.611 Test accuracy 83.920\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.4972 \n",
      "Accuracy: 9029/10000 (90.29%)\n",
      "\n",
      "Round  18, Average loss 0.497 Test accuracy 90.290\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.5020 \n",
      "Accuracy: 8896/10000 (88.96%)\n",
      "\n",
      "Round  19, Average loss 0.502 Test accuracy 88.960\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.4898 \n",
      "Accuracy: 8988/10000 (89.88%)\n",
      "\n",
      "Round  20, Average loss 0.490 Test accuracy 89.880\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.5119 \n",
      "Accuracy: 8797/10000 (87.97%)\n",
      "\n",
      "Round  21, Average loss 0.512 Test accuracy 87.970\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.5865 \n",
      "Accuracy: 8486/10000 (84.86%)\n",
      "\n",
      "Round  22, Average loss 0.586 Test accuracy 84.860\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.4493 \n",
      "Accuracy: 9085/10000 (90.85%)\n",
      "\n",
      "Round  23, Average loss 0.449 Test accuracy 90.850\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.5316 \n",
      "Accuracy: 8671/10000 (86.71%)\n",
      "\n",
      "Round  24, Average loss 0.532 Test accuracy 86.710\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.4957 \n",
      "Accuracy: 8876/10000 (88.76%)\n",
      "\n",
      "Round  25, Average loss 0.496 Test accuracy 88.760\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.4819 \n",
      "Accuracy: 9052/10000 (90.52%)\n",
      "\n",
      "Round  26, Average loss 0.482 Test accuracy 90.520\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.5505 \n",
      "Accuracy: 8454/10000 (84.54%)\n",
      "\n",
      "Round  27, Average loss 0.551 Test accuracy 84.540\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.4558 \n",
      "Accuracy: 9063/10000 (90.63%)\n",
      "\n",
      "Round  28, Average loss 0.456 Test accuracy 90.630\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.5085 \n",
      "Accuracy: 8865/10000 (88.65%)\n",
      "\n",
      "Round  29, Average loss 0.508 Test accuracy 88.650\n",
      "1 !!!\n",
      "z_array: [-0.94 -0.73  0.73  0.94]\n",
      "0.4486236179368535\n",
      "0.49983046821646326\n",
      "0.4998304682164621\n",
      "0.4486236179368556\n",
      "@BACC_Enc: N,K,T, m_i= 4 4 5 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 4 5 15000 \n",
      "\n",
      "(T, sigma)= 5 1 )  0 -th Trial!!\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 14.9017 \n",
      "Accuracy: 1010/10000 (10.10%)\n",
      "\n",
      "Round   0, Average loss 14.902 Test accuracy 10.100\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 6.1003 \n",
      "Accuracy: 913/10000 (9.13%)\n",
      "\n",
      "Round   1, Average loss 6.100 Test accuracy 9.130\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 3.1299 \n",
      "Accuracy: 2403/10000 (24.03%)\n",
      "\n",
      "Round   2, Average loss 3.130 Test accuracy 24.030\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.1211 \n",
      "Accuracy: 5713/10000 (57.13%)\n",
      "\n",
      "Round   3, Average loss 2.121 Test accuracy 57.130\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6793 \n",
      "Accuracy: 6092/10000 (60.92%)\n",
      "\n",
      "Round   4, Average loss 1.679 Test accuracy 60.920\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8428 \n",
      "Accuracy: 4490/10000 (44.90%)\n",
      "\n",
      "Round   5, Average loss 1.843 Test accuracy 44.900\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.4765 \n",
      "Accuracy: 5939/10000 (59.39%)\n",
      "\n",
      "Round   6, Average loss 1.477 Test accuracy 59.390\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.5836 \n",
      "Accuracy: 3956/10000 (39.56%)\n",
      "\n",
      "Round   7, Average loss 1.584 Test accuracy 39.560\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.9048 \n",
      "Accuracy: 6955/10000 (69.55%)\n",
      "\n",
      "Round   8, Average loss 0.905 Test accuracy 69.550\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.2221 \n",
      "Accuracy: 5199/10000 (51.99%)\n",
      "\n",
      "Round   9, Average loss 2.222 Test accuracy 51.990\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.3952 \n",
      "Accuracy: 3415/10000 (34.15%)\n",
      "\n",
      "Round  10, Average loss 2.395 Test accuracy 34.150\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.0757 \n",
      "Accuracy: 5288/10000 (52.88%)\n",
      "\n",
      "Round  11, Average loss 2.076 Test accuracy 52.880\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.0978 \n",
      "Accuracy: 6371/10000 (63.71%)\n",
      "\n",
      "Round  12, Average loss 1.098 Test accuracy 63.710\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.9247 \n",
      "Accuracy: 6976/10000 (69.76%)\n",
      "\n",
      "Round  13, Average loss 0.925 Test accuracy 69.760\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8458 \n",
      "Accuracy: 5300/10000 (53.00%)\n",
      "\n",
      "Round  14, Average loss 1.846 Test accuracy 53.000\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.2624 \n",
      "Accuracy: 6462/10000 (64.62%)\n",
      "\n",
      "Round  15, Average loss 1.262 Test accuracy 64.620\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.8284 \n",
      "Accuracy: 7170/10000 (71.70%)\n",
      "\n",
      "Round  16, Average loss 0.828 Test accuracy 71.700\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.3174 \n",
      "Accuracy: 5706/10000 (57.06%)\n",
      "\n",
      "Round  17, Average loss 1.317 Test accuracy 57.060\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6789 \n",
      "Accuracy: 5641/10000 (56.41%)\n",
      "\n",
      "Round  18, Average loss 1.679 Test accuracy 56.410\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.3697 \n",
      "Accuracy: 5732/10000 (57.32%)\n",
      "\n",
      "Round  19, Average loss 1.370 Test accuracy 57.320\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8367 \n",
      "Accuracy: 5033/10000 (50.33%)\n",
      "\n",
      "Round  20, Average loss 1.837 Test accuracy 50.330\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.0057 \n",
      "Accuracy: 6839/10000 (68.39%)\n",
      "\n",
      "Round  21, Average loss 1.006 Test accuracy 68.390\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.9745 \n",
      "Accuracy: 7327/10000 (73.27%)\n",
      "\n",
      "Round  22, Average loss 0.974 Test accuracy 73.270\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.2776 \n",
      "Accuracy: 6115/10000 (61.15%)\n",
      "\n",
      "Round  23, Average loss 1.278 Test accuracy 61.150\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.8756 \n",
      "Accuracy: 7395/10000 (73.95%)\n",
      "\n",
      "Round  24, Average loss 0.876 Test accuracy 73.950\n",
      "selected users: [0 1 2 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.9437 \n",
      "Accuracy: 4549/10000 (45.49%)\n",
      "\n",
      "Round  25, Average loss 1.944 Test accuracy 45.490\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9621 \n",
      "Accuracy: 4538/10000 (45.38%)\n",
      "\n",
      "Round  26, Average loss 1.962 Test accuracy 45.380\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.5629 \n",
      "Accuracy: 5165/10000 (51.65%)\n",
      "\n",
      "Round  27, Average loss 1.563 Test accuracy 51.650\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.5378 \n",
      "Accuracy: 4532/10000 (45.32%)\n",
      "\n",
      "Round  28, Average loss 1.538 Test accuracy 45.320\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.7706 \n",
      "Accuracy: 3791/10000 (37.91%)\n",
      "\n",
      "Round  29, Average loss 1.771 Test accuracy 37.910\n",
      "2 !!!\n",
      "z_array: [-0.94  -0.125  0.125  0.94 ]\n",
      "0.4486236179368535\n",
      "0.4857187736123886\n",
      "0.48571877361238697\n",
      "0.4486236179368556\n",
      "@BACC_Enc: N,K,T, m_i= 4 4 5 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 4 5 15000 \n",
      "\n",
      "(T, sigma)= 5 1 )  0 -th Trial!!\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1009/10000 (10.09%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 10.090\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.2424 \n",
      "Accuracy: 5070/10000 (50.70%)\n",
      "\n",
      "Round   1, Average loss 2.242 Test accuracy 50.700\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.4146 \n",
      "Accuracy: 7038/10000 (70.38%)\n",
      "\n",
      "Round   2, Average loss 1.415 Test accuracy 70.380\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.3003 \n",
      "Accuracy: 9427/10000 (94.27%)\n",
      "\n",
      "Round   3, Average loss 0.300 Test accuracy 94.270\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.3013 \n",
      "Accuracy: 9451/10000 (94.51%)\n",
      "\n",
      "Round   4, Average loss 0.301 Test accuracy 94.510\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.2650 \n",
      "Accuracy: 9476/10000 (94.76%)\n",
      "\n",
      "Round   5, Average loss 0.265 Test accuracy 94.760\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.3133 \n",
      "Accuracy: 9405/10000 (94.05%)\n",
      "\n",
      "Round   6, Average loss 0.313 Test accuracy 94.050\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.2627 \n",
      "Accuracy: 9508/10000 (95.08%)\n",
      "\n",
      "Round   7, Average loss 0.263 Test accuracy 95.080\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.3314 \n",
      "Accuracy: 9370/10000 (93.70%)\n",
      "\n",
      "Round   8, Average loss 0.331 Test accuracy 93.700\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.2940 \n",
      "Accuracy: 9458/10000 (94.58%)\n",
      "\n",
      "Round   9, Average loss 0.294 Test accuracy 94.580\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.2614 \n",
      "Accuracy: 9506/10000 (95.06%)\n",
      "\n",
      "Round  10, Average loss 0.261 Test accuracy 95.060\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.3157 \n",
      "Accuracy: 9466/10000 (94.66%)\n",
      "\n",
      "Round  11, Average loss 0.316 Test accuracy 94.660\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.2752 \n",
      "Accuracy: 9434/10000 (94.34%)\n",
      "\n",
      "Round  12, Average loss 0.275 Test accuracy 94.340\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.2416 \n",
      "Accuracy: 9519/10000 (95.19%)\n",
      "\n",
      "Round  13, Average loss 0.242 Test accuracy 95.190\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.2472 \n",
      "Accuracy: 9494/10000 (94.94%)\n",
      "\n",
      "Round  14, Average loss 0.247 Test accuracy 94.940\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.2721 \n",
      "Accuracy: 9484/10000 (94.84%)\n",
      "\n",
      "Round  15, Average loss 0.272 Test accuracy 94.840\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.2630 \n",
      "Accuracy: 9476/10000 (94.76%)\n",
      "\n",
      "Round  16, Average loss 0.263 Test accuracy 94.760\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.2521 \n",
      "Accuracy: 9499/10000 (94.99%)\n",
      "\n",
      "Round  17, Average loss 0.252 Test accuracy 94.990\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.2482 \n",
      "Accuracy: 9498/10000 (94.98%)\n",
      "\n",
      "Round  18, Average loss 0.248 Test accuracy 94.980\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.2595 \n",
      "Accuracy: 9466/10000 (94.66%)\n",
      "\n",
      "Round  19, Average loss 0.259 Test accuracy 94.660\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.3803 \n",
      "Accuracy: 9427/10000 (94.27%)\n",
      "\n",
      "Round  20, Average loss 0.380 Test accuracy 94.270\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.3021 \n",
      "Accuracy: 9477/10000 (94.77%)\n",
      "\n",
      "Round  21, Average loss 0.302 Test accuracy 94.770\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.2993 \n",
      "Accuracy: 9461/10000 (94.61%)\n",
      "\n",
      "Round  22, Average loss 0.299 Test accuracy 94.610\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.2820 \n",
      "Accuracy: 9473/10000 (94.73%)\n",
      "\n",
      "Round  23, Average loss 0.282 Test accuracy 94.730\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.2762 \n",
      "Accuracy: 9470/10000 (94.70%)\n",
      "\n",
      "Round  24, Average loss 0.276 Test accuracy 94.700\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.2685 \n",
      "Accuracy: 9438/10000 (94.38%)\n",
      "\n",
      "Round  25, Average loss 0.269 Test accuracy 94.380\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.2634 \n",
      "Accuracy: 9491/10000 (94.91%)\n",
      "\n",
      "Round  26, Average loss 0.263 Test accuracy 94.910\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.2612 \n",
      "Accuracy: 9494/10000 (94.94%)\n",
      "\n",
      "Round  27, Average loss 0.261 Test accuracy 94.940\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.2629 \n",
      "Accuracy: 9461/10000 (94.61%)\n",
      "\n",
      "Round  28, Average loss 0.263 Test accuracy 94.610\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 0.2946 \n",
      "Accuracy: 9421/10000 (94.21%)\n",
      "\n",
      "Round  29, Average loss 0.295 Test accuracy 94.210\n",
      "3 !!!\n",
      "z_array: [-0.94  -0.73  -0.534 -0.125  0.125  0.534  0.73   0.94 ]\n",
      "0.4486236179368535\n",
      "0.49983046821646326\n",
      "0.48963480280841937\n",
      "0.4857187736123886\n",
      "0.48571877361238697\n",
      "0.4896348028084205\n",
      "0.4998304682164621\n",
      "0.4486236179368556\n",
      "@BACC_Enc: N,K,T, m_i= 8 4 5 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 8 4 5 15000 \n",
      "\n",
      "(T, sigma)= 5 1 )  0 -th Trial!!\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 2.3025 \n",
      "Accuracy: 1570/10000 (15.70%)\n",
      "\n",
      "Round   0, Average loss 2.302 Test accuracy 15.700\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 2.3023 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   1, Average loss 2.302 Test accuracy 9.800\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 2.2951 \n",
      "Accuracy: 3325/10000 (33.25%)\n",
      "\n",
      "Round   2, Average loss 2.295 Test accuracy 33.250\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 2.2834 \n",
      "Accuracy: 4603/10000 (46.03%)\n",
      "\n",
      "Round   3, Average loss 2.283 Test accuracy 46.030\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 2.2469 \n",
      "Accuracy: 6849/10000 (68.49%)\n",
      "\n",
      "Round   4, Average loss 2.247 Test accuracy 68.490\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 2.2372 \n",
      "Accuracy: 6985/10000 (69.85%)\n",
      "\n",
      "Round   5, Average loss 2.237 Test accuracy 69.850\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 2.1586 \n",
      "Accuracy: 8297/10000 (82.97%)\n",
      "\n",
      "Round   6, Average loss 2.159 Test accuracy 82.970\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 2.0744 \n",
      "Accuracy: 8772/10000 (87.72%)\n",
      "\n",
      "Round   7, Average loss 2.074 Test accuracy 87.720\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 2.0148 \n",
      "Accuracy: 8916/10000 (89.16%)\n",
      "\n",
      "Round   8, Average loss 2.015 Test accuracy 89.160\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.8957 \n",
      "Accuracy: 8740/10000 (87.40%)\n",
      "\n",
      "Round   9, Average loss 1.896 Test accuracy 87.400\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.8660 \n",
      "Accuracy: 9333/10000 (93.33%)\n",
      "\n",
      "Round  10, Average loss 1.866 Test accuracy 93.330\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.9060 \n",
      "Accuracy: 9321/10000 (93.21%)\n",
      "\n",
      "Round  11, Average loss 1.906 Test accuracy 93.210\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.8647 \n",
      "Accuracy: 9156/10000 (91.56%)\n",
      "\n",
      "Round  12, Average loss 1.865 Test accuracy 91.560\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.9895 \n",
      "Accuracy: 8688/10000 (86.88%)\n",
      "\n",
      "Round  13, Average loss 1.989 Test accuracy 86.880\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.9847 \n",
      "Accuracy: 8912/10000 (89.12%)\n",
      "\n",
      "Round  14, Average loss 1.985 Test accuracy 89.120\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.8959 \n",
      "Accuracy: 9271/10000 (92.71%)\n",
      "\n",
      "Round  15, Average loss 1.896 Test accuracy 92.710\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.8891 \n",
      "Accuracy: 9292/10000 (92.92%)\n",
      "\n",
      "Round  16, Average loss 1.889 Test accuracy 92.920\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.8449 \n",
      "Accuracy: 9199/10000 (91.99%)\n",
      "\n",
      "Round  17, Average loss 1.845 Test accuracy 91.990\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.8692 \n",
      "Accuracy: 9353/10000 (93.53%)\n",
      "\n",
      "Round  18, Average loss 1.869 Test accuracy 93.530\n",
      "selected users: [0 1 2 3 4 5 6 7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.8736 \n",
      "Accuracy: 9199/10000 (91.99%)\n",
      "\n",
      "Round  19, Average loss 1.874 Test accuracy 91.990\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.9324 \n",
      "Accuracy: 9358/10000 (93.58%)\n",
      "\n",
      "Round  20, Average loss 1.932 Test accuracy 93.580\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.8969 \n",
      "Accuracy: 9094/10000 (90.94%)\n",
      "\n",
      "Round  21, Average loss 1.897 Test accuracy 90.940\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.8659 \n",
      "Accuracy: 9164/10000 (91.64%)\n",
      "\n",
      "Round  22, Average loss 1.866 Test accuracy 91.640\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.8866 \n",
      "Accuracy: 8781/10000 (87.81%)\n",
      "\n",
      "Round  23, Average loss 1.887 Test accuracy 87.810\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.8758 \n",
      "Accuracy: 9261/10000 (92.61%)\n",
      "\n",
      "Round  24, Average loss 1.876 Test accuracy 92.610\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.8242 \n",
      "Accuracy: 9403/10000 (94.03%)\n",
      "\n",
      "Round  25, Average loss 1.824 Test accuracy 94.030\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.9040 \n",
      "Accuracy: 9103/10000 (91.03%)\n",
      "\n",
      "Round  26, Average loss 1.904 Test accuracy 91.030\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.9271 \n",
      "Accuracy: 9135/10000 (91.35%)\n",
      "\n",
      "Round  27, Average loss 1.927 Test accuracy 91.350\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.9110 \n",
      "Accuracy: 9250/10000 (92.50%)\n",
      "\n",
      "Round  28, Average loss 1.911 Test accuracy 92.500\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.8761 \n",
      "Accuracy: 9361/10000 (93.61%)\n",
      "\n",
      "Round  29, Average loss 1.876 Test accuracy 93.610\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "K = 4\n",
    "T = 5\n",
    "sigma = 1\n",
    "Noise_Alloc = [0,2,4,6,8]\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "\n",
    "N_array = [4, 4, 4, 8]\n",
    "B_array = [0.5]\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 30\n",
    "\n",
    "\n",
    "\n",
    "loss_test_arr_K4_G1 = np.zeros((len(N_array),len(B_array),N_trials,N_epochs))\n",
    "acc_test_arr_K4_G1  = np.zeros((len(N_array),len(B_array),N_trials,N_epochs))\n",
    "\n",
    "for N_idx in range(len(N_array)):\n",
    "    \n",
    "    N = N_array[N_idx]\n",
    "    \n",
    "    \n",
    "           \n",
    "        \n",
    "    # print(\"alpha_array: \",alpha_array,'\\n')\n",
    "    \n",
    "    \n",
    "    for B_idx in range(len(B_array)):\n",
    "        \n",
    "        B = B_array[B_idx]\n",
    "        z_array = []\n",
    "#         while(len(z_array)<N):\n",
    "#             z_tmp = np.random.uniform(-1,1,1)\n",
    "#             MIS_tmp = MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_tmp], 1,sigma)\n",
    "#             if MIS_tmp < B and MIS_tmp > 0.1:\n",
    "#                 z_array.append(z_tmp[0])\n",
    "#         \n",
    "#         z_array = np.sort(z_array)\n",
    "        print(N_idx,'!!!')\n",
    "        if N_idx==0:\n",
    "            z_array = np.array([-0.94,-0.534,0.534, 0.94])\n",
    "        elif N_idx==1:\n",
    "            z_array = np.array([-0.94, -0.73, 0.73, 0.94])\n",
    "        elif N_idx==2:\n",
    "            z_array = np.array([-0.94, -0.125, 0.125, 0.94])\n",
    "        else:\n",
    "            z_array = np.array([-0.94, -0.73, -0.534, -0.125, 0.125, 0.534, 0.73, 0.94])\n",
    "#             z_array = np.array([-0.9, -0.81, -0.22, -0.20, 0.20, 0.22, 0.81, 0.9])\n",
    "            \n",
    "        print('z_array:',z_array)\n",
    "        for j in range(len(z_array)):\n",
    "            print(MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_array[j]], 1,sigma))\n",
    "        \n",
    "        \n",
    "        _Noise_label = np.ones((15000*T,10)) * 0.1\n",
    "\n",
    "        X_tilde,a,b = BACC_Enc_Data_v3(encoding_input_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde,a,b = BACC_Enc_Data_v3(encoding_label_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "\n",
    "        m = N # m is the number of received result @ master\n",
    "    #     print('number of results:',m)\n",
    "\n",
    "        for trial_idx in range(N_trials):\n",
    "            print('(T, sigma)=',T,sigma,') ',trial_idx,'-th Trial!!')\n",
    "\n",
    "            net_glob = CNNMnist2(args=args)\n",
    "            net_glob.cuda()\n",
    "            net_glob.train()\n",
    "\n",
    "            # copy weights\n",
    "            w_glob = net_glob.state_dict()\n",
    "\n",
    "            for iter in range(N_epochs): #args.epochs\n",
    "                w_locals, loss_locals = [], []\n",
    "                idxs_users = np.random.choice(range(N), m, replace=False)\n",
    "                idxs_users = np.sort(idxs_users)\n",
    "                print('selected users:',idxs_users)\n",
    "                \n",
    "                coded_net = BACC_Enc_Model_withNoise_v3(net_glob.cuda(), N, K, T, 0.01, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "                dec_z_array = []\n",
    "                for idx in idxs_users: #for idx in range(N):\n",
    "            #         print(idx)\n",
    "                    local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[idx,:,:], label=y_tilde[idx,:,:])\n",
    "                    w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "#                     w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                    w_locals.append(copy.deepcopy(w))\n",
    "                    loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                    dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "                # update global weights\n",
    "                #w_glob = FedAvg(w_locals)\n",
    "                w_glob = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "\n",
    "                # copy weight to net_glob\n",
    "                net_glob.load_state_dict(w_glob)\n",
    "\n",
    "                # print loss\n",
    "            #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "            #     loss_train_arr.append(loss_train)\n",
    "\n",
    "                acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "                acc_test_arr_K4_G1[N_idx][B_idx][trial_idx][iter] = acc_test\n",
    "                loss_test_arr_K4_G1[N_idx][B_idx][trial_idx][iter] = loss_test\n",
    "                if iter % 1 ==0:\n",
    "                    print('Round {:3d}, Average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_test, acc_test))\n",
    "                #print(loss_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. With Grouping, N=4, N_i=2, G=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2 3\n",
      "##########################################\n",
      "###### 0 -th Trial!! ###########\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.2922 \n",
      "Accuracy: 2742/10000 (27.42%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.2554 \n",
      "Accuracy: 3894/10000 (38.94%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.3262 \n",
      "Accuracy: 7200/10000 (72.00%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.6624 \n",
      "Accuracy: 8873/10000 (88.73%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.4450 \n",
      "Accuracy: 9446/10000 (94.46%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3529 \n",
      "Accuracy: 9474/10000 (94.74%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3906 \n",
      "Accuracy: 9476/10000 (94.76%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3426 \n",
      "Accuracy: 9502/10000 (95.02%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3325 \n",
      "Accuracy: 9516/10000 (95.16%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3553 \n",
      "Accuracy: 9527/10000 (95.27%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3242 \n",
      "Accuracy: 9565/10000 (95.65%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3068 \n",
      "Accuracy: 9569/10000 (95.69%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3130 \n",
      "Accuracy: 9546/10000 (95.46%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3697 \n",
      "Accuracy: 9507/10000 (95.07%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3316 \n",
      "Accuracy: 9528/10000 (95.28%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2945 \n",
      "Accuracy: 9530/10000 (95.30%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3098 \n",
      "Accuracy: 9529/10000 (95.29%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2964 \n",
      "Accuracy: 9515/10000 (95.15%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2905 \n",
      "Accuracy: 9521/10000 (95.21%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3015 \n",
      "Accuracy: 9524/10000 (95.24%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2967 \n",
      "Accuracy: 9518/10000 (95.18%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2678 \n",
      "Accuracy: 9563/10000 (95.63%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2688 \n",
      "Accuracy: 9555/10000 (95.55%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2546 \n",
      "Accuracy: 9568/10000 (95.68%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2670 \n",
      "Accuracy: 9570/10000 (95.70%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2567 \n",
      "Accuracy: 9520/10000 (95.20%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2519 \n",
      "Accuracy: 9562/10000 (95.62%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2475 \n",
      "Accuracy: 9556/10000 (95.56%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2605 \n",
      "Accuracy: 9556/10000 (95.56%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3143 \n",
      "Accuracy: 9488/10000 (94.88%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "G = 2\n",
    "N = 4 # N should be divisible by G\n",
    "K = 4 # N should be divisible by G\n",
    "\n",
    "size_per_group = int(60000/G)\n",
    "\n",
    "X_group = np.reshape(encoding_input_array_np, (G,size_per_group,28*28))\n",
    "y_group = np.reshape(encoding_label_array_np, (G,size_per_group,args.num_classes)) \n",
    "\n",
    "N_i = int(N/G) # = 2\n",
    "K_i = int(K/G) # = 2\n",
    "T = 3\n",
    "sigma = 1\n",
    "Noise_Alloc = [0,2,4]\n",
    "m = N_i # number of selected workers (if there is no straggler, m=N_i)\n",
    "\n",
    "print(N_i,K_i,T)\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K_i+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K_i+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K_i+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "z_array = np.array([-0.81, 0.81])\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 30\n",
    "\n",
    "loss_test_arr_G2 = np.zeros((N_trials,N_epochs))\n",
    "acc_test_arr_G2  = np.zeros((N_trials,N_epochs))\n",
    "\n",
    "for trial_idx in range(N_trials):\n",
    "    \n",
    "    print('##########################################')\n",
    "    print('######',trial_idx,'-th Trial!! ###########')\n",
    "    \n",
    "    net_glob = CNNMnist2(args=args)\n",
    "    net_glob.cuda()\n",
    "    net_glob.train()\n",
    "    \n",
    "    # copy weights\n",
    "    w_glob = net_glob.state_dict()\n",
    "    \n",
    "    X_tilde = np.empty((N,Size_submatrices,28*28))\n",
    "    y_tilde = np.empty((N,Size_submatrices,10))\n",
    "    \n",
    "    for G_idx in range(G):\n",
    "        \n",
    "        _Noise_label = np.ones((size_per_group*T,10)) * 0.1\n",
    "        \n",
    "        X_tilde_tmp,a,b = BACC_Enc_Data_v3(X_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde_tmp,a,b = BACC_Enc_Data_v3(y_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "        \n",
    "        stt_pos = G_idx * N_i\n",
    "        end_pos = (G_idx+1) * N_i\n",
    "        \n",
    "        X_tilde[stt_pos:end_pos,:,:] = X_tilde_tmp\n",
    "        y_tilde[stt_pos:end_pos,:,:] = y_tilde_tmp\n",
    "        \n",
    "   \n",
    "\n",
    "    for iter in range(N_epochs): #args.epochs\n",
    "        \n",
    "        w_group_array = []\n",
    "        for G_idx in range(G):\n",
    "            w_locals, loss_locals = [], []\n",
    "            idxs_users = np.random.choice(range(N_i), m, replace=False)\n",
    "            idxs_users = np.sort(idxs_users)\n",
    "            print('selected users:',idxs_users)\n",
    "\n",
    "            coded_net = BACC_Enc_Model_withNoise_v3(net_glob.cuda(), N_i, K_i, T, 0.01, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "            dec_z_array = []\n",
    "            for idx in idxs_users: #for idx in range(N):\n",
    "        #         print(idx)\n",
    "                local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[G_idx*N_i+idx,:,:], label=y_tilde[G_idx*N_i+idx,:,:])\n",
    "                w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "#                 w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                w_locals.append(copy.deepcopy(w))\n",
    "                loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "            # update global weights\n",
    "            #w_glob = FedAvg(w_locals)\n",
    "            w_group = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "            \n",
    "            w_group_array.append(copy.deepcopy(w_group))\n",
    "        \n",
    "        w_glob = copy.deepcopy(w_group_array[0])\n",
    "        for k in w_glob.keys():\n",
    "            for G_idx in range(1,G):\n",
    "                w_glob[k] += w_group_array[G_idx][k]\n",
    "            w_glob[k] = torch.div(w_glob[k], len(w_group_array))\n",
    "        \n",
    "        # copy weight to net_glob\n",
    "        net_glob.load_state_dict(w_glob)\n",
    "\n",
    "        # print loss\n",
    "    #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "    #     loss_train_arr.append(loss_train)\n",
    "\n",
    "        acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "        acc_test_arr_G2[trial_idx][iter] = acc_test\n",
    "        loss_test_arr_G2[trial_idx][iter] = loss_test\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. With Grouping, N=8, N_i=4, G=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2 3\n",
      "##########################################\n",
      "###### 0 -th Trial!! ###########\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 15000 \n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.3017 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.3014 \n",
      "Accuracy: 1828/10000 (18.28%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.3012 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.2969 \n",
      "Accuracy: 3021/10000 (30.21%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.2764 \n",
      "Accuracy: 5190/10000 (51.90%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.2188 \n",
      "Accuracy: 5264/10000 (52.64%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.1921 \n",
      "Accuracy: 5235/10000 (52.35%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.1554 \n",
      "Accuracy: 5730/10000 (57.30%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.0841 \n",
      "Accuracy: 7809/10000 (78.09%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.0817 \n",
      "Accuracy: 7686/10000 (76.86%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9836 \n",
      "Accuracy: 9151/10000 (91.51%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9573 \n",
      "Accuracy: 8965/10000 (89.65%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9295 \n",
      "Accuracy: 9351/10000 (93.51%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9021 \n",
      "Accuracy: 9367/10000 (93.67%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9236 \n",
      "Accuracy: 9385/10000 (93.85%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8886 \n",
      "Accuracy: 9350/10000 (93.50%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8534 \n",
      "Accuracy: 9420/10000 (94.20%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8475 \n",
      "Accuracy: 9457/10000 (94.57%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8547 \n",
      "Accuracy: 9360/10000 (93.60%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8596 \n",
      "Accuracy: 9410/10000 (94.10%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9589 \n",
      "Accuracy: 9301/10000 (93.01%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9261 \n",
      "Accuracy: 9333/10000 (93.33%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8444 \n",
      "Accuracy: 9399/10000 (93.99%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8707 \n",
      "Accuracy: 9403/10000 (94.03%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9353 \n",
      "Accuracy: 9406/10000 (94.06%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8539 \n",
      "Accuracy: 9320/10000 (93.20%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8646 \n",
      "Accuracy: 9359/10000 (93.59%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8296 \n",
      "Accuracy: 9402/10000 (94.02%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8863 \n",
      "Accuracy: 9373/10000 (93.73%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "N = 8\n",
    "K = 4\n",
    "G = 2\n",
    "\n",
    "N_i = int(N/G) # = 2\n",
    "K_i = int(K/G) # = 2\n",
    "T = 3\n",
    "sigma = 1\n",
    "Noise_Alloc = [0,2,4]\n",
    "m = N_i # number of selected workers (if there is no straggler, m=N_i)\n",
    "\n",
    "print(N_i,K_i,T)\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K_i+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K_i+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K_i+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "z_array = np.array([-0.81, -0.22, 0.22, 0.81])\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 30\n",
    "\n",
    "loss_test_arr_K4_G2_N8 = np.zeros((N_trials,N_epochs))\n",
    "acc_test_arr_K4_G2_N8  = np.zeros((N_trials,N_epochs))\n",
    "\n",
    "for trial_idx in range(N_trials):\n",
    "    \n",
    "    print('##########################################')\n",
    "    print('######',trial_idx,'-th Trial!! ###########')\n",
    "    \n",
    "    net_glob = CNNMnist2(args=args)\n",
    "    net_glob.cuda()\n",
    "    net_glob.train()\n",
    "    \n",
    "    # copy weights\n",
    "    w_glob = net_glob.state_dict()\n",
    "    \n",
    "    X_tilde = np.empty((N,Size_submatrices,28*28))\n",
    "    y_tilde = np.empty((N,Size_submatrices,10))\n",
    "    \n",
    "    for G_idx in range(G):\n",
    "        \n",
    "        _Noise_label = np.ones((size_per_group*T,10)) * 0.1\n",
    "        \n",
    "        X_tilde_tmp,a,b = BACC_Enc_Data_v3(X_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde_tmp,a,b = BACC_Enc_Data_v3(y_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "        \n",
    "        stt_pos = G_idx * N_i\n",
    "        end_pos = (G_idx+1) * N_i\n",
    "        \n",
    "        X_tilde[stt_pos:end_pos,:,:] = X_tilde_tmp\n",
    "        y_tilde[stt_pos:end_pos,:,:] = y_tilde_tmp\n",
    "        \n",
    "   \n",
    "\n",
    "    for iter in range(N_epochs): #args.epochs\n",
    "        \n",
    "        w_group_array = []\n",
    "        for G_idx in range(G):\n",
    "            w_locals, loss_locals = [], []\n",
    "            idxs_users = np.random.choice(range(N_i), m, replace=False)\n",
    "            idxs_users = np.sort(idxs_users)\n",
    "            print('selected users:',idxs_users)\n",
    "\n",
    "            coded_net = BACC_Enc_Model_withNoise_v3(net_glob.cuda(), N_i, K_i, T, 0.01, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "            dec_z_array = []\n",
    "            for idx in idxs_users: #for idx in range(N):\n",
    "        #         print(idx)\n",
    "                local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[G_idx*N_i+idx,:,:], label=y_tilde[G_idx*N_i+idx,:,:])\n",
    "                w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "#                 w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                w_locals.append(copy.deepcopy(w))\n",
    "                loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "            # update global weights\n",
    "            #w_glob = FedAvg(w_locals)\n",
    "            w_group = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "            \n",
    "            w_group_array.append(copy.deepcopy(w_group))\n",
    "        \n",
    "        w_glob = copy.deepcopy(w_group_array[0])\n",
    "        for k in w_glob.keys():\n",
    "            for G_idx in range(1,G):\n",
    "                w_glob[k] += w_group_array[G_idx][k]\n",
    "            w_glob[k] = torch.div(w_glob[k], len(w_group_array))\n",
    "        \n",
    "        # copy weight to net_glob\n",
    "        net_glob.load_state_dict(w_glob)\n",
    "\n",
    "        # print loss\n",
    "    #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "    #     loss_train_arr.append(loss_train)\n",
    "\n",
    "        acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "        acc_test_arr_K4_G2_N8[trial_idx][iter] = acc_test\n",
    "        loss_test_arr_K4_G2_N8[trial_idx][iter] = loss_test\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. With Grouping, N=8, N_i=2, G=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "464 86 75 63 312\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.9960065472726245 0.024246284094422223\n",
      "-0.9945806304055294 0.02379138948912273\n",
      "-0.993493820294377 0.02344658388246322\n",
      "-0.9915948270662338 0.02284811569959494\n",
      "-0.9898654400800284 0.02230760460521098\n",
      "-0.9897014327908784 0.022256570490235782\n",
      "-0.988038212984107 0.021741257534331246\n",
      "-0.9855355914753883 0.020973626381079663\n",
      "-0.9836359338139788 0.02039727443095157\n",
      "-0.9833772703799692 0.0203192250498566\n",
      "-0.9788318268364826 0.018964774051525496\n",
      "-0.9759626600991576 0.01812685312430741\n",
      "-0.9747165405337235 0.01776714244762389\n",
      "-0.9733447938473296 0.0173741628752302\n",
      "-0.9711924567729937 0.016763972611583335\n",
      "-0.9674330021665809 0.015717351079513994\n",
      "-0.9652858824012636 0.015130801675177045\n",
      "-0.9652457929172171 0.015119928807473135\n",
      "-0.9642087780812734 0.014839687334542639\n",
      "-0.9638938590998354 0.014754971483028234\n",
      "-0.9621416084416963 0.014286926257770411\n",
      "-0.9599432872341076 0.013707798213239565\n",
      "-0.9593561339584071 0.01355465651173764\n",
      "-0.9586057977696858 0.013359906725444062\n",
      "-0.9575513224099064 0.01308803759138225\n",
      "-0.9566128944599299 0.012847890368682946\n",
      "-0.9563593223783295 0.012783293384640643\n",
      "-0.9537677825661035 0.012130327459587198\n",
      "-0.9527619326889922 0.011880475625196098\n",
      "-0.9522503461694536 0.011754175491524728\n",
      "-0.9509199093815408 0.01142819117370528\n",
      "-0.9489235102356512 0.010945804989857163\n",
      "-0.9489146201023797 0.010943675227370754\n",
      "-0.9486355149673011 0.010876894699892867\n",
      "-0.9441450555372011 0.009824941106922351\n",
      "-0.9440315481445896 0.009798905742771381\n",
      "-0.9431204409975924 0.009590930512940648\n",
      "-0.9384849004561346 0.008560945000614325\n",
      "-0.9353661165618867 0.007895070065038822\n",
      "-0.9334356784710904 0.007494116361763292\n",
      "-0.9294193710605347 0.0066880396284560405\n",
      "-0.9287312968042334 0.006553819169319708\n",
      "-0.928156674453134 0.0064426103624047855\n",
      "-0.9273253207631929 0.00628314226921327\n",
      "-0.9272479915889329 0.006268395297950148\n",
      "-0.926270605288434 0.0060832734056211375\n",
      "-0.9254229122526492 0.005924630609969325\n",
      "-0.9250762187559896 0.005860263437083356\n",
      "-0.9214114790749022 0.005198392006075601\n",
      "-0.9177640507391578 0.004573921969528565\n",
      "-0.9172985630024091 0.004496736153564054\n",
      "-0.9112896952048579 0.0035526844484318463\n",
      "-0.9091511367721421 0.0032406434437721975\n",
      "-0.9034026079726294 0.0024663826552649196\n",
      "-0.9013394443102645 0.00221200583366624\n",
      "-0.9010903885902473 0.0021821554107416294\n",
      "-0.9006432276255458 0.0021290275425957404\n",
      "-0.9002751889997387 0.0020857509141149026\n",
      "-0.8994749412387182 0.0019930617240613863\n",
      "-0.8981987633555839 0.0018492672416088919\n",
      "-0.8972842525999662 0.0017492852211808015\n",
      "-0.8964077288501675 0.0016558731535465897\n",
      "-0.8957239258660441 0.0015846521046735621\n",
      "-0.8931840390980881 0.0013329209213731224\n",
      "-0.8926223416768824 0.0012800010250540923\n",
      "-0.8892747773126126 0.0009855996930602762\n",
      "-0.8887842081837298 0.0009455084210809261\n",
      "-0.8884247278384758 0.0009166305561977698\n",
      "-0.8867946122879025 0.0007910215281170476\n",
      "-0.8851400399451881 0.0006725578743725231\n",
      "-0.880891316408106 0.00041078989552159026\n",
      "-0.87937271812583 0.00033230809892147997\n",
      "-0.8775446635187523 0.0002485586337917972\n",
      "-0.8755708451189232 0.00017145301569292318\n",
      "-0.8707915993619828 4.321952808778851e-05\n",
      "-0.8685560448678977 1.2247210583772913e-05\n",
      "-0.8643350148690097 5.518008338818848e-06\n",
      "-0.8612335742189288 4.4660676367535035e-05\n",
      "-0.8571527552412392 0.00015457165832655917\n",
      "-0.8546747818618503 0.0002544221293141915\n",
      "-0.8518040963103946 0.000402053514377647\n",
      "-0.851664959918669 0.0004100913672900415\n",
      "-0.8512928415299652 0.0004319924550760918\n",
      "-0.8500832937076315 0.0005072585243121605\n",
      "-0.8479140148393618 0.0006580015111916468\n",
      "-0.8442432408577909 0.0009599009703416575\n",
      "-0.8434562824219709 0.0010323990487838594\n",
      "-0.8428518071099351 0.001089971289832426\n",
      "-0.8405397325276998 0.0013254209790844376\n",
      "-0.8380872582453796 0.0016018925008366549\n",
      "-0.8371934918094168 0.0017095742422622636\n",
      "-0.836460007064002 0.0018007340570339954\n",
      "-0.8357818406296116 0.0018872670524583402\n",
      "-0.8278943177475091 0.003055646222840166\n",
      "-0.8275899571624084 0.0031068240161004714\n",
      "-0.8240557203448955 0.0037351042046638895\n",
      "-0.8216817246043513 0.004192757968841471\n",
      "-0.8195179063070686 0.004635271869851411\n",
      "-0.8177724153023145 0.005010109634543575\n",
      "-0.8164174746976263 0.005312206208265919\n",
      "-0.8142707831538196 0.005810966139795478\n",
      "-0.8059256466885918 0.007990353696330305\n",
      "-0.8019371062704233 0.009171271579242048\n",
      "-0.8017446892860718 0.009230579659577784\n",
      "-0.8013944013020895 0.00933910542184131\n",
      "-0.7970035116462422 0.010761172953931442\n",
      "-0.7969361592288569 0.010783884841507413\n",
      "-0.7940070679801889 0.011798269214752967\n",
      "-0.7939610874013523 0.01181461113046833\n",
      "-0.7927153943622698 0.012262303368612575\n",
      "-0.7863302406476715 0.014709689156291101\n",
      "-0.7861940942187877 0.014764698033685061\n",
      "-0.7853926551141446 0.01509093930699445\n",
      "-0.7842452503044544 0.015565262916058191\n",
      "-0.7830628469977039 0.01606304269113221\n",
      "-0.7830500703703882 0.016068471578967385\n",
      "-0.7810265913171464 0.01694185733776075\n",
      "-0.7804914637184772 0.017177372651483544\n",
      "-0.7794706145264236 0.017631964253136758\n",
      "-0.7786030561473591 0.018023792415954507\n",
      "-0.7785080123421619 0.01806702669259122\n",
      "-0.7760643149244848 0.019199678585332797\n",
      "-0.7742617693183347 0.020061342994392238\n",
      "-0.7701685425557581 0.022101941589016728\n",
      "-0.7694329978469616 0.02248114662604093\n",
      "-0.767579793057886 0.02345367735544513\n",
      "-0.7668040118536148 0.023868121684107752\n",
      "-0.7651254659080755 0.024779773447290543\n",
      "-0.7648859088000144 0.024911555041193478\n",
      "-0.7645290573075263 0.0251086393510418\n",
      "-0.7635757843336592 0.025639701110646458\n",
      "-0.7628983612310787 0.026021157043338246\n",
      "-0.7620154870436571 0.026523399895848745\n",
      "-0.757772519383723 0.02901849204912195\n",
      "-0.757481213388365 0.02919478966023204\n",
      "-0.7559160898739841 0.030153112015065996\n",
      "-0.7545433815745475 0.031009139192518163\n",
      "-0.7475224521927215 0.03561853363378694\n",
      "-0.7465585098410124 0.03628210263608596\n",
      "-0.7459387255602143 0.03671273721777908\n",
      "-0.7414940593976542 0.03989326936050083\n",
      "-0.741437548267436 0.039934760949785486\n",
      "-0.7326446150276911 0.04672083355407704\n",
      "-0.7317940448154432 0.04741269325351549\n",
      "-0.7210418906744682 0.056717446888047585\n",
      "-0.7148932233935394 0.06251886853903253\n",
      "-0.7146049351014072 0.0627997170925192\n",
      "-0.7145111054256839 0.062891298112328\n",
      "-0.7114435933815635 0.06593230483927998\n",
      "-0.7097220307267329 0.06767929619455494\n",
      "-0.7074023096331026 0.07007967128176974\n",
      "-0.6970855801038922 0.08141619155850607\n",
      "-0.6947921525509828 0.08408670133949604\n",
      "-0.6921672983538785 0.08721197443837737\n",
      "-0.6893527387235037 0.09064580486856683\n",
      "-0.6864914090679413 0.0942256056207767\n",
      "-0.683553882181652 0.09799530149421107\n",
      "-0.6826721667300948 0.0991456935419845\n",
      "-0.6814176574245694 0.10079763794054841\n",
      "-0.6799889449522871 0.10270078451043177\n",
      "-0.6786446101527721 0.10451287961147472\n",
      "-0.6762104301813785 0.10784713077452608\n",
      "-0.6676235657319698 0.12016618085248865\n",
      "-0.6650152538426515 0.12408372509790837\n",
      "-0.6634195543838055 0.12652150812529012\n",
      "-0.6615698044166243 0.1293868433626009\n",
      "-0.6602325724985059 0.1314848282605226\n",
      "-0.6585565732295964 0.13414601030507908\n",
      "-0.6514602391744215 0.1458104441564064\n",
      "-0.6513647297798824 0.1459718708980515\n",
      "-0.6501186411277327 0.14808889657163296\n",
      "-0.643203992237068 0.16020978903060298\n",
      "-0.6377400748632449 0.17024396943213083\n",
      "-0.6346415843825848 0.17611712080554234\n",
      "-0.6337943205602881 0.17774645596111224\n",
      "-0.6320728254001602 0.1810881243556788\n",
      "-0.62748887609959 0.19019175664576085\n",
      "-0.6240219703212386 0.19727808871487731\n",
      "-0.6233245847798896 0.1987247139764284\n",
      "-0.6231766967743015 0.1990324028879655\n",
      "-0.6230651059382086 0.1992647861969731\n",
      "-0.6224091628088331 0.20063446175332628\n",
      "-0.6197305879878638 0.20629355701675203\n",
      "-0.6191547876292742 0.20752397544620524\n",
      "-0.6177277058047326 0.21059483135573492\n",
      "-0.6161742619036339 0.21397234074418423\n",
      "-0.610931585581834 0.22564122333174286\n",
      "-0.6075027642322661 0.23350138931777653\n",
      "-0.6074498485912232 0.23362412357313334\n",
      "-0.6046500364021239 0.24018064536748676\n",
      "-0.6043485397890436 0.24089403276815513\n",
      "-0.6042614544922738 0.24110035708540936\n",
      "-0.6018530636413688 0.2468539697461398\n",
      "-0.6015866397907907 0.24749611629839705\n",
      "-0.59789365931517 0.2565142202608112\n",
      "-0.5942821248406844 0.26554669232205597\n",
      "-0.5930460516646412 0.26868705835118406\n",
      "-0.5923510056038308 0.27046392656345114\n",
      "-0.5921840133199785 0.27089202470628837\n",
      "-0.5869088932332016 0.2846535471817776\n",
      "-0.5861945933176951 0.2865527584108762\n",
      "-0.5842322713036341 0.291814590949219\n",
      "-0.5830157038360027 0.29510951770965504\n",
      "-0.5825356275426228 0.29641668044693714\n",
      "-0.5801751170304696 0.302901276805625\n",
      "-0.5765552894905575 0.3130318547632427\n",
      "-0.5743885379619535 0.3192048046904517\n",
      "-0.5736081288314454 0.3214482865901416\n",
      "-0.571346630645098 0.32801011911905253\n",
      "-0.5681707745812321 0.3373781217446336\n",
      "-0.5678978300195912 0.3381916404876065\n",
      "-0.5678430046428715 0.3383552099611822\n",
      "-0.5673716969851934 0.3397635615092783\n",
      "-0.5644270697162543 0.34865312890562394\n",
      "-0.5627060674489635 0.3539213154984553\n",
      "-0.5618101849680588 0.3566850469539195\n",
      "-0.5614232963785988 0.35788309967736553\n",
      "-0.554883217383197 0.3785522125334278\n",
      "-0.552944100435542 0.3848332342693606\n",
      "-0.5499902081355448 0.39453700435745737\n",
      "-0.5476835498436403 0.40222936963635664\n",
      "-0.5464829932922062 0.40627314287923594\n",
      "-0.5442888863208757 0.4137347615148996\n",
      "-0.5432285325259116 0.4173739511651098\n",
      "-0.5415709629018419 0.42310634832034916\n",
      "-0.5408864858871654 0.4254890256855376\n",
      "-0.540410372270542 0.42715175731994304\n",
      "-0.5403469032916357 0.42737374331861305\n",
      "-0.5361532640670228 0.44221572788433683\n",
      "-0.5330092383225757 0.45357004336362666\n",
      "-0.5309692632268748 0.46104220355981596\n",
      "-0.5303886676924152 0.4631840362896655\n",
      "-0.5270167036666229 0.4757570708496776\n",
      "-0.5266484757785208 0.47714395826125205\n",
      "-0.522599860098774 0.49257403466058186\n",
      "-0.5225551519951219 0.49274629028409733\n",
      "-0.5218497722484856 0.4954694567942743\n",
      "-0.519211943052208 0.5057434575593702\n",
      "-0.5191534512761686 0.5059728983459365\n",
      "-0.5175374351775714 0.5123398599756712\n",
      "-0.5142325385138304 0.5255295665440983\n",
      "-0.5137875073409865 0.5273230586933103\n",
      "-0.513545998780268 0.5282980808946802\n",
      "-0.5131296752135721 0.5299817354107504\n",
      "-0.512710969327054 0.531678686917808\n",
      "-0.5109534148153929 0.5388419451883917\n",
      "-0.509068390576086 0.546597064443393\n",
      "-0.5080358489539891 0.5508768705913415\n",
      "-0.5078201626621157 0.5517737284126838\n",
      "-0.501827009299058 0.5770911135987797\n",
      "-0.5003856780324394 0.5832948623502393\n",
      "-0.4956710529334396 0.6039020216476434\n",
      "-0.4951368473640885 0.6062675338692287\n",
      "-0.4927853323011153 0.6167546462893548\n",
      "-0.49275816931783756 0.6168764956170353\n",
      "-0.49201446737305354 0.6202189520104672\n",
      "-0.4836105196062168 0.6588419261310104\n",
      "-0.48355336204677535 0.6591100136801915\n",
      "-0.4818595747738603 0.6670878104935191\n",
      "-0.47893149038432603 0.6810320609958078\n",
      "-0.47444715355328304 0.7027655975452836\n",
      "-0.4725600490620754 0.7120492181109345\n",
      "-0.4665942392213387 0.7419393913341935\n",
      "-0.4642325151898896 0.7540011514217717\n",
      "-0.46065042389299604 0.7725455254784569\n",
      "-0.4592407374866623 0.7799264787221886\n",
      "-0.4591626973460954 0.7803364629916602\n",
      "-0.4587769612020025 0.7823650515584469\n",
      "-0.4549960317532731 0.80243639249341\n",
      "-0.45481813689092276 0.8033891640795605\n",
      "-0.45399900018611383 0.8077860852869073\n",
      "-0.44571606928766405 0.8531553463773425\n",
      "-0.44526131687029014 0.8556944316028253\n",
      "-0.4427212122214994 0.8699699850236307\n",
      "-0.4414437666742008 0.8772091200041714\n",
      "-0.4410216297193983 0.8796101457505631\n",
      "-0.44021159586416125 0.8842297452919285\n",
      "-0.4379499237322817 0.8972138047929733\n",
      "-0.4361370301778944 0.907713014873376\n",
      "-0.4345826243394526 0.9167803409682991\n",
      "-0.4307931605691939 0.9391386632744325\n",
      "-0.4243439791720751 0.9780221542788481\n",
      "-0.4226027310826319 0.9887018275231834\n",
      "-0.42161613694457634 0.9947873855345551\n",
      "-0.41988596209208695 1.0055198700417467\n",
      "-0.41809649524846537 1.016701253142603\n",
      "-0.41774285030088665 1.0189207700749578\n",
      "-0.4117355263462621 1.0571197265951335\n",
      "-0.4113619361575829 1.0595263964373087\n",
      "-0.41101783123935287 1.0617463606174542\n",
      "-0.41046704311529614 1.0653061938992021\n",
      "-0.4087174032615364 1.0766673324921106\n",
      "-0.4080213632950276 1.0812094447268774\n",
      "-0.4077573232119336 1.082935825147162\n",
      "-0.40736723541775155 1.0854897196945412\n",
      "-0.39978955482000145 1.1359035545647436\n",
      "-0.39712766332974314 1.153978313970794\n",
      "-0.3962794393680853 1.159778162594294\n",
      "-0.39323960510663847 1.1807240009111453\n",
      "-0.3931861889116923 1.1810943140447385\n",
      "-0.3924837752772019 1.1859711220027909\n",
      "-0.39244628556880623 1.1862317896734145\n",
      "-0.39105152900460793 1.1959569565785213\n",
      "-0.38769738043135793 1.2195632837185655\n",
      "-0.38535528780769135 1.2362312518235492\n",
      "-0.38072653016984326 1.2696225108939192\n",
      "-0.3792451433917341 1.2804360896862004\n",
      "-0.3738393303111449 1.3204241283823652\n",
      "-0.3734097370387994 1.3236376725202375\n",
      "-0.3729388585101441 1.3271661267694437\n",
      "-0.3707127955870413 1.3439330774815226\n",
      "-0.3699635550468685 1.3496085798372328\n",
      "-0.369303248564963 1.354623867963035\n",
      "-0.368952999268942 1.3572892768392821\n",
      "-0.3670433867761347 1.371884169016349\n",
      "-0.3669799773320439 1.3723706190545828\n",
      "-0.3654442711288932 1.3841877519938346\n",
      "-0.3644566753153331 1.3918236889200108\n",
      "-0.3609613004871304 1.419079960672264\n",
      "-0.36004654930870417 1.4262726771210876\n",
      "-0.35992098314885235 1.427261947684112\n",
      "-0.3578552198677427 1.4436044501194005\n",
      "-0.35731669106067687 1.4478857608511495\n",
      "-0.354756383311424 1.4683593125523386\n",
      "-0.3540866852163338 1.4737471498370474\n",
      "-0.349096550456397 1.5143225421990296\n",
      "-0.34634747421525147 1.5370014468030329\n",
      "-0.34378497024055776 1.558351860325525\n",
      "-0.3404196416406271 1.5867028933707237\n",
      "-0.3383589879477815 1.6042389149452967\n",
      "-0.3378085559383639 1.6089458449220568\n",
      "-0.3376117989775531 1.610630718727604\n",
      "-0.3355411954980907 1.6284366758584918\n",
      "-0.3352739137668146 1.6307451394638055\n",
      "-0.3349862817963887 1.6332319262193051\n",
      "-0.3345310947562985 1.637172781214005\n",
      "-0.33442875233064195 1.6380597449853374\n",
      "-0.3338133167620987 1.6434006111582524\n",
      "-0.3287552212039948 1.687761027186107\n",
      "-0.3276465201264118 1.6975962827013158\n",
      "-0.327633055765451 1.6977159734267402\n",
      "-0.32654402004660343 1.7074167199581978\n",
      "-0.32592272883109863 1.712968542035171\n",
      "-0.32476485061089577 1.7233494735336699\n",
      "-0.32396980247278684 1.7305033177423959\n",
      "-0.32254903413838276 1.7433400416108689\n",
      "-0.32202977426240187 1.748048489204444\n",
      "-0.3188077517531045 1.7774681529661898\n",
      "-0.31510749948055716 1.8116912561156953\n",
      "-0.3132277449533565 1.8292577910577432\n",
      "-0.308027684023779 1.8784974441977023\n",
      "-0.3067932451441473 1.8903270611147485\n",
      "-0.30677556291498376 1.8904969053764822\n",
      "-0.3049451760220707 1.9081391501142595\n",
      "-0.30395440934387774 1.917739058837561\n",
      "-0.30137002137140967 1.942947987117845\n",
      "-0.2977848667504477 1.9783247691521337\n",
      "-0.2948617775618263 2.0075226697876403\n",
      "-0.29380456599837057 2.018162098639523\n",
      "-0.29318739090251533 2.0243927472945105\n",
      "-0.2914217078855723 2.04229832447579\n",
      "-0.28983430901471063 2.058498112204863\n",
      "-0.2880050714717346 2.0772869028810064\n",
      "-0.28597432672823286 2.09829855705168\n",
      "-0.28434766494466523 2.1152465335358044\n",
      "-0.2835281127880942 2.1238251607535696\n",
      "-0.28198195465772335 2.140082656531876\n",
      "-0.281651088433589 2.1435741167854117\n",
      "-0.2727956528163631 2.2386868796014436\n",
      "-0.2726955807023106 2.2397803785193444\n",
      "-0.2721297266650784 2.245971512966047\n",
      "-0.2704900352898958 2.2639887129570506\n",
      "-0.2659518590859742 2.314459412304207\n",
      "-0.2646838637608355 2.328722281229118\n",
      "-0.26443134873795016 2.3315711598295907\n",
      "-0.26378574913362196 2.338867714439303\n",
      "-0.26355622186805583 2.3414663025621127\n",
      "-0.2627292536935675 2.350848337686692\n",
      "-0.26200584398906934 2.35908065556728\n",
      "-0.25885240306186796 2.3952433777709383\n",
      "-0.2583409882907328 2.4011509704833545\n",
      "-0.2567285361837659 2.419856302489756\n",
      "-0.25625942944665914 2.425320886960245\n",
      "-0.255809748865226 2.4305688288468072\n",
      "-0.253500245499799 2.4576714830450554\n",
      "-0.25327262700584385 2.4603563184214368\n",
      "-0.25117882886851883 2.4851695731555026\n",
      "-0.2503407737105041 2.4951603795910953\n",
      "-0.2500801890422957 2.4982738594104004\n",
      "-0.24909555551143225 2.5100681775534204\n",
      "-0.24689972009653638 2.5365421494324654\n",
      "-0.23561940096194722 2.676400361274393\n",
      "-0.2338762111524022 2.698610350389738\n",
      "-0.23340449589817402 2.704648868441771\n",
      "-0.2331465652912721 2.707955833601758\n",
      "-0.2308052542511594 2.7381417957234846\n",
      "-0.22916896775884799 2.759419306512311\n",
      "-0.2277217625686483 2.7783641022592658\n",
      "-0.22702909271317795 2.7874738363033007\n",
      "-0.2266819120529251 2.792050186846071\n",
      "-0.22589938949627553 2.8023904738290777\n",
      "-0.22339400198960102 2.8357365950975115\n",
      "-0.2218844411443266 2.856007211525604\n",
      "-0.22101706879474636 2.86771601472127\n",
      "-0.21763593257562452 2.913794466175185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.21601643520851632 2.936114907964704\n",
      "-0.21482291810189036 2.952669700116319\n",
      "-0.2138619533665631 2.9660645131722596\n",
      "-0.21102352244253209 3.0059761127642646\n",
      "-0.20869144038840615 3.0391627126063656\n",
      "-0.20825602812902444 3.0453989380058952\n",
      "-0.20780185836614273 3.0519173954357286\n",
      "-0.20194191698088693 3.1372908490862805\n",
      "-0.20051748488093213 3.158407685309725\n",
      "-0.19999485756051416 3.1661920921550553\n",
      "-0.19584463945328867 3.228719489839688\n",
      "-0.19121690454129636 3.299975417605105\n",
      "-0.1907532743252467 3.3072060677754025\n",
      "-0.19039175227583716 3.3128560868763848\n",
      "-0.1899624246263356 3.319579310327534\n",
      "-0.18739512473707354 3.360092530780653\n",
      "-0.18698528972676431 3.3666095921654775\n",
      "-0.18373507469116102 3.4187881242197378\n",
      "-0.18113884958256787 3.4611126560919256\n",
      "-0.17937794199732138 3.490154460430079\n",
      "-0.1775175647730476 3.521137433791579\n",
      "-0.1745138247141691 3.571830065883337\n",
      "-0.17318041639174186 3.5946038161512948\n",
      "-0.1730496697540147 3.5968459868799765\n",
      "-0.17061225489905563 3.6389472109770202\n",
      "-0.1698676542007398 3.651924402190272\n",
      "-0.16667876290010897 3.7081297470809536\n",
      "-0.16663412975778447 3.7089237611529584\n",
      "-0.16388165747089478 3.758290455082035\n",
      "-0.16134293921286758 3.8045377296306246\n",
      "-0.16034731916147615 3.8228666364919985\n",
      "-0.1554701777973686 3.9142696661660907\n",
      "-0.15512134965426072 3.920912714623039\n",
      "-0.1544262087080721 3.9341940155859514\n",
      "-0.1539574225631617 3.943183221557536\n",
      "-0.15279295834292972 3.965627238502236\n",
      "-0.1516188767038722 3.9884244338004335\n",
      "-0.1482706420078459 4.054385999217995\n",
      "-0.14762107676972103 4.067349157680982\n",
      "-0.14654864303450688 4.088872443662735\n",
      "-0.14508284864670773 4.118537784877769\n",
      "-0.14408219020792434 4.1389563478436955\n",
      "-0.14222510908742825 4.177216261262762\n",
      "-0.12885473270156678 4.467857176836912\n",
      "-0.12737067958085557 4.50190168754186\n",
      "-0.12632826669855102 4.526045767072451\n",
      "-0.12406074435451364 4.57924037942648\n",
      "-0.12247947894484512 4.616897609555609\n",
      "-0.1169004065042405 4.753644377320308\n",
      "-0.11630875617268432 4.768516691945354\n",
      "-0.11607167470248547 4.774496884930057\n",
      "-0.11479492265584978 4.806907831832677\n",
      "-0.11158944547215865 4.889853803291047\n",
      "-0.10419776844347028 5.090341577361408\n",
      "-0.10379673365458353 5.101612546236675\n",
      "-0.10358249658595997 5.107651070796307\n",
      "-0.10251548897725349 5.137908932444713\n",
      "-0.09966063265246916 5.2204060686476\n",
      "-0.09824516007456552 5.262170581134257\n",
      "-0.09706961332727704 5.29730619687294\n",
      "-0.0951856808744187 5.35449331855257\n",
      "-0.09407358907338503 5.38877404137709\n",
      "-0.09284685401211656 5.427052674749528\n",
      "-0.09093529378432064 5.487700662917937\n",
      "-0.08730555768937709 5.6063867288654246\n",
      "-0.08510451776671468 5.680741695531406\n",
      "-0.0801506465236177 5.855259288894152\n",
      "-0.07888958321340844 5.901381029369314\n",
      "-0.07631976640692417 5.997661310693022\n",
      "-0.07567026022164325 6.022501680225787\n",
      "-0.07543517202023664 6.031544500964281\n",
      "-0.07497431934511112 6.049352456746846\n",
      "-0.07132025665411179 6.194498676691586\n",
      "-0.0712619061214772 6.19687556480098\n",
      "-0.0621838204196532 6.592292320004527\n",
      "-0.06203819019250889 6.599091176774213\n",
      "-0.055287633123298985 6.932965208187793\n",
      "-0.05453721695020941 6.972550740576291\n",
      "-0.05243467003212965 7.086411908066587\n",
      "-0.050261274425404734 7.208978269131467\n",
      "-0.045652230365068824 7.487334158461812\n",
      "-0.04523180575881125 7.514101748198834\n",
      "-0.045087233079868616 7.52336361932249\n",
      "-0.04427356831635443 7.576047522424207\n",
      "-0.04215724103713603 7.717723646730618\n",
      "-0.039781448938477126 7.885461501709488\n",
      "-0.03960164309941194 7.898559621976151\n",
      "-0.03257617562948956 8.463009167744369\n",
      "-0.03067377227334589 8.636861753397543\n",
      "-0.02997496963135604 8.703437196023318\n",
      "-0.02827959695965121 8.871619136838884\n",
      "-0.027216226754827044 8.98232082297187\n",
      "-0.025025479790692362 9.224678245496804\n",
      "-0.02258289218899079 9.521235679660549\n",
      "-0.02191536999979249 9.607867008260929\n",
      "-0.018736229700298646 10.060339380014078\n",
      "-0.011889406571026884 11.373049727370466\n",
      "-0.010544020774902574 11.71961089707352\n",
      "-0.009863888801644682 11.91203092463786\n",
      "-0.009293672982454204 12.08386722878808\n",
      "-0.00783568280350333 12.576295005224482\n",
      "-0.007012023682803514 12.89677477410104\n",
      "-0.00690052395097851 12.943027676867725\n",
      "-0.0061579210698161635 13.271571389275236\n",
      "-0.005604410009902194 13.543345650063864\n",
      "-0.0022732828729954413 16.14696669909923\n",
      "-0.0017633563720784728 16.879876946948773\n",
      "-0.00026180848783052824 22.38336338754723\n",
      "0.002802618448648264 15.54296526589659\n",
      "0.009876200162191706 11.908431377321065\n",
      "0.010933640050372118 11.614897481498566\n",
      "0.014844983466974382 10.73230043397808\n",
      "0.015919841871257168 10.53053592684028\n",
      "0.020358756396169797 9.820580257654312\n",
      "0.02811680640249503 8.888294308457313\n",
      "0.029968146499587123 8.704094845492644\n",
      "0.032516459645937745 8.468310686183575\n",
      "0.03542662360800253 8.220606872879955\n",
      "0.03671724327091885 8.117182398363834\n",
      "0.038367435431398755 7.990098013629239\n",
      "0.04049942488909308 7.833741098174609\n",
      "0.04293464508257405 7.664874924058176\n",
      "0.0429568296366627 7.663380810795576\n",
      "0.04407933166547817 7.588766543242127\n",
      "0.04520045709836551 7.516107558657922\n",
      "0.04630837562208168 7.446044766588089\n",
      "0.047502152497545636 7.372394294226124\n",
      "0.048306894628800334 7.323776708597522\n",
      "0.05273546345350244 7.069847774241199\n",
      "0.05464380090438414 6.966895464333344\n",
      "0.05888736403206929 6.750196098423754\n",
      "0.06093572478289566 6.651079401417009\n",
      "0.06481547849790825 6.472075349369398\n",
      "0.06755254647627296 6.352063364264896\n",
      "0.0693855654241522 6.274350813792097\n",
      "0.07877917161550152 5.905453714163623\n",
      "0.07927669118386582 5.887146251032372\n",
      "0.0796363156007307 5.873983335059137\n",
      "0.08305679689098033 5.751635404837313\n",
      "0.08449266350475493 5.701747212273808\n",
      "0.08476681418849119 5.69231693127598\n",
      "0.08728497050608963 5.6070736247937285\n",
      "0.08740180598489222 5.603177493348104\n",
      "0.08844938921207302 5.568470477164282\n",
      "0.08888795681976314 5.554060259254287\n",
      "0.08956943200818435 5.531806691752851\n",
      "0.0901342082695733 5.513489560780984\n",
      "0.09354407349523353 5.405236464277287\n",
      "0.09365209602353053 5.401870681424807\n",
      "0.09400412123939184 5.390928582237116\n",
      "0.09501950418288718 5.359590759682905\n",
      "0.0960885824180957 5.326948500885703\n",
      "0.09617257055793771 5.3243991816061325\n",
      "0.09832367026322175 5.259838698161833\n",
      "0.10768418440239769 4.994105101551049\n",
      "0.10937260524713288 4.948587314435092\n",
      "0.11121584616794111 4.899671869666019\n",
      "0.11147005860928605 4.89298776218009\n",
      "0.11293317166453876 4.854804054573368\n",
      "0.11565104955747696 4.785136145189147\n",
      "0.11831972654693201 4.71826345084252\n",
      "0.1201589652525088 4.673023953049451\n",
      "0.1204806338009985 4.665181159422316\n",
      "0.12247418912355723 4.617024375544194\n",
      "0.12469156256453728 4.564347621046434\n",
      "0.12610814605197151 4.531168859252067\n",
      "0.12647968035263446 4.522526789912983\n",
      "0.12835977184098746 4.4791692234724705\n",
      "0.13005398902059073 4.440622669132593\n",
      "0.13029862270281267 4.435097076424212\n",
      "0.13051466188743888 4.430225710799677\n",
      "0.14007982186335965 4.2220198493484515\n",
      "0.1403481738943837 4.216379233592974\n",
      "0.14622114810714115 4.0954754649580245\n",
      "0.14691240063698707 4.081554979041206\n",
      "0.14701469798735856 4.079500286355925\n",
      "0.14950867503754695 4.0298301860842125\n",
      "0.14963720423241966 4.027292131691493\n",
      "0.15222021444551315 3.9767270233994854\n",
      "0.15265506551945807 3.9682959274798346\n",
      "0.15596152566996846 3.904936847519393\n",
      "0.15693889425603613 3.886456458243003\n",
      "0.1581969146123079 3.8628322224431337\n",
      "0.15825772764507362 3.8616948193826417\n",
      "0.1587720900447398 3.852091390725342\n",
      "0.16051769393691084 3.8197223417677586\n",
      "0.16080589670673184 3.814410834944607\n",
      "0.16093834017900277 3.8119730135192005\n",
      "0.16106940391467184 3.809562492929111\n",
      "0.16525378559064974 3.733581630916407\n",
      "0.16639681020011032 3.7131490743365787\n",
      "0.16797104396102336 3.685228846201484\n",
      "0.16802105855518623 3.6843459430658125\n",
      "0.16846379479765794 3.6765413981530988\n",
      "0.16994728934098702 3.650533871582063\n",
      "0.1703839674859946 3.6429200747600907\n",
      "0.17319910220046064 3.59428350712772\n",
      "0.17374430250098327 3.584952426825959\n",
      "0.1746806335163016 3.5689929069720576\n",
      "0.1757871246084819 3.5502391554729997\n",
      "0.17742658581396498 3.5226606395177633\n",
      "0.18071796683584473 3.4680291911116146\n",
      "0.18306950040820125 3.429583180113075\n",
      "0.18369395656380538 3.419453929440993\n",
      "0.18662632458173567 3.372329099891738\n",
      "0.19095420602939872 3.304070304252776\n",
      "0.19393343922220496 3.257947401447322\n",
      "0.19537204992344548 3.2359209530254858\n",
      "0.19537330282175236 3.235901838647824\n",
      "0.1995260575307236 3.1731915946549996\n",
      "0.20132721645714047 3.1463858001709055\n",
      "0.20185418755864037 3.138587222374562\n",
      "0.20448288563789685 3.09997898490421\n",
      "0.2102598585995883 3.0168037674572585\n",
      "0.2130471239937033 2.9774686353266677\n",
      "0.21507688206884468 2.9491395274632133\n",
      "0.215911942089271 2.9375607016309337\n",
      "0.21978633860868646 2.8844077468932814\n",
      "0.22347272047661093 2.8346832616510396\n",
      "0.22572637193010148 2.8046815190528473\n",
      "0.22593168556431653 2.801963011499392\n",
      "0.2270175536530974 2.7876258266924903\n",
      "0.22752969899624453 2.7808872970404925\n",
      "0.22759117428652909 2.7800794487969163\n",
      "0.2283186254692131 2.7705363803067327\n",
      "0.23155096298458933 2.728494614240234\n",
      "0.23328999958027463 2.706116392408996\n",
      "0.23524356529206236 2.681174983776032\n",
      "0.23911108074853726 2.632402650242875\n",
      "0.24050450375464316 2.6150239939145195\n",
      "0.2425837190959954 2.5892794335391964\n",
      "0.24287239374239378 2.5857226727465052\n",
      "0.24419992715564653 2.5694208039269117\n",
      "0.24426047339734502 2.5686794403128985\n",
      "0.24654492629335478 2.540842090376418\n",
      "0.24710001453667418 2.5341174341564874\n",
      "0.2503013162779584 2.4956316072009015\n",
      "0.2510439031257099 2.486775784248998\n",
      "0.2518741359523393 2.4769062726387454\n",
      "0.25453826509788113 2.4454588819423577\n",
      "0.2546592651643049 2.4440385999570946\n",
      "0.25934837303371805 2.3895256781085217\n",
      "0.25998329062484515 2.3822225920334477\n",
      "0.2622755103003074 2.3560091320537566\n",
      "0.26371684603372403 2.339647552503328\n",
      "0.26441352546739605 2.331772349234485\n",
      "0.264928614473416 2.32596369905762\n",
      "0.2658818684119928 2.315244842039165\n",
      "0.2674049715530804 2.2982012876028386\n",
      "0.268266817341009 2.2886021355052417\n",
      "0.2697899254198861 2.2717166945525005\n",
      "0.271471127650287 2.25319452324439\n",
      "0.27311162921431364 2.235236951843816\n",
      "0.27696202368511424 2.1935338331937215\n",
      "0.2770202877428307 2.1929075252565546\n",
      "0.27862416236435217 2.1757216786226987\n",
      "0.2789456884334147 2.1722891782150624\n",
      "0.2796428067757972 2.164861505154649\n",
      "0.28481737845285493 2.1103418796323314\n",
      "0.2859308832277183 2.098749828623797\n",
      "0.28732166584707586 2.0843398853036974\n",
      "0.2882085758736461 2.07519019402161\n",
      "0.2898555029598435 2.0582811822853504\n",
      "0.2944203218137511 2.011960188863356\n",
      "0.2963681622972796 1.9924357955652459\n",
      "0.2989479694548831 1.9667956587936293\n",
      "0.2994259257479108 1.9620725275264745\n",
      "0.30184141506546336 1.9383316905753543\n",
      "0.3025660076479568 1.9312516960519588\n",
      "0.3084195540784276 1.8747535180203407\n",
      "0.30907048735803966 1.8685465639012506\n",
      "0.3112727932446886 1.8476576967155196\n",
      "0.31178312325031876 1.842841590840189\n",
      "0.31219589895782063 1.8389528079033137\n",
      "0.31315714702685726 1.8299199306712257\n",
      "0.3161719021553935 1.8017985357701698\n",
      "0.317712579086036 1.7875483202262448\n",
      "0.3194910524360177 1.7711996514275994\n",
      "0.32418237493158597 1.728588523998266\n",
      "0.3280309871664362 1.6941811043728006\n",
      "0.32846177883957295 1.6903602111168057\n",
      "0.33029404584547084 1.6741770217254235\n",
      "0.33224982099019784 1.6570239365625916\n",
      "0.33297621547700373 1.6506847530427475\n",
      "0.3354702186616856 1.6290494663491173\n",
      "0.3372630274856583 1.6136203572006913\n",
      "0.3379424604443646 1.6077998953781203\n",
      "0.3413536764324152 1.5787985056158087\n",
      "0.34219669950713016 1.5716878815574054\n",
      "0.34367831035920005 1.55924496647834\n",
      "0.3468413633604517 1.5329098521700306\n",
      "0.34751199557175827 1.5273661090501274\n",
      "0.3485720492657485 1.518631521603418\n",
      "0.34962953266858277 1.5099525389190969\n",
      "0.35011136479163807 1.5060094165259015\n",
      "0.35182443467695235 1.4920477971903399\n",
      "0.3553417109009891 1.4636613372029246\n",
      "0.35964781849270855 1.4294156897475971\n",
      "0.3601235190181604 1.4256665044554704\n",
      "0.36169019375416167 1.4133664136873774\n",
      "0.36422992929647613 1.3935808933615266\n",
      "0.3647759264018666 1.3893521605277317\n",
      "0.36521921801590773 1.3859253121706891\n",
      "0.37021683234329483 1.3476881871182098\n",
      "0.37112828940033005 1.3407926972817352\n",
      "0.3712937324982515 1.3395436335969757\n",
      "0.37175202901939275 1.3360877020704989\n",
      "0.3725167887940455 1.3303342489167975\n",
      "0.37418402656769945 1.317849472763068\n",
      "0.37736427582453036 1.2942550813390943\n",
      "0.38184624667063094 1.2614899894906355\n",
      "0.3843640023051551 1.2433318583296986\n",
      "0.3854417966494812 1.2356128847558363\n",
      "0.38591854523875346 1.232208809897723\n",
      "0.3873835492271578 1.2217879003081105\n",
      "0.3912582049845519 1.1945125083984565\n",
      "0.3920974400111996 1.18865917328376\n",
      "0.39213543128307426 1.1883946556948848\n",
      "0.4094257673389814 1.0720579188190702\n",
      "0.4100294550072692 1.068140079444556\n",
      "0.4103686423425059 1.065943013192788\n",
      "0.41071478324384625 1.0637040232251573\n",
      "0.4114189428896915 1.0591589221874702\n",
      "0.4136915637248517 1.0445785661183449\n",
      "0.4145935227327362 1.0388293045690755\n",
      "0.41502150872544763 1.036108653990854\n",
      "0.41546950115864045 1.0332659309168895\n",
      "0.4161373259356358 1.0290379525314062\n",
      "0.42052650167920946 1.0015375531450637\n",
      "0.4249208822766948 0.9745008818428749\n",
      "0.4264656783755445 0.9651135609208945\n",
      "0.4268687760877481 0.9626740207528753\n",
      "0.42934160749259775 0.9477985919629565\n",
      "0.4334531490985021 0.9234067516824126\n",
      "0.43392830150174255 0.9206152389031715\n",
      "0.43570237254985233 0.9102424409784801\n",
      "0.43613087223388547 0.9077488172754725\n",
      "0.4362126109127118 0.9072736639383205\n",
      "0.4369610484928841 0.9029306650709633\n",
      "0.4377065485031242 0.8986185481735762\n",
      "0.4404784816239202 0.8827059162319472\n",
      "0.4409197756339993 0.8801901275437\n",
      "0.4427917644167525 0.8695713423179345\n",
      "0.4437252527392488 0.8643083276174137\n",
      "0.44665754103123567 0.8479147196491461\n",
      "0.45698149107896 0.7918539628595902\n",
      "0.4588606778085056 0.7819244854865438\n",
      "0.45983472221752275 0.776810707834711\n",
      "0.4606318263914311 0.7726425937061758\n",
      "0.4620031035693484 0.7655072231842007\n",
      "0.46646615263509106 0.7425902081163172\n",
      "0.4683915167100978 0.7328476383832598\n",
      "0.4805712407738896 0.6731992539493197\n",
      "0.48369118926807153 0.658463684153779\n",
      "0.4845761703204876 0.6543238050655996\n",
      "0.48483503686348817 0.6531161684231381\n",
      "0.48500750995623876 0.6523123993210008\n",
      "0.48564521691649176 0.6493463077761091\n",
      "0.4858908046036854 0.6482064625640998\n",
      "0.4893052187270366 0.6324985701380745\n",
      "0.49420361639846155 0.6104149627085437\n",
      "0.4986062030542717 0.5910160160739965\n",
      "0.500312383675724 0.5836115331926854\n",
      "0.504530503533192 0.5655753793223713\n",
      "0.5093001890734048 0.5456393810372712\n",
      "0.5126998888762337 0.5317236442292723\n",
      "0.5135810921445243 0.5281563256202303\n",
      "0.5201157843173336 0.5022069993486918\n",
      "0.5221138468105979 0.49444878632686534\n",
      "0.5261689953320778 0.47895397643369836\n",
      "0.5276802886798131 0.4732646709730768\n",
      "0.5303043325454844 0.46349571209253243\n",
      "0.5310818958348702 0.46062747958051553\n",
      "0.5422837200742594 0.4206348902574886\n",
      "0.5455466350317084 0.409446166723819\n",
      "0.5461586081914414 0.40737048066560644\n",
      "0.546574466932986 0.4059640676098541\n",
      "0.5466231764389204 0.40579955103648024\n",
      "0.5475208304445627 0.40277583709790915\n",
      "0.5479430398469594 0.4013589576256241\n",
      "0.5498119200566094 0.39512796732225963\n",
      "0.5499554767170685 0.3946520797131756\n",
      "0.5507656389039157 0.39197371873465736\n",
      "0.5555638372910645 0.376364262942\n",
      "0.5589728041283386 0.36553509062547335\n",
      "0.5607806310808159 0.3598792509731057\n",
      "0.5613352703801999 0.35815606618269324\n",
      "0.5632216395367231 0.3523374433225608\n",
      "0.5637904218191014 0.35059571317131877\n",
      "0.5690578408624936 0.3347433999582873\n",
      "0.5745209866222261 0.3188251089502244\n",
      "0.5773769532627715 0.31071239570744014\n",
      "0.5791600973298348 0.30571908355649885\n",
      "0.5867646937334576 0.28503625923620357\n",
      "0.5869172067115 0.2846314934910631\n",
      "0.5876253419974997 0.28275723944757736\n",
      "0.588618555319911 0.28014262128345035\n",
      "0.5903936074810268 0.27551084904395373\n",
      "0.5914906812133514 0.2726743626120916\n",
      "0.5919330325693264 0.2715362997846082\n",
      "0.5924951854192624 0.2700946800326043\n",
      "0.5953377884465898 0.262884494328763\n",
      "0.5961270197141983 0.26090607642504926\n",
      "0.5965196856689357 0.25992553272061003\n",
      "0.6004161935356973 0.2503306068160269\n",
      "0.6005535202247325 0.2499969056412168\n",
      "0.6005967006002515 0.24989204058462428\n",
      "0.6028866116526583 0.24437356275645047\n",
      "0.6049154677002571 0.23955378083854892\n",
      "0.6071231746380792 0.23438278979312424\n",
      "0.6089635735381929 0.23013036368331036\n",
      "0.6090734069371762 0.22987824834436887\n",
      "0.6095669003973556 0.22874777482792574\n",
      "0.6114556260023352 0.22445593570687633\n",
      "0.6118402268124132 0.22358872264996912\n",
      "0.6138662603751597 0.21905774931694588\n",
      "0.61668276569971 0.21286274806587818\n",
      "0.619460315329009 0.20687048292382007\n",
      "0.6211007184206399 0.203385572915639\n",
      "0.6222674233196046 0.2009312602965873\n",
      "0.6241003775899898 0.19711588970041638\n",
      "0.6244997255951175 0.1962911643591726\n",
      "0.6278911658943194 0.18938075849033426\n",
      "0.6280926071579165 0.18897553842725373\n",
      "0.630339823519154 0.18449453339897162\n",
      "0.6320069772751633 0.18121677672428574\n",
      "0.6327843364377146 0.17970190660776847\n",
      "0.6349351143596309 0.1755549950327094\n",
      "0.637303852228317 0.17106274980467043\n",
      "0.638788577847327 0.16828671324509994\n",
      "0.6401016403349398 0.16585695117880167\n",
      "0.646286564032335 0.15472752973548204\n",
      "0.6471395244353486 0.15323303405650365\n",
      "0.6493585820081933 0.149390187529304\n",
      "0.6499894677787699 0.14830951774364093\n",
      "0.6510544719552602 0.14649708004302398\n",
      "0.6536894572509679 0.14207639808802117\n",
      "0.6572690505047105 0.13621447077240423\n",
      "0.660196148227318 0.13154228768805104\n",
      "0.6662514746679526 0.12221663335793087\n",
      "0.6669545929519265 0.12116303072131267\n",
      "0.6671153820070723 0.12092293902833126\n",
      "0.669656701931628 0.11716982718084473\n",
      "0.6703655316204462 0.11613688931746914\n",
      "0.6721538131090754 0.11355763628223295\n",
      "0.6733244559431926 0.11188980944268946\n",
      "0.6757658489419254 0.10846354015327826\n",
      "0.6853088036609887 0.09573163352429721\n",
      "0.685634866223835 0.09531484303999324\n",
      "0.6888585081752172 0.09125769610547563\n",
      "0.6902679367014981 0.08951978337426601\n",
      "0.6933133558532802 0.08583833121388022\n",
      "0.694061291810331 0.08494947943919791\n",
      "0.6954574714455886 0.08330624649266245\n",
      "0.6969160341818024 0.08161170872027852\n",
      "0.7055486722107545 0.07203642527853875\n",
      "0.7063408248448251 0.07119597850864567\n",
      "0.7145618397791342 0.06284176908102151\n",
      "0.720048358083244 0.05763069805821803\n",
      "0.7202577940364505 0.05743741762036483\n",
      "0.7207754526275565 0.05696145079005376\n",
      "0.7217161618360592 0.05610290359136878\n",
      "0.7239118927348109 0.05413086105180329\n",
      "0.7248914303632332 0.05326542131519905\n",
      "0.7257505980735701 0.05251354625308315\n",
      "0.7265732349732827 0.05179992953543536\n",
      "0.7286028102691879 0.050065467096016954\n",
      "0.7317855637312196 0.0474196238396466\n",
      "0.734732095342437 0.04504966897178722\n",
      "0.7363811418141699 0.043756208218944055\n",
      "0.7380515021750591 0.042469828712316825\n",
      "0.7385679130163303 0.04207694334622932\n",
      "0.7398841799574092 0.0410857441220603\n",
      "0.7399377663739493 0.0410457013255184\n",
      "0.7404522772098328 0.04066245918641884\n",
      "0.7408242824937377 0.04038674946722024\n",
      "0.7418271746662228 0.039649230979473\n",
      "0.7423062881882057 0.03929985588747066\n",
      "0.7452013913385016 0.03722912283198204\n",
      "0.7515680947177303 0.032914888391857446\n",
      "0.7526399821339138 0.03222032705097406\n",
      "0.7528132808088497 0.032108880782881884\n",
      "0.7547001406515756 0.03091064607165872\n",
      "0.7553444336653734 0.030507829958512707\n",
      "0.762817602218796 0.02606685855966507\n",
      "0.7636934370404038 0.02557379609615743\n",
      "0.7636965669040272 0.02557204424611027\n",
      "0.7641816669966854 0.025301394859700392\n",
      "0.7645660114817103 0.025088186767807035\n",
      "0.7646753365854988 0.02502773849620889\n",
      "0.7649520407814843 0.024875133582805593\n",
      "0.7660284896110592 0.024286778546994816\n",
      "0.7678725653664507 0.02329839743748599\n",
      "0.7686308435194058 0.022899085436308295\n",
      "0.7767452280444751 0.018879989510792068\n",
      "0.7784580776150682 0.018089765835528175\n",
      "0.781736163104735 0.01663250380532327\n",
      "0.7836514527084426 0.015814101312802493\n",
      "0.7847922286671338 0.01533808066007989\n",
      "0.7848511360381354 0.01531373021953819\n",
      "0.7869553707503443 0.01445864210684504\n",
      "0.7886727388817123 0.013781845678713012\n",
      "0.7942766521436357 0.011702718045377397\n",
      "0.7979972303512031 0.01042925887014701\n",
      "0.7983212627854466 0.010322311225069958\n",
      "0.8042643628361015 0.008471046752470807\n",
      "0.8088162867665252 0.007191441627173008\n",
      "0.8123813444662091 0.006270617984692927\n",
      "0.8140213595522947 0.005870530582875094\n",
      "0.8156476706908002 0.005488209644346817\n",
      "0.817762413342924 0.0050123038971291835\n",
      "0.819623661171275 0.004613076486085116\n",
      "0.8213952396725062 0.004249947907915284\n",
      "0.8217659176560765 0.0041760315479531405\n",
      "0.8223212109719262 0.004066629587560421\n",
      "0.8256252098215271 0.003448329744890442\n",
      "0.828850116984367 0.00289791769913034\n",
      "0.8317263954793466 0.002450364032889065\n",
      "0.8335629993698412 0.002185604784872869\n",
      "0.8348571373336566 0.0020087565814626658\n",
      "0.8382762390786622 0.0015796001458026883\n",
      "0.839039887369246 0.0014912056926547775\n",
      "0.8409611270252606 0.001280697756233774\n",
      "0.841819546947904 0.0011920947457897096\n",
      "0.8453229576274839 0.0008649256915657723\n",
      "0.8478483707494306 0.0006628809319913537\n",
      "0.8488116750020487 0.0005931557178341773\n",
      "0.8511137318380408 0.0004427439489191146\n",
      "0.8546374536708161 0.00025612043734032875\n",
      "0.8557200798872964 0.0002092123593089299\n",
      "0.8557611816264874 0.00020752708727110386\n",
      "0.8606680261594242 5.5897852753423475e-05\n",
      "0.8624000794773359 2.5494408548185157e-05\n",
      "0.8625265070801225 2.374031775389704e-05\n",
      "0.878332010933659 0.00028318454726602803\n",
      "0.8788859640609843 0.000308858231453394\n",
      "0.8794498174566139 0.0003360986658285375\n",
      "0.879490019419823 0.0003380834412447583\n",
      "0.8811789449547314 0.00042655762588741003\n",
      "0.882606916427517 0.0005090641402433842\n",
      "0.8845928882592098 0.0006354030535131446\n",
      "0.8862455656421757 0.0007506972449434463\n",
      "0.8896093955224502 0.0010133968303042216\n",
      "0.8917096365264148 0.0011961566039335688\n",
      "0.8946035636270688 0.0014711127550036905\n",
      "0.8959103991057544 0.0016039299817053644\n",
      "0.8968014086929474 0.0016975343898431646\n",
      "0.8981538556754096 0.001844297689890355\n",
      "0.8993395218266853 0.0019775683522352838\n",
      "0.9011342322624016 0.0021873968065363983\n",
      "0.9030624811284789 0.002423579164509354\n",
      "0.9030718855881914 0.0024247580896431717\n",
      "0.9038449945397582 0.00252256447350216\n",
      "0.9054989522083197 0.002737683704900016\n",
      "0.9056728532697418 0.002760764176312562\n",
      "0.9076477195976158 0.003028986203155395\n",
      "0.9098314155066434 0.003338515033886175\n",
      "0.9113122764005368 0.0035560473952984395\n",
      "0.9115636118538628 0.0035935734658824555\n",
      "0.9116721629404627 0.00360983499296198\n",
      "0.9117780006230563 0.003625721435753835\n",
      "0.9145157026145758 0.004047348647333318\n",
      "0.9175268070356801 0.004534511230437829\n",
      "0.9211762663795904 0.005157079926563941\n",
      "0.9303375464148569 0.006868925714954736\n",
      "0.9314199991453447 0.007084771727936622\n",
      "0.9360937479334985 0.00804843685022384\n",
      "0.9404007622257176 0.00898087584510627\n",
      "0.9420025338179523 0.00933821011423656\n",
      "0.9434762334976703 0.009671932551309692\n",
      "0.9466195983245747 0.010399373309324349\n",
      "0.9472700460051295 0.010552519045351436\n",
      "0.9527721108857488 0.011882993743507153\n",
      "0.953057651550447 0.011953722006672949\n",
      "0.9573078553106082 0.013025569826948746\n",
      "0.9574852997498045 0.013071086472944235\n",
      "0.9585530394606165 0.013346253710959634\n",
      "0.9620939529711914 0.014274276165215746\n",
      "0.9633551962163862 0.01461048726666087\n",
      "0.9707722878254457 0.016645778526475318\n",
      "0.9740666834466365 0.017580577217771596\n",
      "0.9792184497679182 0.019078704744572895\n",
      "0.9802109911298529 0.019372284160606337\n",
      "0.9806016378749247 0.019488262100239295\n",
      "0.9810505397500717 0.019621833733342132\n",
      "0.9840015642274929 0.020507776148642003\n",
      "0.98827027186536 0.021812911047697923\n",
      "0.9886217402598401 0.021921586745220635\n",
      "0.9910157348471242 0.022666641495663283\n",
      "0.9911928070090803 0.022722080553522195\n",
      "0.9949691347252534 0.023915049496871992\n",
      "0.9969040849883399 0.024534057301401943\n",
      "0.9972295144072054 0.024638671082411145\n",
      "0.9976229353971329 0.02476533466586868\n",
      "0.9990659726658317 0.025231723059649564\n"
     ]
    }
   ],
   "source": [
    "K = 1\n",
    "T = 2\n",
    "Noise_Alloc = [0,2]\n",
    "sigma = 1\n",
    "\n",
    "N = 2\n",
    "\n",
    "j_array = np.array(range(K+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K+T)))\n",
    "\n",
    "i_array = np.array(range(N))\n",
    "beta_array = np.cos(i_array*2*math.pi/(N-1)/2) # np.cos(i_array*2*math.pi/N/2)\n",
    "# print(\"z_array: \",z_array,'\\n')\n",
    "\n",
    "N = 1000\n",
    "z_array = np.random.uniform(-1,1,N) #np.cos(i_array[1:]*2*math.pi/(K+T)/2) # np.cos(i_array*2*math.pi/N/2)\n",
    "z_array = np.sort(z_array)\n",
    "MIS_array = np.zeros((N))\n",
    "MIS_LCC_array = np.zeros((N))\n",
    "# print(z_array)\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "B = [0.5, 1, 1.5, 2]\n",
    "\n",
    "z_array_0 = []\n",
    "z_array_1 = []\n",
    "z_array_2 = []\n",
    "z_array_3 = []\n",
    "z_array_4 = []\n",
    "\n",
    "for j in range(len(z_array)):\n",
    "    MIS_array[j] = MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_array[j]], 1,sigma)\n",
    "    MIS_LCC_array[j] = MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_array[j]], 1,sigma, _is_LCC=True)\n",
    "    \n",
    "    if MIS_array[j] < B[0]:\n",
    "        z_array_0.append(z_array[j])\n",
    "    elif MIS_array[j] < B[1]:\n",
    "        z_array_1.append(z_array[j])\n",
    "    elif MIS_array[j] < B[2]:\n",
    "        z_array_2.append(z_array[j])\n",
    "    elif MIS_array[j] < B[3]:\n",
    "        z_array_3.append(z_array[j])\n",
    "    else:\n",
    "        z_array_4.append(z_array[j])\n",
    "#     print('(beta index, MIS) = ',j,',',MIS_array[j])\n",
    "#     print()\n",
    "\n",
    "\n",
    "\n",
    "print(len(z_array_0),len(z_array_1),len(z_array_2),len(z_array_3),len(z_array_4))\n",
    "\n",
    "\n",
    "plt.plot(z_array, MIS_array, label='Mutual Information Security, BACC')\n",
    "plt.plot(z_array, MIS_LCC_array, label='Mutual Information Security, LCC')\n",
    "plt.plot(alpha_array[Signal_Alloc],0*np.ones(len(Signal_Alloc)),'g*',label='alpha_i, for X')\n",
    "plt.plot(alpha_array[Noise_Alloc],0*np.ones(len(Noise_Alloc)),'r*',label='alpha_i, for N')\n",
    "# plt.plot(beta_array,0*np.ones(len(beta_array)),'b.',label='beta_i')\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('MIS')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "    \n",
    "for i in range(len(z_array)):\n",
    "    print(z_array[i],MIS_array[i])\n",
    "    \n",
    "# print(alpha_array[Signal_Alloc])\n",
    "# print(alpha_array[Noise_Alloc])\n",
    "# print(alpha_array)\n",
    "\n",
    "# plt.plot((2*j_array[Signal_Alloc]+1)/(K+T),alpha_array[Signal_Alloc],'g*',label='alpha_i, for X')\n",
    "# plt.plot((2*j_array[Noise_Alloc]+1)/(K+T),alpha_array[Noise_Alloc],'r*',label='alpha_i, for N')\n",
    "# plt.plot(2*i_array[1:]/(K+T), z_array,'b.',label='beta_i')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1 2\n",
      "##########################################\n",
      "###### 0 -th Trial!! ###########\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 958/10000 (9.58%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3025 \n",
      "Accuracy: 794/10000 (7.94%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 974/10000 (9.74%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3024 \n",
      "Accuracy: 1135/10000 (11.35%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 974/10000 (9.74%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3027 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3025 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3024 \n",
      "Accuracy: 974/10000 (9.74%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3025 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 974/10000 (9.74%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3025 \n",
      "Accuracy: 1030/10000 (10.30%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1028/10000 (10.28%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3024 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3025 \n",
      "Accuracy: 974/10000 (9.74%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3011 \n",
      "Accuracy: 1048/10000 (10.48%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3006 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.2846 \n",
      "Accuracy: 4591/10000 (45.91%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.1880 \n",
      "Accuracy: 6945/10000 (69.45%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.9889 \n",
      "Accuracy: 8943/10000 (89.43%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.9235 \n",
      "Accuracy: 9103/10000 (91.03%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.9263 \n",
      "Accuracy: 9118/10000 (91.18%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.9325 \n",
      "Accuracy: 9095/10000 (90.95%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.9255 \n",
      "Accuracy: 9203/10000 (92.03%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.9287 \n",
      "Accuracy: 9197/10000 (91.97%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.9232 \n",
      "Accuracy: 9227/10000 (92.27%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.9331 \n",
      "Accuracy: 9181/10000 (91.81%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.9318 \n",
      "Accuracy: 9214/10000 (92.14%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "N = 8\n",
    "K = 4\n",
    "G = 4\n",
    "\n",
    "size_per_group = int(60000/G)\n",
    "\n",
    "X_group = np.reshape(encoding_input_array_np, (G,size_per_group,28*28))\n",
    "y_group = np.reshape(encoding_label_array_np, (G,size_per_group,args.num_classes)) \n",
    "\n",
    "\n",
    "N_i = int(N/G) # = 2\n",
    "K_i = int(K/G) # = 2\n",
    "T = 2\n",
    "sigma = 1\n",
    "Noise_Alloc = [0,2]\n",
    "m = N_i # number of selected workers (if there is no straggler, m=N_i)\n",
    "\n",
    "print(N_i,K_i,T)\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K_i+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K_i+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K_i+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "z_array = np.array([-0.521, 0.521])\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 30\n",
    "\n",
    "loss_test_arr_K4_G4_N8 = np.zeros((N_trials,N_epochs))\n",
    "acc_test_arr_K4_G4_N8  = np.zeros((N_trials,N_epochs))\n",
    "\n",
    "for trial_idx in range(N_trials):\n",
    "    \n",
    "    print('##########################################')\n",
    "    print('######',trial_idx,'-th Trial!! ###########')\n",
    "    \n",
    "    net_glob = CNNMnist2(args=args)\n",
    "    net_glob.cuda()\n",
    "    net_glob.train()\n",
    "    \n",
    "    # copy weights\n",
    "    w_glob = net_glob.state_dict()\n",
    "    \n",
    "    X_tilde = np.empty((N,Size_submatrices,28*28))\n",
    "    y_tilde = np.empty((N,Size_submatrices,10))\n",
    "    \n",
    "    for G_idx in range(G):\n",
    "        \n",
    "        _Noise_label = np.ones((size_per_group*T,10)) * 0.1\n",
    "        \n",
    "        X_tilde_tmp,a,b = BACC_Enc_Data_v3(X_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde_tmp,a,b = BACC_Enc_Data_v3(y_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "        \n",
    "        stt_pos = G_idx * N_i\n",
    "        end_pos = (G_idx+1) * N_i\n",
    "        \n",
    "        X_tilde[stt_pos:end_pos,:,:] = X_tilde_tmp\n",
    "        y_tilde[stt_pos:end_pos,:,:] = y_tilde_tmp\n",
    "        \n",
    "   \n",
    "\n",
    "    for iter in range(N_epochs): #args.epochs\n",
    "        \n",
    "        w_group_array = []\n",
    "        for G_idx in range(G):\n",
    "            w_locals, loss_locals = [], []\n",
    "            idxs_users = np.random.choice(range(N_i), m, replace=False)\n",
    "            idxs_users = np.sort(idxs_users)\n",
    "            print('selected users:',idxs_users)\n",
    "\n",
    "            coded_net = BACC_Enc_Model_withNoise_v3(net_glob.cuda(), N_i, K_i, T, 0.01, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "            dec_z_array = []\n",
    "            for idx in idxs_users: #for idx in range(N):\n",
    "        #         print(idx)\n",
    "                local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[G_idx*N_i+idx,:,:], label=y_tilde[G_idx*N_i+idx,:,:])\n",
    "                w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "#                 w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                w_locals.append(copy.deepcopy(w))\n",
    "                loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "            # update global weights\n",
    "            #w_glob = FedAvg(w_locals)\n",
    "            w_group = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "            \n",
    "            w_group_array.append(copy.deepcopy(w_group))\n",
    "        \n",
    "        w_glob = copy.deepcopy(w_group_array[0])\n",
    "        for k in w_glob.keys():\n",
    "            for G_idx in range(1,G):\n",
    "                w_glob[k] += w_group_array[G_idx][k]\n",
    "            w_glob[k] = torch.div(w_glob[k], len(w_group_array))\n",
    "        \n",
    "        # copy weight to net_glob\n",
    "        net_glob.load_state_dict(w_glob)\n",
    "\n",
    "        # print loss\n",
    "    #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "    #     loss_train_arr.append(loss_train)\n",
    "\n",
    "        acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "        acc_test_arr_K4_G4_N8[trial_idx][iter] = acc_test\n",
    "        loss_test_arr_K4_G4_N8[trial_idx][iter] = loss_test\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test_arr_K4_G2_N4 = acc_test_arr_G2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "B_sel = 0\n",
    "\n",
    "# plt.plot(acc_test_arr_v1[0,B_sel,0,0:30],label='N=2, B='+str(B_array[B_sel]))\n",
    "plt.plot(acc_test_arr_K4_G1[2,B_sel,0,0:30],label='w/o Grouping, K=4, N=4, G=1' )\n",
    "plt.plot(acc_test_arr_K4_G1[3,B_sel,0,0:30],label='w/o Grouping, K=4, N=8, G=1' )\n",
    "# plt.plot(acc_test_arr_v1[2,B_sel,0,0:30],label='N=6, B='+str(B_array[B_sel]))\n",
    "# plt.plot(acc_test_arr_v1[3,B_sel,0,0:30],label='N=8, B='+str(B_array[B_sel]))\n",
    "plt.plot(acc_test_arr_K4_G2_N4[0,0:30],label='w/ Grouping,   K=4, N=4, G=2')\n",
    "plt.plot(acc_test_arr_K4_G2_N8[0,0:30],label='w/ Grouping,   K=4, N=8, G=2')\n",
    "plt.plot(acc_test_arr_K4_G4_N8[0,0:30],label='w/ Grouping,   K=4, N=8, G=4')\n",
    "# plt.plot(acc_test_arr_G3[0,0:30],label='Grouping, N=6, G=3, B='+str(B_array[B_sel]))\n",
    "\n",
    "# plt.plot(acc_test_arr_v1[4,B_sel,0,0:30],label='N=7, B='+str(B_array[B_sel]))\n",
    "# plt.plot(acc_test_arr_v1[5,B_sel,0,0:30],label='N=8, B='+str(B_array[B_sel]))\n",
    "\n",
    "# plt.plot(plot_acc_v2[2,sigma_sel,:],label='T=10')\n",
    "# plt.plot(plot_acc_v2[3,sigma_sel,:],label='T=11')\n",
    "\n",
    "# plt.plot(acc_test_arr_uncoded,'r',label='uncoded')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "# plt.title(title_name)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Test accuracy(%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Without model encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z_array: [-0.94  -0.73  -0.534 -0.125  0.125  0.534  0.73   0.94 ]\n",
      "0.4486236179368535\n",
      "0.49983046821646326\n",
      "0.48963480280841937\n",
      "0.4857187736123886\n",
      "0.48571877361238697\n",
      "0.4896348028084205\n",
      "0.4998304682164621\n",
      "0.4486236179368556\n",
      "@BACC_Enc: N,K,T, m_i= 8 4 5 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 8 4 5 15000 \n",
      "\n",
      "(T, sigma)= 5 1 )  0 -th Trial!!\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   1, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 2.2877 \n",
      "Accuracy: 3944/10000 (39.44%)\n",
      "\n",
      "Round   2, Average loss 2.288 Test accuracy 39.440\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 2.0699 \n",
      "Accuracy: 7961/10000 (79.61%)\n",
      "\n",
      "Round   3, Average loss 2.070 Test accuracy 79.610\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.7928 \n",
      "Accuracy: 9264/10000 (92.64%)\n",
      "\n",
      "Round   4, Average loss 1.793 Test accuracy 92.640\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.7944 \n",
      "Accuracy: 9352/10000 (93.52%)\n",
      "\n",
      "Round   5, Average loss 1.794 Test accuracy 93.520\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.7905 \n",
      "Accuracy: 9338/10000 (93.38%)\n",
      "\n",
      "Round   6, Average loss 1.791 Test accuracy 93.380\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.7860 \n",
      "Accuracy: 9381/10000 (93.81%)\n",
      "\n",
      "Round   7, Average loss 1.786 Test accuracy 93.810\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.7889 \n",
      "Accuracy: 9403/10000 (94.03%)\n",
      "\n",
      "Round   8, Average loss 1.789 Test accuracy 94.030\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.7936 \n",
      "Accuracy: 9412/10000 (94.12%)\n",
      "\n",
      "Round   9, Average loss 1.794 Test accuracy 94.120\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.8002 \n",
      "Accuracy: 9412/10000 (94.12%)\n",
      "\n",
      "Round  10, Average loss 1.800 Test accuracy 94.120\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.7667 \n",
      "Accuracy: 9398/10000 (93.98%)\n",
      "\n",
      "Round  11, Average loss 1.767 Test accuracy 93.980\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.8284 \n",
      "Accuracy: 9358/10000 (93.58%)\n",
      "\n",
      "Round  12, Average loss 1.828 Test accuracy 93.580\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.8112 \n",
      "Accuracy: 9403/10000 (94.03%)\n",
      "\n",
      "Round  13, Average loss 1.811 Test accuracy 94.030\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.7871 \n",
      "Accuracy: 9352/10000 (93.52%)\n",
      "\n",
      "Round  14, Average loss 1.787 Test accuracy 93.520\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.9026 \n",
      "Accuracy: 9219/10000 (92.19%)\n",
      "\n",
      "Round  15, Average loss 1.903 Test accuracy 92.190\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.8916 \n",
      "Accuracy: 9286/10000 (92.86%)\n",
      "\n",
      "Round  16, Average loss 1.892 Test accuracy 92.860\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.7630 \n",
      "Accuracy: 9417/10000 (94.17%)\n",
      "\n",
      "Round  17, Average loss 1.763 Test accuracy 94.170\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.7902 \n",
      "Accuracy: 9397/10000 (93.97%)\n",
      "\n",
      "Round  18, Average loss 1.790 Test accuracy 93.970\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.7999 \n",
      "Accuracy: 9450/10000 (94.50%)\n",
      "\n",
      "Round  19, Average loss 1.800 Test accuracy 94.500\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.7676 \n",
      "Accuracy: 9429/10000 (94.29%)\n",
      "\n",
      "Round  20, Average loss 1.768 Test accuracy 94.290\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.7842 \n",
      "Accuracy: 9435/10000 (94.35%)\n",
      "\n",
      "Round  21, Average loss 1.784 Test accuracy 94.350\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.7863 \n",
      "Accuracy: 9451/10000 (94.51%)\n",
      "\n",
      "Round  22, Average loss 1.786 Test accuracy 94.510\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.8083 \n",
      "Accuracy: 9404/10000 (94.04%)\n",
      "\n",
      "Round  23, Average loss 1.808 Test accuracy 94.040\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.8002 \n",
      "Accuracy: 9399/10000 (93.99%)\n",
      "\n",
      "Round  24, Average loss 1.800 Test accuracy 93.990\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.8325 \n",
      "Accuracy: 9422/10000 (94.22%)\n",
      "\n",
      "Round  25, Average loss 1.832 Test accuracy 94.220\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.7755 \n",
      "Accuracy: 9407/10000 (94.07%)\n",
      "\n",
      "Round  26, Average loss 1.776 Test accuracy 94.070\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.7625 \n",
      "Accuracy: 9430/10000 (94.30%)\n",
      "\n",
      "Round  27, Average loss 1.763 Test accuracy 94.300\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.7824 \n",
      "Accuracy: 9419/10000 (94.19%)\n",
      "\n",
      "Round  28, Average loss 1.782 Test accuracy 94.190\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.7719 \n",
      "Accuracy: 9339/10000 (93.39%)\n",
      "\n",
      "Round  29, Average loss 1.772 Test accuracy 93.390\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "K = 4\n",
    "T = 5\n",
    "sigma = 1\n",
    "Noise_Alloc = [0,2,4,6,8]\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "\n",
    "N_array = [8]\n",
    "B_array = [0.5]\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 30\n",
    "\n",
    "\n",
    "\n",
    "loss_test_arr_K4_G1_v2 = np.zeros((len(N_array),len(B_array),N_trials,N_epochs))\n",
    "acc_test_arr_K4_G1_v2  = np.zeros((len(N_array),len(B_array),N_trials,N_epochs))\n",
    "\n",
    "for N_idx in range(len(N_array)):\n",
    "    \n",
    "    N = N_array[N_idx]\n",
    "    \n",
    "    \n",
    "           \n",
    "        \n",
    "    # print(\"alpha_array: \",alpha_array,'\\n')\n",
    "    \n",
    "    \n",
    "    for B_idx in range(len(B_array)):\n",
    "        \n",
    "        B = B_array[B_idx]\n",
    "        z_array = []\n",
    "#         while(len(z_array)<N):\n",
    "#             z_tmp = np.random.uniform(-1,1,1)\n",
    "#             MIS_tmp = MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_tmp], 1,sigma)\n",
    "#             if MIS_tmp < B and MIS_tmp > 0.1:\n",
    "#                 z_array.append(z_tmp[0])\n",
    "#         \n",
    "#         z_array = np.sort(z_array)\n",
    "\n",
    "        if N==2:\n",
    "            z_array = np.array([-0.81, 0.81])\n",
    "        elif N ==4:\n",
    "            z_array = np.array([-0.81, -0.22, 0.22, 0.81])\n",
    "        elif N ==5:\n",
    "            z_array = np.array([-0.81, -0.22, 0, 0.22, 0.81])\n",
    "        elif N ==6:\n",
    "            z_array = np.array([-0.9, -0.81, -0.22, 0.22, 0.81, 0.9])\n",
    "        elif N ==7:\n",
    "            z_array = np.array([-0.9, -0.82, -0.21, 0, 0.21, 0.82, 0.9])\n",
    "        else:\n",
    "            z_array = np.array([-0.94, -0.73, -0.534, -0.125, 0.125, 0.534, 0.73, 0.94])\n",
    "#             z_array = np.array([-0.9, -0.81, -0.22, -0.20, 0.20, 0.22, 0.81, 0.9])\n",
    "            \n",
    "        print('z_array:',z_array)\n",
    "        for j in range(len(z_array)):\n",
    "            print(MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_array[j]], 1,sigma))\n",
    "        \n",
    "        \n",
    "        _Noise_label = np.ones((15000*T,10)) * 0.1\n",
    "\n",
    "        X_tilde,a,b = BACC_Enc_Data_v3(encoding_input_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde,a,b = BACC_Enc_Data_v3(encoding_label_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "\n",
    "        m = N # m is the number of received result @ master\n",
    "    #     print('number of results:',m)\n",
    "\n",
    "        for trial_idx in range(N_trials):\n",
    "            print('(T, sigma)=',T,sigma,') ',trial_idx,'-th Trial!!')\n",
    "\n",
    "            net_glob = CNNMnist2(args=args)\n",
    "            net_glob.cuda()\n",
    "            net_glob.train()\n",
    "\n",
    "            # copy weights\n",
    "            w_glob = net_glob.state_dict()\n",
    "\n",
    "            for iter in range(N_epochs): #args.epochs\n",
    "                w_locals, loss_locals = [], []\n",
    "                idxs_users = np.random.choice(range(N), m, replace=False)\n",
    "                idxs_users = np.sort(idxs_users)\n",
    "                print('selected users:',idxs_users)\n",
    "                \n",
    "                coded_net = BACC_Enc_Model_withNoise_v3(net_glob.cuda(), N, K, T, 0.01, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "                dec_z_array = []\n",
    "                for idx in idxs_users: #for idx in range(N):\n",
    "            #         print(idx)\n",
    "                    local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[idx,:,:], label=y_tilde[idx,:,:])\n",
    "#                     w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "                    w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                    w_locals.append(copy.deepcopy(w))\n",
    "                    loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                    dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "                # update global weights\n",
    "                #w_glob = FedAvg(w_locals)\n",
    "                w_glob = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "\n",
    "                # copy weight to net_glob\n",
    "                net_glob.load_state_dict(w_glob)\n",
    "\n",
    "                # print loss\n",
    "            #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "            #     loss_train_arr.append(loss_train)\n",
    "\n",
    "                acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "                acc_test_arr_K4_G1_v2[N_idx][B_idx][trial_idx][iter] = acc_test\n",
    "                loss_test_arr_K4_G1_v2[N_idx][B_idx][trial_idx][iter] = loss_test\n",
    "                if iter % 1 ==0:\n",
    "                    print('Round {:3d}, Average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_test, acc_test))\n",
    "                #print(loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2 3\n",
      "##########################################\n",
      "###### 0 -th Trial!! ###########\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.2202 \n",
      "Accuracy: 4077/10000 (40.77%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.7585 \n",
      "Accuracy: 8011/10000 (80.11%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2700 \n",
      "Accuracy: 9568/10000 (95.68%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1997 \n",
      "Accuracy: 9657/10000 (96.57%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2082 \n",
      "Accuracy: 9679/10000 (96.79%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1947 \n",
      "Accuracy: 9698/10000 (96.98%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2030 \n",
      "Accuracy: 9723/10000 (97.23%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1999 \n",
      "Accuracy: 9734/10000 (97.34%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2276 \n",
      "Accuracy: 9734/10000 (97.34%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2207 \n",
      "Accuracy: 9753/10000 (97.53%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2127 \n",
      "Accuracy: 9746/10000 (97.46%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2390 \n",
      "Accuracy: 9750/10000 (97.50%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2313 \n",
      "Accuracy: 9753/10000 (97.53%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2331 \n",
      "Accuracy: 9752/10000 (97.52%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2413 \n",
      "Accuracy: 9751/10000 (97.51%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2361 \n",
      "Accuracy: 9747/10000 (97.47%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2493 \n",
      "Accuracy: 9745/10000 (97.45%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2544 \n",
      "Accuracy: 9746/10000 (97.46%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2613 \n",
      "Accuracy: 9750/10000 (97.50%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2671 \n",
      "Accuracy: 9745/10000 (97.45%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2708 \n",
      "Accuracy: 9740/10000 (97.40%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2621 \n",
      "Accuracy: 9755/10000 (97.55%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2776 \n",
      "Accuracy: 9752/10000 (97.52%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2759 \n",
      "Accuracy: 9750/10000 (97.50%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2884 \n",
      "Accuracy: 9743/10000 (97.43%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2711 \n",
      "Accuracy: 9742/10000 (97.42%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2798 \n",
      "Accuracy: 9745/10000 (97.45%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2802 \n",
      "Accuracy: 9742/10000 (97.42%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2836 \n",
      "Accuracy: 9732/10000 (97.32%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2929 \n",
      "Accuracy: 9737/10000 (97.37%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "G = 2\n",
    "N = 4 # N should be divisible by G\n",
    "K = 4 # N should be divisible by G\n",
    "\n",
    "size_per_group = int(60000/G)\n",
    "\n",
    "X_group = np.reshape(encoding_input_array_np, (G,size_per_group,28*28))\n",
    "y_group = np.reshape(encoding_label_array_np, (G,size_per_group,args.num_classes)) \n",
    "\n",
    "N_i = int(N/G) # = 2\n",
    "K_i = int(K/G) # = 2\n",
    "T = 3\n",
    "sigma = 1\n",
    "Noise_Alloc = [0,2,4]\n",
    "m = N_i # number of selected workers (if there is no straggler, m=N_i)\n",
    "\n",
    "print(N_i,K_i,T)\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K_i+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K_i+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K_i+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "z_array = np.array([-0.81, 0.81])\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 30\n",
    "\n",
    "loss_test_arr_G2_N4_v2 = np.zeros((N_trials,N_epochs))\n",
    "acc_test_arr_G2_N4_v2  = np.zeros((N_trials,N_epochs))\n",
    "\n",
    "for trial_idx in range(N_trials):\n",
    "    \n",
    "    print('##########################################')\n",
    "    print('######',trial_idx,'-th Trial!! ###########')\n",
    "    \n",
    "    net_glob = CNNMnist2(args=args)\n",
    "    net_glob.cuda()\n",
    "    net_glob.train()\n",
    "    \n",
    "    # copy weights\n",
    "    w_glob = net_glob.state_dict()\n",
    "    \n",
    "    X_tilde = np.empty((N,Size_submatrices,28*28))\n",
    "    y_tilde = np.empty((N,Size_submatrices,10))\n",
    "    \n",
    "    for G_idx in range(G):\n",
    "        \n",
    "        _Noise_label = np.ones((size_per_group*T,10)) * 0.1\n",
    "        \n",
    "        X_tilde_tmp,a,b = BACC_Enc_Data_v3(X_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde_tmp,a,b = BACC_Enc_Data_v3(y_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "        \n",
    "        stt_pos = G_idx * N_i\n",
    "        end_pos = (G_idx+1) * N_i\n",
    "        \n",
    "        X_tilde[stt_pos:end_pos,:,:] = X_tilde_tmp\n",
    "        y_tilde[stt_pos:end_pos,:,:] = y_tilde_tmp\n",
    "        \n",
    "   \n",
    "\n",
    "    for iter in range(N_epochs): #args.epochs\n",
    "        \n",
    "        w_group_array = []\n",
    "        for G_idx in range(G):\n",
    "            w_locals, loss_locals = [], []\n",
    "            idxs_users = np.random.choice(range(N_i), m, replace=False)\n",
    "            idxs_users = np.sort(idxs_users)\n",
    "            print('selected users:',idxs_users)\n",
    "\n",
    "#             coded_net = BACC_Enc_Model_withNoise_v3(net_glob.cuda(), N_i, K_i, T, 0.01, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "            dec_z_array = []\n",
    "            for idx in idxs_users: #for idx in range(N):\n",
    "        #         print(idx)\n",
    "                local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[G_idx*N_i+idx,:,:], label=y_tilde[G_idx*N_i+idx,:,:])\n",
    "#                 w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "                w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                w_locals.append(copy.deepcopy(w))\n",
    "                loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "            # update global weights\n",
    "            #w_glob = FedAvg(w_locals)\n",
    "            w_group = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "            \n",
    "            w_group_array.append(copy.deepcopy(w_group))\n",
    "        \n",
    "        w_glob = copy.deepcopy(w_group_array[0])\n",
    "        for k in w_glob.keys():\n",
    "            for G_idx in range(1,G):\n",
    "                w_glob[k] += w_group_array[G_idx][k]\n",
    "            w_glob[k] = torch.div(w_glob[k], len(w_group_array))\n",
    "        \n",
    "        # copy weight to net_glob\n",
    "        net_glob.load_state_dict(w_glob)\n",
    "\n",
    "        # print loss\n",
    "    #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "    #     loss_train_arr.append(loss_train)\n",
    "\n",
    "        acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "        acc_test_arr_G2_N4_v2[trial_idx][iter] = acc_test\n",
    "        loss_test_arr_G2_N4_v2[trial_idx][iter] = loss_test\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2 3\n",
      "##########################################\n",
      "###### 0 -th Trial!! ###########\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 15000 \n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.3025 \n",
      "Accuracy: 974/10000 (9.74%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.2943 \n",
      "Accuracy: 2544/10000 (25.44%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.0631 \n",
      "Accuracy: 6656/10000 (66.56%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9568 \n",
      "Accuracy: 8542/10000 (85.42%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8062 \n",
      "Accuracy: 9612/10000 (96.12%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.7800 \n",
      "Accuracy: 9607/10000 (96.07%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8027 \n",
      "Accuracy: 9622/10000 (96.22%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.7711 \n",
      "Accuracy: 9628/10000 (96.28%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.7664 \n",
      "Accuracy: 9609/10000 (96.09%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.7680 \n",
      "Accuracy: 9612/10000 (96.12%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.7728 \n",
      "Accuracy: 9601/10000 (96.01%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8523 \n",
      "Accuracy: 9533/10000 (95.33%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.7927 \n",
      "Accuracy: 9604/10000 (96.04%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8505 \n",
      "Accuracy: 9528/10000 (95.28%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8085 \n",
      "Accuracy: 9595/10000 (95.95%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8481 \n",
      "Accuracy: 9562/10000 (95.62%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8205 \n",
      "Accuracy: 9600/10000 (96.00%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8440 \n",
      "Accuracy: 9582/10000 (95.82%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8253 \n",
      "Accuracy: 9595/10000 (95.95%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8130 \n",
      "Accuracy: 9581/10000 (95.81%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8652 \n",
      "Accuracy: 9552/10000 (95.52%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8122 \n",
      "Accuracy: 9614/10000 (96.14%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8160 \n",
      "Accuracy: 9585/10000 (95.85%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8972 \n",
      "Accuracy: 9468/10000 (94.68%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8030 \n",
      "Accuracy: 9609/10000 (96.09%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8643 \n",
      "Accuracy: 9546/10000 (95.46%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8635 \n",
      "Accuracy: 9497/10000 (94.97%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8276 \n",
      "Accuracy: 9598/10000 (95.98%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8942 \n",
      "Accuracy: 9453/10000 (94.53%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "N = 8\n",
    "K = 4\n",
    "G = 2\n",
    "\n",
    "N_i = int(N/G) # = 2\n",
    "K_i = int(K/G) # = 2\n",
    "T = 3\n",
    "sigma = 1\n",
    "Noise_Alloc = [0,2,4]\n",
    "m = N_i # number of selected workers (if there is no straggler, m=N_i)\n",
    "\n",
    "print(N_i,K_i,T)\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K_i+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K_i+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K_i+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "z_array = np.array([-0.81, -0.22, 0.22, 0.81])\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 30\n",
    "\n",
    "loss_test_arr_K4_G2_N8_v2 = np.zeros((N_trials,N_epochs))\n",
    "acc_test_arr_K4_G2_N8_v2  = np.zeros((N_trials,N_epochs))\n",
    "\n",
    "for trial_idx in range(N_trials):\n",
    "    \n",
    "    print('##########################################')\n",
    "    print('######',trial_idx,'-th Trial!! ###########')\n",
    "    \n",
    "    net_glob = CNNMnist2(args=args)\n",
    "    net_glob.cuda()\n",
    "    net_glob.train()\n",
    "    \n",
    "    # copy weights\n",
    "    w_glob = net_glob.state_dict()\n",
    "    \n",
    "    X_tilde = np.empty((N,Size_submatrices,28*28))\n",
    "    y_tilde = np.empty((N,Size_submatrices,10))\n",
    "    \n",
    "    for G_idx in range(G):\n",
    "        \n",
    "        _Noise_label = np.ones((size_per_group*T,10)) * 0.1\n",
    "        \n",
    "        X_tilde_tmp,a,b = BACC_Enc_Data_v3(X_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde_tmp,a,b = BACC_Enc_Data_v3(y_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "        \n",
    "        stt_pos = G_idx * N_i\n",
    "        end_pos = (G_idx+1) * N_i\n",
    "        \n",
    "        X_tilde[stt_pos:end_pos,:,:] = X_tilde_tmp\n",
    "        y_tilde[stt_pos:end_pos,:,:] = y_tilde_tmp\n",
    "        \n",
    "   \n",
    "\n",
    "    for iter in range(N_epochs): #args.epochs\n",
    "        \n",
    "        w_group_array = []\n",
    "        for G_idx in range(G):\n",
    "            w_locals, loss_locals = [], []\n",
    "            idxs_users = np.random.choice(range(N_i), m, replace=False)\n",
    "            idxs_users = np.sort(idxs_users)\n",
    "            print('selected users:',idxs_users)\n",
    "\n",
    "#             coded_net = BACC_Enc_Model_withNoise_v3(net_glob.cuda(), N_i, K_i, T, 0.01, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "            dec_z_array = []\n",
    "            for idx in idxs_users: #for idx in range(N):\n",
    "        #         print(idx)\n",
    "                local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[G_idx*N_i+idx,:,:], label=y_tilde[G_idx*N_i+idx,:,:])\n",
    "#                 w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "                w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                w_locals.append(copy.deepcopy(w))\n",
    "                loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "            # update global weights\n",
    "            #w_glob = FedAvg(w_locals)\n",
    "            w_group = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "            \n",
    "            w_group_array.append(copy.deepcopy(w_group))\n",
    "        \n",
    "        w_glob = copy.deepcopy(w_group_array[0])\n",
    "        for k in w_glob.keys():\n",
    "            for G_idx in range(1,G):\n",
    "                w_glob[k] += w_group_array[G_idx][k]\n",
    "            w_glob[k] = torch.div(w_glob[k], len(w_group_array))\n",
    "        \n",
    "        # copy weight to net_glob\n",
    "        net_glob.load_state_dict(w_glob)\n",
    "\n",
    "        # print loss\n",
    "    #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "    #     loss_train_arr.append(loss_train)\n",
    "\n",
    "        acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "        acc_test_arr_K4_G2_N8_v2[trial_idx][iter] = acc_test\n",
    "        loss_test_arr_K4_G2_N8_v2[trial_idx][iter] = loss_test\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1 2\n",
      "##########################################\n",
      "###### 0 -th Trial!! ###########\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.2967 \n",
      "Accuracy: 4766/10000 (47.66%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.9995 \n",
      "Accuracy: 7548/10000 (75.48%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.8282 \n",
      "Accuracy: 9493/10000 (94.93%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7811 \n",
      "Accuracy: 9636/10000 (96.36%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7788 \n",
      "Accuracy: 9639/10000 (96.39%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7531 \n",
      "Accuracy: 9632/10000 (96.32%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7798 \n",
      "Accuracy: 9646/10000 (96.46%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.8117 \n",
      "Accuracy: 9660/10000 (96.60%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7924 \n",
      "Accuracy: 9684/10000 (96.84%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7672 \n",
      "Accuracy: 9678/10000 (96.78%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7768 \n",
      "Accuracy: 9657/10000 (96.57%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.8813 \n",
      "Accuracy: 9592/10000 (95.92%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.8124 \n",
      "Accuracy: 9658/10000 (96.58%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7945 \n",
      "Accuracy: 9658/10000 (96.58%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.8279 \n",
      "Accuracy: 9681/10000 (96.81%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.8005 \n",
      "Accuracy: 9667/10000 (96.67%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7784 \n",
      "Accuracy: 9642/10000 (96.42%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.8515 \n",
      "Accuracy: 9594/10000 (95.94%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7832 \n",
      "Accuracy: 9662/10000 (96.62%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7906 \n",
      "Accuracy: 9662/10000 (96.62%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.8648 \n",
      "Accuracy: 9641/10000 (96.41%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7804 \n",
      "Accuracy: 9661/10000 (96.61%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.8232 \n",
      "Accuracy: 9613/10000 (96.13%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7817 \n",
      "Accuracy: 9664/10000 (96.64%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.8109 \n",
      "Accuracy: 9643/10000 (96.43%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7931 \n",
      "Accuracy: 9662/10000 (96.62%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.8087 \n",
      "Accuracy: 9677/10000 (96.77%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.8037 \n",
      "Accuracy: 9701/10000 (97.01%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.8532 \n",
      "Accuracy: 9631/10000 (96.31%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "N = 8\n",
    "K = 4\n",
    "G = 4\n",
    "\n",
    "size_per_group = int(60000/G)\n",
    "\n",
    "X_group = np.reshape(encoding_input_array_np, (G,size_per_group,28*28))\n",
    "y_group = np.reshape(encoding_label_array_np, (G,size_per_group,args.num_classes)) \n",
    "\n",
    "\n",
    "N_i = int(N/G) # = 2\n",
    "K_i = int(K/G) # = 2\n",
    "T = 2\n",
    "sigma = 1\n",
    "Noise_Alloc = [0,2]\n",
    "m = N_i # number of selected workers (if there is no straggler, m=N_i)\n",
    "\n",
    "print(N_i,K_i,T)\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K_i+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K_i+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K_i+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "z_array = np.array([-0.521, 0.521])\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 30\n",
    "\n",
    "loss_test_arr_K4_G4_N8_v2 = np.zeros((N_trials,N_epochs))\n",
    "acc_test_arr_K4_G4_N8_v2  = np.zeros((N_trials,N_epochs))\n",
    "\n",
    "for trial_idx in range(N_trials):\n",
    "    \n",
    "    print('##########################################')\n",
    "    print('######',trial_idx,'-th Trial!! ###########')\n",
    "    \n",
    "    net_glob = CNNMnist2(args=args)\n",
    "    net_glob.cuda()\n",
    "    net_glob.train()\n",
    "    \n",
    "    # copy weights\n",
    "    w_glob = net_glob.state_dict()\n",
    "    \n",
    "    X_tilde = np.empty((N,Size_submatrices,28*28))\n",
    "    y_tilde = np.empty((N,Size_submatrices,10))\n",
    "    \n",
    "    for G_idx in range(G):\n",
    "        \n",
    "        _Noise_label = np.ones((size_per_group*T,10)) * 0.1\n",
    "        \n",
    "        X_tilde_tmp,a,b = BACC_Enc_Data_v3(X_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde_tmp,a,b = BACC_Enc_Data_v3(y_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "        \n",
    "        stt_pos = G_idx * N_i\n",
    "        end_pos = (G_idx+1) * N_i\n",
    "        \n",
    "        X_tilde[stt_pos:end_pos,:,:] = X_tilde_tmp\n",
    "        y_tilde[stt_pos:end_pos,:,:] = y_tilde_tmp\n",
    "        \n",
    "   \n",
    "\n",
    "    for iter in range(N_epochs): #args.epochs\n",
    "        \n",
    "        w_group_array = []\n",
    "        for G_idx in range(G):\n",
    "            w_locals, loss_locals = [], []\n",
    "            idxs_users = np.random.choice(range(N_i), m, replace=False)\n",
    "            idxs_users = np.sort(idxs_users)\n",
    "            print('selected users:',idxs_users)\n",
    "\n",
    "            coded_net = BACC_Enc_Model_withNoise_v3(net_glob.cuda(), N_i, K_i, T, 0.01, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "            dec_z_array = []\n",
    "            for idx in idxs_users: #for idx in range(N):\n",
    "        #         print(idx)\n",
    "                local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[G_idx*N_i+idx,:,:], label=y_tilde[G_idx*N_i+idx,:,:])\n",
    "#                 w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "                w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                w_locals.append(copy.deepcopy(w))\n",
    "                loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "            # update global weights\n",
    "            #w_glob = FedAvg(w_locals)\n",
    "            w_group = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "            \n",
    "            w_group_array.append(copy.deepcopy(w_group))\n",
    "        \n",
    "        w_glob = copy.deepcopy(w_group_array[0])\n",
    "        for k in w_glob.keys():\n",
    "            for G_idx in range(1,G):\n",
    "                w_glob[k] += w_group_array[G_idx][k]\n",
    "            w_glob[k] = torch.div(w_glob[k], len(w_group_array))\n",
    "        \n",
    "        # copy weight to net_glob\n",
    "        net_glob.load_state_dict(w_glob)\n",
    "\n",
    "        # print loss\n",
    "    #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "    #     loss_train_arr.append(loss_train)\n",
    "\n",
    "        acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "        acc_test_arr_K4_G4_N8_v2[trial_idx][iter] = acc_test\n",
    "        loss_test_arr_K4_G4_N8_v2[trial_idx][iter] = loss_test\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "B_sel = 0\n",
    "\n",
    "# plt.plot(acc_test_arr_v1[0,B_sel,0,0:30],label='N=2, B='+str(B_array[B_sel]))\n",
    "plt.plot(acc_test_arr_K4_G1_v2[0,B_sel,0,0:30],label='w/o Grouping, K=4, N=8, G=1' )\n",
    "# plt.plot(acc_test_arr_v1[2,B_sel,0,0:30],label='N=6, B='+str(B_array[B_sel]))\n",
    "# plt.plot(acc_test_arr_v1[3,B_sel,0,0:30],label='N=8, B='+str(B_array[B_sel]))\n",
    "plt.plot(acc_test_arr_G2_N4_v2[0,0:30],label='w/ Grouping,   K=4, N=4, G=2')\n",
    "plt.plot(acc_test_arr_K4_G2_N8_v2[0,0:30],label='w/ Grouping,   K=4, N=8, G=2')\n",
    "plt.plot(acc_test_arr_K4_G4_N8_v2[0,0:30],label='w/ Grouping,   K=4, N=8, G=4')\n",
    "# plt.plot(acc_test_arr_G3[0,0:30],label='Grouping, N=6, G=3, B='+str(B_array[B_sel]))\n",
    "\n",
    "# plt.plot(acc_test_arr_v1[4,B_sel,0,0:30],label='N=7, B='+str(B_array[B_sel]))\n",
    "# plt.plot(acc_test_arr_v1[5,B_sel,0,0:30],label='N=8, B='+str(B_array[B_sel]))\n",
    "\n",
    "# plt.plot(plot_acc_v2[2,sigma_sel,:],label='T=10')\n",
    "# plt.plot(plot_acc_v2[3,sigma_sel,:],label='T=11')\n",
    "\n",
    "# plt.plot(acc_test_arr_uncoded,'r',label='uncoded')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "# plt.title(title_name)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Test accuracy(%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Fix the model encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 !!!\n",
      "z_array: [-0.94  -0.534  0.534  0.94 ]\n",
      "0.4486236179368535\n",
      "0.48963480280841937\n",
      "0.4896348028084205\n",
      "0.4486236179368556\n",
      "@BACC_Enc: N,K,T, m_i= 4 4 5 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 4 5 15000 \n",
      "\n",
      "(T, sigma)= 5 1 )  0 -th Trial!!\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.012616732120513917\n",
      "conv1.bias 0.012467783875763416\n",
      "conv2.weight 0.00041515901684761045\n",
      "conv2.bias 0.00041941372910514474\n",
      "fc1.weight 0.00032443224918097255\n",
      "fc1.bias 0.00031529138796031474\n",
      "\n",
      "Test set: Average loss: 2.2659 \n",
      "Accuracy: 2814/10000 (28.14%)\n",
      "\n",
      "Round   0, Average loss 2.266 Test accuracy 28.140\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0011088134348392488\n",
      "conv1.bias 0.007187981624156237\n",
      "conv2.weight 0.0008096638321876526\n",
      "conv2.bias 0.0017347087850794196\n",
      "fc1.weight 0.0003073048545047641\n",
      "fc1.bias 0.0022951560094952583\n",
      "\n",
      "Test set: Average loss: 1.5107 \n",
      "Accuracy: 6144/10000 (61.44%)\n",
      "\n",
      "Round   1, Average loss 1.511 Test accuracy 61.440\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0005380341038107872\n",
      "conv1.bias 0.020101509988307953\n",
      "conv2.weight 0.00028612170368433\n",
      "conv2.bias 0.004101229831576347\n",
      "fc1.weight 0.0007278656587004662\n",
      "fc1.bias 0.002369406074285507\n",
      "\n",
      "Test set: Average loss: 0.6433 \n",
      "Accuracy: 8845/10000 (88.45%)\n",
      "\n",
      "Round   2, Average loss 0.643 Test accuracy 88.450\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0006736280024051667\n",
      "conv1.bias 0.017079882323741913\n",
      "conv2.weight 0.0004514079913496971\n",
      "conv2.bias 0.0042719170451164246\n",
      "fc1.weight 0.001019554864615202\n",
      "fc1.bias 0.004782037436962127\n",
      "\n",
      "Test set: Average loss: 0.5713 \n",
      "Accuracy: 8923/10000 (89.23%)\n",
      "\n",
      "Round   3, Average loss 0.571 Test accuracy 89.230\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0007506133615970611\n",
      "conv1.bias 0.017611443996429443\n",
      "conv2.weight 0.0005531184747815132\n",
      "conv2.bias 0.004335781559348106\n",
      "fc1.weight 0.0007970871403813363\n",
      "fc1.bias 0.0052139896899461744\n",
      "\n",
      "Test set: Average loss: 0.5456 \n",
      "Accuracy: 8935/10000 (89.35%)\n",
      "\n",
      "Round   4, Average loss 0.546 Test accuracy 89.350\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0007013208419084549\n",
      "conv1.bias 0.014970610849559307\n",
      "conv2.weight 0.0006201102957129478\n",
      "conv2.bias 0.004173900932073593\n",
      "fc1.weight 0.0010427767410874367\n",
      "fc1.bias 0.006302203238010407\n",
      "\n",
      "Test set: Average loss: 0.5032 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "Round   5, Average loss 0.503 Test accuracy 89.050\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0008669610321521759\n",
      "conv1.bias 0.011255930177867413\n",
      "conv2.weight 0.0007103893905878067\n",
      "conv2.bias 0.0038855639286339283\n",
      "fc1.weight 0.0010332875885069371\n",
      "fc1.bias 0.004717053100466728\n",
      "\n",
      "Test set: Average loss: 0.5041 \n",
      "Accuracy: 9017/10000 (90.17%)\n",
      "\n",
      "Round   6, Average loss 0.504 Test accuracy 90.170\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.001125257834792137\n",
      "conv1.bias 0.012311127036809921\n",
      "conv2.weight 0.0005646313354372978\n",
      "conv2.bias 0.003747542854398489\n",
      "fc1.weight 0.0009210167452692985\n",
      "fc1.bias 0.0037931259721517563\n",
      "\n",
      "Test set: Average loss: 0.5126 \n",
      "Accuracy: 8888/10000 (88.88%)\n",
      "\n",
      "Round   7, Average loss 0.513 Test accuracy 88.880\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0008782562613487243\n",
      "conv1.bias 0.012083476409316063\n",
      "conv2.weight 0.0006263855099678039\n",
      "conv2.bias 0.0038963300175964832\n",
      "fc1.weight 0.0009121110662817955\n",
      "fc1.bias 0.003023902513086796\n",
      "\n",
      "Test set: Average loss: 0.4914 \n",
      "Accuracy: 9011/10000 (90.11%)\n",
      "\n",
      "Round   8, Average loss 0.491 Test accuracy 90.110\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0010480347275733947\n",
      "conv1.bias 0.015298513695597649\n",
      "conv2.weight 0.0004923412203788757\n",
      "conv2.bias 0.0034346110187470913\n",
      "fc1.weight 0.0009250521659851074\n",
      "fc1.bias 0.0038743335753679276\n",
      "\n",
      "Test set: Average loss: 0.4429 \n",
      "Accuracy: 9010/10000 (90.10%)\n",
      "\n",
      "Round   9, Average loss 0.443 Test accuracy 90.100\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00091193288564682\n",
      "conv1.bias 0.012504668906331062\n",
      "conv2.weight 0.0006189513579010963\n",
      "conv2.bias 0.0037506914231926203\n",
      "fc1.weight 0.0008652660995721817\n",
      "fc1.bias 0.003910943865776062\n",
      "\n",
      "Test set: Average loss: 0.4964 \n",
      "Accuracy: 8939/10000 (89.39%)\n",
      "\n",
      "Round  10, Average loss 0.496 Test accuracy 89.390\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0010419411957263947\n",
      "conv1.bias 0.016786713153123856\n",
      "conv2.weight 0.0004625040292739868\n",
      "conv2.bias 0.003379519795998931\n",
      "fc1.weight 0.0009046537801623344\n",
      "fc1.bias 0.0031574808061122896\n",
      "\n",
      "Test set: Average loss: 0.4784 \n",
      "Accuracy: 8982/10000 (89.82%)\n",
      "\n",
      "Round  11, Average loss 0.478 Test accuracy 89.820\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0008226795494556426\n",
      "conv1.bias 0.015319054014980793\n",
      "conv2.weight 0.0005249115452170372\n",
      "conv2.bias 0.003383337054401636\n",
      "fc1.weight 0.001004927046597004\n",
      "fc1.bias 0.005055638030171394\n",
      "\n",
      "Test set: Average loss: 0.4767 \n",
      "Accuracy: 8958/10000 (89.58%)\n",
      "\n",
      "Round  12, Average loss 0.477 Test accuracy 89.580\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0010237888246774672\n",
      "conv1.bias 0.01624615490436554\n",
      "conv2.weight 0.0005748789012432099\n",
      "conv2.bias 0.0034289564937353134\n",
      "fc1.weight 0.0009445114061236382\n",
      "fc1.bias 0.0040868926793336865\n",
      "\n",
      "Test set: Average loss: 0.4674 \n",
      "Accuracy: 8997/10000 (89.97%)\n",
      "\n",
      "Round  13, Average loss 0.467 Test accuracy 89.970\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0011223405599594116\n",
      "conv1.bias 0.02013796754181385\n",
      "conv2.weight 0.0005528997629880905\n",
      "conv2.bias 0.0035752691328525543\n",
      "fc1.weight 0.0009271282702684403\n",
      "fc1.bias 0.0032888073474168777\n",
      "\n",
      "Test set: Average loss: 0.5290 \n",
      "Accuracy: 8669/10000 (86.69%)\n",
      "\n",
      "Round  14, Average loss 0.529 Test accuracy 86.690\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0008835156261920929\n",
      "conv1.bias 0.01572185568511486\n",
      "conv2.weight 0.0005914030224084854\n",
      "conv2.bias 0.0035320266615599394\n",
      "fc1.weight 0.0008666806854307652\n",
      "fc1.bias 0.001658937893807888\n",
      "\n",
      "Test set: Average loss: 0.4459 \n",
      "Accuracy: 9046/10000 (90.46%)\n",
      "\n",
      "Round  15, Average loss 0.446 Test accuracy 90.460\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0010400862991809844\n",
      "conv1.bias 0.0171672236174345\n",
      "conv2.weight 0.000554521456360817\n",
      "conv2.bias 0.0033319536596536636\n",
      "fc1.weight 0.0009295501746237278\n",
      "fc1.bias 0.003404400870203972\n",
      "\n",
      "Test set: Average loss: 0.5662 \n",
      "Accuracy: 8498/10000 (84.98%)\n",
      "\n",
      "Round  16, Average loss 0.566 Test accuracy 84.980\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0009943540394306183\n",
      "conv1.bias 0.01696774549782276\n",
      "conv2.weight 0.0005204035341739655\n",
      "conv2.bias 0.0032746801152825356\n",
      "fc1.weight 0.0010009376332163812\n",
      "fc1.bias 0.0019999565556645393\n",
      "\n",
      "Test set: Average loss: 0.4469 \n",
      "Accuracy: 9030/10000 (90.30%)\n",
      "\n",
      "Round  17, Average loss 0.447 Test accuracy 90.300\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0010021334141492844\n",
      "conv1.bias 0.01747874915599823\n",
      "conv2.weight 0.0007556972652673721\n",
      "conv2.bias 0.003632190404459834\n",
      "fc1.weight 0.0010147710330784322\n",
      "fc1.bias 0.0035193350166082384\n",
      "\n",
      "Test set: Average loss: 0.4916 \n",
      "Accuracy: 8774/10000 (87.74%)\n",
      "\n",
      "Round  18, Average loss 0.492 Test accuracy 87.740\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.000991797298192978\n",
      "conv1.bias 0.014718778431415558\n",
      "conv2.weight 0.0007407407462596893\n",
      "conv2.bias 0.003772536525502801\n",
      "fc1.weight 0.0009746367111802101\n",
      "fc1.bias 0.00176271703094244\n",
      "\n",
      "Test set: Average loss: 0.4458 \n",
      "Accuracy: 9072/10000 (90.72%)\n",
      "\n",
      "Round  19, Average loss 0.446 Test accuracy 90.720\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0009947255998849869\n",
      "conv1.bias 0.018306240439414978\n",
      "conv2.weight 0.0006952577829360962\n",
      "conv2.bias 0.0034106713719666004\n",
      "fc1.weight 0.0009480312466621399\n",
      "fc1.bias 0.003031587228178978\n",
      "\n",
      "Test set: Average loss: 0.4357 \n",
      "Accuracy: 9102/10000 (91.02%)\n",
      "\n",
      "Round  20, Average loss 0.436 Test accuracy 91.020\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0009008637070655823\n",
      "conv1.bias 0.017185073345899582\n",
      "conv2.weight 0.0007346707582473755\n",
      "conv2.bias 0.0038551022298634052\n",
      "fc1.weight 0.0010014401748776435\n",
      "fc1.bias 0.004456796497106552\n",
      "\n",
      "Test set: Average loss: 0.4320 \n",
      "Accuracy: 9077/10000 (90.77%)\n",
      "\n",
      "Round  21, Average loss 0.432 Test accuracy 90.770\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.000962710827589035\n",
      "conv1.bias 0.01489319559186697\n",
      "conv2.weight 0.0007116552442312241\n",
      "conv2.bias 0.0036591722164303064\n",
      "fc1.weight 0.0010622390545904636\n",
      "fc1.bias 0.003667541965842247\n",
      "\n",
      "Test set: Average loss: 0.4340 \n",
      "Accuracy: 9094/10000 (90.94%)\n",
      "\n",
      "Round  22, Average loss 0.434 Test accuracy 90.940\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0009922576695680619\n",
      "conv1.bias 0.012166274711489677\n",
      "conv2.weight 0.0006924880295991897\n",
      "conv2.bias 0.0040560076013207436\n",
      "fc1.weight 0.0009124505333602429\n",
      "fc1.bias 0.002550361305475235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.4283 \n",
      "Accuracy: 9088/10000 (90.88%)\n",
      "\n",
      "Round  23, Average loss 0.428 Test accuracy 90.880\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.001175582855939865\n",
      "conv1.bias 0.014385771937668324\n",
      "conv2.weight 0.00045420754700899125\n",
      "conv2.bias 0.0033040321432054043\n",
      "fc1.weight 0.0008197194896638393\n",
      "fc1.bias 0.002220488712191582\n",
      "\n",
      "Test set: Average loss: 0.5204 \n",
      "Accuracy: 8802/10000 (88.02%)\n",
      "\n",
      "Round  24, Average loss 0.520 Test accuracy 88.020\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0008326202630996704\n",
      "conv1.bias 0.015181787312030792\n",
      "conv2.weight 0.0005527037754654885\n",
      "conv2.bias 0.0035513113252818584\n",
      "fc1.weight 0.0008252165280282497\n",
      "fc1.bias 0.003114721179008484\n",
      "\n",
      "Test set: Average loss: 0.4303 \n",
      "Accuracy: 9069/10000 (90.69%)\n",
      "\n",
      "Round  25, Average loss 0.430 Test accuracy 90.690\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.000979602113366127\n",
      "conv1.bias 0.014020392671227455\n",
      "conv2.weight 0.0004993779584765434\n",
      "conv2.bias 0.003304543439298868\n",
      "fc1.weight 0.0010365263558924198\n",
      "fc1.bias 0.004761309176683426\n",
      "\n",
      "Test set: Average loss: 0.4164 \n",
      "Accuracy: 9116/10000 (91.16%)\n",
      "\n",
      "Round  26, Average loss 0.416 Test accuracy 91.160\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0010244286060333253\n",
      "conv1.bias 0.018857557326555252\n",
      "conv2.weight 0.0005751978605985641\n",
      "conv2.bias 0.0035367340315133333\n",
      "fc1.weight 0.0008923263289034366\n",
      "fc1.bias 0.004031198099255562\n",
      "\n",
      "Test set: Average loss: 0.4327 \n",
      "Accuracy: 9137/10000 (91.37%)\n",
      "\n",
      "Round  27, Average loss 0.433 Test accuracy 91.370\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.001095193549990654\n",
      "conv1.bias 0.018560998141765594\n",
      "conv2.weight 0.0004840956255793571\n",
      "conv2.bias 0.0031937523745000362\n",
      "fc1.weight 0.0009205508977174759\n",
      "fc1.bias 0.002157170698046684\n",
      "\n",
      "Test set: Average loss: 0.4966 \n",
      "Accuracy: 8738/10000 (87.38%)\n",
      "\n",
      "Round  28, Average loss 0.497 Test accuracy 87.380\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0008656271547079086\n",
      "conv1.bias 0.01572367176413536\n",
      "conv2.weight 0.0006005741655826568\n",
      "conv2.bias 0.0034423237666487694\n",
      "fc1.weight 0.000865910854190588\n",
      "fc1.bias 0.0022095650434494018\n",
      "\n",
      "Test set: Average loss: 0.4394 \n",
      "Accuracy: 9087/10000 (90.87%)\n",
      "\n",
      "Round  29, Average loss 0.439 Test accuracy 90.870\n",
      "(T, sigma)= 5 1 )  1 -th Trial!!\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.013101818561553956\n",
      "conv1.bias 0.010638292878866196\n",
      "conv2.weight 0.0004211060702800751\n",
      "conv2.bias 0.00047577935038134456\n",
      "fc1.weight 0.00033169554080814123\n",
      "fc1.bias 0.0003490913659334183\n",
      "\n",
      "Test set: Average loss: 2.0817 \n",
      "Accuracy: 5591/10000 (55.91%)\n",
      "\n",
      "Round   0, Average loss 2.082 Test accuracy 55.910\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0008848880231380463\n",
      "conv1.bias 0.004274944309145212\n",
      "conv2.weight 0.00121164470911026\n",
      "conv2.bias 0.0023621683940291405\n",
      "fc1.weight 0.0008488518185913562\n",
      "fc1.bias 0.0008518108166754246\n",
      "\n",
      "Test set: Average loss: 1.5255 \n",
      "Accuracy: 5948/10000 (59.48%)\n",
      "\n",
      "Round   1, Average loss 1.525 Test accuracy 59.480\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0007431503385305404\n",
      "conv1.bias 0.010771417990326881\n",
      "conv2.weight 0.0005153579637408257\n",
      "conv2.bias 0.0031155962496995926\n",
      "fc1.weight 0.0006524287164211273\n",
      "fc1.bias 0.0011460176669061185\n",
      "\n",
      "Test set: Average loss: 0.5234 \n",
      "Accuracy: 8931/10000 (89.31%)\n",
      "\n",
      "Round   2, Average loss 0.523 Test accuracy 89.310\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0009340228140354156\n",
      "conv1.bias 0.019687339663505554\n",
      "conv2.weight 0.000388520248234272\n",
      "conv2.bias 0.004302583634853363\n",
      "fc1.weight 0.0008180320262908935\n",
      "fc1.bias 0.004587198421359062\n",
      "\n",
      "Test set: Average loss: 0.5831 \n",
      "Accuracy: 8745/10000 (87.45%)\n",
      "\n",
      "Round   3, Average loss 0.583 Test accuracy 87.450\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0009214411675930024\n",
      "conv1.bias 0.017927633598446846\n",
      "conv2.weight 0.00043295107781887054\n",
      "conv2.bias 0.0035929414443671703\n",
      "fc1.weight 0.0010498938150703906\n",
      "fc1.bias 0.003927014768123627\n",
      "\n",
      "Test set: Average loss: 0.4633 \n",
      "Accuracy: 9022/10000 (90.22%)\n",
      "\n",
      "Round   4, Average loss 0.463 Test accuracy 90.220\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0011436136811971665\n",
      "conv1.bias 0.018071306869387627\n",
      "conv2.weight 0.00047854196280241014\n",
      "conv2.bias 0.0034766749013215303\n",
      "fc1.weight 0.000979819055646658\n",
      "fc1.bias 0.004409298300743103\n",
      "\n",
      "Test set: Average loss: 0.4995 \n",
      "Accuracy: 8934/10000 (89.34%)\n",
      "\n",
      "Round   5, Average loss 0.500 Test accuracy 89.340\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0009396757185459137\n",
      "conv1.bias 0.015320508740842342\n",
      "conv2.weight 0.0005544200539588928\n",
      "conv2.bias 0.0035103815607726574\n",
      "fc1.weight 0.0010335685685276986\n",
      "fc1.bias 0.004095043241977692\n",
      "\n",
      "Test set: Average loss: 0.4721 \n",
      "Accuracy: 9003/10000 (90.03%)\n",
      "\n",
      "Round   6, Average loss 0.472 Test accuracy 90.030\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0010501423478126526\n",
      "conv1.bias 0.0179155096411705\n",
      "conv2.weight 0.0004599675536155701\n",
      "conv2.bias 0.0033831149339675903\n",
      "fc1.weight 0.001178334467113018\n",
      "fc1.bias 0.0037781115621328353\n",
      "\n",
      "Test set: Average loss: 0.4526 \n",
      "Accuracy: 9038/10000 (90.38%)\n",
      "\n",
      "Round   7, Average loss 0.453 Test accuracy 90.380\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0012198073416948318\n",
      "conv1.bias 0.015599316917359829\n",
      "conv2.weight 0.0005042010173201561\n",
      "conv2.bias 0.0035151024349033833\n",
      "fc1.weight 0.0008326306007802487\n",
      "fc1.bias 0.0024148104712367058\n",
      "\n",
      "Test set: Average loss: 0.4694 \n",
      "Accuracy: 9062/10000 (90.62%)\n",
      "\n",
      "Round   8, Average loss 0.469 Test accuracy 90.620\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0011440965533256531\n",
      "conv1.bias 0.01752557046711445\n",
      "conv2.weight 0.0004453082382678986\n",
      "conv2.bias 0.003449941985309124\n",
      "fc1.weight 0.0010082839988172054\n",
      "fc1.bias 0.002788912691175938\n",
      "\n",
      "Test set: Average loss: 0.4381 \n",
      "Accuracy: 9066/10000 (90.66%)\n",
      "\n",
      "Round   9, Average loss 0.438 Test accuracy 90.660\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0010891870409250259\n",
      "conv1.bias 0.016595233231782913\n",
      "conv2.weight 0.0005830997228622437\n",
      "conv2.bias 0.0034704788122326136\n",
      "fc1.weight 0.0007654889021068811\n",
      "fc1.bias 0.0025540262460708616\n",
      "\n",
      "Test set: Average loss: 0.4377 \n",
      "Accuracy: 9119/10000 (91.19%)\n",
      "\n",
      "Round  10, Average loss 0.438 Test accuracy 91.190\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.001088165044784546\n",
      "conv1.bias 0.014458605088293552\n",
      "conv2.weight 0.0005118620395660401\n",
      "conv2.bias 0.003639903385192156\n",
      "fc1.weight 0.000887422077357769\n",
      "fc1.bias 0.0031537264585494995\n",
      "\n",
      "Test set: Average loss: 0.4234 \n",
      "Accuracy: 9134/10000 (91.34%)\n",
      "\n",
      "Round  11, Average loss 0.423 Test accuracy 91.340\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0009357954561710357\n",
      "conv1.bias 0.013800082728266716\n",
      "conv2.weight 0.0006801547855138779\n",
      "conv2.bias 0.003584335558116436\n",
      "fc1.weight 0.0009717202745378017\n",
      "fc1.bias 0.005754975229501724\n",
      "\n",
      "Test set: Average loss: 0.4750 \n",
      "Accuracy: 9153/10000 (91.53%)\n",
      "\n",
      "Round  12, Average loss 0.475 Test accuracy 91.530\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0012505191564559936\n",
      "conv1.bias 0.016272759065032005\n",
      "conv2.weight 0.00044931069016456603\n",
      "conv2.bias 0.003071233630180359\n",
      "fc1.weight 0.0008754158392548561\n",
      "fc1.bias 0.003609830141067505\n",
      "\n",
      "Test set: Average loss: 0.4329 \n",
      "Accuracy: 9121/10000 (91.21%)\n",
      "\n",
      "Round  13, Average loss 0.433 Test accuracy 91.210\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0010974907875061035\n",
      "conv1.bias 0.016494670882821083\n",
      "conv2.weight 0.0005257540568709373\n",
      "conv2.bias 0.0032223497983068228\n",
      "fc1.weight 0.0007779666222631932\n",
      "fc1.bias 0.0050074603408575055\n",
      "\n",
      "Test set: Average loss: 0.4380 \n",
      "Accuracy: 9141/10000 (91.41%)\n",
      "\n",
      "Round  14, Average loss 0.438 Test accuracy 91.410\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0012140151858329772\n",
      "conv1.bias 0.02214476838707924\n",
      "conv2.weight 0.0003684085980057716\n",
      "conv2.bias 0.002926021348685026\n",
      "fc1.weight 0.0008327618241310119\n",
      "fc1.bias 0.0034974515438079834\n",
      "\n",
      "Test set: Average loss: 0.4610 \n",
      "Accuracy: 8967/10000 (89.67%)\n",
      "\n",
      "Round  15, Average loss 0.461 Test accuracy 89.670\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0009495735168457032\n",
      "conv1.bias 0.018197938799858093\n",
      "conv2.weight 0.0005012129619717597\n",
      "conv2.bias 0.0034044445492327213\n",
      "fc1.weight 0.0008813914842903614\n",
      "fc1.bias 0.0034780945628881455\n",
      "\n",
      "Test set: Average loss: 0.4229 \n",
      "Accuracy: 9108/10000 (91.08%)\n",
      "\n",
      "Round  16, Average loss 0.423 Test accuracy 91.080\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0012487706542015076\n",
      "conv1.bias 0.018534060567617416\n",
      "conv2.weight 0.00045451220124959947\n",
      "conv2.bias 0.0030718492344021797\n",
      "fc1.weight 0.001113755814731121\n",
      "fc1.bias 0.003959468379616737\n",
      "\n",
      "Test set: Average loss: 0.4484 \n",
      "Accuracy: 9046/10000 (90.46%)\n",
      "\n",
      "Round  17, Average loss 0.448 Test accuracy 90.460\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0012473617494106293\n",
      "conv1.bias 0.015502994880080223\n",
      "conv2.weight 0.000457974299788475\n",
      "conv2.bias 0.0030043884180486202\n",
      "fc1.weight 0.0009986855089664458\n",
      "fc1.bias 0.0031252872198820115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.4433 \n",
      "Accuracy: 9119/10000 (91.19%)\n",
      "\n",
      "Round  18, Average loss 0.443 Test accuracy 91.190\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0008736420422792435\n",
      "conv1.bias 0.015051376074552536\n",
      "conv2.weight 0.0006572893261909485\n",
      "conv2.bias 0.0032999401446431875\n",
      "fc1.weight 0.0010530371218919754\n",
      "fc1.bias 0.004160522669553757\n",
      "\n",
      "Test set: Average loss: 0.4432 \n",
      "Accuracy: 9081/10000 (90.81%)\n",
      "\n",
      "Round  19, Average loss 0.443 Test accuracy 90.810\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0016315473616123199\n",
      "conv1.bias 0.020036358386278152\n",
      "conv2.weight 0.0004684704914689064\n",
      "conv2.bias 0.002969155553728342\n",
      "fc1.weight 0.0009581705555319787\n",
      "fc1.bias 0.0031894464045763016\n",
      "\n",
      "Test set: Average loss: 0.4704 \n",
      "Accuracy: 9041/10000 (90.41%)\n",
      "\n",
      "Round  20, Average loss 0.470 Test accuracy 90.410\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0015988081693649292\n",
      "conv1.bias 0.02232244238257408\n",
      "conv2.weight 0.0003395748883485794\n",
      "conv2.bias 0.0029189586639404297\n",
      "fc1.weight 0.0008388116024434566\n",
      "fc1.bias 0.0020735226571559906\n",
      "\n",
      "Test set: Average loss: 0.5851 \n",
      "Accuracy: 8570/10000 (85.70%)\n",
      "\n",
      "Round  21, Average loss 0.585 Test accuracy 85.700\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0009280380606651306\n",
      "conv1.bias 0.01781456544995308\n",
      "conv2.weight 0.0004566039890050888\n",
      "conv2.bias 0.003366687800735235\n",
      "fc1.weight 0.0011729450896382331\n",
      "fc1.bias 0.003923773020505905\n",
      "\n",
      "Test set: Average loss: 0.4982 \n",
      "Accuracy: 9046/10000 (90.46%)\n",
      "\n",
      "Round  22, Average loss 0.498 Test accuracy 90.460\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0009332460165023804\n",
      "conv1.bias 0.01998855173587799\n",
      "conv2.weight 0.0007056715339422226\n",
      "conv2.bias 0.0032110982574522495\n",
      "fc1.weight 0.0010091373696923256\n",
      "fc1.bias 0.0051833663135766985\n",
      "\n",
      "Test set: Average loss: 0.4406 \n",
      "Accuracy: 9152/10000 (91.52%)\n",
      "\n",
      "Round  23, Average loss 0.441 Test accuracy 91.520\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0013462454080581665\n",
      "conv1.bias 0.0160856731235981\n",
      "conv2.weight 0.0005644750967621803\n",
      "conv2.bias 0.0034708522725850344\n",
      "fc1.weight 0.0008346529677510262\n",
      "fc1.bias 0.0025686372071504595\n",
      "\n",
      "Test set: Average loss: 0.4336 \n",
      "Accuracy: 9102/10000 (91.02%)\n",
      "\n",
      "Round  24, Average loss 0.434 Test accuracy 91.020\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0008011556416749954\n",
      "conv1.bias 0.013432859443128109\n",
      "conv2.weight 0.0008545134216547013\n",
      "conv2.bias 0.003892831737175584\n",
      "fc1.weight 0.000997050665318966\n",
      "fc1.bias 0.005203569307923317\n",
      "\n",
      "Test set: Average loss: 0.4537 \n",
      "Accuracy: 9048/10000 (90.48%)\n",
      "\n",
      "Round  25, Average loss 0.454 Test accuracy 90.480\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0013711732625961304\n",
      "conv1.bias 0.01928955316543579\n",
      "conv2.weight 0.00047983616590499876\n",
      "conv2.bias 0.0030554458498954773\n",
      "fc1.weight 0.0011410327628254891\n",
      "fc1.bias 0.0030971584841609\n",
      "\n",
      "Test set: Average loss: 0.4351 \n",
      "Accuracy: 9084/10000 (90.84%)\n",
      "\n",
      "Round  26, Average loss 0.435 Test accuracy 90.840\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0013345639407634736\n",
      "conv1.bias 0.01787935569882393\n",
      "conv2.weight 0.000408262237906456\n",
      "conv2.bias 0.003064796794205904\n",
      "fc1.weight 0.0008305136114358902\n",
      "fc1.bias 0.002224344201385975\n",
      "\n",
      "Test set: Average loss: 0.4432 \n",
      "Accuracy: 9079/10000 (90.79%)\n",
      "\n",
      "Round  27, Average loss 0.443 Test accuracy 90.790\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0012912751734256744\n",
      "conv1.bias 0.014204525388777256\n",
      "conv2.weight 0.00039065863937139513\n",
      "conv2.bias 0.003053393680602312\n",
      "fc1.weight 0.0010802080854773522\n",
      "fc1.bias 0.003158317506313324\n",
      "\n",
      "Test set: Average loss: 0.4285 \n",
      "Accuracy: 9110/10000 (91.10%)\n",
      "\n",
      "Round  28, Average loss 0.429 Test accuracy 91.100\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0013357731699943543\n",
      "conv1.bias 0.013198353350162506\n",
      "conv2.weight 0.00045091792941093445\n",
      "conv2.bias 0.0030423463322222233\n",
      "fc1.weight 0.0007267104461789131\n",
      "fc1.bias 0.002228800393640995\n",
      "\n",
      "Test set: Average loss: 0.4657 \n",
      "Accuracy: 9108/10000 (91.08%)\n",
      "\n",
      "Round  29, Average loss 0.466 Test accuracy 91.080\n",
      "(T, sigma)= 5 1 )  2 -th Trial!!\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.013788955211639404\n",
      "conv1.bias 0.014282751828432083\n",
      "conv2.weight 0.0004153047874569893\n",
      "conv2.bias 0.0003723013214766979\n",
      "fc1.weight 0.0003220047801733017\n",
      "fc1.bias 0.000349485594779253\n",
      "\n",
      "Test set: Average loss: 2.2947 \n",
      "Accuracy: 2780/10000 (27.80%)\n",
      "\n",
      "Round   0, Average loss 2.295 Test accuracy 27.800\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0006775252521038055\n",
      "conv1.bias 0.00692728441208601\n",
      "conv2.weight 0.0011910825222730638\n",
      "conv2.bias 0.002902829088270664\n",
      "fc1.weight 0.0004878433886915445\n",
      "fc1.bias 0.0012739649042487144\n",
      "\n",
      "Test set: Average loss: 2.1881 \n",
      "Accuracy: 4378/10000 (43.78%)\n",
      "\n",
      "Round   1, Average loss 2.188 Test accuracy 43.780\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0005218520015478134\n",
      "conv1.bias 0.015243024565279484\n",
      "conv2.weight 0.00022057821974158287\n",
      "conv2.bias 0.002562684705480933\n",
      "fc1.weight 0.00044434200972318647\n",
      "fc1.bias 0.0012745206244289876\n",
      "\n",
      "Test set: Average loss: 1.4143 \n",
      "Accuracy: 5268/10000 (52.68%)\n",
      "\n",
      "Round   2, Average loss 1.414 Test accuracy 52.680\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0007712885737419129\n",
      "conv1.bias 0.029324907809495926\n",
      "conv2.weight 0.00021894536912441253\n",
      "conv2.bias 0.0028221257962286472\n",
      "fc1.weight 0.0006072560790926218\n",
      "fc1.bias 0.0012192116118967533\n",
      "\n",
      "Test set: Average loss: 0.5835 \n",
      "Accuracy: 8481/10000 (84.81%)\n",
      "\n",
      "Round   3, Average loss 0.584 Test accuracy 84.810\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0008941326290369034\n",
      "conv1.bias 0.026982242241501808\n",
      "conv2.weight 0.0002904944866895676\n",
      "conv2.bias 0.0028071007691323757\n",
      "fc1.weight 0.0009419301524758339\n",
      "fc1.bias 0.003687584400177002\n",
      "\n",
      "Test set: Average loss: 0.5166 \n",
      "Accuracy: 9052/10000 (90.52%)\n",
      "\n",
      "Round   4, Average loss 0.517 Test accuracy 90.520\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0008683975785970688\n",
      "conv1.bias 0.023985303938388824\n",
      "conv2.weight 0.0004078921675682068\n",
      "conv2.bias 0.0033017704263329506\n",
      "fc1.weight 0.0010197412222623826\n",
      "fc1.bias 0.006955470144748688\n",
      "\n",
      "Test set: Average loss: 0.4735 \n",
      "Accuracy: 8995/10000 (89.95%)\n",
      "\n",
      "Round   5, Average loss 0.474 Test accuracy 89.950\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0011368809640407561\n",
      "conv1.bias 0.023273054510354996\n",
      "conv2.weight 0.0003867543488740921\n",
      "conv2.bias 0.002828498836606741\n",
      "fc1.weight 0.0008063831366598607\n",
      "fc1.bias 0.003987319022417069\n",
      "\n",
      "Test set: Average loss: 0.4473 \n",
      "Accuracy: 9124/10000 (91.24%)\n",
      "\n",
      "Round   6, Average loss 0.447 Test accuracy 91.240\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.000885273814201355\n",
      "conv1.bias 0.018911238759756088\n",
      "conv2.weight 0.00041962746530771255\n",
      "conv2.bias 0.00319122988730669\n",
      "fc1.weight 0.0013538600876927375\n",
      "fc1.bias 0.005567489936947822\n",
      "\n",
      "Test set: Average loss: 0.4515 \n",
      "Accuracy: 9128/10000 (91.28%)\n",
      "\n",
      "Round   7, Average loss 0.451 Test accuracy 91.280\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0009605956822633743\n",
      "conv1.bias 0.019559549167752266\n",
      "conv2.weight 0.0005029749125242233\n",
      "conv2.bias 0.0036407108418643475\n",
      "fc1.weight 0.0008764090016484261\n",
      "fc1.bias 0.003102697245776653\n",
      "\n",
      "Test set: Average loss: 0.4735 \n",
      "Accuracy: 9041/10000 (90.41%)\n",
      "\n",
      "Round   8, Average loss 0.474 Test accuracy 90.410\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0006903903931379319\n",
      "conv1.bias 0.01550249196588993\n",
      "conv2.weight 0.0006815361231565475\n",
      "conv2.bias 0.0036503137089312077\n",
      "fc1.weight 0.0009592696093022824\n",
      "fc1.bias 0.002269083634018898\n",
      "\n",
      "Test set: Average loss: 0.4431 \n",
      "Accuracy: 9119/10000 (91.19%)\n",
      "\n",
      "Round   9, Average loss 0.443 Test accuracy 91.190\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0011059608310461043\n",
      "conv1.bias 0.015682123601436615\n",
      "conv2.weight 0.0004858516529202461\n",
      "conv2.bias 0.0028425229247659445\n",
      "fc1.weight 0.001079077646136284\n",
      "fc1.bias 0.004604754596948623\n",
      "\n",
      "Test set: Average loss: 0.4172 \n",
      "Accuracy: 9144/10000 (91.44%)\n",
      "\n",
      "Round  10, Average loss 0.417 Test accuracy 91.440\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0011002370715141297\n",
      "conv1.bias 0.019699303433299065\n",
      "conv2.weight 0.000534478910267353\n",
      "conv2.bias 0.003240876831114292\n",
      "fc1.weight 0.0010944003239274024\n",
      "fc1.bias 0.006555106490850449\n",
      "\n",
      "Test set: Average loss: 0.4351 \n",
      "Accuracy: 9123/10000 (91.23%)\n",
      "\n",
      "Round  11, Average loss 0.435 Test accuracy 91.230\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0012398770451545716\n",
      "conv1.bias 0.019383572041988373\n",
      "conv2.weight 0.00039995573461055757\n",
      "conv2.bias 0.0029224688187241554\n",
      "fc1.weight 0.0009143159724771977\n",
      "fc1.bias 0.0036663543432950974\n",
      "\n",
      "Test set: Average loss: 0.4601 \n",
      "Accuracy: 8814/10000 (88.14%)\n",
      "\n",
      "Round  12, Average loss 0.460 Test accuracy 88.140\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0007134545594453812\n",
      "conv1.bias 0.014802759513258934\n",
      "conv2.weight 0.0007127737998962403\n",
      "conv2.bias 0.003423672867938876\n",
      "fc1.weight 0.000920846126973629\n",
      "fc1.bias 0.0037033461034297944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.4158 \n",
      "Accuracy: 9168/10000 (91.68%)\n",
      "\n",
      "Round  13, Average loss 0.416 Test accuracy 91.680\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.001324448436498642\n",
      "conv1.bias 0.022601254284381866\n",
      "conv2.weight 0.00047714821994304657\n",
      "conv2.bias 0.002766302553936839\n",
      "fc1.weight 0.0011894961819052697\n",
      "fc1.bias 0.003943328559398651\n",
      "\n",
      "Test set: Average loss: 0.4159 \n",
      "Accuracy: 9141/10000 (91.41%)\n",
      "\n",
      "Round  14, Average loss 0.416 Test accuracy 91.410\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0011974452435970307\n",
      "conv1.bias 0.018507227301597595\n",
      "conv2.weight 0.0004209556430578232\n",
      "conv2.bias 0.0026745202485471964\n",
      "fc1.weight 0.000991460494697094\n",
      "fc1.bias 0.003810575231909752\n",
      "\n",
      "Test set: Average loss: 0.4201 \n",
      "Accuracy: 9126/10000 (91.26%)\n",
      "\n",
      "Round  15, Average loss 0.420 Test accuracy 91.260\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0011593854427337646\n",
      "conv1.bias 0.022156964987516403\n",
      "conv2.weight 0.00042661584913730624\n",
      "conv2.bias 0.003273359499871731\n",
      "fc1.weight 0.0011763926595449448\n",
      "fc1.bias 0.003286499157547951\n",
      "\n",
      "Test set: Average loss: 0.5298 \n",
      "Accuracy: 9104/10000 (91.04%)\n",
      "\n",
      "Round  16, Average loss 0.530 Test accuracy 91.040\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0007966822385787964\n",
      "conv1.bias 0.014640596695244312\n",
      "conv2.weight 0.0008571042865514755\n",
      "conv2.bias 0.003909546881914139\n",
      "fc1.weight 0.0009940369985997678\n",
      "fc1.bias 0.0025591662153601645\n",
      "\n",
      "Test set: Average loss: 0.4843 \n",
      "Accuracy: 9149/10000 (91.49%)\n",
      "\n",
      "Round  17, Average loss 0.484 Test accuracy 91.490\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0010190846025943756\n",
      "conv1.bias 0.012101857922971249\n",
      "conv2.weight 0.0008323747664690017\n",
      "conv2.bias 0.0030544965993613005\n",
      "fc1.weight 0.0011538645252585412\n",
      "fc1.bias 0.0032928679138422012\n",
      "\n",
      "Test set: Average loss: 0.6235 \n",
      "Accuracy: 8660/10000 (86.60%)\n",
      "\n",
      "Round  18, Average loss 0.623 Test accuracy 86.600\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0010261503607034684\n",
      "conv1.bias 0.01231477502733469\n",
      "conv2.weight 0.0009038829058408737\n",
      "conv2.bias 0.0031853732652962208\n",
      "fc1.weight 0.0011400152929127217\n",
      "fc1.bias 0.002000102959573269\n",
      "\n",
      "Test set: Average loss: 0.4583 \n",
      "Accuracy: 9176/10000 (91.76%)\n",
      "\n",
      "Round  19, Average loss 0.458 Test accuracy 91.760\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0011479400098323822\n",
      "conv1.bias 0.015007048845291138\n",
      "conv2.weight 0.0007920181751251221\n",
      "conv2.bias 0.0032079052180051804\n",
      "fc1.weight 0.001228834129869938\n",
      "fc1.bias 0.004873045161366463\n",
      "\n",
      "Test set: Average loss: 0.4622 \n",
      "Accuracy: 9116/10000 (91.16%)\n",
      "\n",
      "Round  20, Average loss 0.462 Test accuracy 91.160\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0013145972788333894\n",
      "conv1.bias 0.01648702472448349\n",
      "conv2.weight 0.0006507768481969833\n",
      "conv2.bias 0.0031133657321333885\n",
      "fc1.weight 0.0009645759128034115\n",
      "fc1.bias 0.002342558652162552\n",
      "\n",
      "Test set: Average loss: 0.8610 \n",
      "Accuracy: 7608/10000 (76.08%)\n",
      "\n",
      "Round  21, Average loss 0.861 Test accuracy 76.080\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0009179885685443878\n",
      "conv1.bias 0.013209996744990349\n",
      "conv2.weight 0.0011864954978227616\n",
      "conv2.bias 0.0037698340602219105\n",
      "fc1.weight 0.0009626039303839206\n",
      "fc1.bias 0.0015995360910892487\n",
      "\n",
      "Test set: Average loss: 0.6468 \n",
      "Accuracy: 8374/10000 (83.74%)\n",
      "\n",
      "Round  22, Average loss 0.647 Test accuracy 83.740\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0014519420266151427\n",
      "conv1.bias 0.016189996153116226\n",
      "conv2.weight 0.0005595963448286057\n",
      "conv2.bias 0.0029548732563853264\n",
      "fc1.weight 0.0010736720636487008\n",
      "fc1.bias 0.003644862025976181\n",
      "\n",
      "Test set: Average loss: 0.4238 \n",
      "Accuracy: 9180/10000 (91.80%)\n",
      "\n",
      "Round  23, Average loss 0.424 Test accuracy 91.800\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.001346927285194397\n",
      "conv1.bias 0.01796618290245533\n",
      "conv2.weight 0.0005454240739345551\n",
      "conv2.bias 0.0034137810580432415\n",
      "fc1.weight 0.0008668620139360428\n",
      "fc1.bias 0.003787250071763992\n",
      "\n",
      "Test set: Average loss: 0.4614 \n",
      "Accuracy: 9128/10000 (91.28%)\n",
      "\n",
      "Round  24, Average loss 0.461 Test accuracy 91.280\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0009900674223899842\n",
      "conv1.bias 0.015432052314281464\n",
      "conv2.weight 0.0006360789388418198\n",
      "conv2.bias 0.003175678662955761\n",
      "fc1.weight 0.0010338901542127133\n",
      "fc1.bias 0.002768351137638092\n",
      "\n",
      "Test set: Average loss: 0.4308 \n",
      "Accuracy: 9109/10000 (91.09%)\n",
      "\n",
      "Round  25, Average loss 0.431 Test accuracy 91.090\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0009996821731328965\n",
      "conv1.bias 0.016332484781742096\n",
      "conv2.weight 0.0005263437703251838\n",
      "conv2.bias 0.0030220877379179\n",
      "fc1.weight 0.000928336288779974\n",
      "fc1.bias 0.0028436210006475447\n",
      "\n",
      "Test set: Average loss: 0.4432 \n",
      "Accuracy: 9054/10000 (90.54%)\n",
      "\n",
      "Round  26, Average loss 0.443 Test accuracy 90.540\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0009076274186372757\n",
      "conv1.bias 0.01702997088432312\n",
      "conv2.weight 0.0005799355357885361\n",
      "conv2.bias 0.0029496997594833374\n",
      "fc1.weight 0.0010766632854938506\n",
      "fc1.bias 0.002648085355758667\n",
      "\n",
      "Test set: Average loss: 0.4216 \n",
      "Accuracy: 9243/10000 (92.43%)\n",
      "\n",
      "Round  27, Average loss 0.422 Test accuracy 92.430\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.000804373025894165\n",
      "conv1.bias 0.015478302724659443\n",
      "conv2.weight 0.0006943131983280182\n",
      "conv2.bias 0.0033932533115148544\n",
      "fc1.weight 0.0008983317762613297\n",
      "fc1.bias 0.0019942129030823706\n",
      "\n",
      "Test set: Average loss: 0.4134 \n",
      "Accuracy: 9203/10000 (92.03%)\n",
      "\n",
      "Round  28, Average loss 0.413 Test accuracy 92.030\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0010279252380132676\n",
      "conv1.bias 0.016034919768571854\n",
      "conv2.weight 0.0007645518332719803\n",
      "conv2.bias 0.0034204265102744102\n",
      "fc1.weight 0.0010141436010599135\n",
      "fc1.bias 0.0033300086855888368\n",
      "\n",
      "Test set: Average loss: 0.4352 \n",
      "Accuracy: 9110/10000 (91.10%)\n",
      "\n",
      "Round  29, Average loss 0.435 Test accuracy 91.100\n",
      "(T, sigma)= 5 1 )  3 -th Trial!!\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.013220643997192383\n",
      "conv1.bias 0.014793898910284042\n",
      "conv2.weight 0.0004192323237657547\n",
      "conv2.bias 0.00043462388566695154\n",
      "fc1.weight 0.00032378947362303733\n",
      "fc1.bias 0.0002555126091465354\n",
      "\n",
      "Test set: Average loss: 1.9564 \n",
      "Accuracy: 6020/10000 (60.20%)\n",
      "\n",
      "Round   0, Average loss 1.956 Test accuracy 60.200\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0006607252359390258\n",
      "conv1.bias 0.008851136080920696\n",
      "conv2.weight 0.001425585299730301\n",
      "conv2.bias 0.0026703791227191687\n",
      "fc1.weight 0.0007461305242031813\n",
      "fc1.bias 0.0006669909693300724\n",
      "\n",
      "Test set: Average loss: 2.0819 \n",
      "Accuracy: 6649/10000 (66.49%)\n",
      "\n",
      "Round   1, Average loss 2.082 Test accuracy 66.490\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004362831264734268\n",
      "conv1.bias 0.009672424755990505\n",
      "conv2.weight 0.0006213671341538429\n",
      "conv2.bias 0.0029666952323168516\n",
      "fc1.weight 0.0007334291934967041\n",
      "fc1.bias 0.0010320426896214484\n",
      "\n",
      "Test set: Average loss: 2.0922 \n",
      "Accuracy: 4711/10000 (47.11%)\n",
      "\n",
      "Round   2, Average loss 2.092 Test accuracy 47.110\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0009692209959030151\n",
      "conv1.bias 0.01501160766929388\n",
      "conv2.weight 0.00046896178275346756\n",
      "conv2.bias 0.004383467603474855\n",
      "fc1.weight 0.0007003001868724823\n",
      "fc1.bias 0.0005309334024786949\n",
      "\n",
      "Test set: Average loss: 2.1346 \n",
      "Accuracy: 4860/10000 (48.60%)\n",
      "\n",
      "Round   3, Average loss 2.135 Test accuracy 48.600\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0005399307981133461\n",
      "conv1.bias 0.016992032527923584\n",
      "conv2.weight 0.00035938695073127745\n",
      "conv2.bias 0.004548382014036179\n",
      "fc1.weight 0.0007329360116273165\n",
      "fc1.bias 0.004032076895236969\n",
      "\n",
      "Test set: Average loss: 0.6924 \n",
      "Accuracy: 8533/10000 (85.33%)\n",
      "\n",
      "Round   4, Average loss 0.692 Test accuracy 85.330\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.000782884880900383\n",
      "conv1.bias 0.015432918444275856\n",
      "conv2.weight 0.00047103807330131533\n",
      "conv2.bias 0.00494019640609622\n",
      "fc1.weight 0.0007632104679942131\n",
      "fc1.bias 0.008323506265878678\n",
      "\n",
      "Test set: Average loss: 0.4992 \n",
      "Accuracy: 8942/10000 (89.42%)\n",
      "\n",
      "Round   5, Average loss 0.499 Test accuracy 89.420\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0012474357336759567\n",
      "conv1.bias 0.013801909983158112\n",
      "conv2.weight 0.0004695824533700943\n",
      "conv2.bias 0.004139141645282507\n",
      "fc1.weight 0.0007252945564687252\n",
      "fc1.bias 0.006903731077909469\n",
      "\n",
      "Test set: Average loss: 0.8192 \n",
      "Accuracy: 7779/10000 (77.79%)\n",
      "\n",
      "Round   6, Average loss 0.819 Test accuracy 77.790\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0009828902781009673\n",
      "conv1.bias 0.011837675236165524\n",
      "conv2.weight 0.0007277842611074448\n",
      "conv2.bias 0.004090474918484688\n",
      "fc1.weight 0.0010237384587526322\n",
      "fc1.bias 0.0037846971303224564\n",
      "\n",
      "Test set: Average loss: 0.4719 \n",
      "Accuracy: 8984/10000 (89.84%)\n",
      "\n",
      "Round   7, Average loss 0.472 Test accuracy 89.840\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0013626953959465027\n",
      "conv1.bias 0.015816092491149902\n",
      "conv2.weight 0.0005533542484045028\n",
      "conv2.bias 0.003395477309823036\n",
      "fc1.weight 0.0008952190168201923\n",
      "fc1.bias 0.006606195867061615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.4758 \n",
      "Accuracy: 8948/10000 (89.48%)\n",
      "\n",
      "Round   8, Average loss 0.476 Test accuracy 89.480\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0011985206604003906\n",
      "conv1.bias 0.01862991228699684\n",
      "conv2.weight 0.0005391519889235497\n",
      "conv2.bias 0.0038108541630208492\n",
      "fc1.weight 0.0008593806996941566\n",
      "fc1.bias 0.005797397717833519\n",
      "\n",
      "Test set: Average loss: 0.5074 \n",
      "Accuracy: 8965/10000 (89.65%)\n",
      "\n",
      "Round   9, Average loss 0.507 Test accuracy 89.650\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0010294896364212037\n",
      "conv1.bias 0.017067531123757362\n",
      "conv2.weight 0.0006431344151496887\n",
      "conv2.bias 0.0037859799340367317\n",
      "fc1.weight 0.0006634674966335297\n",
      "fc1.bias 0.003930662199854851\n",
      "\n",
      "Test set: Average loss: 0.5180 \n",
      "Accuracy: 8963/10000 (89.63%)\n",
      "\n",
      "Round  10, Average loss 0.518 Test accuracy 89.630\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0009827519953250884\n",
      "conv1.bias 0.01579016074538231\n",
      "conv2.weight 0.0007383423298597336\n",
      "conv2.bias 0.003875371068716049\n",
      "fc1.weight 0.0008583059534430504\n",
      "fc1.bias 0.004334656894207001\n",
      "\n",
      "Test set: Average loss: 0.4825 \n",
      "Accuracy: 8965/10000 (89.65%)\n",
      "\n",
      "Round  11, Average loss 0.482 Test accuracy 89.650\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0014319398999214172\n",
      "conv1.bias 0.019460229203104973\n",
      "conv2.weight 0.0003830131143331528\n",
      "conv2.bias 0.0031474665738642216\n",
      "fc1.weight 0.0008177939802408218\n",
      "fc1.bias 0.003406568244099617\n",
      "\n",
      "Test set: Average loss: 0.5111 \n",
      "Accuracy: 8811/10000 (88.11%)\n",
      "\n",
      "Round  12, Average loss 0.511 Test accuracy 88.110\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0009096390008926391\n",
      "conv1.bias 0.012658135034143925\n",
      "conv2.weight 0.0007346901297569275\n",
      "conv2.bias 0.0035445531830191612\n",
      "fc1.weight 0.0008610542863607407\n",
      "fc1.bias 0.0022500233724713327\n",
      "\n",
      "Test set: Average loss: 0.4773 \n",
      "Accuracy: 9039/10000 (90.39%)\n",
      "\n",
      "Round  13, Average loss 0.477 Test accuracy 90.390\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.002142977863550186\n",
      "conv1.bias 0.022115133702754974\n",
      "conv2.weight 0.00026095073670148847\n",
      "conv2.bias 0.002771242056041956\n",
      "fc1.weight 0.0008850664831697941\n",
      "fc1.bias 0.0034570049494504927\n",
      "\n",
      "Test set: Average loss: 0.5783 \n",
      "Accuracy: 8467/10000 (84.67%)\n",
      "\n",
      "Round  14, Average loss 0.578 Test accuracy 84.670\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0017586088180541993\n",
      "conv1.bias 0.01954718865454197\n",
      "conv2.weight 0.0002359546162188053\n",
      "conv2.bias 0.0027390739414840937\n",
      "fc1.weight 0.0007961686700582504\n",
      "fc1.bias 0.0016873188316822052\n",
      "\n",
      "Test set: Average loss: 0.7408 \n",
      "Accuracy: 7919/10000 (79.19%)\n",
      "\n",
      "Round  15, Average loss 0.741 Test accuracy 79.190\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0015618982911109924\n",
      "conv1.bias 0.01722208596765995\n",
      "conv2.weight 0.0004131678864359856\n",
      "conv2.bias 0.0032817558385431767\n",
      "fc1.weight 0.0012475603260099887\n",
      "fc1.bias 0.0016664637252688408\n",
      "\n",
      "Test set: Average loss: 0.4592 \n",
      "Accuracy: 8999/10000 (89.99%)\n",
      "\n",
      "Round  16, Average loss 0.459 Test accuracy 89.990\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0013156361877918244\n",
      "conv1.bias 0.015709461644291878\n",
      "conv2.weight 0.00046018488705158235\n",
      "conv2.bias 0.0037240551318973303\n",
      "fc1.weight 0.0006971933878958225\n",
      "fc1.bias 0.0049755915999412535\n",
      "\n",
      "Test set: Average loss: 0.4700 \n",
      "Accuracy: 9017/10000 (90.17%)\n",
      "\n",
      "Round  17, Average loss 0.470 Test accuracy 90.170\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0012415355443954467\n",
      "conv1.bias 0.019450047984719276\n",
      "conv2.weight 0.00040258094668388367\n",
      "conv2.bias 0.00357221532613039\n",
      "fc1.weight 0.0011191627010703087\n",
      "fc1.bias 0.003859829157590866\n",
      "\n",
      "Test set: Average loss: 0.5061 \n",
      "Accuracy: 8902/10000 (89.02%)\n",
      "\n",
      "Round  18, Average loss 0.506 Test accuracy 89.020\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0014708960056304932\n",
      "conv1.bias 0.017875557765364647\n",
      "conv2.weight 0.0003913082927465439\n",
      "conv2.bias 0.003618720220401883\n",
      "fc1.weight 0.0006909467279911041\n",
      "fc1.bias 0.0031750995665788652\n",
      "\n",
      "Test set: Average loss: 0.4994 \n",
      "Accuracy: 8792/10000 (87.92%)\n",
      "\n",
      "Round  19, Average loss 0.499 Test accuracy 87.920\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.001441691815853119\n",
      "conv1.bias 0.014695713296532631\n",
      "conv2.weight 0.0003289614990353584\n",
      "conv2.bias 0.0031288196332752705\n",
      "fc1.weight 0.0008717063814401626\n",
      "fc1.bias 0.001685675047338009\n",
      "\n",
      "Test set: Average loss: 0.4540 \n",
      "Accuracy: 8989/10000 (89.89%)\n",
      "\n",
      "Round  20, Average loss 0.454 Test accuracy 89.890\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0013537058234214783\n",
      "conv1.bias 0.012538621202111244\n",
      "conv2.weight 0.00042814139276742936\n",
      "conv2.bias 0.0034045549109578133\n",
      "fc1.weight 0.0007590339984744788\n",
      "fc1.bias 0.004217297211289406\n",
      "\n",
      "Test set: Average loss: 0.4756 \n",
      "Accuracy: 8952/10000 (89.52%)\n",
      "\n",
      "Round  21, Average loss 0.476 Test accuracy 89.520\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00131346195936203\n",
      "conv1.bias 0.014741955325007439\n",
      "conv2.weight 0.0005435309186577797\n",
      "conv2.bias 0.003412896301597357\n",
      "fc1.weight 0.0009358589537441731\n",
      "fc1.bias 0.00402982197701931\n",
      "\n",
      "Test set: Average loss: 0.4782 \n",
      "Accuracy: 9031/10000 (90.31%)\n",
      "\n",
      "Round  22, Average loss 0.478 Test accuracy 90.310\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0011548611521720886\n",
      "conv1.bias 0.019684866070747375\n",
      "conv2.weight 0.0005000262334942817\n",
      "conv2.bias 0.0035934008192270994\n",
      "fc1.weight 0.0007564004044979811\n",
      "fc1.bias 0.0033803287893533706\n",
      "\n",
      "Test set: Average loss: 0.4691 \n",
      "Accuracy: 8996/10000 (89.96%)\n",
      "\n",
      "Round  23, Average loss 0.469 Test accuracy 89.960\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0012269437313079834\n",
      "conv1.bias 0.017144301906228065\n",
      "conv2.weight 0.000493396371603012\n",
      "conv2.bias 0.0036098232958465815\n",
      "fc1.weight 0.0008242769166827202\n",
      "fc1.bias 0.0038543295115232466\n",
      "\n",
      "Test set: Average loss: 0.4983 \n",
      "Accuracy: 8974/10000 (89.74%)\n",
      "\n",
      "Round  24, Average loss 0.498 Test accuracy 89.740\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0014451372623443604\n",
      "conv1.bias 0.014881748706102371\n",
      "conv2.weight 0.0004231104254722595\n",
      "conv2.bias 0.0030058047268539667\n",
      "fc1.weight 0.000759377982467413\n",
      "fc1.bias 0.0034797728061676026\n",
      "\n",
      "Test set: Average loss: 0.4409 \n",
      "Accuracy: 9024/10000 (90.24%)\n",
      "\n",
      "Round  25, Average loss 0.441 Test accuracy 90.240\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0010722749680280685\n",
      "conv1.bias 0.017338665202260017\n",
      "conv2.weight 0.00038110718131065366\n",
      "conv2.bias 0.003408798947930336\n",
      "fc1.weight 0.0010313299484550954\n",
      "fc1.bias 0.0030903998762369157\n",
      "\n",
      "Test set: Average loss: 0.4690 \n",
      "Accuracy: 8989/10000 (89.89%)\n",
      "\n",
      "Round  26, Average loss 0.469 Test accuracy 89.890\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0014657646417617798\n",
      "conv1.bias 0.024544335901737213\n",
      "conv2.weight 0.00040656641125679015\n",
      "conv2.bias 0.003450085874646902\n",
      "fc1.weight 0.0007053195498883724\n",
      "fc1.bias 0.003691619262099266\n",
      "\n",
      "Test set: Average loss: 0.5526 \n",
      "Accuracy: 8828/10000 (88.28%)\n",
      "\n",
      "Round  27, Average loss 0.553 Test accuracy 88.280\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0010164336860179902\n",
      "conv1.bias 0.01649980992078781\n",
      "conv2.weight 0.0007914922386407852\n",
      "conv2.bias 0.003883230732753873\n",
      "fc1.weight 0.0007806655950844288\n",
      "fc1.bias 0.004503266140818596\n",
      "\n",
      "Test set: Average loss: 0.5033 \n",
      "Accuracy: 8884/10000 (88.84%)\n",
      "\n",
      "Round  28, Average loss 0.503 Test accuracy 88.840\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0015048620104789734\n",
      "conv1.bias 0.014506197534501553\n",
      "conv2.weight 0.0005725739523768425\n",
      "conv2.bias 0.0032111534383147955\n",
      "fc1.weight 0.0012553749606013299\n",
      "fc1.bias 0.0035674691200256346\n",
      "\n",
      "Test set: Average loss: 0.5050 \n",
      "Accuracy: 8893/10000 (88.93%)\n",
      "\n",
      "Round  29, Average loss 0.505 Test accuracy 88.930\n",
      "(T, sigma)= 5 1 )  4 -th Trial!!\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.01305637240409851\n",
      "conv1.bias 0.015570824965834618\n",
      "conv2.weight 0.0004153258726000786\n",
      "conv2.bias 0.0005266438820399344\n",
      "fc1.weight 0.0003280916018411517\n",
      "fc1.bias 0.0003194771474227309\n",
      "\n",
      "Test set: Average loss: 2.0838 \n",
      "Accuracy: 6103/10000 (61.03%)\n",
      "\n",
      "Round   0, Average loss 2.084 Test accuracy 61.030\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0007696331292390824\n",
      "conv1.bias 0.009207175113260746\n",
      "conv2.weight 0.0009018775075674057\n",
      "conv2.bias 0.002350994385778904\n",
      "fc1.weight 0.0005280002485960722\n",
      "fc1.bias 0.0018505165353417397\n",
      "\n",
      "Test set: Average loss: 1.2021 \n",
      "Accuracy: 7143/10000 (71.43%)\n",
      "\n",
      "Round   1, Average loss 1.202 Test accuracy 71.430\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0006064807623624802\n",
      "conv1.bias 0.01409156434237957\n",
      "conv2.weight 0.0003435085341334343\n",
      "conv2.bias 0.0030872100032866\n",
      "fc1.weight 0.0007874651812016964\n",
      "fc1.bias 0.0019839372485876083\n",
      "\n",
      "Test set: Average loss: 0.6894 \n",
      "Accuracy: 7914/10000 (79.14%)\n",
      "\n",
      "Round   2, Average loss 0.689 Test accuracy 79.140\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0008444315940141678\n",
      "conv1.bias 0.01800638809800148\n",
      "conv2.weight 0.00034166984260082246\n",
      "conv2.bias 0.003597323317080736\n",
      "fc1.weight 0.001055794581770897\n",
      "fc1.bias 0.004812772199511528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.4624 \n",
      "Accuracy: 9060/10000 (90.60%)\n",
      "\n",
      "Round   3, Average loss 0.462 Test accuracy 90.600\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0009344609081745148\n",
      "conv1.bias 0.018278449773788452\n",
      "conv2.weight 0.0005759219452738762\n",
      "conv2.bias 0.003783551510423422\n",
      "fc1.weight 0.0009343058802187443\n",
      "fc1.bias 0.003958434984087944\n",
      "\n",
      "Test set: Average loss: 0.4095 \n",
      "Accuracy: 9138/10000 (91.38%)\n",
      "\n",
      "Round   4, Average loss 0.410 Test accuracy 91.380\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0008392563462257385\n",
      "conv1.bias 0.015229474753141403\n",
      "conv2.weight 0.0005753423273563385\n",
      "conv2.bias 0.003619065973907709\n",
      "fc1.weight 0.0010392120108008384\n",
      "fc1.bias 0.003698892518877983\n",
      "\n",
      "Test set: Average loss: 0.3973 \n",
      "Accuracy: 9144/10000 (91.44%)\n",
      "\n",
      "Round   5, Average loss 0.397 Test accuracy 91.440\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0010322915762662887\n",
      "conv1.bias 0.01623060181736946\n",
      "conv2.weight 0.0005421213433146477\n",
      "conv2.bias 0.003654582193121314\n",
      "fc1.weight 0.0010152162984013557\n",
      "fc1.bias 0.002686408720910549\n",
      "\n",
      "Test set: Average loss: 0.4073 \n",
      "Accuracy: 9072/10000 (90.72%)\n",
      "\n",
      "Round   6, Average loss 0.407 Test accuracy 90.720\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.000857292115688324\n",
      "conv1.bias 0.01172160916030407\n",
      "conv2.weight 0.0006110330671072006\n",
      "conv2.bias 0.003242640756070614\n",
      "fc1.weight 0.0010261145420372487\n",
      "fc1.bias 0.0020795373246073725\n",
      "\n",
      "Test set: Average loss: 0.3668 \n",
      "Accuracy: 9206/10000 (92.06%)\n",
      "\n",
      "Round   7, Average loss 0.367 Test accuracy 92.060\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0010494852811098099\n",
      "conv1.bias 0.01571713760495186\n",
      "conv2.weight 0.0005276322737336158\n",
      "conv2.bias 0.0029921685345470905\n",
      "fc1.weight 0.001126452162861824\n",
      "fc1.bias 0.002190626598894596\n",
      "\n",
      "Test set: Average loss: 0.3916 \n",
      "Accuracy: 9150/10000 (91.50%)\n",
      "\n",
      "Round   8, Average loss 0.392 Test accuracy 91.500\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0007860977947711944\n",
      "conv1.bias 0.011768577620387077\n",
      "conv2.weight 0.0007047152519226074\n",
      "conv2.bias 0.0037470683455467224\n",
      "fc1.weight 0.0010734719224274158\n",
      "fc1.bias 0.0026196608319878577\n",
      "\n",
      "Test set: Average loss: 0.3630 \n",
      "Accuracy: 9156/10000 (91.56%)\n",
      "\n",
      "Round   9, Average loss 0.363 Test accuracy 91.560\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0010155150294303894\n",
      "conv1.bias 0.01354600302875042\n",
      "conv2.weight 0.0005902324989438056\n",
      "conv2.bias 0.00326011935248971\n",
      "fc1.weight 0.001105298288166523\n",
      "fc1.bias 0.0023308247327804565\n",
      "\n",
      "Test set: Average loss: 0.3690 \n",
      "Accuracy: 9157/10000 (91.57%)\n",
      "\n",
      "Round  10, Average loss 0.369 Test accuracy 91.570\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0009948303550481796\n",
      "conv1.bias 0.012430250644683838\n",
      "conv2.weight 0.0005702446028590202\n",
      "conv2.bias 0.003341731848195195\n",
      "fc1.weight 0.0009942657314240932\n",
      "fc1.bias 0.002280715107917786\n",
      "\n",
      "Test set: Average loss: 0.4252 \n",
      "Accuracy: 9041/10000 (90.41%)\n",
      "\n",
      "Round  11, Average loss 0.425 Test accuracy 90.410\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0008500120043754577\n",
      "conv1.bias 0.01262738462537527\n",
      "conv2.weight 0.000665297731757164\n",
      "conv2.bias 0.0032477229833602905\n",
      "fc1.weight 0.0010579683817923068\n",
      "fc1.bias 0.0034256838262081146\n",
      "\n",
      "Test set: Average loss: 0.4093 \n",
      "Accuracy: 9157/10000 (91.57%)\n",
      "\n",
      "Round  12, Average loss 0.409 Test accuracy 91.570\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0010524887591600418\n",
      "conv1.bias 0.013885746710002422\n",
      "conv2.weight 0.0006542155146598816\n",
      "conv2.bias 0.0030216937884688377\n",
      "fc1.weight 0.0012104885652661323\n",
      "fc1.bias 0.005370093509554863\n",
      "\n",
      "Test set: Average loss: 0.4092 \n",
      "Accuracy: 9164/10000 (91.64%)\n",
      "\n",
      "Round  13, Average loss 0.409 Test accuracy 91.640\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0014696556329727173\n",
      "conv1.bias 0.020690809935331345\n",
      "conv2.weight 0.000521862730383873\n",
      "conv2.bias 0.0030891960486769676\n",
      "fc1.weight 0.000997663289308548\n",
      "fc1.bias 0.002564219757914543\n",
      "\n",
      "Test set: Average loss: 0.4302 \n",
      "Accuracy: 9094/10000 (90.94%)\n",
      "\n",
      "Round  14, Average loss 0.430 Test accuracy 90.940\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0007649879157543183\n",
      "conv1.bias 0.013768721371889114\n",
      "conv2.weight 0.0006572915613651275\n",
      "conv2.bias 0.002973803784698248\n",
      "fc1.weight 0.0010571405291557312\n",
      "fc1.bias 0.0029557082802057267\n",
      "\n",
      "Test set: Average loss: 0.4044 \n",
      "Accuracy: 9155/10000 (91.55%)\n",
      "\n",
      "Round  15, Average loss 0.404 Test accuracy 91.550\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0013957664370536805\n",
      "conv1.bias 0.019540956243872643\n",
      "conv2.weight 0.00047562237828969957\n",
      "conv2.bias 0.002865135669708252\n",
      "fc1.weight 0.0011230974458158016\n",
      "fc1.bias 0.002969961799681187\n",
      "\n",
      "Test set: Average loss: 0.3898 \n",
      "Accuracy: 9254/10000 (92.54%)\n",
      "\n",
      "Round  16, Average loss 0.390 Test accuracy 92.540\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0010680729150772095\n",
      "conv1.bias 0.016217073425650597\n",
      "conv2.weight 0.0005392060056328774\n",
      "conv2.bias 0.003085110802203417\n",
      "fc1.weight 0.0010262452997267246\n",
      "fc1.bias 0.0017948703840374946\n",
      "\n",
      "Test set: Average loss: 0.4317 \n",
      "Accuracy: 9168/10000 (91.68%)\n",
      "\n",
      "Round  17, Average loss 0.432 Test accuracy 91.680\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.000922439843416214\n",
      "conv1.bias 0.0138840451836586\n",
      "conv2.weight 0.0006011339277029038\n",
      "conv2.bias 0.0031330930069088936\n",
      "fc1.weight 0.0010418089106678963\n",
      "fc1.bias 0.0017709014937281609\n",
      "\n",
      "Test set: Average loss: 0.3530 \n",
      "Accuracy: 9287/10000 (92.87%)\n",
      "\n",
      "Round  18, Average loss 0.353 Test accuracy 92.870\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0007942039519548416\n",
      "conv1.bias 0.013304798863828182\n",
      "conv2.weight 0.0006427714228630066\n",
      "conv2.bias 0.0028584173414856195\n",
      "fc1.weight 0.001106326188892126\n",
      "fc1.bias 0.0034807849675416946\n",
      "\n",
      "Test set: Average loss: 0.3981 \n",
      "Accuracy: 9178/10000 (91.78%)\n",
      "\n",
      "Round  19, Average loss 0.398 Test accuracy 91.780\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0010335909575223924\n",
      "conv1.bias 0.015596214681863785\n",
      "conv2.weight 0.0006675203144550323\n",
      "conv2.bias 0.0030101218726485968\n",
      "fc1.weight 0.0011128765530884267\n",
      "fc1.bias 0.004657899588346481\n",
      "\n",
      "Test set: Average loss: 0.4028 \n",
      "Accuracy: 9212/10000 (92.12%)\n",
      "\n",
      "Round  20, Average loss 0.403 Test accuracy 92.120\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0012415555119514466\n",
      "conv1.bias 0.018117906525731087\n",
      "conv2.weight 0.000573476180434227\n",
      "conv2.bias 0.0030461386777460575\n",
      "fc1.weight 0.0009596940129995346\n",
      "fc1.bias 0.003592391684651375\n",
      "\n",
      "Test set: Average loss: 0.3753 \n",
      "Accuracy: 9210/10000 (92.10%)\n",
      "\n",
      "Round  21, Average loss 0.375 Test accuracy 92.100\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0009413355588912964\n",
      "conv1.bias 0.016341015696525574\n",
      "conv2.weight 0.0007517576217651367\n",
      "conv2.bias 0.0033665294758975506\n",
      "fc1.weight 0.0010751226916909217\n",
      "fc1.bias 0.003917609900236129\n",
      "\n",
      "Test set: Average loss: 0.3950 \n",
      "Accuracy: 9123/10000 (91.23%)\n",
      "\n",
      "Round  22, Average loss 0.395 Test accuracy 91.230\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0012664347887039185\n",
      "conv1.bias 0.017950501292943954\n",
      "conv2.weight 0.0006133085489273072\n",
      "conv2.bias 0.0026253110263496637\n",
      "fc1.weight 0.0011134902015328407\n",
      "fc1.bias 0.003037458471953869\n",
      "\n",
      "Test set: Average loss: 0.4151 \n",
      "Accuracy: 9172/10000 (91.72%)\n",
      "\n",
      "Round  23, Average loss 0.415 Test accuracy 91.720\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0012280215322971345\n",
      "conv1.bias 0.02051742747426033\n",
      "conv2.weight 0.0005254220589995384\n",
      "conv2.bias 0.00288926693610847\n",
      "fc1.weight 0.0009604490362107754\n",
      "fc1.bias 0.0022038506343960763\n",
      "\n",
      "Test set: Average loss: 0.3900 \n",
      "Accuracy: 9159/10000 (91.59%)\n",
      "\n",
      "Round  24, Average loss 0.390 Test accuracy 91.590\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0011263781785964967\n",
      "conv1.bias 0.015017695724964142\n",
      "conv2.weight 0.00059877909719944\n",
      "conv2.bias 0.0033651175908744335\n",
      "fc1.weight 0.001029573567211628\n",
      "fc1.bias 0.003442876413464546\n",
      "\n",
      "Test set: Average loss: 0.4196 \n",
      "Accuracy: 9145/10000 (91.45%)\n",
      "\n",
      "Round  25, Average loss 0.420 Test accuracy 91.450\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0011011329293251037\n",
      "conv1.bias 0.017980262637138367\n",
      "conv2.weight 0.0005677718296647072\n",
      "conv2.bias 0.0031048727687448263\n",
      "fc1.weight 0.0010061382316052913\n",
      "fc1.bias 0.002201608568429947\n",
      "\n",
      "Test set: Average loss: 0.3829 \n",
      "Accuracy: 9247/10000 (92.47%)\n",
      "\n",
      "Round  26, Average loss 0.383 Test accuracy 92.470\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.001431979238986969\n",
      "conv1.bias 0.022408872842788696\n",
      "conv2.weight 0.0004619777575135231\n",
      "conv2.bias 0.0028773255180567503\n",
      "fc1.weight 0.0010327269323170186\n",
      "fc1.bias 0.00255440529435873\n",
      "\n",
      "Test set: Average loss: 0.3907 \n",
      "Accuracy: 9170/10000 (91.70%)\n",
      "\n",
      "Round  27, Average loss 0.391 Test accuracy 91.700\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0009700661897659302\n",
      "conv1.bias 0.018266597762703896\n",
      "conv2.weight 0.0005617976561188698\n",
      "conv2.bias 0.0030817489605396986\n",
      "fc1.weight 0.0010555699467658997\n",
      "fc1.bias 0.00472572110593319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.3865 \n",
      "Accuracy: 9218/10000 (92.18%)\n",
      "\n",
      "Round  28, Average loss 0.387 Test accuracy 92.180\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0015293341875076295\n",
      "conv1.bias 0.02377348579466343\n",
      "conv2.weight 0.000517510287463665\n",
      "conv2.bias 0.002888001501560211\n",
      "fc1.weight 0.001115255244076252\n",
      "fc1.bias 0.0038615543395280837\n",
      "\n",
      "Test set: Average loss: 0.4165 \n",
      "Accuracy: 9186/10000 (91.86%)\n",
      "\n",
      "Round  29, Average loss 0.417 Test accuracy 91.860\n",
      "1 !!!\n",
      "z_array: [-0.94  -0.73  -0.534 -0.125  0.125  0.534  0.73   0.94 ]\n",
      "0.4486236179368535\n",
      "0.49983046821646326\n",
      "0.48963480280841937\n",
      "0.4857187736123886\n",
      "0.48571877361238697\n",
      "0.4896348028084205\n",
      "0.4998304682164621\n",
      "0.4486236179368556\n",
      "@BACC_Enc: N,K,T, m_i= 8 4 5 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 8 4 5 15000 \n",
      "\n",
      "(T, sigma)= 5 1 )  0 -th Trial!!\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.014268984794616699\n",
      "conv1.bias 0.01169295608997345\n",
      "conv2.weight 0.0004176216199994087\n",
      "conv2.bias 0.00043653909233398736\n",
      "fc1.weight 0.0003244195831939578\n",
      "fc1.bias 0.0003057722933590412\n",
      "\n",
      "Test set: Average loss: 2.3023 \n",
      "Accuracy: 1833/10000 (18.33%)\n",
      "\n",
      "Round   0, Average loss 2.302 Test accuracy 18.330\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0003131096065044403\n",
      "conv1.bias 0.0009442858281545341\n",
      "conv2.weight 0.00026686189696192744\n",
      "conv2.bias 0.0004811444377992302\n",
      "fc1.weight 6.426118779927492e-05\n",
      "fc1.bias 0.00044413148425519465\n",
      "\n",
      "Test set: Average loss: 2.2996 \n",
      "Accuracy: 2417/10000 (24.17%)\n",
      "\n",
      "Round   1, Average loss 2.300 Test accuracy 24.170\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00011496199294924736\n",
      "conv1.bias 0.0036908816546201706\n",
      "conv2.weight 8.228883147239686e-05\n",
      "conv2.bias 0.0011048223823308945\n",
      "fc1.weight 0.00010194967035204172\n",
      "fc1.bias 0.0008958472870290279\n",
      "\n",
      "Test set: Average loss: 2.2969 \n",
      "Accuracy: 2484/10000 (24.84%)\n",
      "\n",
      "Round   2, Average loss 2.297 Test accuracy 24.840\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0003742799162864685\n",
      "conv1.bias 0.001630469225347042\n",
      "conv2.weight 6.559327710419893e-05\n",
      "conv2.bias 0.0010798515286296606\n",
      "fc1.weight 0.00013586044078692794\n",
      "fc1.bias 0.0005260111764073371\n",
      "\n",
      "Test set: Average loss: 2.2993 \n",
      "Accuracy: 1135/10000 (11.35%)\n",
      "\n",
      "Round   3, Average loss 2.299 Test accuracy 11.350\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0002944783866405487\n",
      "conv1.bias 0.0017290201503783464\n",
      "conv2.weight 6.837458815425634e-05\n",
      "conv2.bias 0.0010633887723088264\n",
      "fc1.weight 0.00013551110168918967\n",
      "fc1.bias 0.0007251320406794548\n",
      "\n",
      "Test set: Average loss: 2.2705 \n",
      "Accuracy: 4495/10000 (44.95%)\n",
      "\n",
      "Round   4, Average loss 2.270 Test accuracy 44.950\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0003426792845129967\n",
      "conv1.bias 0.001550573855638504\n",
      "conv2.weight 6.158105563372373e-05\n",
      "conv2.bias 0.0008061306434683502\n",
      "fc1.weight 0.00012011795770376921\n",
      "fc1.bias 0.0004272507503628731\n",
      "\n",
      "Test set: Average loss: 2.2872 \n",
      "Accuracy: 1433/10000 (14.33%)\n",
      "\n",
      "Round   5, Average loss 2.287 Test accuracy 14.330\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00022736115381121635\n",
      "conv1.bias 0.0017964612925425172\n",
      "conv2.weight 7.496973499655724e-05\n",
      "conv2.bias 0.000996299204416573\n",
      "fc1.weight 0.00011111635249108076\n",
      "fc1.bias 0.00024986679200083016\n",
      "\n",
      "Test set: Average loss: 2.2852 \n",
      "Accuracy: 5252/10000 (52.52%)\n",
      "\n",
      "Round   6, Average loss 2.285 Test accuracy 52.520\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00034264054149389265\n",
      "conv1.bias 0.0014595906250178814\n",
      "conv2.weight 7.653974462300539e-05\n",
      "conv2.bias 0.0007874269504100084\n",
      "fc1.weight 0.0001628589117899537\n",
      "fc1.bias 0.001221233792603016\n",
      "\n",
      "Test set: Average loss: 2.3006 \n",
      "Accuracy: 1943/10000 (19.43%)\n",
      "\n",
      "Round   7, Average loss 2.301 Test accuracy 19.430\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00012657435610890388\n",
      "conv1.bias 0.0031412511598318815\n",
      "conv2.weight 4.334178287535906e-05\n",
      "conv2.bias 0.001016876776702702\n",
      "fc1.weight 0.00012642930960282683\n",
      "fc1.bias 0.00022992328740656376\n",
      "\n",
      "Test set: Average loss: 2.3020 \n",
      "Accuracy: 1412/10000 (14.12%)\n",
      "\n",
      "Round   8, Average loss 2.302 Test accuracy 14.120\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00016798194497823714\n",
      "conv1.bias 0.0021198666654527187\n",
      "conv2.weight 7.107548881322146e-05\n",
      "conv2.bias 0.001249640597961843\n",
      "fc1.weight 0.00022257817909121513\n",
      "fc1.bias 0.0005326104816049338\n",
      "\n",
      "Test set: Average loss: 2.3010 \n",
      "Accuracy: 1361/10000 (13.61%)\n",
      "\n",
      "Round   9, Average loss 2.301 Test accuracy 13.610\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0002002188190817833\n",
      "conv1.bias 0.0020133766811341047\n",
      "conv2.weight 3.627144731581211e-05\n",
      "conv2.bias 0.0008175584371201694\n",
      "fc1.weight 0.00010949417483061553\n",
      "fc1.bias 0.0003286793828010559\n",
      "\n",
      "Test set: Average loss: 2.2799 \n",
      "Accuracy: 3480/10000 (34.80%)\n",
      "\n",
      "Round  10, Average loss 2.280 Test accuracy 34.800\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00025015302002429963\n",
      "conv1.bias 0.002497836947441101\n",
      "conv2.weight 9.66675765812397e-05\n",
      "conv2.bias 0.0011326996609568596\n",
      "fc1.weight 0.0001988079398870468\n",
      "fc1.bias 0.0004736010916531086\n",
      "\n",
      "Test set: Average loss: 2.2955 \n",
      "Accuracy: 2294/10000 (22.94%)\n",
      "\n",
      "Round  11, Average loss 2.295 Test accuracy 22.940\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00023130081593990325\n",
      "conv1.bias 0.00224514352157712\n",
      "conv2.weight 4.501534625887871e-05\n",
      "conv2.bias 0.0009223626693710685\n",
      "fc1.weight 0.00010714271338656545\n",
      "fc1.bias 0.000313852378167212\n",
      "\n",
      "Test set: Average loss: 2.2598 \n",
      "Accuracy: 4344/10000 (43.44%)\n",
      "\n",
      "Round  12, Average loss 2.260 Test accuracy 43.440\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0003431330621242523\n",
      "conv1.bias 0.001695991144515574\n",
      "conv2.weight 7.16119771823287e-05\n",
      "conv2.bias 0.0006985953077673912\n",
      "fc1.weight 0.0003174509853124619\n",
      "fc1.bias 0.00046367431059479713\n",
      "\n",
      "Test set: Average loss: 2.2550 \n",
      "Accuracy: 5187/10000 (51.87%)\n",
      "\n",
      "Round  13, Average loss 2.255 Test accuracy 51.870\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00015496532432734966\n",
      "conv1.bias 0.002664486411958933\n",
      "conv2.weight 5.9418147429823875e-05\n",
      "conv2.bias 0.001229217741638422\n",
      "fc1.weight 0.000110255042091012\n",
      "fc1.bias 0.00028634760528802874\n",
      "\n",
      "Test set: Average loss: 2.2764 \n",
      "Accuracy: 3513/10000 (35.13%)\n",
      "\n",
      "Round  14, Average loss 2.276 Test accuracy 35.130\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0002496381103992462\n",
      "conv1.bias 0.0015715962508693337\n",
      "conv2.weight 6.717672105878592e-05\n",
      "conv2.bias 0.0007377754081971943\n",
      "fc1.weight 0.00026665001641958954\n",
      "fc1.bias 0.0006661145482212306\n",
      "\n",
      "Test set: Average loss: 2.2065 \n",
      "Accuracy: 6122/10000 (61.22%)\n",
      "\n",
      "Round  15, Average loss 2.206 Test accuracy 61.220\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0003056209348142147\n",
      "conv1.bias 0.002488108817487955\n",
      "conv2.weight 5.901368334889412e-05\n",
      "conv2.bias 0.0008656118297949433\n",
      "fc1.weight 0.00012909816578030587\n",
      "fc1.bias 0.0004356383346021175\n",
      "\n",
      "Test set: Average loss: 2.1199 \n",
      "Accuracy: 5524/10000 (55.24%)\n",
      "\n",
      "Round  16, Average loss 2.120 Test accuracy 55.240\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00031794779002666476\n",
      "conv1.bias 0.002047694521024823\n",
      "conv2.weight 5.69462962448597e-05\n",
      "conv2.bias 0.0008402992971241474\n",
      "fc1.weight 0.00026635269168764355\n",
      "fc1.bias 0.0005612705368548632\n",
      "\n",
      "Test set: Average loss: 2.1810 \n",
      "Accuracy: 6925/10000 (69.25%)\n",
      "\n",
      "Round  17, Average loss 2.181 Test accuracy 69.250\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.000179869681596756\n",
      "conv1.bias 0.002253095619380474\n",
      "conv2.weight 4.368963651359081e-05\n",
      "conv2.bias 0.0007852241978980601\n",
      "fc1.weight 0.00015449372585862875\n",
      "fc1.bias 0.0005799909587949515\n",
      "\n",
      "Test set: Average loss: 2.0763 \n",
      "Accuracy: 7829/10000 (78.29%)\n",
      "\n",
      "Round  18, Average loss 2.076 Test accuracy 78.290\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0003126700222492218\n",
      "conv1.bias 0.002303013112396002\n",
      "conv2.weight 6.148220971226693e-05\n",
      "conv2.bias 0.0007802455220371485\n",
      "fc1.weight 0.0003184111090376973\n",
      "fc1.bias 0.00045247268863022325\n",
      "\n",
      "Test set: Average loss: 2.0790 \n",
      "Accuracy: 6732/10000 (67.32%)\n",
      "\n",
      "Round  19, Average loss 2.079 Test accuracy 67.320\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0002633274905383587\n",
      "conv1.bias 0.0027577783912420273\n",
      "conv2.weight 6.115220952779054e-05\n",
      "conv2.bias 0.0008705538930371404\n",
      "fc1.weight 0.00021693839225918056\n",
      "fc1.bias 0.0012956999242305755\n",
      "\n",
      "Test set: Average loss: 2.0970 \n",
      "Accuracy: 6511/10000 (65.11%)\n",
      "\n",
      "Round  20, Average loss 2.097 Test accuracy 65.110\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00041862394660711287\n",
      "conv1.bias 0.0025028628297150135\n",
      "conv2.weight 6.549335550516844e-05\n",
      "conv2.bias 0.0007449659751728177\n",
      "fc1.weight 0.0001902994350530207\n",
      "fc1.bias 0.0005613338202238083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.0973 \n",
      "Accuracy: 7203/10000 (72.03%)\n",
      "\n",
      "Round  21, Average loss 2.097 Test accuracy 72.030\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00032542333006858827\n",
      "conv1.bias 0.001976609928533435\n",
      "conv2.weight 9.268948808312416e-05\n",
      "conv2.bias 0.0007242119172587991\n",
      "fc1.weight 0.00015902143204584717\n",
      "fc1.bias 0.0006433987058699131\n",
      "\n",
      "Test set: Average loss: 2.1340 \n",
      "Accuracy: 6070/10000 (60.70%)\n",
      "\n",
      "Round  22, Average loss 2.134 Test accuracy 60.700\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0003886595368385315\n",
      "conv1.bias 0.0015512711834162474\n",
      "conv2.weight 5.6357337161898616e-05\n",
      "conv2.bias 0.0006658534985035658\n",
      "fc1.weight 0.00024015111848711966\n",
      "fc1.bias 0.0006502380128949881\n",
      "\n",
      "Test set: Average loss: 2.1523 \n",
      "Accuracy: 5811/10000 (58.11%)\n",
      "\n",
      "Round  23, Average loss 2.152 Test accuracy 58.110\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00027584198862314226\n",
      "conv1.bias 0.0020328075625002384\n",
      "conv2.weight 8.551175706088543e-05\n",
      "conv2.bias 0.0008602143498137593\n",
      "fc1.weight 0.0001800594269298017\n",
      "fc1.bias 0.0004255218431353569\n",
      "\n",
      "Test set: Average loss: 2.2532 \n",
      "Accuracy: 4438/10000 (44.38%)\n",
      "\n",
      "Round  24, Average loss 2.253 Test accuracy 44.380\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00030974937602877616\n",
      "conv1.bias 0.001198371173813939\n",
      "conv2.weight 5.682486575096846e-05\n",
      "conv2.bias 0.0006603692891076207\n",
      "fc1.weight 0.000185163295827806\n",
      "fc1.bias 0.0010663773864507675\n",
      "\n",
      "Test set: Average loss: 2.2898 \n",
      "Accuracy: 3858/10000 (38.58%)\n",
      "\n",
      "Round  25, Average loss 2.290 Test accuracy 38.580\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00022043056786060332\n",
      "conv1.bias 0.001941402442753315\n",
      "conv2.weight 3.8665325846523046e-05\n",
      "conv2.bias 0.0007740802248008549\n",
      "fc1.weight 0.00014979527331888677\n",
      "fc1.bias 0.0006883175112307072\n",
      "\n",
      "Test set: Average loss: 2.2951 \n",
      "Accuracy: 2104/10000 (21.04%)\n",
      "\n",
      "Round  26, Average loss 2.295 Test accuracy 21.040\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0002175798825919628\n",
      "conv1.bias 0.0013152605388313532\n",
      "conv2.weight 1.6982099041342735e-05\n",
      "conv2.bias 0.0004982042592018843\n",
      "fc1.weight 0.00010182014666497708\n",
      "fc1.bias 0.0005210121162235737\n",
      "\n",
      "Test set: Average loss: 2.2872 \n",
      "Accuracy: 3457/10000 (34.57%)\n",
      "\n",
      "Round  27, Average loss 2.287 Test accuracy 34.570\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00039522912353277204\n",
      "conv1.bias 0.0008414470357820392\n",
      "conv2.weight 6.334185134619474e-05\n",
      "conv2.bias 0.0006820459384471178\n",
      "fc1.weight 0.0002387954154983163\n",
      "fc1.bias 0.0002568337833508849\n",
      "\n",
      "Test set: Average loss: 2.2363 \n",
      "Accuracy: 4662/10000 (46.62%)\n",
      "\n",
      "Round  28, Average loss 2.236 Test accuracy 46.620\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00032449211925268174\n",
      "conv1.bias 0.002501179464161396\n",
      "conv2.weight 5.8809584006667135e-05\n",
      "conv2.bias 0.0008780879434198141\n",
      "fc1.weight 9.436974069103599e-05\n",
      "fc1.bias 0.00027462546713650224\n",
      "\n",
      "Test set: Average loss: 2.2943 \n",
      "Accuracy: 2626/10000 (26.26%)\n",
      "\n",
      "Round  29, Average loss 2.294 Test accuracy 26.260\n",
      "(T, sigma)= 5 1 )  1 -th Trial!!\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.013284789323806763\n",
      "conv1.bias 0.011925027705729008\n",
      "conv2.weight 0.00041490159928798677\n",
      "conv2.bias 0.00041508645517751575\n",
      "fc1.weight 0.0003259962424635887\n",
      "fc1.bias 0.00035539499949663875\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00022541675716638565\n",
      "conv1.bias 0.0018818483222275972\n",
      "conv2.weight 0.0002405884861946106\n",
      "conv2.bias 0.0005642196047119796\n",
      "fc1.weight 5.009395536035299e-05\n",
      "fc1.bias 0.00033915690146386624\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1347/10000 (13.47%)\n",
      "\n",
      "Round   1, Average loss 2.303 Test accuracy 13.470\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00012094439938664436\n",
      "conv1.bias 0.002533520106226206\n",
      "conv2.weight 8.118873462080956e-05\n",
      "conv2.bias 0.0010359552688896656\n",
      "fc1.weight 5.6164641864597795e-05\n",
      "fc1.bias 0.0005021088756620884\n",
      "\n",
      "Test set: Average loss: 2.3022 \n",
      "Accuracy: 1143/10000 (11.43%)\n",
      "\n",
      "Round   2, Average loss 2.302 Test accuracy 11.430\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0003005698323249817\n",
      "conv1.bias 0.0011058025993406773\n",
      "conv2.weight 5.767982453107834e-05\n",
      "conv2.bias 0.0007978104986250401\n",
      "fc1.weight 0.00028473972342908383\n",
      "fc1.bias 0.00047325859777629374\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   3, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00012972687371075155\n",
      "conv1.bias 0.0026161475107073784\n",
      "conv2.weight 0.00017590675503015518\n",
      "conv2.bias 0.0021940856240689754\n",
      "fc1.weight 6.57075783237815e-05\n",
      "fc1.bias 0.00042371246963739394\n",
      "\n",
      "Test set: Average loss: 2.2927 \n",
      "Accuracy: 3882/10000 (38.82%)\n",
      "\n",
      "Round   4, Average loss 2.293 Test accuracy 38.820\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0003192390501499176\n",
      "conv1.bias 0.0013527658302336931\n",
      "conv2.weight 5.644851829856634e-05\n",
      "conv2.bias 0.0012762690894305706\n",
      "fc1.weight 0.00014125960879027844\n",
      "fc1.bias 0.00029278146103024484\n",
      "\n",
      "Test set: Average loss: 2.2868 \n",
      "Accuracy: 4382/10000 (43.82%)\n",
      "\n",
      "Round   5, Average loss 2.287 Test accuracy 43.820\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00032724305987358093\n",
      "conv1.bias 0.0013177173677831888\n",
      "conv2.weight 5.235968623310328e-05\n",
      "conv2.bias 0.0008999889832921326\n",
      "fc1.weight 0.00018971890676766634\n",
      "fc1.bias 0.0004374210722744465\n",
      "\n",
      "Test set: Average loss: 2.3030 \n",
      "Accuracy: 820/10000 (8.20%)\n",
      "\n",
      "Round   6, Average loss 2.303 Test accuracy 8.200\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00018621493130922318\n",
      "conv1.bias 0.0019762858282774687\n",
      "conv2.weight 6.120439618825913e-05\n",
      "conv2.bias 0.0011064037680625916\n",
      "fc1.weight 0.00016353519167751075\n",
      "fc1.bias 0.0006347693968564271\n",
      "\n",
      "Test set: Average loss: 2.2200 \n",
      "Accuracy: 5483/10000 (54.83%)\n",
      "\n",
      "Round   7, Average loss 2.220 Test accuracy 54.830\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0005043085664510726\n",
      "conv1.bias 0.0011643811594694853\n",
      "conv2.weight 0.00016778336837887763\n",
      "conv2.bias 0.0011450970778241754\n",
      "fc1.weight 0.00025503865908831357\n",
      "fc1.bias 0.0017136536538600922\n",
      "\n",
      "Test set: Average loss: 2.2591 \n",
      "Accuracy: 4510/10000 (45.10%)\n",
      "\n",
      "Round   8, Average loss 2.259 Test accuracy 45.100\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00022891495376825332\n",
      "conv1.bias 0.002177936490625143\n",
      "conv2.weight 0.00014511152170598507\n",
      "conv2.bias 0.001384316012263298\n",
      "fc1.weight 0.00013552443124353886\n",
      "fc1.bias 0.0015306576155126096\n",
      "\n",
      "Test set: Average loss: 2.2665 \n",
      "Accuracy: 4756/10000 (47.56%)\n",
      "\n",
      "Round   9, Average loss 2.267 Test accuracy 47.560\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0002309119701385498\n",
      "conv1.bias 0.0012407798785716295\n",
      "conv2.weight 8.011444471776485e-05\n",
      "conv2.bias 0.0010341391898691654\n",
      "fc1.weight 0.00016987579874694347\n",
      "fc1.bias 0.0013542006723582744\n",
      "\n",
      "Test set: Average loss: 2.2936 \n",
      "Accuracy: 1278/10000 (12.78%)\n",
      "\n",
      "Round  10, Average loss 2.294 Test accuracy 12.780\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00019323244690895081\n",
      "conv1.bias 0.0017537784297019243\n",
      "conv2.weight 6.887326948344708e-05\n",
      "conv2.bias 0.000901169260032475\n",
      "fc1.weight 0.00021077105775475503\n",
      "fc1.bias 0.00030561021994799373\n",
      "\n",
      "Test set: Average loss: 2.2330 \n",
      "Accuracy: 5321/10000 (53.21%)\n",
      "\n",
      "Round  11, Average loss 2.233 Test accuracy 53.210\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0004517369717359543\n",
      "conv1.bias 0.0016149526927620173\n",
      "conv2.weight 0.0001271765399724245\n",
      "conv2.bias 0.0008496895898133516\n",
      "fc1.weight 0.00016641166293993593\n",
      "fc1.bias 0.002560659870505333\n",
      "\n",
      "Test set: Average loss: 2.2305 \n",
      "Accuracy: 5049/10000 (50.49%)\n",
      "\n",
      "Round  12, Average loss 2.230 Test accuracy 50.490\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0003344804048538208\n",
      "conv1.bias 0.0009473561076447368\n",
      "conv2.weight 0.0001438325084745884\n",
      "conv2.bias 0.0008934621000662446\n",
      "fc1.weight 0.00012365311849862337\n",
      "fc1.bias 0.001710273139178753\n",
      "\n",
      "Test set: Average loss: 2.2714 \n",
      "Accuracy: 4074/10000 (40.74%)\n",
      "\n",
      "Round  13, Average loss 2.271 Test accuracy 40.740\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00024022128432989122\n",
      "conv1.bias 0.0024005358573049307\n",
      "conv2.weight 4.6707293950021264e-05\n",
      "conv2.bias 0.0008141737780533731\n",
      "fc1.weight 0.0002075193915516138\n",
      "fc1.bias 0.0012947422452270984\n",
      "\n",
      "Test set: Average loss: 2.2907 \n",
      "Accuracy: 2988/10000 (29.88%)\n",
      "\n",
      "Round  14, Average loss 2.291 Test accuracy 29.880\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00018733665347099305\n",
      "conv1.bias 0.002061467617750168\n",
      "conv2.weight 5.9524537064135075e-05\n",
      "conv2.bias 0.0009720273083075881\n",
      "fc1.weight 0.0001136606908403337\n",
      "fc1.bias 0.000702871521934867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.2729 \n",
      "Accuracy: 4620/10000 (46.20%)\n",
      "\n",
      "Round  15, Average loss 2.273 Test accuracy 46.200\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0002219069004058838\n",
      "conv1.bias 0.0015664056409150362\n",
      "conv2.weight 5.517376586794853e-05\n",
      "conv2.bias 0.0009361236589029431\n",
      "fc1.weight 0.0001511220703832805\n",
      "fc1.bias 0.0006426018662750721\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  16, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0003776770457625389\n",
      "conv1.bias 0.001058701192960143\n",
      "conv2.weight 0.0001921354793012142\n",
      "conv2.bias 0.0007832938572391868\n",
      "fc1.weight 0.00021689170971512796\n",
      "fc1.bias 0.00018706183182075619\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1028/10000 (10.28%)\n",
      "\n",
      "Round  17, Average loss 2.303 Test accuracy 10.280\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00011312151327729225\n",
      "conv1.bias 0.0017861549276858568\n",
      "conv2.weight 8.611440658569336e-05\n",
      "conv2.bias 0.001080622198060155\n",
      "fc1.weight 5.8384559815749525e-05\n",
      "fc1.bias 0.00014859528746455908\n",
      "\n",
      "Test set: Average loss: 2.2692 \n",
      "Accuracy: 4234/10000 (42.34%)\n",
      "\n",
      "Round  18, Average loss 2.269 Test accuracy 42.340\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0004292339086532593\n",
      "conv1.bias 0.00086172204464674\n",
      "conv2.weight 9.078901261091233e-05\n",
      "conv2.bias 0.0012400441337376833\n",
      "fc1.weight 0.00011946344748139381\n",
      "fc1.bias 0.0004123649559915066\n",
      "\n",
      "Test set: Average loss: 2.3007 \n",
      "Accuracy: 1360/10000 (13.60%)\n",
      "\n",
      "Round  19, Average loss 2.301 Test accuracy 13.600\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00014382408000528812\n",
      "conv1.bias 0.0020356765016913414\n",
      "conv2.weight 5.280537996441126e-05\n",
      "conv2.bias 0.0009201230714097619\n",
      "fc1.weight 0.00011548789916560054\n",
      "fc1.bias 0.0004639807157218456\n",
      "\n",
      "Test set: Average loss: 2.2892 \n",
      "Accuracy: 3254/10000 (32.54%)\n",
      "\n",
      "Round  20, Average loss 2.289 Test accuracy 32.540\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0002739034965634346\n",
      "conv1.bias 0.0014875808265060186\n",
      "conv2.weight 5.6457966566085814e-05\n",
      "conv2.bias 0.0009583005448803306\n",
      "fc1.weight 0.00011493819765746593\n",
      "fc1.bias 0.00042560440488159657\n",
      "\n",
      "Test set: Average loss: 2.2943 \n",
      "Accuracy: 2545/10000 (25.45%)\n",
      "\n",
      "Round  21, Average loss 2.294 Test accuracy 25.450\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00030138663947582245\n",
      "conv1.bias 0.0007834057323634624\n",
      "conv2.weight 6.268692202866078e-05\n",
      "conv2.bias 0.0007916471222415566\n",
      "fc1.weight 0.00021140221506357192\n",
      "fc1.bias 0.00038248994387686254\n",
      "\n",
      "Test set: Average loss: 2.2972 \n",
      "Accuracy: 2476/10000 (24.76%)\n",
      "\n",
      "Round  22, Average loss 2.297 Test accuracy 24.760\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00027554573491215704\n",
      "conv1.bias 0.0019191132159903646\n",
      "conv2.weight 0.00010662533342838288\n",
      "conv2.bias 0.000760571681894362\n",
      "fc1.weight 0.00011772741563618184\n",
      "fc1.bias 0.0007818211801350117\n",
      "\n",
      "Test set: Average loss: 2.3017 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  23, Average loss 2.302 Test accuracy 9.800\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00014947259798645974\n",
      "conv1.bias 0.0036284467205405235\n",
      "conv2.weight 0.00010567829012870788\n",
      "conv2.bias 0.0013501052744686604\n",
      "fc1.weight 0.00013168007135391235\n",
      "fc1.bias 0.0007268278393894434\n",
      "\n",
      "Test set: Average loss: 2.2979 \n",
      "Accuracy: 985/10000 (9.85%)\n",
      "\n",
      "Round  24, Average loss 2.298 Test accuracy 9.850\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00030639033764600753\n",
      "conv1.bias 0.0007974578184075654\n",
      "conv2.weight 5.113450810313225e-05\n",
      "conv2.bias 0.0009682560921646655\n",
      "fc1.weight 0.00010752087691798807\n",
      "fc1.bias 0.0008033724501729011\n",
      "\n",
      "Test set: Average loss: 2.2933 \n",
      "Accuracy: 2875/10000 (28.75%)\n",
      "\n",
      "Round  25, Average loss 2.293 Test accuracy 28.750\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00035053033381700517\n",
      "conv1.bias 0.001189940725453198\n",
      "conv2.weight 5.226020235568285e-05\n",
      "conv2.bias 0.0007482898654416203\n",
      "fc1.weight 0.00021447737235575915\n",
      "fc1.bias 0.0016157573089003562\n",
      "\n",
      "Test set: Average loss: 2.2676 \n",
      "Accuracy: 4070/10000 (40.70%)\n",
      "\n",
      "Round  26, Average loss 2.268 Test accuracy 40.700\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00035757377743721007\n",
      "conv1.bias 0.0014555079396814108\n",
      "conv2.weight 3.4968173131346703e-05\n",
      "conv2.bias 0.0009231833973899484\n",
      "fc1.weight 0.00011553321965038777\n",
      "fc1.bias 0.0005757452920079232\n",
      "\n",
      "Test set: Average loss: 2.2587 \n",
      "Accuracy: 4774/10000 (47.74%)\n",
      "\n",
      "Round  27, Average loss 2.259 Test accuracy 47.740\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0002958833053708077\n",
      "conv1.bias 0.0014434817712754011\n",
      "conv2.weight 6.719470489770174e-05\n",
      "conv2.bias 0.0007927190163172781\n",
      "fc1.weight 0.00027652850840240715\n",
      "fc1.bias 0.0007748712785542011\n",
      "\n",
      "Test set: Average loss: 2.3022 \n",
      "Accuracy: 1754/10000 (17.54%)\n",
      "\n",
      "Round  28, Average loss 2.302 Test accuracy 17.540\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0002306104637682438\n",
      "conv1.bias 0.002248168457299471\n",
      "conv2.weight 3.0112506356090306e-05\n",
      "conv2.bias 0.0006622521905228496\n",
      "fc1.weight 0.0001367720076814294\n",
      "fc1.bias 0.0002615846460685134\n",
      "\n",
      "Test set: Average loss: 2.3016 \n",
      "Accuracy: 1957/10000 (19.57%)\n",
      "\n",
      "Round  29, Average loss 2.302 Test accuracy 19.570\n",
      "(T, sigma)= 5 1 )  2 -th Trial!!\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.013088257312774658\n",
      "conv1.bias 0.015008548274636269\n",
      "conv2.weight 0.0004154669493436813\n",
      "conv2.bias 0.0005060059484094381\n",
      "fc1.weight 0.0003205461660400033\n",
      "fc1.bias 0.0003220518119633198\n",
      "\n",
      "Test set: Average loss: 2.3018 \n",
      "Accuracy: 1706/10000 (17.06%)\n",
      "\n",
      "Round   0, Average loss 2.302 Test accuracy 17.060\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00030086252838373184\n",
      "conv1.bias 0.0019704331643879414\n",
      "conv2.weight 0.00026913901790976525\n",
      "conv2.bias 0.0004908806295134127\n",
      "fc1.weight 7.248287438414991e-05\n",
      "fc1.bias 0.0001800304977223277\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   1, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00012419654056429862\n",
      "conv1.bias 0.002492851810529828\n",
      "conv2.weight 4.49052732437849e-05\n",
      "conv2.bias 0.0008378088241443038\n",
      "fc1.weight 7.125424453988672e-05\n",
      "fc1.bias 0.00033073350787162783\n",
      "\n",
      "Test set: Average loss: 2.3013 \n",
      "Accuracy: 1904/10000 (19.04%)\n",
      "\n",
      "Round   2, Average loss 2.301 Test accuracy 19.040\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0002716059051454067\n",
      "conv1.bias 0.002363016130402684\n",
      "conv2.weight 0.00010580694302916526\n",
      "conv2.bias 0.001243048463948071\n",
      "fc1.weight 9.918055729940534e-05\n",
      "fc1.bias 0.0005822765175253153\n",
      "\n",
      "Test set: Average loss: 2.2888 \n",
      "Accuracy: 3110/10000 (31.10%)\n",
      "\n",
      "Round   3, Average loss 2.289 Test accuracy 31.100\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00032772112637758255\n",
      "conv1.bias 0.0010261074639856815\n",
      "conv2.weight 6.874264683574438e-05\n",
      "conv2.bias 0.001140235224738717\n",
      "fc1.weight 7.815604913048446e-05\n",
      "fc1.bias 0.0003235669806599617\n",
      "\n",
      "Test set: Average loss: 2.3016 \n",
      "Accuracy: 1643/10000 (16.43%)\n",
      "\n",
      "Round   4, Average loss 2.302 Test accuracy 16.430\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00026891689747571946\n",
      "conv1.bias 0.001188740716315806\n",
      "conv2.weight 5.582010373473167e-05\n",
      "conv2.bias 0.0007583439582958817\n",
      "fc1.weight 0.0002888823160901666\n",
      "fc1.bias 0.0007070647086948156\n",
      "\n",
      "Test set: Average loss: 2.3020 \n",
      "Accuracy: 1135/10000 (11.35%)\n",
      "\n",
      "Round   5, Average loss 2.302 Test accuracy 11.350\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00029369190335273745\n",
      "conv1.bias 0.0005310896667651832\n",
      "conv2.weight 7.815996184945106e-05\n",
      "conv2.bias 0.0010739309946075082\n",
      "fc1.weight 0.00013930401764810084\n",
      "fc1.bias 0.0004947897046804428\n",
      "\n",
      "Test set: Average loss: 2.2932 \n",
      "Accuracy: 1591/10000 (15.91%)\n",
      "\n",
      "Round   6, Average loss 2.293 Test accuracy 15.910\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00035538453608751296\n",
      "conv1.bias 0.0013707464095205069\n",
      "conv2.weight 4.581477493047714e-05\n",
      "conv2.bias 0.0009422161383554339\n",
      "fc1.weight 0.00010434420546516776\n",
      "fc1.bias 0.0006803903728723526\n",
      "\n",
      "Test set: Average loss: 2.3009 \n",
      "Accuracy: 1135/10000 (11.35%)\n",
      "\n",
      "Round   7, Average loss 2.301 Test accuracy 11.350\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0002686063200235367\n",
      "conv1.bias 0.0009518783772364259\n",
      "conv2.weight 6.698552519083023e-05\n",
      "conv2.bias 0.0007368297083303332\n",
      "fc1.weight 0.00021197721362113952\n",
      "fc1.bias 0.0006998366676270962\n",
      "\n",
      "Test set: Average loss: 2.3015 \n",
      "Accuracy: 1135/10000 (11.35%)\n",
      "\n",
      "Round   8, Average loss 2.302 Test accuracy 11.350\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0002346271649003029\n",
      "conv1.bias 0.0018851474160328507\n",
      "conv2.weight 0.0001184355840086937\n",
      "conv2.bias 0.001287562306970358\n",
      "fc1.weight 0.00013140402734279633\n",
      "fc1.bias 0.0009409936144948005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.3016 \n",
      "Accuracy: 2331/10000 (23.31%)\n",
      "\n",
      "Round   9, Average loss 2.302 Test accuracy 23.310\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0004921611398458481\n",
      "conv1.bias 0.0009109588572755456\n",
      "conv2.weight 6.337635684758425e-05\n",
      "conv2.bias 0.0010249370243400335\n",
      "fc1.weight 0.00013810942182317375\n",
      "fc1.bias 0.00032491902820765974\n",
      "\n",
      "Test set: Average loss: 2.3019 \n",
      "Accuracy: 912/10000 (9.12%)\n",
      "\n",
      "Round  10, Average loss 2.302 Test accuracy 9.120\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00012477299198508264\n",
      "conv1.bias 0.0015717362985014915\n",
      "conv2.weight 6.831612903624773e-05\n",
      "conv2.bias 0.0010691401548683643\n",
      "fc1.weight 0.00011890687746927143\n",
      "fc1.bias 0.00027617586310952903\n",
      "\n",
      "Test set: Average loss: 2.3025 \n",
      "Accuracy: 992/10000 (9.92%)\n",
      "\n",
      "Round  11, Average loss 2.303 Test accuracy 9.920\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0001664341241121292\n",
      "conv1.bias 0.0024730912409722805\n",
      "conv2.weight 2.9840916395187377e-05\n",
      "conv2.bias 0.00080812384840101\n",
      "fc1.weight 0.0001414005644619465\n",
      "fc1.bias 0.00036685550585389135\n",
      "\n",
      "Test set: Average loss: 2.3007 \n",
      "Accuracy: 1175/10000 (11.75%)\n",
      "\n",
      "Round  12, Average loss 2.301 Test accuracy 11.750\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00024879168719053266\n",
      "conv1.bias 0.0007680805865675211\n",
      "conv2.weight 6.914353463798761e-05\n",
      "conv2.bias 0.0010584243573248386\n",
      "fc1.weight 8.424150873906911e-05\n",
      "fc1.bias 0.00033315292093902824\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  13, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00010012680664658546\n",
      "conv1.bias 0.0027972888201475143\n",
      "conv2.weight 6.58706936519593e-06\n",
      "conv2.bias 0.0008229857194237411\n",
      "fc1.weight 0.00019867513328790664\n",
      "fc1.bias 0.0004793971311300993\n",
      "\n",
      "Test set: Average loss: 2.2864 \n",
      "Accuracy: 1142/10000 (11.42%)\n",
      "\n",
      "Round  14, Average loss 2.286 Test accuracy 11.420\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00019695909693837166\n",
      "conv1.bias 0.0008461110992357135\n",
      "conv2.weight 4.5775952748954296e-05\n",
      "conv2.bias 0.0009746933938004076\n",
      "fc1.weight 0.0001342258066870272\n",
      "fc1.bias 0.0007576136849820614\n",
      "\n",
      "Test set: Average loss: 2.3024 \n",
      "Accuracy: 1725/10000 (17.25%)\n",
      "\n",
      "Round  15, Average loss 2.302 Test accuracy 17.250\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00011439936235547066\n",
      "conv1.bias 0.0010897349566221237\n",
      "conv2.weight 2.873746445402503e-05\n",
      "conv2.bias 0.0009018603013828397\n",
      "fc1.weight 0.00017871150048449637\n",
      "fc1.bias 0.000411691702902317\n",
      "\n",
      "Test set: Average loss: 2.3004 \n",
      "Accuracy: 1955/10000 (19.55%)\n",
      "\n",
      "Round  16, Average loss 2.300 Test accuracy 19.550\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00022176481783390045\n",
      "conv1.bias 0.0018494755495339632\n",
      "conv2.weight 5.2706967107951644e-05\n",
      "conv2.bias 0.0009190341224893928\n",
      "fc1.weight 7.604375132359564e-05\n",
      "fc1.bias 0.0003335284302011132\n",
      "\n",
      "Test set: Average loss: 2.2292 \n",
      "Accuracy: 5299/10000 (52.99%)\n",
      "\n",
      "Round  17, Average loss 2.229 Test accuracy 52.990\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00046395361423492434\n",
      "conv1.bias 0.00126201706007123\n",
      "conv2.weight 0.00010125131346285344\n",
      "conv2.bias 0.0012157727032899857\n",
      "fc1.weight 0.00013043152866885067\n",
      "fc1.bias 0.00042952937074005606\n",
      "\n",
      "Test set: Average loss: 2.3018 \n",
      "Accuracy: 1771/10000 (17.71%)\n",
      "\n",
      "Round  18, Average loss 2.302 Test accuracy 17.710\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00024429528042674064\n",
      "conv1.bias 0.001139140920713544\n",
      "conv2.weight 6.352880969643592e-05\n",
      "conv2.bias 0.000779499823693186\n",
      "fc1.weight 0.00016190619207918645\n",
      "fc1.bias 0.0004106624983251095\n",
      "\n",
      "Test set: Average loss: 2.2326 \n",
      "Accuracy: 4672/10000 (46.72%)\n",
      "\n",
      "Round  19, Average loss 2.233 Test accuracy 46.720\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0005620800331234932\n",
      "conv1.bias 0.0012793546775355935\n",
      "conv2.weight 7.464624010026454e-05\n",
      "conv2.bias 0.0007855485309846699\n",
      "fc1.weight 0.00020903637632727624\n",
      "fc1.bias 0.001861386001110077\n",
      "\n",
      "Test set: Average loss: 2.1860 \n",
      "Accuracy: 6318/10000 (63.18%)\n",
      "\n",
      "Round  20, Average loss 2.186 Test accuracy 63.180\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00039314493536949155\n",
      "conv1.bias 0.0013165364507585764\n",
      "conv2.weight 0.00013561168685555458\n",
      "conv2.bias 0.0008225131314247847\n",
      "fc1.weight 9.696301422081887e-05\n",
      "fc1.bias 0.0017952816560864449\n",
      "\n",
      "Test set: Average loss: 2.1742 \n",
      "Accuracy: 5456/10000 (54.56%)\n",
      "\n",
      "Round  21, Average loss 2.174 Test accuracy 54.560\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00034279845654964445\n",
      "conv1.bias 0.002242711838334799\n",
      "conv2.weight 8.283521048724652e-05\n",
      "conv2.bias 0.0009942323667928576\n",
      "fc1.weight 0.0001253199065104127\n",
      "fc1.bias 0.0012167836539447308\n",
      "\n",
      "Test set: Average loss: 2.1938 \n",
      "Accuracy: 5365/10000 (53.65%)\n",
      "\n",
      "Round  22, Average loss 2.194 Test accuracy 53.650\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.000518156997859478\n",
      "conv1.bias 0.0010890925768762827\n",
      "conv2.weight 6.714944262057542e-05\n",
      "conv2.bias 0.000779921596404165\n",
      "fc1.weight 0.00021137665025889875\n",
      "fc1.bias 0.0018893243744969369\n",
      "\n",
      "Test set: Average loss: 2.1495 \n",
      "Accuracy: 6148/10000 (61.48%)\n",
      "\n",
      "Round  23, Average loss 2.150 Test accuracy 61.480\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00046643845736980436\n",
      "conv1.bias 0.0011764022056013346\n",
      "conv2.weight 9.97739750891924e-05\n",
      "conv2.bias 0.0007490887655876577\n",
      "fc1.weight 0.000341438758186996\n",
      "fc1.bias 0.0026316490024328233\n",
      "\n",
      "Test set: Average loss: 2.1337 \n",
      "Accuracy: 5704/10000 (57.04%)\n",
      "\n",
      "Round  24, Average loss 2.134 Test accuracy 57.040\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00041498586535453796\n",
      "conv1.bias 0.002347598783671856\n",
      "conv2.weight 8.105418644845486e-05\n",
      "conv2.bias 0.000979335280135274\n",
      "fc1.weight 0.00011217701248824596\n",
      "fc1.bias 0.0018987247720360755\n",
      "\n",
      "Test set: Average loss: 2.2424 \n",
      "Accuracy: 5217/10000 (52.17%)\n",
      "\n",
      "Round  25, Average loss 2.242 Test accuracy 52.170\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00030360292643308637\n",
      "conv1.bias 0.0013349144719541073\n",
      "conv2.weight 7.295623887330293e-05\n",
      "conv2.bias 0.0007905795355327427\n",
      "fc1.weight 0.00026377835310995577\n",
      "fc1.bias 0.0006505252793431282\n",
      "\n",
      "Test set: Average loss: 2.2107 \n",
      "Accuracy: 5242/10000 (52.42%)\n",
      "\n",
      "Round  26, Average loss 2.211 Test accuracy 52.420\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.000331730917096138\n",
      "conv1.bias 0.0018643501680344343\n",
      "conv2.weight 4.796923603862524e-05\n",
      "conv2.bias 0.0008036770741455257\n",
      "fc1.weight 0.00016815117560327054\n",
      "fc1.bias 0.000996308121830225\n",
      "\n",
      "Test set: Average loss: 2.1929 \n",
      "Accuracy: 5021/10000 (50.21%)\n",
      "\n",
      "Round  27, Average loss 2.193 Test accuracy 50.210\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0006074382737278938\n",
      "conv1.bias 0.0012631742283701897\n",
      "conv2.weight 7.371503394097089e-05\n",
      "conv2.bias 0.0007105267141014338\n",
      "fc1.weight 0.00015185133088380098\n",
      "fc1.bias 0.0008140232414007187\n",
      "\n",
      "Test set: Average loss: 2.1244 \n",
      "Accuracy: 6521/10000 (65.21%)\n",
      "\n",
      "Round  28, Average loss 2.124 Test accuracy 65.210\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0005883798375725747\n",
      "conv1.bias 0.0014341012574732304\n",
      "conv2.weight 0.00014301168732345105\n",
      "conv2.bias 0.0007764843758195639\n",
      "fc1.weight 0.0002456754213199019\n",
      "fc1.bias 0.0013199908658862113\n",
      "\n",
      "Test set: Average loss: 2.1828 \n",
      "Accuracy: 6656/10000 (66.56%)\n",
      "\n",
      "Round  29, Average loss 2.183 Test accuracy 66.560\n",
      "(T, sigma)= 5 1 )  3 -th Trial!!\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0135064959526062\n",
      "conv1.bias 0.01578088477253914\n",
      "conv2.weight 0.0004180039465427399\n",
      "conv2.bias 0.0004239577683620155\n",
      "fc1.weight 0.00032518121879547833\n",
      "fc1.bias 0.0004250603262335062\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1008/10000 (10.08%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 10.080\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00023613441735506057\n",
      "conv1.bias 0.002043088898062706\n",
      "conv2.weight 0.00017778130248188973\n",
      "conv2.bias 0.00046704430133104324\n",
      "fc1.weight 4.2450701585039496e-05\n",
      "fc1.bias 0.00041856053285300733\n",
      "\n",
      "Test set: Average loss: 2.2954 \n",
      "Accuracy: 2506/10000 (25.06%)\n",
      "\n",
      "Round   1, Average loss 2.295 Test accuracy 25.060\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0001718263328075409\n",
      "conv1.bias 0.0025788284838199615\n",
      "conv2.weight 7.318113930523395e-05\n",
      "conv2.bias 0.0010914829326793551\n",
      "fc1.weight 8.295461884699762e-05\n",
      "fc1.bias 0.00044275149703025817\n",
      "\n",
      "Test set: Average loss: 2.2953 \n",
      "Accuracy: 2615/10000 (26.15%)\n",
      "\n",
      "Round   2, Average loss 2.295 Test accuracy 26.150\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0002674524486064911\n",
      "conv1.bias 0.0013877269811928272\n",
      "conv2.weight 6.346803158521652e-05\n",
      "conv2.bias 0.0009462127345614135\n",
      "fc1.weight 0.00011933869682252407\n",
      "fc1.bias 0.0004032411612570286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.3020 \n",
      "Accuracy: 1028/10000 (10.28%)\n",
      "\n",
      "Round   3, Average loss 2.302 Test accuracy 10.280\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0003184841573238373\n",
      "conv1.bias 0.0018667385447770357\n",
      "conv2.weight 6.596929393708706e-05\n",
      "conv2.bias 0.0011054433416575193\n",
      "fc1.weight 0.00012640187051147223\n",
      "fc1.bias 0.0008877666667103767\n",
      "\n",
      "Test set: Average loss: 2.3025 \n",
      "Accuracy: 1028/10000 (10.28%)\n",
      "\n",
      "Round   4, Average loss 2.303 Test accuracy 10.280\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0002659502997994423\n",
      "conv1.bias 0.0017717157024890184\n",
      "conv2.weight 7.148840464651584e-05\n",
      "conv2.bias 0.0008910291362553835\n",
      "fc1.weight 9.089966770261526e-05\n",
      "fc1.bias 0.0007561927661299706\n",
      "\n",
      "Test set: Average loss: 2.3021 \n",
      "Accuracy: 1386/10000 (13.86%)\n",
      "\n",
      "Round   5, Average loss 2.302 Test accuracy 13.860\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00023030621930956842\n",
      "conv1.bias 0.0010378805454820395\n",
      "conv2.weight 7.66385719180107e-05\n",
      "conv2.bias 0.0011980815324932337\n",
      "fc1.weight 0.00010583600960671902\n",
      "fc1.bias 0.0002629425609484315\n",
      "\n",
      "Test set: Average loss: 2.2915 \n",
      "Accuracy: 1659/10000 (16.59%)\n",
      "\n",
      "Round   6, Average loss 2.292 Test accuracy 16.590\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00020669503137469292\n",
      "conv1.bias 0.0016907267272472382\n",
      "conv2.weight 9.258201345801353e-05\n",
      "conv2.bias 0.000984685029834509\n",
      "fc1.weight 0.00020405226387083532\n",
      "fc1.bias 0.0010706569999456405\n",
      "\n",
      "Test set: Average loss: 2.3016 \n",
      "Accuracy: 1561/10000 (15.61%)\n",
      "\n",
      "Round   7, Average loss 2.302 Test accuracy 15.610\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00038472909480333326\n",
      "conv1.bias 0.0011921031400561333\n",
      "conv2.weight 8.423393592238426e-05\n",
      "conv2.bias 0.0006743965204805136\n",
      "fc1.weight 0.00016010809922590853\n",
      "fc1.bias 0.0004005616996437311\n",
      "\n",
      "Test set: Average loss: 2.2952 \n",
      "Accuracy: 2993/10000 (29.93%)\n",
      "\n",
      "Round   8, Average loss 2.295 Test accuracy 29.930\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0003358999639749527\n",
      "conv1.bias 0.001624684315174818\n",
      "conv2.weight 3.803530475124717e-05\n",
      "conv2.bias 0.0008649438386783004\n",
      "fc1.weight 0.00010830555111169815\n",
      "fc1.bias 0.0004441550001502037\n",
      "\n",
      "Test set: Average loss: 2.2780 \n",
      "Accuracy: 3621/10000 (36.21%)\n",
      "\n",
      "Round   9, Average loss 2.278 Test accuracy 36.210\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.000394645631313324\n",
      "conv1.bias 0.0010360297746956348\n",
      "conv2.weight 6.936308927834034e-05\n",
      "conv2.bias 0.0007546963170170784\n",
      "fc1.weight 0.00014705175999552011\n",
      "fc1.bias 0.0007458992768079042\n",
      "\n",
      "Test set: Average loss: 2.2781 \n",
      "Accuracy: 4632/10000 (46.32%)\n",
      "\n",
      "Round  10, Average loss 2.278 Test accuracy 46.320\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0003442687168717384\n",
      "conv1.bias 0.0013627696316689253\n",
      "conv2.weight 8.725843392312527e-05\n",
      "conv2.bias 0.0007828170200809836\n",
      "fc1.weight 0.0001709850155748427\n",
      "fc1.bias 0.0003756985068321228\n",
      "\n",
      "Test set: Average loss: 2.3021 \n",
      "Accuracy: 1623/10000 (16.23%)\n",
      "\n",
      "Round  11, Average loss 2.302 Test accuracy 16.230\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00022752556949853897\n",
      "conv1.bias 0.0012318362714722753\n",
      "conv2.weight 1.284765312448144e-05\n",
      "conv2.bias 0.0005636960850097239\n",
      "fc1.weight 0.00011203517206013203\n",
      "fc1.bias 0.0003355069551616907\n",
      "\n",
      "Test set: Average loss: 2.2979 \n",
      "Accuracy: 1659/10000 (16.59%)\n",
      "\n",
      "Round  12, Average loss 2.298 Test accuracy 16.590\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00014151046052575112\n",
      "conv1.bias 0.0014642102178186178\n",
      "conv2.weight 8.570441044867039e-05\n",
      "conv2.bias 0.0010643790010362864\n",
      "fc1.weight 8.446836727671325e-05\n",
      "fc1.bias 0.00017873067408800126\n",
      "\n",
      "Test set: Average loss: 2.2933 \n",
      "Accuracy: 2857/10000 (28.57%)\n",
      "\n",
      "Round  13, Average loss 2.293 Test accuracy 28.570\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0001970198005437851\n",
      "conv1.bias 0.0015854435041546822\n",
      "conv2.weight 3.6845048889517784e-05\n",
      "conv2.bias 0.0008070585317909718\n",
      "fc1.weight 0.000187202007509768\n",
      "fc1.bias 0.0008478928357362747\n",
      "\n",
      "Test set: Average loss: 2.2891 \n",
      "Accuracy: 1806/10000 (18.06%)\n",
      "\n",
      "Round  14, Average loss 2.289 Test accuracy 18.060\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00029214756563305855\n",
      "conv1.bias 0.0016000890173017979\n",
      "conv2.weight 7.844781503081321e-05\n",
      "conv2.bias 0.0009250863804481924\n",
      "fc1.weight 0.00014823497040197253\n",
      "fc1.bias 0.0011631038039922714\n",
      "\n",
      "Test set: Average loss: 2.2990 \n",
      "Accuracy: 1486/10000 (14.86%)\n",
      "\n",
      "Round  15, Average loss 2.299 Test accuracy 14.860\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00050161711871624\n",
      "conv1.bias 0.0010526410769671202\n",
      "conv2.weight 0.00019904134795069694\n",
      "conv2.bias 0.0007942236261442304\n",
      "fc1.weight 0.00020035181660205126\n",
      "fc1.bias 0.000452818488702178\n",
      "\n",
      "Test set: Average loss: 2.3027 \n",
      "Accuracy: 974/10000 (9.74%)\n",
      "\n",
      "Round  16, Average loss 2.303 Test accuracy 9.740\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00021465554833412172\n",
      "conv1.bias 0.002629632595926523\n",
      "conv2.weight 7.57427839562297e-05\n",
      "conv2.bias 0.0013445919612422585\n",
      "fc1.weight 7.754435646347701e-05\n",
      "fc1.bias 0.0004996397998183966\n",
      "\n",
      "Test set: Average loss: 2.3019 \n",
      "Accuracy: 1810/10000 (18.10%)\n",
      "\n",
      "Round  17, Average loss 2.302 Test accuracy 18.100\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0005508817732334137\n",
      "conv1.bias 0.0010525916004553437\n",
      "conv2.weight 7.949937134981156e-05\n",
      "conv2.bias 0.0011124416487291455\n",
      "fc1.weight 0.00016941326903179287\n",
      "fc1.bias 0.0009569840505719185\n",
      "\n",
      "Test set: Average loss: 2.3022 \n",
      "Accuracy: 1135/10000 (11.35%)\n",
      "\n",
      "Round  18, Average loss 2.302 Test accuracy 11.350\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00022150581702589988\n",
      "conv1.bias 0.0012088126968592405\n",
      "conv2.weight 7.776513695716857e-05\n",
      "conv2.bias 0.0011906663421541452\n",
      "fc1.weight 6.0220080194994806e-05\n",
      "fc1.bias 0.0009124243631958961\n",
      "\n",
      "Test set: Average loss: 2.2983 \n",
      "Accuracy: 1135/10000 (11.35%)\n",
      "\n",
      "Round  19, Average loss 2.298 Test accuracy 11.350\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0003609292209148407\n",
      "conv1.bias 0.000992741552181542\n",
      "conv2.weight 7.047052960842848e-05\n",
      "conv2.bias 0.0011841973755508661\n",
      "fc1.weight 0.0002580266445875168\n",
      "fc1.bias 0.001169283501803875\n",
      "\n",
      "Test set: Average loss: 2.2889 \n",
      "Accuracy: 3739/10000 (37.39%)\n",
      "\n",
      "Round  20, Average loss 2.289 Test accuracy 37.390\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00022745182737708092\n",
      "conv1.bias 0.0023489962331950665\n",
      "conv2.weight 4.601967055350542e-05\n",
      "conv2.bias 0.0010046882089227438\n",
      "fc1.weight 0.00010580157395452261\n",
      "fc1.bias 0.00046468460932374\n",
      "\n",
      "Test set: Average loss: 2.3023 \n",
      "Accuracy: 1464/10000 (14.64%)\n",
      "\n",
      "Round  21, Average loss 2.302 Test accuracy 14.640\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.000469677597284317\n",
      "conv1.bias 0.0013273772783577442\n",
      "conv2.weight 0.0001231348607689142\n",
      "conv2.bias 0.0007752180099487305\n",
      "fc1.weight 0.0002582137705758214\n",
      "fc1.bias 0.0005072950851172208\n",
      "\n",
      "Test set: Average loss: 2.3017 \n",
      "Accuracy: 1135/10000 (11.35%)\n",
      "\n",
      "Round  22, Average loss 2.302 Test accuracy 11.350\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0003705042228102684\n",
      "conv1.bias 0.0013981868978589773\n",
      "conv2.weight 7.76775972917676e-05\n",
      "conv2.bias 0.001296374830417335\n",
      "fc1.weight 8.709052926860749e-05\n",
      "fc1.bias 0.0020810062065720557\n",
      "\n",
      "Test set: Average loss: 2.2999 \n",
      "Accuracy: 2244/10000 (22.44%)\n",
      "\n",
      "Round  23, Average loss 2.300 Test accuracy 22.440\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00035912998020648957\n",
      "conv1.bias 0.0009995447471737862\n",
      "conv2.weight 7.443849928677082e-05\n",
      "conv2.bias 0.0011972091160714626\n",
      "fc1.weight 0.00018134278943762183\n",
      "fc1.bias 0.00024005987215787173\n",
      "\n",
      "Test set: Average loss: 2.2940 \n",
      "Accuracy: 2792/10000 (27.92%)\n",
      "\n",
      "Round  24, Average loss 2.294 Test accuracy 27.920\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0001694352738559246\n",
      "conv1.bias 0.0025068498216569424\n",
      "conv2.weight 6.622679065912963e-05\n",
      "conv2.bias 0.0011069148313254118\n",
      "fc1.weight 0.0001678876462392509\n",
      "fc1.bias 0.0013566299341619015\n",
      "\n",
      "Test set: Average loss: 2.2887 \n",
      "Accuracy: 3735/10000 (37.35%)\n",
      "\n",
      "Round  25, Average loss 2.289 Test accuracy 37.350\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0004123857617378235\n",
      "conv1.bias 0.001039179740473628\n",
      "conv2.weight 9.778624400496482e-05\n",
      "conv2.bias 0.000982081750407815\n",
      "fc1.weight 0.00025868762750178573\n",
      "fc1.bias 0.0011810263618826865\n",
      "\n",
      "Test set: Average loss: 2.2986 \n",
      "Accuracy: 1317/10000 (13.17%)\n",
      "\n",
      "Round  26, Average loss 2.299 Test accuracy 13.170\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00046822145581245424\n",
      "conv1.bias 0.0010548514546826482\n",
      "conv2.weight 6.37512793764472e-05\n",
      "conv2.bias 0.0007297953707166016\n",
      "fc1.weight 0.00015967178624123334\n",
      "fc1.bias 0.0005547106266021729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.2956 \n",
      "Accuracy: 2435/10000 (24.35%)\n",
      "\n",
      "Round  27, Average loss 2.296 Test accuracy 24.350\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0001310382876545191\n",
      "conv1.bias 0.005856798961758614\n",
      "conv2.weight 6.887503433972598e-05\n",
      "conv2.bias 0.0010666692396625876\n",
      "fc1.weight 6.871458608657122e-05\n",
      "fc1.bias 0.00022945899982005358\n",
      "\n",
      "Test set: Average loss: 2.3001 \n",
      "Accuracy: 3245/10000 (32.45%)\n",
      "\n",
      "Round  28, Average loss 2.300 Test accuracy 32.450\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0002110094390809536\n",
      "conv1.bias 0.002177502727136016\n",
      "conv2.weight 3.162576584145427e-05\n",
      "conv2.bias 0.0007320907898247242\n",
      "fc1.weight 0.00023501873947679998\n",
      "fc1.bias 0.00031605244148522613\n",
      "\n",
      "Test set: Average loss: 2.2831 \n",
      "Accuracy: 3705/10000 (37.05%)\n",
      "\n",
      "Round  29, Average loss 2.283 Test accuracy 37.050\n",
      "(T, sigma)= 5 1 )  4 -th Trial!!\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.013374913930892945\n",
      "conv1.bias 0.012574296444654465\n",
      "conv2.weight 0.00041922666132450105\n",
      "conv2.bias 0.00042637932347133756\n",
      "fc1.weight 0.0003224487882107496\n",
      "fc1.bias 0.0002909662900492549\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0002858623117208481\n",
      "conv1.bias 0.001524751540273428\n",
      "conv2.weight 0.00021592717617750169\n",
      "conv2.bias 0.00043730036122724414\n",
      "fc1.weight 4.88664023578167e-05\n",
      "fc1.bias 0.00025831861421465874\n",
      "\n",
      "Test set: Average loss: 2.3022 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   1, Average loss 2.302 Test accuracy 9.800\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00017055556178092958\n",
      "conv1.bias 0.0011885021813213825\n",
      "conv2.weight 7.800023071467876e-05\n",
      "conv2.bias 0.0008416989585384727\n",
      "fc1.weight 9.067287319339812e-05\n",
      "fc1.bias 0.0006671080831438303\n",
      "\n",
      "Test set: Average loss: 2.2928 \n",
      "Accuracy: 2106/10000 (21.06%)\n",
      "\n",
      "Round   2, Average loss 2.293 Test accuracy 21.060\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00015826474875211717\n",
      "conv1.bias 0.002404262777417898\n",
      "conv2.weight 9.093163534998894e-05\n",
      "conv2.bias 0.001197155797854066\n",
      "fc1.weight 0.00021419613622128964\n",
      "fc1.bias 0.001181451790034771\n",
      "\n",
      "Test set: Average loss: 2.2674 \n",
      "Accuracy: 4188/10000 (41.88%)\n",
      "\n",
      "Round   3, Average loss 2.267 Test accuracy 41.880\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00030376145616173745\n",
      "conv1.bias 0.0019401306053623557\n",
      "conv2.weight 6.457255687564612e-05\n",
      "conv2.bias 0.0009164608200080693\n",
      "fc1.weight 0.00016433021519333125\n",
      "fc1.bias 0.000310239614918828\n",
      "\n",
      "Test set: Average loss: 2.2913 \n",
      "Accuracy: 2147/10000 (21.47%)\n",
      "\n",
      "Round   4, Average loss 2.291 Test accuracy 21.470\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00029173962771892546\n",
      "conv1.bias 0.0012049798388034105\n",
      "conv2.weight 7.317531388252974e-05\n",
      "conv2.bias 0.0008576866239309311\n",
      "fc1.weight 0.00015105151105672122\n",
      "fc1.bias 0.0004780700895935297\n",
      "\n",
      "Test set: Average loss: 2.3012 \n",
      "Accuracy: 1010/10000 (10.10%)\n",
      "\n",
      "Round   5, Average loss 2.301 Test accuracy 10.100\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0001339175645262003\n",
      "conv1.bias 0.0023159589618444443\n",
      "conv2.weight 6.520645692944526e-05\n",
      "conv2.bias 0.000942870625294745\n",
      "fc1.weight 0.0001281570759601891\n",
      "fc1.bias 0.00045467084273695944\n",
      "\n",
      "Test set: Average loss: 2.3008 \n",
      "Accuracy: 2418/10000 (24.18%)\n",
      "\n",
      "Round   6, Average loss 2.301 Test accuracy 24.180\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00031095270067453383\n",
      "conv1.bias 0.0016994808102026582\n",
      "conv2.weight 6.103589199483394e-05\n",
      "conv2.bias 0.0008416055352427065\n",
      "fc1.weight 0.0001419738633558154\n",
      "fc1.bias 0.0005139915272593498\n",
      "\n",
      "Test set: Average loss: 2.2707 \n",
      "Accuracy: 4061/10000 (40.61%)\n",
      "\n",
      "Round   7, Average loss 2.271 Test accuracy 40.610\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00044589728116989136\n",
      "conv1.bias 0.0010125500848516822\n",
      "conv2.weight 8.915949612855911e-05\n",
      "conv2.bias 0.0008714675786904991\n",
      "fc1.weight 0.00027457894757390024\n",
      "fc1.bias 0.001303944457322359\n",
      "\n",
      "Test set: Average loss: 2.2765 \n",
      "Accuracy: 5371/10000 (53.71%)\n",
      "\n",
      "Round   8, Average loss 2.276 Test accuracy 53.710\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0002850642800331116\n",
      "conv1.bias 0.0014932663179934025\n",
      "conv2.weight 3.221722319722176e-05\n",
      "conv2.bias 0.0006546661606989801\n",
      "fc1.weight 0.00010510684223845601\n",
      "fc1.bias 0.00023858128115534783\n",
      "\n",
      "Test set: Average loss: 2.1433 \n",
      "Accuracy: 6436/10000 (64.36%)\n",
      "\n",
      "Round   9, Average loss 2.143 Test accuracy 64.360\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00045959368348121645\n",
      "conv1.bias 0.002494462765753269\n",
      "conv2.weight 8.329590782523155e-05\n",
      "conv2.bias 0.0007440941408276558\n",
      "fc1.weight 0.00019611672032624484\n",
      "fc1.bias 0.0008427569642663002\n",
      "\n",
      "Test set: Average loss: 2.1650 \n",
      "Accuracy: 6549/10000 (65.49%)\n",
      "\n",
      "Round  10, Average loss 2.165 Test accuracy 65.490\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00027456533163785937\n",
      "conv1.bias 0.0022040996700525284\n",
      "conv2.weight 8.256192319095135e-05\n",
      "conv2.bias 0.0008614183170720935\n",
      "fc1.weight 0.00017776438035070895\n",
      "fc1.bias 0.0008311584591865539\n",
      "\n",
      "Test set: Average loss: 2.1638 \n",
      "Accuracy: 6358/10000 (63.58%)\n",
      "\n",
      "Round  11, Average loss 2.164 Test accuracy 63.580\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00035062272101640704\n",
      "conv1.bias 0.002327212132513523\n",
      "conv2.weight 5.825476720929146e-05\n",
      "conv2.bias 0.0006725706625729799\n",
      "fc1.weight 0.00016295046079903842\n",
      "fc1.bias 0.0007576607167720795\n",
      "\n",
      "Test set: Average loss: 2.1171 \n",
      "Accuracy: 7251/10000 (72.51%)\n",
      "\n",
      "Round  12, Average loss 2.117 Test accuracy 72.510\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00035872943699359893\n",
      "conv1.bias 0.0017459444934502244\n",
      "conv2.weight 8.053497411310673e-05\n",
      "conv2.bias 0.0006435730610974133\n",
      "fc1.weight 0.00028965009842067956\n",
      "fc1.bias 0.00029659452848136426\n",
      "\n",
      "Test set: Average loss: 2.2425 \n",
      "Accuracy: 6171/10000 (61.71%)\n",
      "\n",
      "Round  13, Average loss 2.242 Test accuracy 61.710\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0002466649189591408\n",
      "conv1.bias 0.001855808892287314\n",
      "conv2.weight 5.9500671923160554e-05\n",
      "conv2.bias 0.0006363067077472806\n",
      "fc1.weight 0.0002352011390030384\n",
      "fc1.bias 0.0002658808371052146\n",
      "\n",
      "Test set: Average loss: 2.2390 \n",
      "Accuracy: 5325/10000 (53.25%)\n",
      "\n",
      "Round  14, Average loss 2.239 Test accuracy 53.250\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00031586732715368273\n",
      "conv1.bias 0.0011380871292203665\n",
      "conv2.weight 0.00010470223613083362\n",
      "conv2.bias 0.000897869816981256\n",
      "fc1.weight 0.0001572803594172001\n",
      "fc1.bias 0.0002816086169332266\n",
      "\n",
      "Test set: Average loss: 2.2968 \n",
      "Accuracy: 2609/10000 (26.09%)\n",
      "\n",
      "Round  15, Average loss 2.297 Test accuracy 26.090\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0002400682494044304\n",
      "conv1.bias 0.0018901429139077663\n",
      "conv2.weight 4.2146253399550914e-05\n",
      "conv2.bias 0.0008357539190910757\n",
      "fc1.weight 0.0001805298961699009\n",
      "fc1.bias 0.0003493721131235361\n",
      "\n",
      "Test set: Average loss: 2.2931 \n",
      "Accuracy: 4263/10000 (42.63%)\n",
      "\n",
      "Round  16, Average loss 2.293 Test accuracy 42.630\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0002251903899013996\n",
      "conv1.bias 0.003077339380979538\n",
      "conv2.weight 6.297579035162926e-05\n",
      "conv2.bias 0.0010646861046552658\n",
      "fc1.weight 0.00014670147793367506\n",
      "fc1.bias 0.0003810715628787875\n",
      "\n",
      "Test set: Average loss: 2.2903 \n",
      "Accuracy: 5749/10000 (57.49%)\n",
      "\n",
      "Round  17, Average loss 2.290 Test accuracy 57.490\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0003421790152788162\n",
      "conv1.bias 0.0014154601376503706\n",
      "conv2.weight 6.282454822212458e-05\n",
      "conv2.bias 0.001070789061486721\n",
      "fc1.weight 0.00011911140754818916\n",
      "fc1.bias 0.0002786090131849051\n",
      "\n",
      "Test set: Average loss: 2.2926 \n",
      "Accuracy: 5114/10000 (51.14%)\n",
      "\n",
      "Round  18, Average loss 2.293 Test accuracy 51.140\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00022567279636859894\n",
      "conv1.bias 0.002031417563557625\n",
      "conv2.weight 3.0520381405949594e-05\n",
      "conv2.bias 0.0006874509272165596\n",
      "fc1.weight 0.0001529840170405805\n",
      "fc1.bias 0.0002255330793559551\n",
      "\n",
      "Test set: Average loss: 2.2826 \n",
      "Accuracy: 4887/10000 (48.87%)\n",
      "\n",
      "Round  19, Average loss 2.283 Test accuracy 48.870\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00024388547986745835\n",
      "conv1.bias 0.0009168701362796128\n",
      "conv2.weight 7.373662665486336e-05\n",
      "conv2.bias 0.0009079314768314362\n",
      "fc1.weight 0.00018198956968262792\n",
      "fc1.bias 0.00028113506268709896\n",
      "\n",
      "Test set: Average loss: 2.2840 \n",
      "Accuracy: 2950/10000 (29.50%)\n",
      "\n",
      "Round  20, Average loss 2.284 Test accuracy 29.500\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0002684681490063667\n",
      "conv1.bias 0.0009728936129249632\n",
      "conv2.weight 6.948709022253752e-05\n",
      "conv2.bias 0.0007573489565402269\n",
      "fc1.weight 0.00019389055669307708\n",
      "fc1.bias 0.0008957327343523502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.2961 \n",
      "Accuracy: 1262/10000 (12.62%)\n",
      "\n",
      "Round  21, Average loss 2.296 Test accuracy 12.620\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0005228253453969955\n",
      "conv1.bias 0.0010672332718968391\n",
      "conv2.weight 0.00023090187460184097\n",
      "conv2.bias 0.0007535008480772376\n",
      "fc1.weight 0.0002117895521223545\n",
      "fc1.bias 0.0011671410873532295\n",
      "\n",
      "Test set: Average loss: 2.2992 \n",
      "Accuracy: 2339/10000 (23.39%)\n",
      "\n",
      "Round  22, Average loss 2.299 Test accuracy 23.390\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0003847220167517662\n",
      "conv1.bias 0.001833084737882018\n",
      "conv2.weight 8.484448306262494e-05\n",
      "conv2.bias 0.001161295804195106\n",
      "fc1.weight 8.420962840318679e-05\n",
      "fc1.bias 0.0015136186964809894\n",
      "\n",
      "Test set: Average loss: 2.2260 \n",
      "Accuracy: 7064/10000 (70.64%)\n",
      "\n",
      "Round  23, Average loss 2.226 Test accuracy 70.640\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0004291202127933502\n",
      "conv1.bias 0.00046908273361623287\n",
      "conv2.weight 7.891953922808171e-05\n",
      "conv2.bias 0.000752789550460875\n",
      "fc1.weight 0.0002837427658960223\n",
      "fc1.bias 0.0007643346209079028\n",
      "\n",
      "Test set: Average loss: 2.2940 \n",
      "Accuracy: 3429/10000 (34.29%)\n",
      "\n",
      "Round  24, Average loss 2.294 Test accuracy 34.290\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 9.018203243613243e-05\n",
      "conv1.bias 0.0016668277094140649\n",
      "conv2.weight 0.00011246826499700547\n",
      "conv2.bias 0.0009890981018543243\n",
      "fc1.weight 0.00012803811114281416\n",
      "fc1.bias 0.00038538798689842225\n",
      "\n",
      "Test set: Average loss: 2.2456 \n",
      "Accuracy: 4910/10000 (49.10%)\n",
      "\n",
      "Round  25, Average loss 2.246 Test accuracy 49.100\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0002645675651729107\n",
      "conv1.bias 0.0016703535802662373\n",
      "conv2.weight 9.037191979587078e-05\n",
      "conv2.bias 0.0010363576002418995\n",
      "fc1.weight 0.00012877630069851875\n",
      "fc1.bias 0.00048037669621407985\n",
      "\n",
      "Test set: Average loss: 2.2157 \n",
      "Accuracy: 5860/10000 (58.60%)\n",
      "\n",
      "Round  26, Average loss 2.216 Test accuracy 58.600\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.000402858629822731\n",
      "conv1.bias 0.0011631656670942903\n",
      "conv2.weight 0.00010105015709996223\n",
      "conv2.bias 0.0007743773749098182\n",
      "fc1.weight 0.00019066680688410996\n",
      "fc1.bias 0.001014454383403063\n",
      "\n",
      "Test set: Average loss: 2.2234 \n",
      "Accuracy: 6366/10000 (63.66%)\n",
      "\n",
      "Round  27, Average loss 2.223 Test accuracy 63.660\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.00017029127106070517\n",
      "conv1.bias 0.002443811157718301\n",
      "conv2.weight 7.742030546069145e-05\n",
      "conv2.bias 0.0009377555106766522\n",
      "fc1.weight 0.0001606062753126025\n",
      "fc1.bias 0.000869001541286707\n",
      "\n",
      "Test set: Average loss: 2.2537 \n",
      "Accuracy: 3939/10000 (39.39%)\n",
      "\n",
      "Round  28, Average loss 2.254 Test accuracy 39.390\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "conv1.weight 0.0003160610795021057\n",
      "conv1.bias 0.0017472467152401805\n",
      "conv2.weight 8.004468865692616e-05\n",
      "conv2.bias 0.0007424692739732563\n",
      "fc1.weight 0.00024072681553661824\n",
      "fc1.bias 0.00028231816831976176\n",
      "\n",
      "Test set: Average loss: 2.2471 \n",
      "Accuracy: 5435/10000 (54.35%)\n",
      "\n",
      "Round  29, Average loss 2.247 Test accuracy 54.350\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "K = 4\n",
    "T = 5\n",
    "sigma = 1\n",
    "Noise_Alloc = [0,2,4,6,8]\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "\n",
    "N_array = [4, 8]\n",
    "B_array = [0.5]\n",
    "\n",
    "N_trials = 5\n",
    "N_epochs = 30\n",
    "\n",
    "\n",
    "\n",
    "loss_test_arr_K4_G1_v3 = np.zeros((len(N_array),len(B_array),N_trials,N_epochs))\n",
    "acc_test_arr_K4_G1_v3  = np.zeros((len(N_array),len(B_array),N_trials,N_epochs))\n",
    "\n",
    "for N_idx in range(len(N_array)):\n",
    "    \n",
    "    N = N_array[N_idx]\n",
    "    \n",
    "    \n",
    "           \n",
    "        \n",
    "    # print(\"alpha_array: \",alpha_array,'\\n')\n",
    "    \n",
    "    \n",
    "    for B_idx in range(len(B_array)):\n",
    "        \n",
    "        B = B_array[B_idx]\n",
    "        z_array = []\n",
    "#         while(len(z_array)<N):\n",
    "#             z_tmp = np.random.uniform(-1,1,1)\n",
    "#             MIS_tmp = MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_tmp], 1,sigma)\n",
    "#             if MIS_tmp < B and MIS_tmp > 0.1:\n",
    "#                 z_array.append(z_tmp[0])\n",
    "#         \n",
    "#         z_array = np.sort(z_array)\n",
    "        print(N_idx,'!!!')\n",
    "        if N_idx==0:\n",
    "            z_array = np.array([-0.94,-0.534,0.534, 0.94])\n",
    "#         elif N_idx==1:\n",
    "#             z_array = np.array([-0.94, -0.73, 0.73, 0.94])\n",
    "#         elif N_idx==2:\n",
    "#             z_array = np.array([-0.94, -0.125, 0.125, 0.94])\n",
    "        else:\n",
    "            z_array = np.array([-0.94, -0.73, -0.534, -0.125, 0.125, 0.534, 0.73, 0.94])\n",
    "#             z_array = np.array([-0.9, -0.81, -0.22, -0.20, 0.20, 0.22, 0.81, 0.9])\n",
    "            \n",
    "        print('z_array:',z_array)\n",
    "        for j in range(len(z_array)):\n",
    "            print(MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_array[j]], 1,sigma))\n",
    "        \n",
    "        \n",
    "        _Noise_label = np.ones((15000*T,10)) * 0.1\n",
    "\n",
    "        X_tilde,a,b = BACC_Enc_Data_v3(encoding_input_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde,a,b = BACC_Enc_Data_v3(encoding_label_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "\n",
    "        m = N # m is the number of received result @ master\n",
    "    #     print('number of results:',m)\n",
    "\n",
    "        for trial_idx in range(N_trials):\n",
    "            print('(T, sigma)=',T,sigma,') ',trial_idx,'-th Trial!!')\n",
    "\n",
    "            net_glob = CNNMnist2(args=args)\n",
    "            net_glob.cuda()\n",
    "            net_glob.train()\n",
    "\n",
    "            # copy weights\n",
    "            w_glob = net_glob.state_dict()\n",
    "\n",
    "            for iter in range(N_epochs): #args.epochs\n",
    "                w_locals, loss_locals = [], []\n",
    "                idxs_users = np.random.choice(range(N), m, replace=False)\n",
    "                idxs_users = np.sort(idxs_users)\n",
    "                print('selected users:',idxs_users)\n",
    "                \n",
    "                coded_net = BACC_Enc_Model_withNoise_v4(net_glob.cuda(), N, K, T, 1, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "                dec_z_array = []\n",
    "                for idx in idxs_users: #for idx in range(N):\n",
    "            #         print(idx)\n",
    "                    local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[idx,:,:], label=y_tilde[idx,:,:])\n",
    "                    w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "#                     w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                    w_locals.append(copy.deepcopy(w))\n",
    "                    loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                    dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "                # update global weights\n",
    "                #w_glob = FedAvg(w_locals)\n",
    "                w_glob = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "\n",
    "                # copy weight to net_glob\n",
    "                net_glob.load_state_dict(w_glob)\n",
    "\n",
    "                # print loss\n",
    "            #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "            #     loss_train_arr.append(loss_train)\n",
    "\n",
    "                acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "                acc_test_arr_K4_G1_v3[N_idx][B_idx][trial_idx][iter] = acc_test\n",
    "                loss_test_arr_K4_G1_v3[N_idx][B_idx][trial_idx][iter] = loss_test\n",
    "                if iter % 1 ==0:\n",
    "                    print('Round {:3d}, Average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_test, acc_test))\n",
    "                #print(loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2 3\n",
      "##########################################\n",
      "###### 0 -th Trial!! ###########\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.013985772132873536\n",
      "conv1.bias 0.015298161655664444\n",
      "conv2.weight 0.000414406917989254\n",
      "conv2.bias 0.00038662675069645047\n",
      "fc1.weight 0.00032845877576619386\n",
      "fc1.bias 0.00034686101134866475\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.013985772132873536\n",
      "conv1.bias 0.015298161655664444\n",
      "conv2.weight 0.000414406917989254\n",
      "conv2.bias 0.00038662675069645047\n",
      "fc1.weight 0.00032845877576619386\n",
      "fc1.bias 0.00034686101134866475\n",
      "\n",
      "Test set: Average loss: 2.2987 \n",
      "Accuracy: 1440/10000 (14.40%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018887656927108764\n",
      "conv1.bias 0.008341850712895393\n",
      "conv2.weight 0.0010946141928434372\n",
      "conv2.bias 0.0018832884961739182\n",
      "fc1.weight 0.0003517881967127323\n",
      "fc1.bias 0.0018705600872635842\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018887656927108764\n",
      "conv1.bias 0.008341850712895393\n",
      "conv2.weight 0.0010946141928434372\n",
      "conv2.bias 0.0018832884961739182\n",
      "fc1.weight 0.0003517881967127323\n",
      "fc1.bias 0.0018705600872635842\n",
      "\n",
      "Test set: Average loss: 1.9331 \n",
      "Accuracy: 4154/10000 (41.54%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0008677514642477036\n",
      "conv1.bias 0.01791965216398239\n",
      "conv2.weight 0.0002547328360378742\n",
      "conv2.bias 0.003428058233112097\n",
      "fc1.weight 0.00032466403208673\n",
      "fc1.bias 0.001323069166392088\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0008677514642477036\n",
      "conv1.bias 0.01791965216398239\n",
      "conv2.weight 0.0002547328360378742\n",
      "conv2.bias 0.003428058233112097\n",
      "fc1.weight 0.00032466403208673\n",
      "fc1.bias 0.001323069166392088\n",
      "\n",
      "Test set: Average loss: 0.6780 \n",
      "Accuracy: 8499/10000 (84.99%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001392158716917038\n",
      "conv1.bias 0.01875949278473854\n",
      "conv2.weight 0.0005395462363958359\n",
      "conv2.bias 0.0044867489486932755\n",
      "fc1.weight 0.0011906097643077374\n",
      "fc1.bias 0.0018422765657305717\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001392158716917038\n",
      "conv1.bias 0.01875949278473854\n",
      "conv2.weight 0.0005395462363958359\n",
      "conv2.bias 0.0044867489486932755\n",
      "fc1.weight 0.0011906097643077374\n",
      "fc1.bias 0.0018422765657305717\n",
      "\n",
      "Test set: Average loss: 0.2917 \n",
      "Accuracy: 9524/10000 (95.24%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001968294084072113\n",
      "conv1.bias 0.019547589123249054\n",
      "conv2.weight 0.0005209941416978836\n",
      "conv2.bias 0.0039491974748671055\n",
      "fc1.weight 0.0009322267025709153\n",
      "fc1.bias 0.002533668465912342\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001968294084072113\n",
      "conv1.bias 0.019547589123249054\n",
      "conv2.weight 0.0005209941416978836\n",
      "conv2.bias 0.0039491974748671055\n",
      "fc1.weight 0.0009322267025709153\n",
      "fc1.bias 0.002533668465912342\n",
      "\n",
      "Test set: Average loss: 0.2903 \n",
      "Accuracy: 9581/10000 (95.81%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002078150510787964\n",
      "conv1.bias 0.021974802017211914\n",
      "conv2.weight 0.0005650483816862106\n",
      "conv2.bias 0.003744355868548155\n",
      "fc1.weight 0.0009931857697665692\n",
      "fc1.bias 0.0028815291821956634\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002078150510787964\n",
      "conv1.bias 0.021974802017211914\n",
      "conv2.weight 0.0005650483816862106\n",
      "conv2.bias 0.003744355868548155\n",
      "fc1.weight 0.0009931857697665692\n",
      "fc1.bias 0.0028815291821956634\n",
      "\n",
      "Test set: Average loss: 0.2359 \n",
      "Accuracy: 9618/10000 (96.18%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0023885594308376314\n",
      "conv1.bias 0.023771844804286957\n",
      "conv2.weight 0.00044832289218902586\n",
      "conv2.bias 0.0031164020765572786\n",
      "fc1.weight 0.0009993689134716987\n",
      "fc1.bias 0.003627525269985199\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0023885594308376314\n",
      "conv1.bias 0.023771844804286957\n",
      "conv2.weight 0.00044832289218902586\n",
      "conv2.bias 0.0031164020765572786\n",
      "fc1.weight 0.0009993689134716987\n",
      "fc1.bias 0.003627525269985199\n",
      "\n",
      "Test set: Average loss: 0.2289 \n",
      "Accuracy: 9656/10000 (96.56%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0026550284028053284\n",
      "conv1.bias 0.02384074777364731\n",
      "conv2.weight 0.0005449829250574112\n",
      "conv2.bias 0.003231453476473689\n",
      "fc1.weight 0.0011697392910718918\n",
      "fc1.bias 0.004401078820228577\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0026550284028053284\n",
      "conv1.bias 0.02384074777364731\n",
      "conv2.weight 0.0005449829250574112\n",
      "conv2.bias 0.003231453476473689\n",
      "fc1.weight 0.0011697392910718918\n",
      "fc1.bias 0.004401078820228577\n",
      "\n",
      "Test set: Average loss: 0.2346 \n",
      "Accuracy: 9644/10000 (96.44%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0022680768370628357\n",
      "conv1.bias 0.023496754467487335\n",
      "conv2.weight 0.0005442281067371368\n",
      "conv2.bias 0.003224538639187813\n",
      "fc1.weight 0.0010974833741784096\n",
      "fc1.bias 0.004614651575684548\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0022680768370628357\n",
      "conv1.bias 0.023496754467487335\n",
      "conv2.weight 0.0005442281067371368\n",
      "conv2.bias 0.003224538639187813\n",
      "fc1.weight 0.0010974833741784096\n",
      "fc1.bias 0.004614651575684548\n",
      "\n",
      "Test set: Average loss: 0.2388 \n",
      "Accuracy: 9643/10000 (96.43%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020694829523563384\n",
      "conv1.bias 0.027693234384059906\n",
      "conv2.weight 0.0005479346960783005\n",
      "conv2.bias 0.003038662951439619\n",
      "fc1.weight 0.0010461959056556225\n",
      "fc1.bias 0.005054667219519615\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020694829523563384\n",
      "conv1.bias 0.027693234384059906\n",
      "conv2.weight 0.0005479346960783005\n",
      "conv2.bias 0.003038662951439619\n",
      "fc1.weight 0.0010461959056556225\n",
      "fc1.bias 0.005054667219519615\n",
      "\n",
      "Test set: Average loss: 0.2907 \n",
      "Accuracy: 9630/10000 (96.30%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019298788905143737\n",
      "conv1.bias 0.024036137387156487\n",
      "conv2.weight 0.0006094563752412796\n",
      "conv2.bias 0.0032528142910450697\n",
      "fc1.weight 0.0010337801650166512\n",
      "fc1.bias 0.005368053913116455\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019298788905143737\n",
      "conv1.bias 0.024036137387156487\n",
      "conv2.weight 0.0006094563752412796\n",
      "conv2.bias 0.0032528142910450697\n",
      "fc1.weight 0.0010337801650166512\n",
      "fc1.bias 0.005368053913116455\n",
      "\n",
      "Test set: Average loss: 0.2160 \n",
      "Accuracy: 9642/10000 (96.42%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0022926050424575805\n",
      "conv1.bias 0.024956360459327698\n",
      "conv2.weight 0.0006094218790531159\n",
      "conv2.bias 0.0030027348548173904\n",
      "fc1.weight 0.001270383596420288\n",
      "fc1.bias 0.00618036612868309\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0022926050424575805\n",
      "conv1.bias 0.024956360459327698\n",
      "conv2.weight 0.0006094218790531159\n",
      "conv2.bias 0.0030027348548173904\n",
      "fc1.weight 0.001270383596420288\n",
      "fc1.bias 0.00618036612868309\n",
      "\n",
      "Test set: Average loss: 0.2306 \n",
      "Accuracy: 9641/10000 (96.41%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019842448830604555\n",
      "conv1.bias 0.02571035362780094\n",
      "conv2.weight 0.0006662130355834961\n",
      "conv2.bias 0.003418862586840987\n",
      "fc1.weight 0.001085848920047283\n",
      "fc1.bias 0.006983734667301178\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019842448830604555\n",
      "conv1.bias 0.02571035362780094\n",
      "conv2.weight 0.0006662130355834961\n",
      "conv2.bias 0.003418862586840987\n",
      "fc1.weight 0.001085848920047283\n",
      "fc1.bias 0.006983734667301178\n",
      "\n",
      "Test set: Average loss: 0.2502 \n",
      "Accuracy: 9625/10000 (96.25%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002014793157577515\n",
      "conv1.bias 0.02415516972541809\n",
      "conv2.weight 0.0006504084169864654\n",
      "conv2.bias 0.003324376419186592\n",
      "fc1.weight 0.0011193357408046723\n",
      "fc1.bias 0.006297272443771362\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002014793157577515\n",
      "conv1.bias 0.02415516972541809\n",
      "conv2.weight 0.0006504084169864654\n",
      "conv2.bias 0.003324376419186592\n",
      "fc1.weight 0.0011193357408046723\n",
      "fc1.bias 0.006297272443771362\n",
      "\n",
      "Test set: Average loss: 0.2355 \n",
      "Accuracy: 9648/10000 (96.48%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021917977929115295\n",
      "conv1.bias 0.02552569843828678\n",
      "conv2.weight 0.0006772912293672562\n",
      "conv2.bias 0.0033470699563622475\n",
      "fc1.weight 0.001221601851284504\n",
      "fc1.bias 0.006413359194993973\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021917977929115295\n",
      "conv1.bias 0.02552569843828678\n",
      "conv2.weight 0.0006772912293672562\n",
      "conv2.bias 0.0033470699563622475\n",
      "fc1.weight 0.001221601851284504\n",
      "fc1.bias 0.006413359194993973\n",
      "\n",
      "Test set: Average loss: 0.2331 \n",
      "Accuracy: 9630/10000 (96.30%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0023357489705085753\n",
      "conv1.bias 0.026739154011011124\n",
      "conv2.weight 0.0006120660156011581\n",
      "conv2.bias 0.003292981069535017\n",
      "fc1.weight 0.0012199390679597855\n",
      "fc1.bias 0.005815167352557183\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0023357489705085753\n",
      "conv1.bias 0.026739154011011124\n",
      "conv2.weight 0.0006120660156011581\n",
      "conv2.bias 0.003292981069535017\n",
      "fc1.weight 0.0012199390679597855\n",
      "fc1.bias 0.005815167352557183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.2165 \n",
      "Accuracy: 9656/10000 (96.56%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002330783009529114\n",
      "conv1.bias 0.024221647530794144\n",
      "conv2.weight 0.0006715203821659088\n",
      "conv2.bias 0.0032503921538591385\n",
      "fc1.weight 0.001188305951654911\n",
      "fc1.bias 0.006067048013210297\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002330783009529114\n",
      "conv1.bias 0.024221647530794144\n",
      "conv2.weight 0.0006715203821659088\n",
      "conv2.bias 0.0032503921538591385\n",
      "fc1.weight 0.001188305951654911\n",
      "fc1.bias 0.006067048013210297\n",
      "\n",
      "Test set: Average loss: 0.2682 \n",
      "Accuracy: 9625/10000 (96.25%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002105666697025299\n",
      "conv1.bias 0.02076771855354309\n",
      "conv2.weight 0.0006436654180288315\n",
      "conv2.bias 0.0029430408030748367\n",
      "fc1.weight 0.0010224374011158944\n",
      "fc1.bias 0.007059565931558609\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002105666697025299\n",
      "conv1.bias 0.02076771855354309\n",
      "conv2.weight 0.0006436654180288315\n",
      "conv2.bias 0.0029430408030748367\n",
      "fc1.weight 0.0010224374011158944\n",
      "fc1.bias 0.007059565931558609\n",
      "\n",
      "Test set: Average loss: 0.2409 \n",
      "Accuracy: 9640/10000 (96.40%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020878997445106504\n",
      "conv1.bias 0.023342162370681763\n",
      "conv2.weight 0.0008177248388528824\n",
      "conv2.bias 0.0037796213291585445\n",
      "fc1.weight 0.0009974084794521331\n",
      "fc1.bias 0.010829668492078781\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020878997445106504\n",
      "conv1.bias 0.023342162370681763\n",
      "conv2.weight 0.0008177248388528824\n",
      "conv2.bias 0.0037796213291585445\n",
      "fc1.weight 0.0009974084794521331\n",
      "fc1.bias 0.010829668492078781\n",
      "\n",
      "Test set: Average loss: 0.2288 \n",
      "Accuracy: 9649/10000 (96.49%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021154463291168215\n",
      "conv1.bias 0.02255113609135151\n",
      "conv2.weight 0.0006473350524902343\n",
      "conv2.bias 0.003344440832734108\n",
      "fc1.weight 0.0010265952907502652\n",
      "fc1.bias 0.008226876705884933\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021154463291168215\n",
      "conv1.bias 0.02255113609135151\n",
      "conv2.weight 0.0006473350524902343\n",
      "conv2.bias 0.003344440832734108\n",
      "fc1.weight 0.0010265952907502652\n",
      "fc1.bias 0.008226876705884933\n",
      "\n",
      "Test set: Average loss: 0.2250 \n",
      "Accuracy: 9631/10000 (96.31%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002227683365345001\n",
      "conv1.bias 0.02125580981373787\n",
      "conv2.weight 0.0005822780728340148\n",
      "conv2.bias 0.003283347235992551\n",
      "fc1.weight 0.000872433464974165\n",
      "fc1.bias 0.008358493447303772\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002227683365345001\n",
      "conv1.bias 0.02125580981373787\n",
      "conv2.weight 0.0005822780728340148\n",
      "conv2.bias 0.003283347235992551\n",
      "fc1.weight 0.000872433464974165\n",
      "fc1.bias 0.008358493447303772\n",
      "\n",
      "Test set: Average loss: 0.3062 \n",
      "Accuracy: 9591/10000 (95.91%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002202005684375763\n",
      "conv1.bias 0.020471196621656418\n",
      "conv2.weight 0.0007319679856300354\n",
      "conv2.bias 0.0033391090109944344\n",
      "fc1.weight 0.0011162308044731617\n",
      "fc1.bias 0.006921934336423874\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002202005684375763\n",
      "conv1.bias 0.020471196621656418\n",
      "conv2.weight 0.0007319679856300354\n",
      "conv2.bias 0.0033391090109944344\n",
      "fc1.weight 0.0011162308044731617\n",
      "fc1.bias 0.006921934336423874\n",
      "\n",
      "Test set: Average loss: 0.2256 \n",
      "Accuracy: 9650/10000 (96.50%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002520171105861664\n",
      "conv1.bias 0.023366089910268784\n",
      "conv2.weight 0.0007633772492408752\n",
      "conv2.bias 0.003373306943103671\n",
      "fc1.weight 0.0014847784303128719\n",
      "fc1.bias 0.005471401289105416\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002520171105861664\n",
      "conv1.bias 0.023366089910268784\n",
      "conv2.weight 0.0007633772492408752\n",
      "conv2.bias 0.003373306943103671\n",
      "fc1.weight 0.0014847784303128719\n",
      "fc1.bias 0.005471401289105416\n",
      "\n",
      "Test set: Average loss: 0.2392 \n",
      "Accuracy: 9613/10000 (96.13%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021827872097492217\n",
      "conv1.bias 0.02645769715309143\n",
      "conv2.weight 0.0007563929259777069\n",
      "conv2.bias 0.0034027628134936094\n",
      "fc1.weight 0.0014426834881305695\n",
      "fc1.bias 0.005637077614665031\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021827872097492217\n",
      "conv1.bias 0.02645769715309143\n",
      "conv2.weight 0.0007563929259777069\n",
      "conv2.bias 0.0034027628134936094\n",
      "fc1.weight 0.0014426834881305695\n",
      "fc1.bias 0.005637077614665031\n",
      "\n",
      "Test set: Average loss: 0.2743 \n",
      "Accuracy: 9614/10000 (96.14%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0022120295464992524\n",
      "conv1.bias 0.025241944938898087\n",
      "conv2.weight 0.000643913671374321\n",
      "conv2.bias 0.003305269405245781\n",
      "fc1.weight 0.0010654873214662075\n",
      "fc1.bias 0.005368695035576821\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0022120295464992524\n",
      "conv1.bias 0.025241944938898087\n",
      "conv2.weight 0.000643913671374321\n",
      "conv2.bias 0.003305269405245781\n",
      "fc1.weight 0.0010654873214662075\n",
      "fc1.bias 0.005368695035576821\n",
      "\n",
      "Test set: Average loss: 0.2776 \n",
      "Accuracy: 9621/10000 (96.21%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002152428925037384\n",
      "conv1.bias 0.02257676050066948\n",
      "conv2.weight 0.0007521424442529679\n",
      "conv2.bias 0.003484150394797325\n",
      "fc1.weight 0.0010636022314429283\n",
      "fc1.bias 0.005664753913879395\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002152428925037384\n",
      "conv1.bias 0.02257676050066948\n",
      "conv2.weight 0.0007521424442529679\n",
      "conv2.bias 0.003484150394797325\n",
      "fc1.weight 0.0010636022314429283\n",
      "fc1.bias 0.005664753913879395\n",
      "\n",
      "Test set: Average loss: 0.2586 \n",
      "Accuracy: 9647/10000 (96.47%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0022416993975639342\n",
      "conv1.bias 0.02561790868639946\n",
      "conv2.weight 0.000740688443183899\n",
      "conv2.bias 0.00363801047205925\n",
      "fc1.weight 0.0011489512398838997\n",
      "fc1.bias 0.00617375634610653\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0022416993975639342\n",
      "conv1.bias 0.02561790868639946\n",
      "conv2.weight 0.000740688443183899\n",
      "conv2.bias 0.00363801047205925\n",
      "fc1.weight 0.0011489512398838997\n",
      "fc1.bias 0.00617375634610653\n",
      "\n",
      "Test set: Average loss: 0.2833 \n",
      "Accuracy: 9657/10000 (96.57%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002391307204961777\n",
      "conv1.bias 0.025428704917430878\n",
      "conv2.weight 0.0007240709662437439\n",
      "conv2.bias 0.0033913790248334408\n",
      "fc1.weight 0.0010826068930327893\n",
      "fc1.bias 0.005711111053824425\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002391307204961777\n",
      "conv1.bias 0.025428704917430878\n",
      "conv2.weight 0.0007240709662437439\n",
      "conv2.bias 0.0033913790248334408\n",
      "fc1.weight 0.0010826068930327893\n",
      "fc1.bias 0.005711111053824425\n",
      "\n",
      "Test set: Average loss: 0.2770 \n",
      "Accuracy: 9650/10000 (96.50%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002361500710248947\n",
      "conv1.bias 0.022447142750024796\n",
      "conv2.weight 0.0007174734026193618\n",
      "conv2.bias 0.0034143938682973385\n",
      "fc1.weight 0.0008998118340969085\n",
      "fc1.bias 0.005486821383237838\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002361500710248947\n",
      "conv1.bias 0.022447142750024796\n",
      "conv2.weight 0.0007174734026193618\n",
      "conv2.bias 0.0034143938682973385\n",
      "fc1.weight 0.0008998118340969085\n",
      "fc1.bias 0.005486821383237838\n",
      "\n",
      "Test set: Average loss: 0.2863 \n",
      "Accuracy: 9626/10000 (96.26%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0023418271541595457\n",
      "conv1.bias 0.02337793819606304\n",
      "conv2.weight 0.0009428319334983825\n",
      "conv2.bias 0.003636577632278204\n",
      "fc1.weight 0.0011139943264424801\n",
      "fc1.bias 0.005362026393413544\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0023418271541595457\n",
      "conv1.bias 0.02337793819606304\n",
      "conv2.weight 0.0009428319334983825\n",
      "conv2.bias 0.003636577632278204\n",
      "fc1.weight 0.0011139943264424801\n",
      "fc1.bias 0.005362026393413544\n",
      "\n",
      "Test set: Average loss: 0.2503 \n",
      "Accuracy: 9643/10000 (96.43%)\n",
      "\n",
      "##########################################\n",
      "###### 1 -th Trial!! ###########\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.013331738710403442\n",
      "conv1.bias 0.014053411781787872\n",
      "conv2.weight 0.00041684839874505996\n",
      "conv2.bias 0.00046520138857886195\n",
      "fc1.weight 0.00032223495654761793\n",
      "fc1.bias 0.00039160773158073423\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.013331738710403442\n",
      "conv1.bias 0.014053411781787872\n",
      "conv2.weight 0.00041684839874505996\n",
      "conv2.bias 0.00046520138857886195\n",
      "fc1.weight 0.00032223495654761793\n",
      "fc1.bias 0.00039160773158073423\n",
      "\n",
      "Test set: Average loss: 2.3022 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0023516069352626802\n",
      "conv1.bias 0.0076448628678917885\n",
      "conv2.weight 0.0012011466920375823\n",
      "conv2.bias 0.0019311999203637242\n",
      "fc1.weight 0.0003259460674598813\n",
      "fc1.bias 0.001962039992213249\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0023516069352626802\n",
      "conv1.bias 0.0076448628678917885\n",
      "conv2.weight 0.0012011466920375823\n",
      "conv2.bias 0.0019311999203637242\n",
      "fc1.weight 0.0003259460674598813\n",
      "fc1.bias 0.001962039992213249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.2934 \n",
      "Accuracy: 1930/10000 (19.30%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003993162512779236\n",
      "conv1.bias 0.012991908006370068\n",
      "conv2.weight 0.0001602461375296116\n",
      "conv2.bias 0.0029768578242510557\n",
      "fc1.weight 0.0003064637538045645\n",
      "fc1.bias 0.0013257311657071114\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003993162512779236\n",
      "conv1.bias 0.012991908006370068\n",
      "conv2.weight 0.0001602461375296116\n",
      "conv2.bias 0.0029768578242510557\n",
      "fc1.weight 0.0003064637538045645\n",
      "fc1.bias 0.0013257311657071114\n",
      "\n",
      "Test set: Average loss: 0.8012 \n",
      "Accuracy: 7651/10000 (76.51%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0014443828165531158\n",
      "conv1.bias 0.017082270234823227\n",
      "conv2.weight 0.0003790566697716713\n",
      "conv2.bias 0.0050607966259121895\n",
      "fc1.weight 0.000781919714063406\n",
      "fc1.bias 0.002023254334926605\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0014443828165531158\n",
      "conv1.bias 0.017082270234823227\n",
      "conv2.weight 0.0003790566697716713\n",
      "conv2.bias 0.0050607966259121895\n",
      "fc1.weight 0.000781919714063406\n",
      "fc1.bias 0.002023254334926605\n",
      "\n",
      "Test set: Average loss: 0.3507 \n",
      "Accuracy: 9446/10000 (94.46%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001511591523885727\n",
      "conv1.bias 0.019554778933525085\n",
      "conv2.weight 0.0007809387147426606\n",
      "conv2.bias 0.005483458749949932\n",
      "fc1.weight 0.0011023084633052349\n",
      "fc1.bias 0.003542526066303253\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001511591523885727\n",
      "conv1.bias 0.019554778933525085\n",
      "conv2.weight 0.0007809387147426606\n",
      "conv2.bias 0.005483458749949932\n",
      "fc1.weight 0.0011023084633052349\n",
      "fc1.bias 0.003542526066303253\n",
      "\n",
      "Test set: Average loss: 0.2251 \n",
      "Accuracy: 9659/10000 (96.59%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001946920156478882\n",
      "conv1.bias 0.02380138635635376\n",
      "conv2.weight 0.0005201037973165512\n",
      "conv2.bias 0.0040660728700459\n",
      "fc1.weight 0.0009178043343126774\n",
      "fc1.bias 0.005531446263194084\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001946920156478882\n",
      "conv1.bias 0.02380138635635376\n",
      "conv2.weight 0.0005201037973165512\n",
      "conv2.bias 0.0040660728700459\n",
      "fc1.weight 0.0009178043343126774\n",
      "fc1.bias 0.005531446263194084\n",
      "\n",
      "Test set: Average loss: 0.2754 \n",
      "Accuracy: 9676/10000 (96.76%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00175032377243042\n",
      "conv1.bias 0.0207974873483181\n",
      "conv2.weight 0.0006925217062234879\n",
      "conv2.bias 0.004205205477774143\n",
      "fc1.weight 0.0009332959540188313\n",
      "fc1.bias 0.005264435335993767\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00175032377243042\n",
      "conv1.bias 0.0207974873483181\n",
      "conv2.weight 0.0006925217062234879\n",
      "conv2.bias 0.004205205477774143\n",
      "fc1.weight 0.0009332959540188313\n",
      "fc1.bias 0.005264435335993767\n",
      "\n",
      "Test set: Average loss: 0.2110 \n",
      "Accuracy: 9683/10000 (96.83%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021928276121616365\n",
      "conv1.bias 0.021535925567150116\n",
      "conv2.weight 0.0005663072317838669\n",
      "conv2.bias 0.003625142853707075\n",
      "fc1.weight 0.0009842749685049057\n",
      "fc1.bias 0.006075965985655784\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021928276121616365\n",
      "conv1.bias 0.021535925567150116\n",
      "conv2.weight 0.0005663072317838669\n",
      "conv2.bias 0.003625142853707075\n",
      "fc1.weight 0.0009842749685049057\n",
      "fc1.bias 0.006075965985655784\n",
      "\n",
      "Test set: Average loss: 0.1983 \n",
      "Accuracy: 9688/10000 (96.88%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021351024508476256\n",
      "conv1.bias 0.024797098711133003\n",
      "conv2.weight 0.0005797746405005455\n",
      "conv2.bias 0.003557780757546425\n",
      "fc1.weight 0.001087347511202097\n",
      "fc1.bias 0.005332896113395691\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021351024508476256\n",
      "conv1.bias 0.024797098711133003\n",
      "conv2.weight 0.0005797746405005455\n",
      "conv2.bias 0.003557780757546425\n",
      "fc1.weight 0.001087347511202097\n",
      "fc1.bias 0.005332896113395691\n",
      "\n",
      "Test set: Average loss: 0.2198 \n",
      "Accuracy: 9687/10000 (96.87%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019632819294929504\n",
      "conv1.bias 0.024359047412872314\n",
      "conv2.weight 0.0006335864216089249\n",
      "conv2.bias 0.0036237130407243967\n",
      "fc1.weight 0.001072445884346962\n",
      "fc1.bias 0.00566723458468914\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019632819294929504\n",
      "conv1.bias 0.024359047412872314\n",
      "conv2.weight 0.0006335864216089249\n",
      "conv2.bias 0.0036237130407243967\n",
      "fc1.weight 0.001072445884346962\n",
      "fc1.bias 0.00566723458468914\n",
      "\n",
      "Test set: Average loss: 0.2396 \n",
      "Accuracy: 9675/10000 (96.75%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020407605171203613\n",
      "conv1.bias 0.024580877274274826\n",
      "conv2.weight 0.0005782078579068184\n",
      "conv2.bias 0.0033042263239622116\n",
      "fc1.weight 0.000996107328683138\n",
      "fc1.bias 0.005285576730966568\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020407605171203613\n",
      "conv1.bias 0.024580877274274826\n",
      "conv2.weight 0.0005782078579068184\n",
      "conv2.bias 0.0033042263239622116\n",
      "fc1.weight 0.000996107328683138\n",
      "fc1.bias 0.005285576730966568\n",
      "\n",
      "Test set: Average loss: 0.2562 \n",
      "Accuracy: 9663/10000 (96.63%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020706090331077576\n",
      "conv1.bias 0.02366577461361885\n",
      "conv2.weight 0.0007154566049575806\n",
      "conv2.bias 0.0036658416502177715\n",
      "fc1.weight 0.001014215499162674\n",
      "fc1.bias 0.00677369087934494\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020706090331077576\n",
      "conv1.bias 0.02366577461361885\n",
      "conv2.weight 0.0007154566049575806\n",
      "conv2.bias 0.0036658416502177715\n",
      "fc1.weight 0.001014215499162674\n",
      "fc1.bias 0.00677369087934494\n",
      "\n",
      "Test set: Average loss: 0.2235 \n",
      "Accuracy: 9668/10000 (96.68%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002028374969959259\n",
      "conv1.bias 0.02545447275042534\n",
      "conv2.weight 0.0006163351610302925\n",
      "conv2.bias 0.003241978818550706\n",
      "fc1.weight 0.0010307642631232738\n",
      "fc1.bias 0.005417926609516144\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002028374969959259\n",
      "conv1.bias 0.02545447275042534\n",
      "conv2.weight 0.0006163351610302925\n",
      "conv2.bias 0.003241978818550706\n",
      "fc1.weight 0.0010307642631232738\n",
      "fc1.bias 0.005417926609516144\n",
      "\n",
      "Test set: Average loss: 0.2840 \n",
      "Accuracy: 9644/10000 (96.44%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002092068791389465\n",
      "conv1.bias 0.022924501448869705\n",
      "conv2.weight 0.0006690863519906998\n",
      "conv2.bias 0.003383596893399954\n",
      "fc1.weight 0.0010159118101000786\n",
      "fc1.bias 0.00593155175447464\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002092068791389465\n",
      "conv1.bias 0.022924501448869705\n",
      "conv2.weight 0.0006690863519906998\n",
      "conv2.bias 0.003383596893399954\n",
      "fc1.weight 0.0010159118101000786\n",
      "fc1.bias 0.00593155175447464\n",
      "\n",
      "Test set: Average loss: 0.2016 \n",
      "Accuracy: 9663/10000 (96.63%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002269579917192459\n",
      "conv1.bias 0.024128135293722153\n",
      "conv2.weight 0.0007007401436567307\n",
      "conv2.bias 0.0035976856015622616\n",
      "fc1.weight 0.001057913526892662\n",
      "fc1.bias 0.005827026069164276\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002269579917192459\n",
      "conv1.bias 0.024128135293722153\n",
      "conv2.weight 0.0007007401436567307\n",
      "conv2.bias 0.0035976856015622616\n",
      "fc1.weight 0.001057913526892662\n",
      "fc1.bias 0.005827026069164276\n",
      "\n",
      "Test set: Average loss: 0.3156 \n",
      "Accuracy: 9577/10000 (95.77%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018813827633857728\n",
      "conv1.bias 0.022596638649702072\n",
      "conv2.weight 0.0005788405612111091\n",
      "conv2.bias 0.003170441370457411\n",
      "fc1.weight 0.0010963034816086291\n",
      "fc1.bias 0.005071826279163361\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018813827633857728\n",
      "conv1.bias 0.022596638649702072\n",
      "conv2.weight 0.0005788405612111091\n",
      "conv2.bias 0.003170441370457411\n",
      "fc1.weight 0.0010963034816086291\n",
      "fc1.bias 0.005071826279163361\n",
      "\n",
      "Test set: Average loss: 0.2129 \n",
      "Accuracy: 9628/10000 (96.28%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020796966552734373\n",
      "conv1.bias 0.022596679627895355\n",
      "conv2.weight 0.0007370080798864365\n",
      "conv2.bias 0.003465251997113228\n",
      "fc1.weight 0.0013420690782368183\n",
      "fc1.bias 0.005285273864865303\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020796966552734373\n",
      "conv1.bias 0.022596679627895355\n",
      "conv2.weight 0.0007370080798864365\n",
      "conv2.bias 0.003465251997113228\n",
      "fc1.weight 0.0013420690782368183\n",
      "fc1.bias 0.005285273864865303\n",
      "\n",
      "Test set: Average loss: 0.1972 \n",
      "Accuracy: 9673/10000 (96.73%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020625051856040955\n",
      "conv1.bias 0.025418337434530258\n",
      "conv2.weight 0.000605623833835125\n",
      "conv2.bias 0.003270363435149193\n",
      "fc1.weight 0.0012794959358870984\n",
      "fc1.bias 0.005179006233811378\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020625051856040955\n",
      "conv1.bias 0.025418337434530258\n",
      "conv2.weight 0.000605623833835125\n",
      "conv2.bias 0.003270363435149193\n",
      "fc1.weight 0.0012794959358870984\n",
      "fc1.bias 0.005179006233811378\n",
      "\n",
      "Test set: Average loss: 0.2436 \n",
      "Accuracy: 9678/10000 (96.78%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002075008749961853\n",
      "conv1.bias 0.02256370335817337\n",
      "conv2.weight 0.000553421713411808\n",
      "conv2.bias 0.0030559515580534935\n",
      "fc1.weight 0.0010720791295170784\n",
      "fc1.bias 0.005003728345036507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected users: [0 1]\n",
      "conv1.weight 0.002075008749961853\n",
      "conv1.bias 0.02256370335817337\n",
      "conv2.weight 0.000553421713411808\n",
      "conv2.bias 0.0030559515580534935\n",
      "fc1.weight 0.0010720791295170784\n",
      "fc1.bias 0.005003728345036507\n",
      "\n",
      "Test set: Average loss: 0.2246 \n",
      "Accuracy: 9685/10000 (96.85%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001985015124082565\n",
      "conv1.bias 0.026008613407611847\n",
      "conv2.weight 0.000576024018228054\n",
      "conv2.bias 0.0032684761099517345\n",
      "fc1.weight 0.0009285038337111473\n",
      "fc1.bias 0.005973227694630623\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001985015124082565\n",
      "conv1.bias 0.026008613407611847\n",
      "conv2.weight 0.000576024018228054\n",
      "conv2.bias 0.0032684761099517345\n",
      "fc1.weight 0.0009285038337111473\n",
      "fc1.bias 0.005973227694630623\n",
      "\n",
      "Test set: Average loss: 0.2095 \n",
      "Accuracy: 9705/10000 (97.05%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002176060974597931\n",
      "conv1.bias 0.024369895458221436\n",
      "conv2.weight 0.000552721805870533\n",
      "conv2.bias 0.0031730460468679667\n",
      "fc1.weight 0.0009245790541172027\n",
      "fc1.bias 0.006104464828968048\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002176060974597931\n",
      "conv1.bias 0.024369895458221436\n",
      "conv2.weight 0.000552721805870533\n",
      "conv2.bias 0.0031730460468679667\n",
      "fc1.weight 0.0009245790541172027\n",
      "fc1.bias 0.006104464828968048\n",
      "\n",
      "Test set: Average loss: 0.2107 \n",
      "Accuracy: 9686/10000 (96.86%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00200158566236496\n",
      "conv1.bias 0.02477247267961502\n",
      "conv2.weight 0.0005981118232011796\n",
      "conv2.bias 0.003154422389343381\n",
      "fc1.weight 0.0010241718962788581\n",
      "fc1.bias 0.006145189329981804\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00200158566236496\n",
      "conv1.bias 0.02477247267961502\n",
      "conv2.weight 0.0005981118232011796\n",
      "conv2.bias 0.003154422389343381\n",
      "fc1.weight 0.0010241718962788581\n",
      "fc1.bias 0.006145189329981804\n",
      "\n",
      "Test set: Average loss: 0.2628 \n",
      "Accuracy: 9684/10000 (96.84%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002041507363319397\n",
      "conv1.bias 0.024544421583414078\n",
      "conv2.weight 0.0006010540202260017\n",
      "conv2.bias 0.0032142018899321556\n",
      "fc1.weight 0.001016160473227501\n",
      "fc1.bias 0.004973054304718972\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002041507363319397\n",
      "conv1.bias 0.024544421583414078\n",
      "conv2.weight 0.0006010540202260017\n",
      "conv2.bias 0.0032142018899321556\n",
      "fc1.weight 0.001016160473227501\n",
      "fc1.bias 0.004973054304718972\n",
      "\n",
      "Test set: Average loss: 0.2495 \n",
      "Accuracy: 9659/10000 (96.59%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021814969182014465\n",
      "conv1.bias 0.023366842418909073\n",
      "conv2.weight 0.0007290515303611755\n",
      "conv2.bias 0.0033491903450340033\n",
      "fc1.weight 0.0012897408567368984\n",
      "fc1.bias 0.006096931919455528\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021814969182014465\n",
      "conv1.bias 0.023366842418909073\n",
      "conv2.weight 0.0007290515303611755\n",
      "conv2.bias 0.0033491903450340033\n",
      "fc1.weight 0.0012897408567368984\n",
      "fc1.bias 0.006096931919455528\n",
      "\n",
      "Test set: Average loss: 0.2214 \n",
      "Accuracy: 9681/10000 (96.81%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021245405077934265\n",
      "conv1.bias 0.024841006845235825\n",
      "conv2.weight 0.0006762468069791794\n",
      "conv2.bias 0.003396016312763095\n",
      "fc1.weight 0.0011398563161492347\n",
      "fc1.bias 0.006115119159221649\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021245405077934265\n",
      "conv1.bias 0.024841006845235825\n",
      "conv2.weight 0.0006762468069791794\n",
      "conv2.bias 0.003396016312763095\n",
      "fc1.weight 0.0011398563161492347\n",
      "fc1.bias 0.006115119159221649\n",
      "\n",
      "Test set: Average loss: 0.2437 \n",
      "Accuracy: 9674/10000 (96.74%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0022036829590797425\n",
      "conv1.bias 0.022371163591742516\n",
      "conv2.weight 0.0006731398403644562\n",
      "conv2.bias 0.0033090440556406975\n",
      "fc1.weight 0.0011424859054386615\n",
      "fc1.bias 0.005619573965668678\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0022036829590797425\n",
      "conv1.bias 0.022371163591742516\n",
      "conv2.weight 0.0006731398403644562\n",
      "conv2.bias 0.0033090440556406975\n",
      "fc1.weight 0.0011424859054386615\n",
      "fc1.bias 0.005619573965668678\n",
      "\n",
      "Test set: Average loss: 0.2412 \n",
      "Accuracy: 9637/10000 (96.37%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020990592241287233\n",
      "conv1.bias 0.0226711668074131\n",
      "conv2.weight 0.0008046285063028336\n",
      "conv2.bias 0.003187299007549882\n",
      "fc1.weight 0.0015466578304767608\n",
      "fc1.bias 0.005582255125045776\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020990592241287233\n",
      "conv1.bias 0.0226711668074131\n",
      "conv2.weight 0.0008046285063028336\n",
      "conv2.bias 0.003187299007549882\n",
      "fc1.weight 0.0015466578304767608\n",
      "fc1.bias 0.005582255125045776\n",
      "\n",
      "Test set: Average loss: 0.2922 \n",
      "Accuracy: 9638/10000 (96.38%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002151917815208435\n",
      "conv1.bias 0.0221723485738039\n",
      "conv2.weight 0.0007726138830184936\n",
      "conv2.bias 0.0031369486823678017\n",
      "fc1.weight 0.0012320821173489095\n",
      "fc1.bias 0.004741739109158516\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002151917815208435\n",
      "conv1.bias 0.0221723485738039\n",
      "conv2.weight 0.0007726138830184936\n",
      "conv2.bias 0.0031369486823678017\n",
      "fc1.weight 0.0012320821173489095\n",
      "fc1.bias 0.004741739109158516\n",
      "\n",
      "Test set: Average loss: 0.3143 \n",
      "Accuracy: 9619/10000 (96.19%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018320333957672118\n",
      "conv1.bias 0.024173125624656677\n",
      "conv2.weight 0.0009016471356153488\n",
      "conv2.bias 0.0032616681419312954\n",
      "fc1.weight 0.0012799574062228203\n",
      "fc1.bias 0.004960272833704949\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018320333957672118\n",
      "conv1.bias 0.024173125624656677\n",
      "conv2.weight 0.0009016471356153488\n",
      "conv2.bias 0.0032616681419312954\n",
      "fc1.weight 0.0012799574062228203\n",
      "fc1.bias 0.004960272833704949\n",
      "\n",
      "Test set: Average loss: 0.3525 \n",
      "Accuracy: 9605/10000 (96.05%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0017684268951416015\n",
      "conv1.bias 0.020478390157222748\n",
      "conv2.weight 0.0008792302012443542\n",
      "conv2.bias 0.0029289473313838243\n",
      "fc1.weight 0.0015787143260240556\n",
      "fc1.bias 0.005198407545685768\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0017684268951416015\n",
      "conv1.bias 0.020478390157222748\n",
      "conv2.weight 0.0008792302012443542\n",
      "conv2.bias 0.0029289473313838243\n",
      "fc1.weight 0.0015787143260240556\n",
      "fc1.bias 0.005198407545685768\n",
      "\n",
      "Test set: Average loss: 0.2288 \n",
      "Accuracy: 9636/10000 (96.36%)\n",
      "\n",
      "##########################################\n",
      "###### 2 -th Trial!! ###########\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.01349303960800171\n",
      "conv1.bias 0.011823946610093117\n",
      "conv2.weight 0.00041684988886117933\n",
      "conv2.bias 0.0004254807427059859\n",
      "fc1.weight 0.0003302828874439001\n",
      "fc1.bias 0.00023798327893018723\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.01349303960800171\n",
      "conv1.bias 0.011823946610093117\n",
      "conv2.weight 0.00041684988886117933\n",
      "conv2.bias 0.0004254807427059859\n",
      "fc1.weight 0.0003302828874439001\n",
      "fc1.bias 0.00023798327893018723\n",
      "\n",
      "Test set: Average loss: 2.2695 \n",
      "Accuracy: 3664/10000 (36.64%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018489648401737212\n",
      "conv1.bias 0.006498604081571102\n",
      "conv2.weight 0.001460663229227066\n",
      "conv2.bias 0.0024169611278921366\n",
      "fc1.weight 0.0006619567982852459\n",
      "fc1.bias 0.0013063867576420308\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018489648401737212\n",
      "conv1.bias 0.006498604081571102\n",
      "conv2.weight 0.001460663229227066\n",
      "conv2.bias 0.0024169611278921366\n",
      "fc1.weight 0.0006619567982852459\n",
      "fc1.bias 0.0013063867576420308\n",
      "\n",
      "Test set: Average loss: 1.3759 \n",
      "Accuracy: 6860/10000 (68.60%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00122737780213356\n",
      "conv1.bias 0.012841533869504929\n",
      "conv2.weight 0.00038324203342199326\n",
      "conv2.bias 0.003449180629104376\n",
      "fc1.weight 0.0006616134662181139\n",
      "fc1.bias 0.0026446156203746795\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00122737780213356\n",
      "conv1.bias 0.012841533869504929\n",
      "conv2.weight 0.00038324203342199326\n",
      "conv2.bias 0.003449180629104376\n",
      "fc1.weight 0.0006616134662181139\n",
      "fc1.bias 0.0026446156203746795\n",
      "\n",
      "Test set: Average loss: 0.3586 \n",
      "Accuracy: 9432/10000 (94.32%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002262886166572571\n",
      "conv1.bias 0.021572601050138474\n",
      "conv2.weight 0.0005131908506155014\n",
      "conv2.bias 0.0041620465926826\n",
      "fc1.weight 0.0007516957819461822\n",
      "fc1.bias 0.005383311212062836\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002262886166572571\n",
      "conv1.bias 0.021572601050138474\n",
      "conv2.weight 0.0005131908506155014\n",
      "conv2.bias 0.0041620465926826\n",
      "fc1.weight 0.0007516957819461822\n",
      "fc1.bias 0.005383311212062836\n",
      "\n",
      "Test set: Average loss: 0.3154 \n",
      "Accuracy: 9462/10000 (94.62%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002231355607509613\n",
      "conv1.bias 0.01993565261363983\n",
      "conv2.weight 0.0006848000735044479\n",
      "conv2.bias 0.004106323234736919\n",
      "fc1.weight 0.000924232229590416\n",
      "fc1.bias 0.00712321549654007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected users: [0 1]\n",
      "conv1.weight 0.002231355607509613\n",
      "conv1.bias 0.01993565261363983\n",
      "conv2.weight 0.0006848000735044479\n",
      "conv2.bias 0.004106323234736919\n",
      "fc1.weight 0.000924232229590416\n",
      "fc1.bias 0.00712321549654007\n",
      "\n",
      "Test set: Average loss: 0.3445 \n",
      "Accuracy: 9436/10000 (94.36%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002487565577030182\n",
      "conv1.bias 0.01889164373278618\n",
      "conv2.weight 0.0006236561015248299\n",
      "conv2.bias 0.003589305095374584\n",
      "fc1.weight 0.0007556228898465634\n",
      "fc1.bias 0.007752906531095505\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002487565577030182\n",
      "conv1.bias 0.01889164373278618\n",
      "conv2.weight 0.0006236561015248299\n",
      "conv2.bias 0.003589305095374584\n",
      "fc1.weight 0.0007556228898465634\n",
      "fc1.bias 0.007752906531095505\n",
      "\n",
      "Test set: Average loss: 0.3018 \n",
      "Accuracy: 9482/10000 (94.82%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021683646738529204\n",
      "conv1.bias 0.018746815621852875\n",
      "conv2.weight 0.000526110716164112\n",
      "conv2.bias 0.0034343418665230274\n",
      "fc1.weight 0.0007292776368558407\n",
      "fc1.bias 0.007503948360681534\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021683646738529204\n",
      "conv1.bias 0.018746815621852875\n",
      "conv2.weight 0.000526110716164112\n",
      "conv2.bias 0.0034343418665230274\n",
      "fc1.weight 0.0007292776368558407\n",
      "fc1.bias 0.007503948360681534\n",
      "\n",
      "Test set: Average loss: 0.3170 \n",
      "Accuracy: 9468/10000 (94.68%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021132567524909975\n",
      "conv1.bias 0.01859079673886299\n",
      "conv2.weight 0.0006367814540863037\n",
      "conv2.bias 0.0037859694566577673\n",
      "fc1.weight 0.0007540478836745024\n",
      "fc1.bias 0.008464760333299636\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021132567524909975\n",
      "conv1.bias 0.01859079673886299\n",
      "conv2.weight 0.0006367814540863037\n",
      "conv2.bias 0.0037859694566577673\n",
      "fc1.weight 0.0007540478836745024\n",
      "fc1.bias 0.008464760333299636\n",
      "\n",
      "Test set: Average loss: 0.2882 \n",
      "Accuracy: 9490/10000 (94.90%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0023847009241580964\n",
      "conv1.bias 0.024864258244633675\n",
      "conv2.weight 0.0004310412332415581\n",
      "conv2.bias 0.0032463176175951958\n",
      "fc1.weight 0.0007935088127851486\n",
      "fc1.bias 0.007171467691659927\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0023847009241580964\n",
      "conv1.bias 0.024864258244633675\n",
      "conv2.weight 0.0004310412332415581\n",
      "conv2.bias 0.0032463176175951958\n",
      "fc1.weight 0.0007935088127851486\n",
      "fc1.bias 0.007171467691659927\n",
      "\n",
      "Test set: Average loss: 0.2804 \n",
      "Accuracy: 9493/10000 (94.93%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002140335142612457\n",
      "conv1.bias 0.023074863478541374\n",
      "conv2.weight 0.0005434940010309219\n",
      "conv2.bias 0.0036706901155412197\n",
      "fc1.weight 0.0008418391458690167\n",
      "fc1.bias 0.007143460214138031\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002140335142612457\n",
      "conv1.bias 0.023074863478541374\n",
      "conv2.weight 0.0005434940010309219\n",
      "conv2.bias 0.0036706901155412197\n",
      "fc1.weight 0.0008418391458690167\n",
      "fc1.bias 0.007143460214138031\n",
      "\n",
      "Test set: Average loss: 0.2720 \n",
      "Accuracy: 9497/10000 (94.97%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002375642657279968\n",
      "conv1.bias 0.025749407708644867\n",
      "conv2.weight 0.0006469298154115677\n",
      "conv2.bias 0.0038377882447093725\n",
      "fc1.weight 0.0008600422181189061\n",
      "fc1.bias 0.006453600525856018\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002375642657279968\n",
      "conv1.bias 0.025749407708644867\n",
      "conv2.weight 0.0006469298154115677\n",
      "conv2.bias 0.0038377882447093725\n",
      "fc1.weight 0.0008600422181189061\n",
      "fc1.bias 0.006453600525856018\n",
      "\n",
      "Test set: Average loss: 0.2903 \n",
      "Accuracy: 9485/10000 (94.85%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002573649287223816\n",
      "conv1.bias 0.026414934545755386\n",
      "conv2.weight 0.0004448128119111061\n",
      "conv2.bias 0.0029654596000909805\n",
      "fc1.weight 0.0006921208929270506\n",
      "fc1.bias 0.005910582467913627\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002573649287223816\n",
      "conv1.bias 0.026414934545755386\n",
      "conv2.weight 0.0004448128119111061\n",
      "conv2.bias 0.0029654596000909805\n",
      "fc1.weight 0.0006921208929270506\n",
      "fc1.bias 0.005910582467913627\n",
      "\n",
      "Test set: Average loss: 0.2692 \n",
      "Accuracy: 9533/10000 (95.33%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0028004395961761476\n",
      "conv1.bias 0.023373693227767944\n",
      "conv2.weight 0.0004859452694654465\n",
      "conv2.bias 0.0032787832897156477\n",
      "fc1.weight 0.0008277949877083301\n",
      "fc1.bias 0.005832312256097793\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0028004395961761476\n",
      "conv1.bias 0.023373693227767944\n",
      "conv2.weight 0.0004859452694654465\n",
      "conv2.bias 0.0032787832897156477\n",
      "fc1.weight 0.0008277949877083301\n",
      "fc1.bias 0.005832312256097793\n",
      "\n",
      "Test set: Average loss: 0.2666 \n",
      "Accuracy: 9536/10000 (95.36%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002096022665500641\n",
      "conv1.bias 0.025835879147052765\n",
      "conv2.weight 0.0005737834796309471\n",
      "conv2.bias 0.0032110605388879776\n",
      "fc1.weight 0.000973588041961193\n",
      "fc1.bias 0.006679697334766388\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002096022665500641\n",
      "conv1.bias 0.025835879147052765\n",
      "conv2.weight 0.0005737834796309471\n",
      "conv2.bias 0.0032110605388879776\n",
      "fc1.weight 0.000973588041961193\n",
      "fc1.bias 0.006679697334766388\n",
      "\n",
      "Test set: Average loss: 0.2624 \n",
      "Accuracy: 9531/10000 (95.31%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020916688442230223\n",
      "conv1.bias 0.0218451339751482\n",
      "conv2.weight 0.0006114040687680244\n",
      "conv2.bias 0.0033575287088751793\n",
      "fc1.weight 0.0006526289042085409\n",
      "fc1.bias 0.0069953233003616335\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020916688442230223\n",
      "conv1.bias 0.0218451339751482\n",
      "conv2.weight 0.0006114040687680244\n",
      "conv2.bias 0.0033575287088751793\n",
      "fc1.weight 0.0006526289042085409\n",
      "fc1.bias 0.0069953233003616335\n",
      "\n",
      "Test set: Average loss: 0.2874 \n",
      "Accuracy: 9527/10000 (95.27%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021450284123420717\n",
      "conv1.bias 0.019183874130249023\n",
      "conv2.weight 0.0007738067954778671\n",
      "conv2.bias 0.003936453256756067\n",
      "fc1.weight 0.0006434053648263216\n",
      "fc1.bias 0.007108201831579208\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021450284123420717\n",
      "conv1.bias 0.019183874130249023\n",
      "conv2.weight 0.0007738067954778671\n",
      "conv2.bias 0.003936453256756067\n",
      "fc1.weight 0.0006434053648263216\n",
      "fc1.bias 0.007108201831579208\n",
      "\n",
      "Test set: Average loss: 0.2875 \n",
      "Accuracy: 9519/10000 (95.19%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002621707022190094\n",
      "conv1.bias 0.018726889044046402\n",
      "conv2.weight 0.0007438825815916061\n",
      "conv2.bias 0.003458418184891343\n",
      "fc1.weight 0.0012913557700812816\n",
      "fc1.bias 0.007812700420618057\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002621707022190094\n",
      "conv1.bias 0.018726889044046402\n",
      "conv2.weight 0.0007438825815916061\n",
      "conv2.bias 0.003458418184891343\n",
      "fc1.weight 0.0012913557700812816\n",
      "fc1.bias 0.007812700420618057\n",
      "\n",
      "Test set: Average loss: 0.2662 \n",
      "Accuracy: 9535/10000 (95.35%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002586296796798706\n",
      "conv1.bias 0.01983032375574112\n",
      "conv2.weight 0.0007143289595842361\n",
      "conv2.bias 0.0036240098997950554\n",
      "fc1.weight 0.0009839005768299102\n",
      "fc1.bias 0.00783984810113907\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002586296796798706\n",
      "conv1.bias 0.01983032375574112\n",
      "conv2.weight 0.0007143289595842361\n",
      "conv2.bias 0.0036240098997950554\n",
      "fc1.weight 0.0009839005768299102\n",
      "fc1.bias 0.00783984810113907\n",
      "\n",
      "Test set: Average loss: 0.2797 \n",
      "Accuracy: 9520/10000 (95.20%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0026090866327285766\n",
      "conv1.bias 0.020241480320692062\n",
      "conv2.weight 0.0007986512780189514\n",
      "conv2.bias 0.003813367336988449\n",
      "fc1.weight 0.0009328706189990043\n",
      "fc1.bias 0.006532266736030579\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0026090866327285766\n",
      "conv1.bias 0.020241480320692062\n",
      "conv2.weight 0.0007986512780189514\n",
      "conv2.bias 0.003813367336988449\n",
      "fc1.weight 0.0009328706189990043\n",
      "fc1.bias 0.006532266736030579\n",
      "\n",
      "Test set: Average loss: 0.2595 \n",
      "Accuracy: 9526/10000 (95.26%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0026728618144989014\n",
      "conv1.bias 0.0215691439807415\n",
      "conv2.weight 0.0006999436765909195\n",
      "conv2.bias 0.003410967532545328\n",
      "fc1.weight 0.0010610980913043021\n",
      "fc1.bias 0.0058931615203619\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0026728618144989014\n",
      "conv1.bias 0.0215691439807415\n",
      "conv2.weight 0.0006999436765909195\n",
      "conv2.bias 0.003410967532545328\n",
      "fc1.weight 0.0010610980913043021\n",
      "fc1.bias 0.0058931615203619\n",
      "\n",
      "Test set: Average loss: 0.2788 \n",
      "Accuracy: 9490/10000 (94.90%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00234156996011734\n",
      "conv1.bias 0.019440269097685814\n",
      "conv2.weight 0.0007313045114278793\n",
      "conv2.bias 0.003292451612651348\n",
      "fc1.weight 0.000990171916782856\n",
      "fc1.bias 0.006146462634205818\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00234156996011734\n",
      "conv1.bias 0.019440269097685814\n",
      "conv2.weight 0.0007313045114278793\n",
      "conv2.bias 0.003292451612651348\n",
      "fc1.weight 0.000990171916782856\n",
      "fc1.bias 0.006146462634205818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.2720 \n",
      "Accuracy: 9484/10000 (94.84%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002147302180528641\n",
      "conv1.bias 0.022037770599126816\n",
      "conv2.weight 0.0008286476135253906\n",
      "conv2.bias 0.0037684047129005194\n",
      "fc1.weight 0.0011969459243118762\n",
      "fc1.bias 0.006791721284389496\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002147302180528641\n",
      "conv1.bias 0.022037770599126816\n",
      "conv2.weight 0.0008286476135253906\n",
      "conv2.bias 0.0037684047129005194\n",
      "fc1.weight 0.0011969459243118762\n",
      "fc1.bias 0.006791721284389496\n",
      "\n",
      "Test set: Average loss: 0.2823 \n",
      "Accuracy: 9494/10000 (94.94%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0023594501614570616\n",
      "conv1.bias 0.02139897271990776\n",
      "conv2.weight 0.0007823573052883148\n",
      "conv2.bias 0.0034651258029043674\n",
      "fc1.weight 0.0008513320237398148\n",
      "fc1.bias 0.006118107587099075\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0023594501614570616\n",
      "conv1.bias 0.02139897271990776\n",
      "conv2.weight 0.0007823573052883148\n",
      "conv2.bias 0.0034651258029043674\n",
      "fc1.weight 0.0008513320237398148\n",
      "fc1.bias 0.006118107587099075\n",
      "\n",
      "Test set: Average loss: 0.2730 \n",
      "Accuracy: 9514/10000 (95.14%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.003054834008216858\n",
      "conv1.bias 0.022595437243580818\n",
      "conv2.weight 0.00047683306038379667\n",
      "conv2.bias 0.0031945686787366867\n",
      "fc1.weight 0.0006548985838890076\n",
      "fc1.bias 0.005756859108805657\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.003054834008216858\n",
      "conv1.bias 0.022595437243580818\n",
      "conv2.weight 0.00047683306038379667\n",
      "conv2.bias 0.0031945686787366867\n",
      "fc1.weight 0.0006548985838890076\n",
      "fc1.bias 0.005756859108805657\n",
      "\n",
      "Test set: Average loss: 0.2652 \n",
      "Accuracy: 9544/10000 (95.44%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0027133280038833616\n",
      "conv1.bias 0.02259693667292595\n",
      "conv2.weight 0.0005490098148584366\n",
      "conv2.bias 0.0034225843846797943\n",
      "fc1.weight 0.000807935930788517\n",
      "fc1.bias 0.005521075055003166\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0027133280038833616\n",
      "conv1.bias 0.02259693667292595\n",
      "conv2.weight 0.0005490098148584366\n",
      "conv2.bias 0.0034225843846797943\n",
      "fc1.weight 0.000807935930788517\n",
      "fc1.bias 0.005521075055003166\n",
      "\n",
      "Test set: Average loss: 0.2881 \n",
      "Accuracy: 9501/10000 (95.01%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002386931627988815\n",
      "conv1.bias 0.0217610951513052\n",
      "conv2.weight 0.0006156716495752335\n",
      "conv2.bias 0.0032765413634479046\n",
      "fc1.weight 0.0007937228307127953\n",
      "fc1.bias 0.006098988279700279\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002386931627988815\n",
      "conv1.bias 0.0217610951513052\n",
      "conv2.weight 0.0006156716495752335\n",
      "conv2.bias 0.0032765413634479046\n",
      "fc1.weight 0.0007937228307127953\n",
      "fc1.bias 0.006098988279700279\n",
      "\n",
      "Test set: Average loss: 0.3217 \n",
      "Accuracy: 9462/10000 (94.62%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0025835233926773072\n",
      "conv1.bias 0.020532988011837006\n",
      "conv2.weight 0.0007693430781364441\n",
      "conv2.bias 0.003493850352242589\n",
      "fc1.weight 0.0009833129122853279\n",
      "fc1.bias 0.00500854104757309\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0025835233926773072\n",
      "conv1.bias 0.020532988011837006\n",
      "conv2.weight 0.0007693430781364441\n",
      "conv2.bias 0.003493850352242589\n",
      "fc1.weight 0.0009833129122853279\n",
      "fc1.bias 0.00500854104757309\n",
      "\n",
      "Test set: Average loss: 0.2687 \n",
      "Accuracy: 9517/10000 (95.17%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002882168889045715\n",
      "conv1.bias 0.023884903639554977\n",
      "conv2.weight 0.0005858347192406654\n",
      "conv2.bias 0.003163221525028348\n",
      "fc1.weight 0.0011579500511288642\n",
      "fc1.bias 0.006069495901465416\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002882168889045715\n",
      "conv1.bias 0.023884903639554977\n",
      "conv2.weight 0.0005858347192406654\n",
      "conv2.bias 0.003163221525028348\n",
      "fc1.weight 0.0011579500511288642\n",
      "fc1.bias 0.006069495901465416\n",
      "\n",
      "Test set: Average loss: 0.2833 \n",
      "Accuracy: 9513/10000 (95.13%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0024094077944755554\n",
      "conv1.bias 0.022653017193078995\n",
      "conv2.weight 0.0006902875006198884\n",
      "conv2.bias 0.0034795631654560566\n",
      "fc1.weight 0.0011545466259121895\n",
      "fc1.bias 0.006185925379395485\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0024094077944755554\n",
      "conv1.bias 0.022653017193078995\n",
      "conv2.weight 0.0006902875006198884\n",
      "conv2.bias 0.0034795631654560566\n",
      "fc1.weight 0.0011545466259121895\n",
      "fc1.bias 0.006185925379395485\n",
      "\n",
      "Test set: Average loss: 0.2754 \n",
      "Accuracy: 9526/10000 (95.26%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00238618403673172\n",
      "conv1.bias 0.021476920694112778\n",
      "conv2.weight 0.0006301186233758926\n",
      "conv2.bias 0.0032003605738282204\n",
      "fc1.weight 0.0008701592683792114\n",
      "fc1.bias 0.007351822406053543\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00238618403673172\n",
      "conv1.bias 0.021476920694112778\n",
      "conv2.weight 0.0006301186233758926\n",
      "conv2.bias 0.0032003605738282204\n",
      "fc1.weight 0.0008701592683792114\n",
      "fc1.bias 0.007351822406053543\n",
      "\n",
      "Test set: Average loss: 0.3474 \n",
      "Accuracy: 9491/10000 (94.91%)\n",
      "\n",
      "##########################################\n",
      "###### 3 -th Trial!! ###########\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.013050150871276856\n",
      "conv1.bias 0.012807446531951427\n",
      "conv2.weight 0.0004177437722682953\n",
      "conv2.bias 0.0003207340487278998\n",
      "fc1.weight 0.00032250550575554373\n",
      "fc1.bias 0.00042220759205520154\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.013050150871276856\n",
      "conv1.bias 0.012807446531951427\n",
      "conv2.weight 0.0004177437722682953\n",
      "conv2.bias 0.0003207340487278998\n",
      "fc1.weight 0.00032250550575554373\n",
      "fc1.bias 0.00042220759205520154\n",
      "\n",
      "Test set: Average loss: 2.2942 \n",
      "Accuracy: 2991/10000 (29.91%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002307701110839844\n",
      "conv1.bias 0.006367046386003494\n",
      "conv2.weight 0.0014300572872161866\n",
      "conv2.bias 0.002151121851056814\n",
      "fc1.weight 0.0005017265677452088\n",
      "fc1.bias 0.0016709502786397934\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002307701110839844\n",
      "conv1.bias 0.006367046386003494\n",
      "conv2.weight 0.0014300572872161866\n",
      "conv2.bias 0.002151121851056814\n",
      "fc1.weight 0.0005017265677452088\n",
      "fc1.bias 0.0016709502786397934\n",
      "\n",
      "Test set: Average loss: 2.1482 \n",
      "Accuracy: 3679/10000 (36.79%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006411489844322204\n",
      "conv1.bias 0.016269458457827568\n",
      "conv2.weight 0.00011968267150223255\n",
      "conv2.bias 0.002383581828325987\n",
      "fc1.weight 0.0002580375177785754\n",
      "fc1.bias 0.0018717503175139428\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006411489844322204\n",
      "conv1.bias 0.016269458457827568\n",
      "conv2.weight 0.00011968267150223255\n",
      "conv2.bias 0.002383581828325987\n",
      "fc1.weight 0.0002580375177785754\n",
      "fc1.bias 0.0018717503175139428\n",
      "\n",
      "Test set: Average loss: 1.4148 \n",
      "Accuracy: 5909/10000 (59.09%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00102670818567276\n",
      "conv1.bias 0.01821383461356163\n",
      "conv2.weight 0.00024631379172205923\n",
      "conv2.bias 0.002994171343743801\n",
      "fc1.weight 0.000594159122556448\n",
      "fc1.bias 0.0021795861423015593\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00102670818567276\n",
      "conv1.bias 0.01821383461356163\n",
      "conv2.weight 0.00024631379172205923\n",
      "conv2.bias 0.002994171343743801\n",
      "fc1.weight 0.000594159122556448\n",
      "fc1.bias 0.0021795861423015593\n",
      "\n",
      "Test set: Average loss: 0.4224 \n",
      "Accuracy: 9333/10000 (93.33%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001471196860074997\n",
      "conv1.bias 0.017637187615036964\n",
      "conv2.weight 0.0007240364700555801\n",
      "conv2.bias 0.004560345783829689\n",
      "fc1.weight 0.0011676478199660778\n",
      "fc1.bias 0.0035472944378852843\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001471196860074997\n",
      "conv1.bias 0.017637187615036964\n",
      "conv2.weight 0.0007240364700555801\n",
      "conv2.bias 0.004560345783829689\n",
      "fc1.weight 0.0011676478199660778\n",
      "fc1.bias 0.0035472944378852843\n",
      "\n",
      "Test set: Average loss: 0.2082 \n",
      "Accuracy: 9661/10000 (96.61%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001972948908805847\n",
      "conv1.bias 0.019353410229086876\n",
      "conv2.weight 0.0004417265579104424\n",
      "conv2.bias 0.0034469880629330873\n",
      "fc1.weight 0.000888036098331213\n",
      "fc1.bias 0.004994230717420578\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001972948908805847\n",
      "conv1.bias 0.019353410229086876\n",
      "conv2.weight 0.0004417265579104424\n",
      "conv2.bias 0.0034469880629330873\n",
      "fc1.weight 0.000888036098331213\n",
      "fc1.bias 0.004994230717420578\n",
      "\n",
      "Test set: Average loss: 0.2618 \n",
      "Accuracy: 9643/10000 (96.43%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020297664403915406\n",
      "conv1.bias 0.022143762558698654\n",
      "conv2.weight 0.0006912592798471451\n",
      "conv2.bias 0.004093122202903032\n",
      "fc1.weight 0.0009507261216640473\n",
      "fc1.bias 0.005378545075654983\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020297664403915406\n",
      "conv1.bias 0.022143762558698654\n",
      "conv2.weight 0.0006912592798471451\n",
      "conv2.bias 0.004093122202903032\n",
      "fc1.weight 0.0009507261216640473\n",
      "fc1.bias 0.005378545075654983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.2267 \n",
      "Accuracy: 9671/10000 (96.71%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020183274149894715\n",
      "conv1.bias 0.023140911012887955\n",
      "conv2.weight 0.0006555858999490738\n",
      "conv2.bias 0.0035219553392380476\n",
      "fc1.weight 0.0010284511372447014\n",
      "fc1.bias 0.0062497578561306\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020183274149894715\n",
      "conv1.bias 0.023140911012887955\n",
      "conv2.weight 0.0006555858999490738\n",
      "conv2.bias 0.0035219553392380476\n",
      "fc1.weight 0.0010284511372447014\n",
      "fc1.bias 0.0062497578561306\n",
      "\n",
      "Test set: Average loss: 0.2160 \n",
      "Accuracy: 9690/10000 (96.90%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020747315883636475\n",
      "conv1.bias 0.0235244482755661\n",
      "conv2.weight 0.0005954281613230705\n",
      "conv2.bias 0.0032591535709798336\n",
      "fc1.weight 0.0010651628486812114\n",
      "fc1.bias 0.005930444598197937\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020747315883636475\n",
      "conv1.bias 0.0235244482755661\n",
      "conv2.weight 0.0005954281613230705\n",
      "conv2.bias 0.0032591535709798336\n",
      "fc1.weight 0.0010651628486812114\n",
      "fc1.bias 0.005930444598197937\n",
      "\n",
      "Test set: Average loss: 0.2337 \n",
      "Accuracy: 9677/10000 (96.77%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002157624512910843\n",
      "conv1.bias 0.02639382891356945\n",
      "conv2.weight 0.0005345718562602997\n",
      "conv2.bias 0.003158936509862542\n",
      "fc1.weight 0.0010021486319601535\n",
      "fc1.bias 0.006847218424081802\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002157624512910843\n",
      "conv1.bias 0.02639382891356945\n",
      "conv2.weight 0.0005345718562602997\n",
      "conv2.bias 0.003158936509862542\n",
      "fc1.weight 0.0010021486319601535\n",
      "fc1.bias 0.006847218424081802\n",
      "\n",
      "Test set: Average loss: 0.1898 \n",
      "Accuracy: 9694/10000 (96.94%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020548491179943084\n",
      "conv1.bias 0.0275825597345829\n",
      "conv2.weight 0.0006240179389715195\n",
      "conv2.bias 0.0033702971413731575\n",
      "fc1.weight 0.0011717479676008225\n",
      "fc1.bias 0.00740993469953537\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020548491179943084\n",
      "conv1.bias 0.0275825597345829\n",
      "conv2.weight 0.0006240179389715195\n",
      "conv2.bias 0.0033702971413731575\n",
      "fc1.weight 0.0011717479676008225\n",
      "fc1.bias 0.00740993469953537\n",
      "\n",
      "Test set: Average loss: 0.2091 \n",
      "Accuracy: 9688/10000 (96.88%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001953204572200775\n",
      "conv1.bias 0.025115463882684708\n",
      "conv2.weight 0.0005662966519594193\n",
      "conv2.bias 0.0031470272224396467\n",
      "fc1.weight 0.0010644002817571164\n",
      "fc1.bias 0.006471986323595047\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001953204572200775\n",
      "conv1.bias 0.025115463882684708\n",
      "conv2.weight 0.0005662966519594193\n",
      "conv2.bias 0.0031470272224396467\n",
      "fc1.weight 0.0010644002817571164\n",
      "fc1.bias 0.006471986323595047\n",
      "\n",
      "Test set: Average loss: 0.2010 \n",
      "Accuracy: 9702/10000 (97.02%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019270263612270356\n",
      "conv1.bias 0.025975460186600685\n",
      "conv2.weight 0.0006157715246081352\n",
      "conv2.bias 0.003384602488949895\n",
      "fc1.weight 0.001267180498689413\n",
      "fc1.bias 0.00635685920715332\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019270263612270356\n",
      "conv1.bias 0.025975460186600685\n",
      "conv2.weight 0.0006157715246081352\n",
      "conv2.bias 0.003384602488949895\n",
      "fc1.weight 0.001267180498689413\n",
      "fc1.bias 0.00635685920715332\n",
      "\n",
      "Test set: Average loss: 0.2007 \n",
      "Accuracy: 9708/10000 (97.08%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018534578382968902\n",
      "conv1.bias 0.027348652482032776\n",
      "conv2.weight 0.0005545953288674354\n",
      "conv2.bias 0.003250937443226576\n",
      "fc1.weight 0.000995790958404541\n",
      "fc1.bias 0.0065948814153671265\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018534578382968902\n",
      "conv1.bias 0.027348652482032776\n",
      "conv2.weight 0.0005545953288674354\n",
      "conv2.bias 0.003250937443226576\n",
      "fc1.weight 0.000995790958404541\n",
      "fc1.bias 0.0065948814153671265\n",
      "\n",
      "Test set: Average loss: 0.2181 \n",
      "Accuracy: 9709/10000 (97.09%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002065650820732117\n",
      "conv1.bias 0.02497749589383602\n",
      "conv2.weight 0.0005865586921572685\n",
      "conv2.bias 0.0035194396041333675\n",
      "fc1.weight 0.0010784386657178402\n",
      "fc1.bias 0.006596457958221435\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002065650820732117\n",
      "conv1.bias 0.02497749589383602\n",
      "conv2.weight 0.0005865586921572685\n",
      "conv2.bias 0.0035194396041333675\n",
      "fc1.weight 0.0010784386657178402\n",
      "fc1.bias 0.006596457958221435\n",
      "\n",
      "Test set: Average loss: 0.2470 \n",
      "Accuracy: 9669/10000 (96.69%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0017627964913845063\n",
      "conv1.bias 0.02499864622950554\n",
      "conv2.weight 0.0006726983189582824\n",
      "conv2.bias 0.0034617059864103794\n",
      "fc1.weight 0.0010449117049574851\n",
      "fc1.bias 0.008272388577461242\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0017627964913845063\n",
      "conv1.bias 0.02499864622950554\n",
      "conv2.weight 0.0006726983189582824\n",
      "conv2.bias 0.0034617059864103794\n",
      "fc1.weight 0.0010449117049574851\n",
      "fc1.bias 0.008272388577461242\n",
      "\n",
      "Test set: Average loss: 0.2159 \n",
      "Accuracy: 9670/10000 (96.70%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018404573202133179\n",
      "conv1.bias 0.025318602100014687\n",
      "conv2.weight 0.0006856922805309295\n",
      "conv2.bias 0.003404300194233656\n",
      "fc1.weight 0.0010428357869386672\n",
      "fc1.bias 0.00695783793926239\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018404573202133179\n",
      "conv1.bias 0.025318602100014687\n",
      "conv2.weight 0.0006856922805309295\n",
      "conv2.bias 0.003404300194233656\n",
      "fc1.weight 0.0010428357869386672\n",
      "fc1.bias 0.00695783793926239\n",
      "\n",
      "Test set: Average loss: 0.2546 \n",
      "Accuracy: 9654/10000 (96.54%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020078158378601073\n",
      "conv1.bias 0.02417869307100773\n",
      "conv2.weight 0.0006318172812461853\n",
      "conv2.bias 0.0035343393683433533\n",
      "fc1.weight 0.0009134480729699135\n",
      "fc1.bias 0.00684153214097023\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020078158378601073\n",
      "conv1.bias 0.02417869307100773\n",
      "conv2.weight 0.0006318172812461853\n",
      "conv2.bias 0.0035343393683433533\n",
      "fc1.weight 0.0009134480729699135\n",
      "fc1.bias 0.00684153214097023\n",
      "\n",
      "Test set: Average loss: 0.2507 \n",
      "Accuracy: 9689/10000 (96.89%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019387704133987426\n",
      "conv1.bias 0.024156244471669197\n",
      "conv2.weight 0.0006628860533237457\n",
      "conv2.bias 0.0034115933813154697\n",
      "fc1.weight 0.0011397717520594598\n",
      "fc1.bias 0.00829068273305893\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019387704133987426\n",
      "conv1.bias 0.024156244471669197\n",
      "conv2.weight 0.0006628860533237457\n",
      "conv2.bias 0.0034115933813154697\n",
      "fc1.weight 0.0011397717520594598\n",
      "fc1.bias 0.00829068273305893\n",
      "\n",
      "Test set: Average loss: 0.3673 \n",
      "Accuracy: 9629/10000 (96.29%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018670900166034698\n",
      "conv1.bias 0.02225038781762123\n",
      "conv2.weight 0.0009247709810733795\n",
      "conv2.bias 0.003845231607556343\n",
      "fc1.weight 0.0010024439543485641\n",
      "fc1.bias 0.009651122242212295\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018670900166034698\n",
      "conv1.bias 0.02225038781762123\n",
      "conv2.weight 0.0009247709810733795\n",
      "conv2.bias 0.003845231607556343\n",
      "fc1.weight 0.0010024439543485641\n",
      "fc1.bias 0.009651122242212295\n",
      "\n",
      "Test set: Average loss: 0.2003 \n",
      "Accuracy: 9705/10000 (97.05%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00218775749206543\n",
      "conv1.bias 0.023323340341448784\n",
      "conv2.weight 0.000552310049533844\n",
      "conv2.bias 0.003056033980101347\n",
      "fc1.weight 0.0010400949046015739\n",
      "fc1.bias 0.008857465535402297\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00218775749206543\n",
      "conv1.bias 0.023323340341448784\n",
      "conv2.weight 0.000552310049533844\n",
      "conv2.bias 0.003056033980101347\n",
      "fc1.weight 0.0010400949046015739\n",
      "fc1.bias 0.008857465535402297\n",
      "\n",
      "Test set: Average loss: 0.2972 \n",
      "Accuracy: 9667/10000 (96.67%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0017386484146118164\n",
      "conv1.bias 0.02594432234764099\n",
      "conv2.weight 0.0007562918961048126\n",
      "conv2.bias 0.003716489067301154\n",
      "fc1.weight 0.0010113488882780076\n",
      "fc1.bias 0.009891241788864136\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0017386484146118164\n",
      "conv1.bias 0.02594432234764099\n",
      "conv2.weight 0.0007562918961048126\n",
      "conv2.bias 0.003716489067301154\n",
      "fc1.weight 0.0010113488882780076\n",
      "fc1.bias 0.009891241788864136\n",
      "\n",
      "Test set: Average loss: 0.2076 \n",
      "Accuracy: 9687/10000 (96.87%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0023535820841789246\n",
      "conv1.bias 0.023258784785866737\n",
      "conv2.weight 0.0005683082342147827\n",
      "conv2.bias 0.0034009008668363094\n",
      "fc1.weight 0.0009722362272441387\n",
      "fc1.bias 0.009400856494903565\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0023535820841789246\n",
      "conv1.bias 0.023258784785866737\n",
      "conv2.weight 0.0005683082342147827\n",
      "conv2.bias 0.0034009008668363094\n",
      "fc1.weight 0.0009722362272441387\n",
      "fc1.bias 0.009400856494903565\n",
      "\n",
      "Test set: Average loss: 0.2643 \n",
      "Accuracy: 9658/10000 (96.58%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0017212244868278503\n",
      "conv1.bias 0.023270687088370323\n",
      "conv2.weight 0.0008001218736171722\n",
      "conv2.bias 0.0036746193654835224\n",
      "fc1.weight 0.0010150359012186527\n",
      "fc1.bias 0.008086519688367844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected users: [0 1]\n",
      "conv1.weight 0.0017212244868278503\n",
      "conv1.bias 0.023270687088370323\n",
      "conv2.weight 0.0008001218736171722\n",
      "conv2.bias 0.0036746193654835224\n",
      "fc1.weight 0.0010150359012186527\n",
      "fc1.bias 0.008086519688367844\n",
      "\n",
      "Test set: Average loss: 0.2348 \n",
      "Accuracy: 9676/10000 (96.76%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0022395464777946474\n",
      "conv1.bias 0.027597490698099136\n",
      "conv2.weight 0.0005166953429579735\n",
      "conv2.bias 0.002672189846634865\n",
      "fc1.weight 0.0009874824434518814\n",
      "fc1.bias 0.008476802706718444\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0022395464777946474\n",
      "conv1.bias 0.027597490698099136\n",
      "conv2.weight 0.0005166953429579735\n",
      "conv2.bias 0.002672189846634865\n",
      "fc1.weight 0.0009874824434518814\n",
      "fc1.bias 0.008476802706718444\n",
      "\n",
      "Test set: Average loss: 0.1910 \n",
      "Accuracy: 9690/10000 (96.90%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021031731367111206\n",
      "conv1.bias 0.027072351425886154\n",
      "conv2.weight 0.0006581290811300277\n",
      "conv2.bias 0.0035458230413496494\n",
      "fc1.weight 0.0013074036687612534\n",
      "fc1.bias 0.007162613421678543\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021031731367111206\n",
      "conv1.bias 0.027072351425886154\n",
      "conv2.weight 0.0006581290811300277\n",
      "conv2.bias 0.0035458230413496494\n",
      "fc1.weight 0.0013074036687612534\n",
      "fc1.bias 0.007162613421678543\n",
      "\n",
      "Test set: Average loss: 0.1911 \n",
      "Accuracy: 9723/10000 (97.23%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002293677181005478\n",
      "conv1.bias 0.02784353494644165\n",
      "conv2.weight 0.0005340782552957535\n",
      "conv2.bias 0.003076929599046707\n",
      "fc1.weight 0.0012123984284698962\n",
      "fc1.bias 0.006847152113914489\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002293677181005478\n",
      "conv1.bias 0.02784353494644165\n",
      "conv2.weight 0.0005340782552957535\n",
      "conv2.bias 0.003076929599046707\n",
      "fc1.weight 0.0012123984284698962\n",
      "fc1.bias 0.006847152113914489\n",
      "\n",
      "Test set: Average loss: 0.2099 \n",
      "Accuracy: 9686/10000 (96.86%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018314354121685029\n",
      "conv1.bias 0.026748569682240486\n",
      "conv2.weight 0.0006644701212644577\n",
      "conv2.bias 0.0033118873834609985\n",
      "fc1.weight 0.0014815814793109894\n",
      "fc1.bias 0.00746854767203331\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018314354121685029\n",
      "conv1.bias 0.026748569682240486\n",
      "conv2.weight 0.0006644701212644577\n",
      "conv2.bias 0.0033118873834609985\n",
      "fc1.weight 0.0014815814793109894\n",
      "fc1.bias 0.00746854767203331\n",
      "\n",
      "Test set: Average loss: 0.2309 \n",
      "Accuracy: 9645/10000 (96.45%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020571470260620115\n",
      "conv1.bias 0.028053410351276398\n",
      "conv2.weight 0.0005658788606524468\n",
      "conv2.bias 0.0030663744546473026\n",
      "fc1.weight 0.001137644797563553\n",
      "fc1.bias 0.006901032477617264\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020571470260620115\n",
      "conv1.bias 0.028053410351276398\n",
      "conv2.weight 0.0005658788606524468\n",
      "conv2.bias 0.0030663744546473026\n",
      "fc1.weight 0.001137644797563553\n",
      "fc1.bias 0.006901032477617264\n",
      "\n",
      "Test set: Average loss: 0.2472 \n",
      "Accuracy: 9673/10000 (96.73%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001875094622373581\n",
      "conv1.bias 0.02663736790418625\n",
      "conv2.weight 0.0006673011928796768\n",
      "conv2.bias 0.003315968671813607\n",
      "fc1.weight 0.0013815498910844325\n",
      "fc1.bias 0.006586924940347671\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001875094622373581\n",
      "conv1.bias 0.02663736790418625\n",
      "conv2.weight 0.0006673011928796768\n",
      "conv2.bias 0.003315968671813607\n",
      "fc1.weight 0.0013815498910844325\n",
      "fc1.bias 0.006586924940347671\n",
      "\n",
      "Test set: Average loss: 0.2131 \n",
      "Accuracy: 9692/10000 (96.92%)\n",
      "\n",
      "##########################################\n",
      "###### 4 -th Trial!! ###########\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.013465664386749267\n",
      "conv1.bias 0.012737352401018143\n",
      "conv2.weight 0.0004171285033226013\n",
      "conv2.bias 0.0003834125818684697\n",
      "fc1.weight 0.0003265444189310074\n",
      "fc1.bias 0.00022241366095840931\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.013465664386749267\n",
      "conv1.bias 0.012737352401018143\n",
      "conv2.weight 0.0004171285033226013\n",
      "conv2.bias 0.0003834125818684697\n",
      "fc1.weight 0.0003265444189310074\n",
      "fc1.bias 0.00022241366095840931\n",
      "\n",
      "Test set: Average loss: 2.2621 \n",
      "Accuracy: 4137/10000 (41.37%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021290627121925356\n",
      "conv1.bias 0.005388912279158831\n",
      "conv2.weight 0.00123677134513855\n",
      "conv2.bias 0.002251657657325268\n",
      "fc1.weight 0.00037886237259954214\n",
      "fc1.bias 0.0012850428931415081\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021290627121925356\n",
      "conv1.bias 0.005388912279158831\n",
      "conv2.weight 0.00123677134513855\n",
      "conv2.bias 0.002251657657325268\n",
      "fc1.weight 0.00037886237259954214\n",
      "fc1.bias 0.0012850428931415081\n",
      "\n",
      "Test set: Average loss: 1.6721 \n",
      "Accuracy: 6249/10000 (62.49%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0009982889890670775\n",
      "conv1.bias 0.009091855026781559\n",
      "conv2.weight 0.000583847425878048\n",
      "conv2.bias 0.0031823350582271814\n",
      "fc1.weight 0.0005621267016977071\n",
      "fc1.bias 0.0016503516584634782\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0009982889890670775\n",
      "conv1.bias 0.009091855026781559\n",
      "conv2.weight 0.000583847425878048\n",
      "conv2.bias 0.0031823350582271814\n",
      "fc1.weight 0.0005621267016977071\n",
      "fc1.bias 0.0016503516584634782\n",
      "\n",
      "Test set: Average loss: 0.4634 \n",
      "Accuracy: 9278/10000 (92.78%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0014700967073440552\n",
      "conv1.bias 0.010865041986107826\n",
      "conv2.weight 0.0006085105985403061\n",
      "conv2.bias 0.0038996823132038116\n",
      "fc1.weight 0.0008901209570467472\n",
      "fc1.bias 0.004442282021045685\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0014700967073440552\n",
      "conv1.bias 0.010865041986107826\n",
      "conv2.weight 0.0006085105985403061\n",
      "conv2.bias 0.0038996823132038116\n",
      "fc1.weight 0.0008901209570467472\n",
      "fc1.bias 0.004442282021045685\n",
      "\n",
      "Test set: Average loss: 0.3246 \n",
      "Accuracy: 9512/10000 (95.12%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019991058111190796\n",
      "conv1.bias 0.01349389087408781\n",
      "conv2.weight 0.0006940526515245437\n",
      "conv2.bias 0.004051185678690672\n",
      "fc1.weight 0.0010715847834944724\n",
      "fc1.bias 0.005765024200081825\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019991058111190796\n",
      "conv1.bias 0.01349389087408781\n",
      "conv2.weight 0.0006940526515245437\n",
      "conv2.bias 0.004051185678690672\n",
      "fc1.weight 0.0010715847834944724\n",
      "fc1.bias 0.005765024200081825\n",
      "\n",
      "Test set: Average loss: 0.2695 \n",
      "Accuracy: 9599/10000 (95.99%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0024948722124099733\n",
      "conv1.bias 0.020572427660226822\n",
      "conv2.weight 0.00042234744876623153\n",
      "conv2.bias 0.0033904784359037876\n",
      "fc1.weight 0.001000630483031273\n",
      "fc1.bias 0.008297666907310486\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0024948722124099733\n",
      "conv1.bias 0.020572427660226822\n",
      "conv2.weight 0.00042234744876623153\n",
      "conv2.bias 0.0033904784359037876\n",
      "fc1.weight 0.001000630483031273\n",
      "fc1.bias 0.008297666907310486\n",
      "\n",
      "Test set: Average loss: 0.2508 \n",
      "Accuracy: 9597/10000 (95.97%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0024453288316726683\n",
      "conv1.bias 0.024216435849666595\n",
      "conv2.weight 0.0005236368253827095\n",
      "conv2.bias 0.0035900534130632877\n",
      "fc1.weight 0.0010365863330662251\n",
      "fc1.bias 0.00906650274991989\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0024453288316726683\n",
      "conv1.bias 0.024216435849666595\n",
      "conv2.weight 0.0005236368253827095\n",
      "conv2.bias 0.0035900534130632877\n",
      "fc1.weight 0.0010365863330662251\n",
      "fc1.bias 0.00906650274991989\n",
      "\n",
      "Test set: Average loss: 0.2737 \n",
      "Accuracy: 9556/10000 (95.56%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002062704712152481\n",
      "conv1.bias 0.02264036238193512\n",
      "conv2.weight 0.0006601466983556747\n",
      "conv2.bias 0.003547665663063526\n",
      "fc1.weight 0.001135182287544012\n",
      "fc1.bias 0.008668313175439835\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002062704712152481\n",
      "conv1.bias 0.02264036238193512\n",
      "conv2.weight 0.0006601466983556747\n",
      "conv2.bias 0.003547665663063526\n",
      "fc1.weight 0.001135182287544012\n",
      "fc1.bias 0.008668313175439835\n",
      "\n",
      "Test set: Average loss: 0.2766 \n",
      "Accuracy: 9576/10000 (95.76%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0024154821038246156\n",
      "conv1.bias 0.02240118570625782\n",
      "conv2.weight 0.0005599494650959969\n",
      "conv2.bias 0.0033837053924798965\n",
      "fc1.weight 0.0011194374412298203\n",
      "fc1.bias 0.007281264662742615\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0024154821038246156\n",
      "conv1.bias 0.02240118570625782\n",
      "conv2.weight 0.0005599494650959969\n",
      "conv2.bias 0.0033837053924798965\n",
      "fc1.weight 0.0011194374412298203\n",
      "fc1.bias 0.007281264662742615\n",
      "\n",
      "Test set: Average loss: 0.2576 \n",
      "Accuracy: 9603/10000 (96.03%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0025874069333076477\n",
      "conv1.bias 0.02357310801744461\n",
      "conv2.weight 0.0006044019758701325\n",
      "conv2.bias 0.003539617173373699\n",
      "fc1.weight 0.0008893165737390518\n",
      "fc1.bias 0.007012701779603958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected users: [0 1]\n",
      "conv1.weight 0.0025874069333076477\n",
      "conv1.bias 0.02357310801744461\n",
      "conv2.weight 0.0006044019758701325\n",
      "conv2.bias 0.003539617173373699\n",
      "fc1.weight 0.0008893165737390518\n",
      "fc1.bias 0.007012701779603958\n",
      "\n",
      "Test set: Average loss: 0.3288 \n",
      "Accuracy: 9553/10000 (95.53%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0022605296969413755\n",
      "conv1.bias 0.02041550725698471\n",
      "conv2.weight 0.0006859736889600754\n",
      "conv2.bias 0.0036033452488482\n",
      "fc1.weight 0.0009375860914587975\n",
      "fc1.bias 0.005502787604928017\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0022605296969413755\n",
      "conv1.bias 0.02041550725698471\n",
      "conv2.weight 0.0006859736889600754\n",
      "conv2.bias 0.0036033452488482\n",
      "fc1.weight 0.0009375860914587975\n",
      "fc1.bias 0.005502787604928017\n",
      "\n",
      "Test set: Average loss: 0.2836 \n",
      "Accuracy: 9569/10000 (95.69%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0026042240858078\n",
      "conv1.bias 0.02205973118543625\n",
      "conv2.weight 0.0007648978382349015\n",
      "conv2.bias 0.0034618484787642956\n",
      "fc1.weight 0.0010101241059601307\n",
      "fc1.bias 0.0055862579494714735\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0026042240858078\n",
      "conv1.bias 0.02205973118543625\n",
      "conv2.weight 0.0007648978382349015\n",
      "conv2.bias 0.0034618484787642956\n",
      "fc1.weight 0.0010101241059601307\n",
      "fc1.bias 0.0055862579494714735\n",
      "\n",
      "Test set: Average loss: 0.2602 \n",
      "Accuracy: 9579/10000 (95.79%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0025618740916252135\n",
      "conv1.bias 0.026075081899762154\n",
      "conv2.weight 0.0005847212672233582\n",
      "conv2.bias 0.0030594118870794773\n",
      "fc1.weight 0.0010346458293497562\n",
      "fc1.bias 0.006184443086385727\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0025618740916252135\n",
      "conv1.bias 0.026075081899762154\n",
      "conv2.weight 0.0005847212672233582\n",
      "conv2.bias 0.0030594118870794773\n",
      "fc1.weight 0.0010346458293497562\n",
      "fc1.bias 0.006184443086385727\n",
      "\n",
      "Test set: Average loss: 0.2698 \n",
      "Accuracy: 9571/10000 (95.71%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002058563232421875\n",
      "conv1.bias 0.02606445923447609\n",
      "conv2.weight 0.000697847455739975\n",
      "conv2.bias 0.0033742859959602356\n",
      "fc1.weight 0.0012811150401830674\n",
      "fc1.bias 0.005275321006774902\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002058563232421875\n",
      "conv1.bias 0.02606445923447609\n",
      "conv2.weight 0.000697847455739975\n",
      "conv2.bias 0.0033742859959602356\n",
      "fc1.weight 0.0012811150401830674\n",
      "fc1.bias 0.005275321006774902\n",
      "\n",
      "Test set: Average loss: 0.3910 \n",
      "Accuracy: 9542/10000 (95.42%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021008190512657164\n",
      "conv1.bias 0.023424163460731506\n",
      "conv2.weight 0.0007398088276386261\n",
      "conv2.bias 0.0031843725591897964\n",
      "fc1.weight 0.0010458028875291348\n",
      "fc1.bias 0.005907348543405533\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021008190512657164\n",
      "conv1.bias 0.023424163460731506\n",
      "conv2.weight 0.0007398088276386261\n",
      "conv2.bias 0.0031843725591897964\n",
      "fc1.weight 0.0010458028875291348\n",
      "fc1.bias 0.005907348543405533\n",
      "\n",
      "Test set: Average loss: 0.3351 \n",
      "Accuracy: 9535/10000 (95.35%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020529231429100036\n",
      "conv1.bias 0.02330423705279827\n",
      "conv2.weight 0.0008135811239480972\n",
      "conv2.bias 0.0032533202320337296\n",
      "fc1.weight 0.0013493865728378296\n",
      "fc1.bias 0.0052284661680459974\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020529231429100036\n",
      "conv1.bias 0.02330423705279827\n",
      "conv2.weight 0.0008135811239480972\n",
      "conv2.bias 0.0032533202320337296\n",
      "fc1.weight 0.0013493865728378296\n",
      "fc1.bias 0.0052284661680459974\n",
      "\n",
      "Test set: Average loss: 0.3525 \n",
      "Accuracy: 9552/10000 (95.52%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002242719531059265\n",
      "conv1.bias 0.022615747526288033\n",
      "conv2.weight 0.0009272998571395874\n",
      "conv2.bias 0.0033409700263291597\n",
      "fc1.weight 0.0015106802806258202\n",
      "fc1.bias 0.004017333686351776\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002242719531059265\n",
      "conv1.bias 0.022615747526288033\n",
      "conv2.weight 0.0009272998571395874\n",
      "conv2.bias 0.0033409700263291597\n",
      "fc1.weight 0.0015106802806258202\n",
      "fc1.bias 0.004017333686351776\n",
      "\n",
      "Test set: Average loss: 0.3495 \n",
      "Accuracy: 9554/10000 (95.54%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002177262753248215\n",
      "conv1.bias 0.02002069726586342\n",
      "conv2.weight 0.0010884951800107956\n",
      "conv2.bias 0.0035735443234443665\n",
      "fc1.weight 0.0012247730046510696\n",
      "fc1.bias 0.004201040789484978\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002177262753248215\n",
      "conv1.bias 0.02002069726586342\n",
      "conv2.weight 0.0010884951800107956\n",
      "conv2.bias 0.0035735443234443665\n",
      "fc1.weight 0.0012247730046510696\n",
      "fc1.bias 0.004201040789484978\n",
      "\n",
      "Test set: Average loss: 0.2751 \n",
      "Accuracy: 9569/10000 (95.69%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0024122443795204164\n",
      "conv1.bias 0.02135164849460125\n",
      "conv2.weight 0.0009149426966905594\n",
      "conv2.bias 0.0030315681360661983\n",
      "fc1.weight 0.001517624780535698\n",
      "fc1.bias 0.004512111842632294\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0024122443795204164\n",
      "conv1.bias 0.02135164849460125\n",
      "conv2.weight 0.0009149426966905594\n",
      "conv2.bias 0.0030315681360661983\n",
      "fc1.weight 0.001517624780535698\n",
      "fc1.bias 0.004512111842632294\n",
      "\n",
      "Test set: Average loss: 0.3210 \n",
      "Accuracy: 9563/10000 (95.63%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002240797132253647\n",
      "conv1.bias 0.021775977686047554\n",
      "conv2.weight 0.0009256789088249207\n",
      "conv2.bias 0.003747515846043825\n",
      "fc1.weight 0.0009433543309569359\n",
      "fc1.bias 0.005426094308495522\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002240797132253647\n",
      "conv1.bias 0.021775977686047554\n",
      "conv2.weight 0.0009256789088249207\n",
      "conv2.bias 0.003747515846043825\n",
      "fc1.weight 0.0009433543309569359\n",
      "fc1.bias 0.005426094308495522\n",
      "\n",
      "Test set: Average loss: 0.3009 \n",
      "Accuracy: 9576/10000 (95.76%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002421025037765503\n",
      "conv1.bias 0.018513690680265427\n",
      "conv2.weight 0.0011307887732982635\n",
      "conv2.bias 0.003599933348596096\n",
      "fc1.weight 0.0011334174312651157\n",
      "fc1.bias 0.005790989845991135\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002421025037765503\n",
      "conv1.bias 0.018513690680265427\n",
      "conv2.weight 0.0011307887732982635\n",
      "conv2.bias 0.003599933348596096\n",
      "fc1.weight 0.0011334174312651157\n",
      "fc1.bias 0.005790989845991135\n",
      "\n",
      "Test set: Average loss: 0.2642 \n",
      "Accuracy: 9592/10000 (95.92%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0022034540772438048\n",
      "conv1.bias 0.020631778985261917\n",
      "conv2.weight 0.001068839058279991\n",
      "conv2.bias 0.0037732250057160854\n",
      "fc1.weight 0.0012660947628319263\n",
      "fc1.bias 0.006296807527542114\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0022034540772438048\n",
      "conv1.bias 0.020631778985261917\n",
      "conv2.weight 0.001068839058279991\n",
      "conv2.bias 0.0037732250057160854\n",
      "fc1.weight 0.0012660947628319263\n",
      "fc1.bias 0.006296807527542114\n",
      "\n",
      "Test set: Average loss: 0.2629 \n",
      "Accuracy: 9545/10000 (95.45%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002107437551021576\n",
      "conv1.bias 0.022220788523554802\n",
      "conv2.weight 0.000894152894616127\n",
      "conv2.bias 0.0034739808179438114\n",
      "fc1.weight 0.0011579710990190506\n",
      "fc1.bias 0.005792145431041717\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002107437551021576\n",
      "conv1.bias 0.022220788523554802\n",
      "conv2.weight 0.000894152894616127\n",
      "conv2.bias 0.0034739808179438114\n",
      "fc1.weight 0.0011579710990190506\n",
      "fc1.bias 0.005792145431041717\n",
      "\n",
      "Test set: Average loss: 0.4739 \n",
      "Accuracy: 8829/10000 (88.29%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020601657032966614\n",
      "conv1.bias 0.02532929927110672\n",
      "conv2.weight 0.0006491722166538238\n",
      "conv2.bias 0.003332344349473715\n",
      "fc1.weight 0.0008558774366974831\n",
      "fc1.bias 0.006170661374926567\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020601657032966614\n",
      "conv1.bias 0.02532929927110672\n",
      "conv2.weight 0.0006491722166538238\n",
      "conv2.bias 0.003332344349473715\n",
      "fc1.weight 0.0008558774366974831\n",
      "fc1.bias 0.006170661374926567\n",
      "\n",
      "Test set: Average loss: 0.2594 \n",
      "Accuracy: 9596/10000 (95.96%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002211674451828003\n",
      "conv1.bias 0.022963622584939003\n",
      "conv2.weight 0.0005361299589276314\n",
      "conv2.bias 0.0032593607902526855\n",
      "fc1.weight 0.0009068341925740242\n",
      "fc1.bias 0.005994344875216484\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002211674451828003\n",
      "conv1.bias 0.022963622584939003\n",
      "conv2.weight 0.0005361299589276314\n",
      "conv2.bias 0.0032593607902526855\n",
      "fc1.weight 0.0009068341925740242\n",
      "fc1.bias 0.005994344875216484\n",
      "\n",
      "Test set: Average loss: 0.2287 \n",
      "Accuracy: 9631/10000 (96.31%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0026229578256607056\n",
      "conv1.bias 0.02374027669429779\n",
      "conv2.weight 0.00048332579433918\n",
      "conv2.bias 0.0031813490204513073\n",
      "fc1.weight 0.0011916317977011203\n",
      "fc1.bias 0.006370525807142258\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0026229578256607056\n",
      "conv1.bias 0.02374027669429779\n",
      "conv2.weight 0.00048332579433918\n",
      "conv2.bias 0.0031813490204513073\n",
      "fc1.weight 0.0011916317977011203\n",
      "fc1.bias 0.006370525807142258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.2315 \n",
      "Accuracy: 9620/10000 (96.20%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0023125088214874267\n",
      "conv1.bias 0.02419351041316986\n",
      "conv2.weight 0.0005551575869321824\n",
      "conv2.bias 0.0033952745143324137\n",
      "fc1.weight 0.0008892670273780822\n",
      "fc1.bias 0.006167647242546081\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0023125088214874267\n",
      "conv1.bias 0.02419351041316986\n",
      "conv2.weight 0.0005551575869321824\n",
      "conv2.bias 0.0033952745143324137\n",
      "fc1.weight 0.0008892670273780822\n",
      "fc1.bias 0.006167647242546081\n",
      "\n",
      "Test set: Average loss: 0.2436 \n",
      "Accuracy: 9609/10000 (96.09%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002579784095287323\n",
      "conv1.bias 0.02249250002205372\n",
      "conv2.weight 0.0006011474132537842\n",
      "conv2.bias 0.0032569034956395626\n",
      "fc1.weight 0.0013310758396983147\n",
      "fc1.bias 0.005612050741910934\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002579784095287323\n",
      "conv1.bias 0.02249250002205372\n",
      "conv2.weight 0.0006011474132537842\n",
      "conv2.bias 0.0032569034956395626\n",
      "fc1.weight 0.0013310758396983147\n",
      "fc1.bias 0.005612050741910934\n",
      "\n",
      "Test set: Average loss: 0.2264 \n",
      "Accuracy: 9624/10000 (96.24%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002461530566215515\n",
      "conv1.bias 0.023872192949056625\n",
      "conv2.weight 0.0005735011026263237\n",
      "conv2.bias 0.0033614460844546556\n",
      "fc1.weight 0.0009603617712855339\n",
      "fc1.bias 0.006007150933146477\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002461530566215515\n",
      "conv1.bias 0.023872192949056625\n",
      "conv2.weight 0.0005735011026263237\n",
      "conv2.bias 0.0033614460844546556\n",
      "fc1.weight 0.0009603617712855339\n",
      "fc1.bias 0.006007150933146477\n",
      "\n",
      "Test set: Average loss: 0.2456 \n",
      "Accuracy: 9630/10000 (96.30%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002206782400608063\n",
      "conv1.bias 0.023638561367988586\n",
      "conv2.weight 0.0006556224077939987\n",
      "conv2.bias 0.0035389228723943233\n",
      "fc1.weight 0.0010429431684315204\n",
      "fc1.bias 0.005364224314689636\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002206782400608063\n",
      "conv1.bias 0.023638561367988586\n",
      "conv2.weight 0.0006556224077939987\n",
      "conv2.bias 0.0035389228723943233\n",
      "fc1.weight 0.0010429431684315204\n",
      "fc1.bias 0.005364224314689636\n",
      "\n",
      "Test set: Average loss: 0.2406 \n",
      "Accuracy: 9614/10000 (96.14%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "G = 2\n",
    "N = 4 # N should be divisible by G\n",
    "K = 4 # N should be divisible by G\n",
    "\n",
    "size_per_group = int(60000/G)\n",
    "\n",
    "X_group = np.reshape(encoding_input_array_np, (G,size_per_group,28*28))\n",
    "y_group = np.reshape(encoding_label_array_np, (G,size_per_group,args.num_classes)) \n",
    "\n",
    "N_i = int(N/G) # = 2\n",
    "K_i = int(K/G) # = 2\n",
    "T = 3\n",
    "sigma = 1\n",
    "Noise_Alloc = [0,2,4]\n",
    "m = N_i # number of selected workers (if there is no straggler, m=N_i)\n",
    "\n",
    "print(N_i,K_i,T)\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K_i+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K_i+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K_i+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "z_array = np.array([-0.81, 0.81])\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "N_trials = 5\n",
    "N_epochs = 30\n",
    "\n",
    "loss_test_arr_G2_N4_v3 = np.zeros((N_trials,N_epochs))\n",
    "acc_test_arr_G2_N4_v3  = np.zeros((N_trials,N_epochs))\n",
    "\n",
    "for trial_idx in range(N_trials):\n",
    "    \n",
    "    print('##########################################')\n",
    "    print('######',trial_idx,'-th Trial!! ###########')\n",
    "    \n",
    "    net_glob = CNNMnist2(args=args)\n",
    "    net_glob.cuda()\n",
    "    net_glob.train()\n",
    "    \n",
    "    # copy weights\n",
    "    w_glob = net_glob.state_dict()\n",
    "    \n",
    "    X_tilde = np.empty((N,Size_submatrices,28*28))\n",
    "    y_tilde = np.empty((N,Size_submatrices,10))\n",
    "    \n",
    "    for G_idx in range(G):\n",
    "        \n",
    "        _Noise_label = np.ones((size_per_group*T,10)) * 0.1\n",
    "        \n",
    "        X_tilde_tmp,a,b = BACC_Enc_Data_v3(X_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde_tmp,a,b = BACC_Enc_Data_v3(y_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "        \n",
    "        stt_pos = G_idx * N_i\n",
    "        end_pos = (G_idx+1) * N_i\n",
    "        \n",
    "        X_tilde[stt_pos:end_pos,:,:] = X_tilde_tmp\n",
    "        y_tilde[stt_pos:end_pos,:,:] = y_tilde_tmp\n",
    "        \n",
    "   \n",
    "\n",
    "    for iter in range(N_epochs): #args.epochs\n",
    "        \n",
    "        w_group_array = []\n",
    "        for G_idx in range(G):\n",
    "            w_locals, loss_locals = [], []\n",
    "            idxs_users = np.random.choice(range(N_i), m, replace=False)\n",
    "            idxs_users = np.sort(idxs_users)\n",
    "            print('selected users:',idxs_users)\n",
    "\n",
    "            coded_net = BACC_Enc_Model_withNoise_v4(net_glob.cuda(), N_i, K_i, T, 1, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "            dec_z_array = []\n",
    "            for idx in idxs_users: #for idx in range(N):\n",
    "        #         print(idx)\n",
    "                local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[G_idx*N_i+idx,:,:], label=y_tilde[G_idx*N_i+idx,:,:])\n",
    "                w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "#                 w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                w_locals.append(copy.deepcopy(w))\n",
    "                loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "            # update global weights\n",
    "            #w_glob = FedAvg(w_locals)\n",
    "            w_group = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "            \n",
    "            w_group_array.append(copy.deepcopy(w_group))\n",
    "        \n",
    "        w_glob = copy.deepcopy(w_group_array[0])\n",
    "        for k in w_glob.keys():\n",
    "            for G_idx in range(1,G):\n",
    "                w_glob[k] += w_group_array[G_idx][k]\n",
    "            w_glob[k] = torch.div(w_glob[k], len(w_group_array))\n",
    "        \n",
    "        # copy weight to net_glob\n",
    "        net_glob.load_state_dict(w_glob)\n",
    "\n",
    "        # print loss\n",
    "    #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "    #     loss_train_arr.append(loss_train)\n",
    "\n",
    "        acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "        acc_test_arr_G2_N4_v3[trial_idx][iter] = acc_test\n",
    "        loss_test_arr_G2_N4_v3[trial_idx][iter] = loss_test\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2 3\n",
      "##########################################\n",
      "###### 0 -th Trial!! ###########\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 15000 \n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.013360857963562012\n",
      "conv1.bias 0.015726113691926003\n",
      "conv2.weight 0.0004158143699169159\n",
      "conv2.bias 0.00049850984942168\n",
      "fc1.weight 0.0003233036957681179\n",
      "fc1.bias 0.0003867488820105791\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.013360857963562012\n",
      "conv1.bias 0.015726113691926003\n",
      "conv2.weight 0.0004158143699169159\n",
      "conv2.bias 0.00049850984942168\n",
      "fc1.weight 0.0003233036957681179\n",
      "fc1.bias 0.0003867488820105791\n",
      "\n",
      "Test set: Average loss: 2.3025 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00030148502439260484\n",
      "conv1.bias 0.0016018373426049948\n",
      "conv2.weight 0.00022578096017241477\n",
      "conv2.bias 0.0003886436752509326\n",
      "fc1.weight 5.1529018674045804e-05\n",
      "fc1.bias 0.00043980511836707594\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00030148502439260484\n",
      "conv1.bias 0.0016018373426049948\n",
      "conv2.weight 0.00022578096017241477\n",
      "conv2.bias 0.0003886436752509326\n",
      "fc1.weight 5.1529018674045804e-05\n",
      "fc1.bias 0.00043980511836707594\n",
      "\n",
      "Test set: Average loss: 2.3015 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0001475183665752411\n",
      "conv1.bias 0.001974961720407009\n",
      "conv2.weight 6.271300837397576e-05\n",
      "conv2.bias 0.0008936119847930968\n",
      "fc1.weight 6.0164567548781635e-05\n",
      "fc1.bias 0.00043310397304594515\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0001475183665752411\n",
      "conv1.bias 0.001974961720407009\n",
      "conv2.weight 6.271300837397576e-05\n",
      "conv2.bias 0.0008936119847930968\n",
      "fc1.weight 6.0164567548781635e-05\n",
      "fc1.bias 0.00043310397304594515\n",
      "\n",
      "Test set: Average loss: 2.2991 \n",
      "Accuracy: 1007/10000 (10.07%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0001696690544486046\n",
      "conv1.bias 0.001284029334783554\n",
      "conv2.weight 5.4995217360556126e-05\n",
      "conv2.bias 0.0009985495125874877\n",
      "fc1.weight 0.00018648477271199226\n",
      "fc1.bias 0.0007348739542067051\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0001696690544486046\n",
      "conv1.bias 0.001284029334783554\n",
      "conv2.weight 5.4995217360556126e-05\n",
      "conv2.bias 0.0009985495125874877\n",
      "fc1.weight 0.00018648477271199226\n",
      "fc1.bias 0.0007348739542067051\n",
      "\n",
      "Test set: Average loss: 2.2867 \n",
      "Accuracy: 3884/10000 (38.84%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00021761981770396233\n",
      "conv1.bias 0.0018186894012615085\n",
      "conv2.weight 3.675093874335289e-05\n",
      "conv2.bias 0.0009197060717269778\n",
      "fc1.weight 0.00019027823582291604\n",
      "fc1.bias 0.0008018313907086849\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00021761981770396233\n",
      "conv1.bias 0.0018186894012615085\n",
      "conv2.weight 3.675093874335289e-05\n",
      "conv2.bias 0.0009197060717269778\n",
      "fc1.weight 0.00019027823582291604\n",
      "fc1.bias 0.0008018313907086849\n",
      "\n",
      "Test set: Average loss: 2.2815 \n",
      "Accuracy: 4655/10000 (46.55%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00024830929934978487\n",
      "conv1.bias 0.0013207055162638426\n",
      "conv2.weight 4.425607621669769e-05\n",
      "conv2.bias 0.0008010683231987059\n",
      "fc1.weight 0.00012986247893422841\n",
      "fc1.bias 0.000344274053350091\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00024830929934978487\n",
      "conv1.bias 0.0013207055162638426\n",
      "conv2.weight 4.425607621669769e-05\n",
      "conv2.bias 0.0008010683231987059\n",
      "fc1.weight 0.00012986247893422841\n",
      "fc1.bias 0.000344274053350091\n",
      "\n",
      "Test set: Average loss: 2.3015 \n",
      "Accuracy: 1087/10000 (10.87%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00039769403636455533\n",
      "conv1.bias 0.001456927740946412\n",
      "conv2.weight 5.017978139221668e-05\n",
      "conv2.bias 0.0008717347518540919\n",
      "fc1.weight 0.00012115286663174629\n",
      "fc1.bias 0.0004844234324991703\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00039769403636455533\n",
      "conv1.bias 0.001456927740946412\n",
      "conv2.weight 5.017978139221668e-05\n",
      "conv2.bias 0.0008717347518540919\n",
      "fc1.weight 0.00012115286663174629\n",
      "fc1.bias 0.0004844234324991703\n",
      "\n",
      "Test set: Average loss: 2.2983 \n",
      "Accuracy: 4652/10000 (46.52%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00046076007187366484\n",
      "conv1.bias 0.0010806004283949733\n",
      "conv2.weight 6.811758037656546e-05\n",
      "conv2.bias 0.0007634461508132517\n",
      "fc1.weight 0.00020926306024193764\n",
      "fc1.bias 0.00029970200266689063\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00046076007187366484\n",
      "conv1.bias 0.0010806004283949733\n",
      "conv2.weight 6.811758037656546e-05\n",
      "conv2.bias 0.0007634461508132517\n",
      "fc1.weight 0.00020926306024193764\n",
      "fc1.bias 0.00029970200266689063\n",
      "\n",
      "Test set: Average loss: 2.2984 \n",
      "Accuracy: 3085/10000 (30.85%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00021496396511793136\n",
      "conv1.bias 0.00297386571764946\n",
      "conv2.weight 5.2779130637645725e-05\n",
      "conv2.bias 0.0010136176133528352\n",
      "fc1.weight 5.324339726939797e-05\n",
      "fc1.bias 0.00021470091305673123\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00021496396511793136\n",
      "conv1.bias 0.00297386571764946\n",
      "conv2.weight 5.2779130637645725e-05\n",
      "conv2.bias 0.0010136176133528352\n",
      "fc1.weight 5.324339726939797e-05\n",
      "fc1.bias 0.00021470091305673123\n",
      "\n",
      "Test set: Average loss: 2.2993 \n",
      "Accuracy: 2085/10000 (20.85%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00048488594591617585\n",
      "conv1.bias 0.0010543311946094036\n",
      "conv2.weight 8.314084261655808e-05\n",
      "conv2.bias 0.0008084862492978573\n",
      "fc1.weight 0.00022274055518209935\n",
      "fc1.bias 0.00027262573130428793\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00048488594591617585\n",
      "conv1.bias 0.0010543311946094036\n",
      "conv2.weight 8.314084261655808e-05\n",
      "conv2.bias 0.0008084862492978573\n",
      "fc1.weight 0.00022274055518209935\n",
      "fc1.bias 0.00027262573130428793\n",
      "\n",
      "Test set: Average loss: 2.3021 \n",
      "Accuracy: 1200/10000 (12.00%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00020881641656160356\n",
      "conv1.bias 0.0014789457200095057\n",
      "conv2.weight 0.0001025821641087532\n",
      "conv2.bias 0.0013939656782895327\n",
      "fc1.weight 0.0001039789873175323\n",
      "fc1.bias 0.00044461446814239027\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00020881641656160356\n",
      "conv1.bias 0.0014789457200095057\n",
      "conv2.weight 0.0001025821641087532\n",
      "conv2.bias 0.0013939656782895327\n",
      "fc1.weight 0.0001039789873175323\n",
      "fc1.bias 0.00044461446814239027\n",
      "\n",
      "Test set: Average loss: 2.2864 \n",
      "Accuracy: 4605/10000 (46.05%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0002612621523439884\n",
      "conv1.bias 0.001585255260579288\n",
      "conv2.weight 5.150313023477793e-05\n",
      "conv2.bias 0.0008779074996709824\n",
      "fc1.weight 0.00010575770866125823\n",
      "fc1.bias 0.0004897521343082189\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0002612621523439884\n",
      "conv1.bias 0.001585255260579288\n",
      "conv2.weight 5.150313023477793e-05\n",
      "conv2.bias 0.0008779074996709824\n",
      "fc1.weight 0.00010575770866125823\n",
      "fc1.bias 0.0004897521343082189\n",
      "\n",
      "Test set: Average loss: 2.3022 \n",
      "Accuracy: 1387/10000 (13.87%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004026016592979431\n",
      "conv1.bias 0.0027070236392319202\n",
      "conv2.weight 9.854335337877273e-05\n",
      "conv2.bias 0.001085782772861421\n",
      "fc1.weight 0.00018655506428331137\n",
      "fc1.bias 0.000646281847730279\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004026016592979431\n",
      "conv1.bias 0.0027070236392319202\n",
      "conv2.weight 9.854335337877273e-05\n",
      "conv2.bias 0.001085782772861421\n",
      "fc1.weight 0.00018655506428331137\n",
      "fc1.bias 0.000646281847730279\n",
      "\n",
      "Test set: Average loss: 2.3018 \n",
      "Accuracy: 2262/10000 (22.62%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00043036594986915587\n",
      "conv1.bias 0.001286786631681025\n",
      "conv2.weight 4.1340328752994534e-05\n",
      "conv2.bias 0.0010109108407050371\n",
      "fc1.weight 7.863193168304861e-05\n",
      "fc1.bias 0.00030266875401139257\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00043036594986915587\n",
      "conv1.bias 0.001286786631681025\n",
      "conv2.weight 4.1340328752994534e-05\n",
      "conv2.bias 0.0010109108407050371\n",
      "fc1.weight 7.863193168304861e-05\n",
      "fc1.bias 0.00030266875401139257\n",
      "\n",
      "Test set: Average loss: 2.2975 \n",
      "Accuracy: 1461/10000 (14.61%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00041036538779735563\n",
      "conv1.bias 0.001109145232476294\n",
      "conv2.weight 6.396046373993158e-05\n",
      "conv2.bias 0.0010957071790471673\n",
      "fc1.weight 0.00014371091965585946\n",
      "fc1.bias 0.0009325693361461162\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00041036538779735563\n",
      "conv1.bias 0.001109145232476294\n",
      "conv2.weight 6.396046373993158e-05\n",
      "conv2.bias 0.0010957071790471673\n",
      "fc1.weight 0.00014371091965585946\n",
      "fc1.bias 0.0009325693361461162\n",
      "\n",
      "Test set: Average loss: 2.2984 \n",
      "Accuracy: 3147/10000 (31.47%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0003283201158046722\n",
      "conv1.bias 0.001091659301891923\n",
      "conv2.weight 6.336478050798177e-05\n",
      "conv2.bias 0.0008060099789872766\n",
      "fc1.weight 9.47850407101214e-05\n",
      "fc1.bias 0.0009265058673918247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0003283201158046722\n",
      "conv1.bias 0.001091659301891923\n",
      "conv2.weight 6.336478050798177e-05\n",
      "conv2.bias 0.0008060099789872766\n",
      "fc1.weight 9.47850407101214e-05\n",
      "fc1.bias 0.0009265058673918247\n",
      "\n",
      "Test set: Average loss: 2.2867 \n",
      "Accuracy: 3413/10000 (34.13%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0003733266144990921\n",
      "conv1.bias 0.0008480274118483067\n",
      "conv2.weight 0.0001255156844854355\n",
      "conv2.bias 0.000790985650382936\n",
      "fc1.weight 0.00019972478039562703\n",
      "fc1.bias 0.0007092123851180077\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0003733266144990921\n",
      "conv1.bias 0.0008480274118483067\n",
      "conv2.weight 0.0001255156844854355\n",
      "conv2.bias 0.000790985650382936\n",
      "fc1.weight 0.00019972478039562703\n",
      "fc1.bias 0.0007092123851180077\n",
      "\n",
      "Test set: Average loss: 2.3001 \n",
      "Accuracy: 1776/10000 (17.76%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00017615810036659241\n",
      "conv1.bias 0.0028825614135712385\n",
      "conv2.weight 4.657822195440531e-05\n",
      "conv2.bias 0.0010293383384123445\n",
      "fc1.weight 7.281202124431729e-05\n",
      "fc1.bias 0.00031120525673031805\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00017615810036659241\n",
      "conv1.bias 0.0028825614135712385\n",
      "conv2.weight 4.657822195440531e-05\n",
      "conv2.bias 0.0010293383384123445\n",
      "fc1.weight 7.281202124431729e-05\n",
      "fc1.bias 0.00031120525673031805\n",
      "\n",
      "Test set: Average loss: 2.3017 \n",
      "Accuracy: 1154/10000 (11.54%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00046921830624341965\n",
      "conv1.bias 0.0012839555274695158\n",
      "conv2.weight 0.00011591224931180477\n",
      "conv2.bias 0.0007993720355443656\n",
      "fc1.weight 0.00020686101634055377\n",
      "fc1.bias 0.0005878631491214037\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00046921830624341965\n",
      "conv1.bias 0.0012839555274695158\n",
      "conv2.weight 0.00011591224931180477\n",
      "conv2.bias 0.0007993720355443656\n",
      "fc1.weight 0.00020686101634055377\n",
      "fc1.bias 0.0005878631491214037\n",
      "\n",
      "Test set: Average loss: 2.3025 \n",
      "Accuracy: 1485/10000 (14.85%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004307505488395691\n",
      "conv1.bias 0.0007261749124154449\n",
      "conv2.weight 0.00010272035375237465\n",
      "conv2.bias 0.0009212162112817168\n",
      "fc1.weight 0.00011444523697718978\n",
      "fc1.bias 0.00032793446443974973\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004307505488395691\n",
      "conv1.bias 0.0007261749124154449\n",
      "conv2.weight 0.00010272035375237465\n",
      "conv2.bias 0.0009212162112817168\n",
      "fc1.weight 0.00011444523697718978\n",
      "fc1.bias 0.00032793446443974973\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00022626983001828194\n",
      "conv1.bias 0.0014823104720562696\n",
      "conv2.weight 4.8353984020650387e-05\n",
      "conv2.bias 0.0013245369773358107\n",
      "fc1.weight 5.36865321919322e-05\n",
      "fc1.bias 0.00039661633782088755\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00022626983001828194\n",
      "conv1.bias 0.0014823104720562696\n",
      "conv2.weight 4.8353984020650387e-05\n",
      "conv2.bias 0.0013245369773358107\n",
      "fc1.weight 5.36865321919322e-05\n",
      "fc1.bias 0.00039661633782088755\n",
      "\n",
      "Test set: Average loss: 2.2980 \n",
      "Accuracy: 3859/10000 (38.59%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0003056318312883377\n",
      "conv1.bias 0.0012380258413031697\n",
      "conv2.weight 8.329336531460286e-05\n",
      "conv2.bias 0.001187048153951764\n",
      "fc1.weight 0.00018890243954956533\n",
      "fc1.bias 0.00047053107991814616\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0003056318312883377\n",
      "conv1.bias 0.0012380258413031697\n",
      "conv2.weight 8.329336531460286e-05\n",
      "conv2.bias 0.001187048153951764\n",
      "fc1.weight 0.00018890243954956533\n",
      "fc1.bias 0.00047053107991814616\n",
      "\n",
      "Test set: Average loss: 2.2996 \n",
      "Accuracy: 2056/10000 (20.56%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0001530453935265541\n",
      "conv1.bias 0.001796051044948399\n",
      "conv2.weight 9.94817353785038e-05\n",
      "conv2.bias 0.0010568092111498117\n",
      "fc1.weight 0.00011816533515229821\n",
      "fc1.bias 0.0007003381848335266\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0001530453935265541\n",
      "conv1.bias 0.001796051044948399\n",
      "conv2.weight 9.94817353785038e-05\n",
      "conv2.bias 0.0010568092111498117\n",
      "fc1.weight 0.00011816533515229821\n",
      "fc1.bias 0.0007003381848335266\n",
      "\n",
      "Test set: Average loss: 2.2991 \n",
      "Accuracy: 3054/10000 (30.54%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.000376582071185112\n",
      "conv1.bias 0.0015485307667404413\n",
      "conv2.weight 7.68962549045682e-05\n",
      "conv2.bias 0.0007971382001414895\n",
      "fc1.weight 0.00022631241008639336\n",
      "fc1.bias 0.0004079745151102543\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.000376582071185112\n",
      "conv1.bias 0.0015485307667404413\n",
      "conv2.weight 7.68962549045682e-05\n",
      "conv2.bias 0.0007971382001414895\n",
      "fc1.weight 0.00022631241008639336\n",
      "fc1.bias 0.0004079745151102543\n",
      "\n",
      "Test set: Average loss: 2.2994 \n",
      "Accuracy: 1794/10000 (17.94%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00028125431388616564\n",
      "conv1.bias 0.0016820381861180067\n",
      "conv2.weight 3.8148872554302216e-05\n",
      "conv2.bias 0.0011397707276046276\n",
      "fc1.weight 0.00011871836613863707\n",
      "fc1.bias 0.0005938281770795584\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00028125431388616564\n",
      "conv1.bias 0.0016820381861180067\n",
      "conv2.weight 3.8148872554302216e-05\n",
      "conv2.bias 0.0011397707276046276\n",
      "fc1.weight 0.00011871836613863707\n",
      "fc1.bias 0.0005938281770795584\n",
      "\n",
      "Test set: Average loss: 2.3014 \n",
      "Accuracy: 1270/10000 (12.70%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.000399300716817379\n",
      "conv1.bias 0.00132360786665231\n",
      "conv2.weight 0.00011027253232896328\n",
      "conv2.bias 0.0008550420170649886\n",
      "fc1.weight 0.00021088237408548594\n",
      "fc1.bias 0.0004166950471699238\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.000399300716817379\n",
      "conv1.bias 0.00132360786665231\n",
      "conv2.weight 0.00011027253232896328\n",
      "conv2.bias 0.0008550420170649886\n",
      "fc1.weight 0.00021088237408548594\n",
      "fc1.bias 0.0004166950471699238\n",
      "\n",
      "Test set: Average loss: 2.2946 \n",
      "Accuracy: 1226/10000 (12.26%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004054616391658783\n",
      "conv1.bias 0.0013696355745196342\n",
      "conv2.weight 4.31143119931221e-05\n",
      "conv2.bias 0.0009018982527777553\n",
      "fc1.weight 0.00017190573271363974\n",
      "fc1.bias 0.0010984130203723907\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004054616391658783\n",
      "conv1.bias 0.0013696355745196342\n",
      "conv2.weight 4.31143119931221e-05\n",
      "conv2.bias 0.0009018982527777553\n",
      "fc1.weight 0.00017190573271363974\n",
      "fc1.bias 0.0010984130203723907\n",
      "\n",
      "Test set: Average loss: 2.2926 \n",
      "Accuracy: 2441/10000 (24.41%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00020049463957548142\n",
      "conv1.bias 0.0024450146593153477\n",
      "conv2.weight 6.451118271797895e-05\n",
      "conv2.bias 0.0011585716856643558\n",
      "fc1.weight 9.066866477951408e-05\n",
      "fc1.bias 0.0003795532509684563\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00020049463957548142\n",
      "conv1.bias 0.0024450146593153477\n",
      "conv2.weight 6.451118271797895e-05\n",
      "conv2.bias 0.0011585716856643558\n",
      "fc1.weight 9.066866477951408e-05\n",
      "fc1.bias 0.0003795532509684563\n",
      "\n",
      "Test set: Average loss: 2.2935 \n",
      "Accuracy: 2748/10000 (27.48%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00036698400974273683\n",
      "conv1.bias 0.001358417677693069\n",
      "conv2.weight 8.114068768918514e-05\n",
      "conv2.bias 0.0008601376321166754\n",
      "fc1.weight 0.00015769817400723696\n",
      "fc1.bias 0.0006924166344106197\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00036698400974273683\n",
      "conv1.bias 0.001358417677693069\n",
      "conv2.weight 8.114068768918514e-05\n",
      "conv2.bias 0.0008601376321166754\n",
      "fc1.weight 0.00015769817400723696\n",
      "fc1.bias 0.0006924166344106197\n",
      "\n",
      "Test set: Average loss: 2.3014 \n",
      "Accuracy: 1135/10000 (11.35%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004271923005580902\n",
      "conv1.bias 0.0010786595521494746\n",
      "conv2.weight 0.00015530874021351337\n",
      "conv2.bias 0.0008290966507047415\n",
      "fc1.weight 0.0001575200818479061\n",
      "fc1.bias 0.0020092779770493506\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004271923005580902\n",
      "conv1.bias 0.0010786595521494746\n",
      "conv2.weight 0.00015530874021351337\n",
      "conv2.bias 0.0008290966507047415\n",
      "fc1.weight 0.0001575200818479061\n",
      "fc1.bias 0.0020092779770493506\n",
      "\n",
      "Test set: Average loss: 2.3025 \n",
      "Accuracy: 1010/10000 (10.10%)\n",
      "\n",
      "##########################################\n",
      "###### 1 -th Trial!! ###########\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 15000 \n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.013503371477127076\n",
      "conv1.bias 0.0184007715433836\n",
      "conv2.weight 0.0004143151268362999\n",
      "conv2.bias 0.00040011273813433945\n",
      "fc1.weight 0.00032447539269924163\n",
      "fc1.bias 0.00020022373646497727\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.013503371477127076\n",
      "conv1.bias 0.0184007715433836\n",
      "conv2.weight 0.0004143151268362999\n",
      "conv2.bias 0.00040011273813433945\n",
      "fc1.weight 0.00032447539269924163\n",
      "fc1.bias 0.00020022373646497727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.3018 \n",
      "Accuracy: 2773/10000 (27.73%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004284814745187759\n",
      "conv1.bias 0.001629732782021165\n",
      "conv2.weight 0.0003004690445959568\n",
      "conv2.bias 0.0005272729904390872\n",
      "fc1.weight 6.074075936339796e-05\n",
      "fc1.bias 0.0002808247460052371\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004284814745187759\n",
      "conv1.bias 0.001629732782021165\n",
      "conv2.weight 0.0003004690445959568\n",
      "conv2.bias 0.0005272729904390872\n",
      "fc1.weight 6.074075936339796e-05\n",
      "fc1.bias 0.0002808247460052371\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1009/10000 (10.09%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00011034272611141205\n",
      "conv1.bias 0.00246730400249362\n",
      "conv2.weight 7.445518393069506e-05\n",
      "conv2.bias 0.0009056192357093096\n",
      "fc1.weight 5.987745244055987e-05\n",
      "fc1.bias 0.0004278399981558323\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00011034272611141205\n",
      "conv1.bias 0.00246730400249362\n",
      "conv2.weight 7.445518393069506e-05\n",
      "conv2.bias 0.0009056192357093096\n",
      "fc1.weight 5.987745244055987e-05\n",
      "fc1.bias 0.0004278399981558323\n",
      "\n",
      "Test set: Average loss: 2.2755 \n",
      "Accuracy: 4800/10000 (48.00%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0005666171014308929\n",
      "conv1.bias 0.0006167381070554256\n",
      "conv2.weight 6.028643809258938e-05\n",
      "conv2.bias 0.0008485297439619899\n",
      "fc1.weight 0.0001694479607976973\n",
      "fc1.bias 0.0006285844836384058\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0005666171014308929\n",
      "conv1.bias 0.0006167381070554256\n",
      "conv2.weight 6.028643809258938e-05\n",
      "conv2.bias 0.0008485297439619899\n",
      "fc1.weight 0.0001694479607976973\n",
      "fc1.bias 0.0006285844836384058\n",
      "\n",
      "Test set: Average loss: 2.2976 \n",
      "Accuracy: 1680/10000 (16.80%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00016482004895806312\n",
      "conv1.bias 0.0035936846397817135\n",
      "conv2.weight 7.073523011058569e-05\n",
      "conv2.bias 0.0009634346351958811\n",
      "fc1.weight 0.0001396180596202612\n",
      "fc1.bias 0.00026840777136385443\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00016482004895806312\n",
      "conv1.bias 0.0035936846397817135\n",
      "conv2.weight 7.073523011058569e-05\n",
      "conv2.bias 0.0009634346351958811\n",
      "fc1.weight 0.0001396180596202612\n",
      "fc1.bias 0.00026840777136385443\n",
      "\n",
      "Test set: Average loss: 2.2681 \n",
      "Accuracy: 4833/10000 (48.33%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00037042979151010515\n",
      "conv1.bias 0.0027959593571722507\n",
      "conv2.weight 8.5067730396986e-05\n",
      "conv2.bias 0.0010245823068544269\n",
      "fc1.weight 0.00014845823170617223\n",
      "fc1.bias 0.0009399794973433018\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00037042979151010515\n",
      "conv1.bias 0.0027959593571722507\n",
      "conv2.weight 8.5067730396986e-05\n",
      "conv2.bias 0.0010245823068544269\n",
      "fc1.weight 0.00014845823170617223\n",
      "fc1.bias 0.0009399794973433018\n",
      "\n",
      "Test set: Average loss: 2.2838 \n",
      "Accuracy: 1976/10000 (19.76%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0002660885825753212\n",
      "conv1.bias 0.0017967126332223415\n",
      "conv2.weight 6.98795448988676e-05\n",
      "conv2.bias 0.0009477523271925747\n",
      "fc1.weight 0.00013391480315476655\n",
      "fc1.bias 0.0005366436205804348\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0002660885825753212\n",
      "conv1.bias 0.0017967126332223415\n",
      "conv2.weight 6.98795448988676e-05\n",
      "conv2.bias 0.0009477523271925747\n",
      "fc1.weight 0.00013391480315476655\n",
      "fc1.bias 0.0005366436205804348\n",
      "\n",
      "Test set: Average loss: 2.2544 \n",
      "Accuracy: 4938/10000 (49.38%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00032607529312372207\n",
      "conv1.bias 0.001209613517858088\n",
      "conv2.weight 5.8757094666361806e-05\n",
      "conv2.bias 0.0007493331795558333\n",
      "fc1.weight 0.00017051444156095384\n",
      "fc1.bias 0.0007080047857016325\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00032607529312372207\n",
      "conv1.bias 0.001209613517858088\n",
      "conv2.weight 5.8757094666361806e-05\n",
      "conv2.bias 0.0007493331795558333\n",
      "fc1.weight 0.00017051444156095384\n",
      "fc1.bias 0.0007080047857016325\n",
      "\n",
      "Test set: Average loss: 2.2386 \n",
      "Accuracy: 5500/10000 (55.00%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0005570501834154129\n",
      "conv1.bias 0.0013470612466335297\n",
      "conv2.weight 0.00012101162225008011\n",
      "conv2.bias 0.0007913345471024513\n",
      "fc1.weight 0.0001260549179278314\n",
      "fc1.bias 0.001505280937999487\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0005570501834154129\n",
      "conv1.bias 0.0013470612466335297\n",
      "conv2.weight 0.00012101162225008011\n",
      "conv2.bias 0.0007913345471024513\n",
      "fc1.weight 0.0001260549179278314\n",
      "fc1.bias 0.001505280937999487\n",
      "\n",
      "Test set: Average loss: 2.2296 \n",
      "Accuracy: 4043/10000 (40.43%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00041520867496728896\n",
      "conv1.bias 0.0010261785937473178\n",
      "conv2.weight 0.00013638592325150966\n",
      "conv2.bias 0.0007993000908754766\n",
      "fc1.weight 0.0001001868979074061\n",
      "fc1.bias 0.0014494712464511394\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00041520867496728896\n",
      "conv1.bias 0.0010261785937473178\n",
      "conv2.weight 0.00013638592325150966\n",
      "conv2.bias 0.0007993000908754766\n",
      "fc1.weight 0.0001001868979074061\n",
      "fc1.bias 0.0014494712464511394\n",
      "\n",
      "Test set: Average loss: 2.2391 \n",
      "Accuracy: 4537/10000 (45.37%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0005095428973436356\n",
      "conv1.bias 0.0010606329888105392\n",
      "conv2.weight 5.3587472066283227e-05\n",
      "conv2.bias 0.000768085359595716\n",
      "fc1.weight 0.0003395743202418089\n",
      "fc1.bias 0.0008911122567951679\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0005095428973436356\n",
      "conv1.bias 0.0010606329888105392\n",
      "conv2.weight 5.3587472066283227e-05\n",
      "conv2.bias 0.000768085359595716\n",
      "fc1.weight 0.0003395743202418089\n",
      "fc1.bias 0.0008911122567951679\n",
      "\n",
      "Test set: Average loss: 2.2181 \n",
      "Accuracy: 5392/10000 (53.92%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004950971528887749\n",
      "conv1.bias 0.0012044282630085945\n",
      "conv2.weight 5.7869870215654376e-05\n",
      "conv2.bias 0.0007443695794790983\n",
      "fc1.weight 0.0002192438580095768\n",
      "fc1.bias 0.0010349024087190628\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004950971528887749\n",
      "conv1.bias 0.0012044282630085945\n",
      "conv2.weight 5.7869870215654376e-05\n",
      "conv2.bias 0.0007443695794790983\n",
      "fc1.weight 0.0002192438580095768\n",
      "fc1.bias 0.0010349024087190628\n",
      "\n",
      "Test set: Average loss: 2.1928 \n",
      "Accuracy: 6020/10000 (60.20%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004367412254214287\n",
      "conv1.bias 0.0012094780104234815\n",
      "conv2.weight 0.00012601204216480255\n",
      "conv2.bias 0.0007946605910547078\n",
      "fc1.weight 9.592751157470048e-05\n",
      "fc1.bias 0.002437062934041023\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004367412254214287\n",
      "conv1.bias 0.0012094780104234815\n",
      "conv2.weight 0.00012601204216480255\n",
      "conv2.bias 0.0007946605910547078\n",
      "fc1.weight 9.592751157470048e-05\n",
      "fc1.bias 0.002437062934041023\n",
      "\n",
      "Test set: Average loss: 2.1726 \n",
      "Accuracy: 5567/10000 (55.67%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0005381765216588974\n",
      "conv1.bias 0.0013561362866312265\n",
      "conv2.weight 0.00012621015310287475\n",
      "conv2.bias 0.0007105765398591757\n",
      "fc1.weight 0.00024186423979699612\n",
      "fc1.bias 0.0013427607715129851\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0005381765216588974\n",
      "conv1.bias 0.0013561362866312265\n",
      "conv2.weight 0.00012621015310287475\n",
      "conv2.bias 0.0007105765398591757\n",
      "fc1.weight 0.00024186423979699612\n",
      "fc1.bias 0.0013427607715129851\n",
      "\n",
      "Test set: Average loss: 2.1866 \n",
      "Accuracy: 6589/10000 (65.89%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00039577580988407133\n",
      "conv1.bias 0.0014121822314336896\n",
      "conv2.weight 7.779870182275772e-05\n",
      "conv2.bias 0.0007223585853353143\n",
      "fc1.weight 0.00012659461935982108\n",
      "fc1.bias 0.0009367862716317177\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00039577580988407133\n",
      "conv1.bias 0.0014121822314336896\n",
      "conv2.weight 7.779870182275772e-05\n",
      "conv2.bias 0.0007223585853353143\n",
      "fc1.weight 0.00012659461935982108\n",
      "fc1.bias 0.0009367862716317177\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0003469180688261986\n",
      "conv1.bias 0.0013231579214334488\n",
      "conv2.weight 0.00021205434575676918\n",
      "conv2.bias 0.0007719772402197123\n",
      "fc1.weight 0.00010165788698941469\n",
      "fc1.bias 0.00024510929360985754\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0003469180688261986\n",
      "conv1.bias 0.0013231579214334488\n",
      "conv2.weight 0.00021205434575676918\n",
      "conv2.bias 0.0007719772402197123\n",
      "fc1.weight 0.00010165788698941469\n",
      "fc1.bias 0.00024510929360985754\n",
      "\n",
      "Test set: Average loss: 2.2900 \n",
      "Accuracy: 3173/10000 (31.73%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00041426196694374087\n",
      "conv1.bias 0.001487781759351492\n",
      "conv2.weight 0.00013910039328038693\n",
      "conv2.bias 0.0012089009396731853\n",
      "fc1.weight 7.109149591997266e-05\n",
      "fc1.bias 0.00020103415008634328\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00041426196694374087\n",
      "conv1.bias 0.001487781759351492\n",
      "conv2.weight 0.00013910039328038693\n",
      "conv2.bias 0.0012089009396731853\n",
      "fc1.weight 7.109149591997266e-05\n",
      "fc1.bias 0.00020103415008634328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.2181 \n",
      "Accuracy: 6094/10000 (60.94%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00032815489917993547\n",
      "conv1.bias 0.0018729703733697534\n",
      "conv2.weight 3.39680421166122e-05\n",
      "conv2.bias 0.0006966111250221729\n",
      "fc1.weight 0.00010159848025068641\n",
      "fc1.bias 0.0004100938327610493\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00032815489917993547\n",
      "conv1.bias 0.0018729703733697534\n",
      "conv2.weight 3.39680421166122e-05\n",
      "conv2.bias 0.0006966111250221729\n",
      "fc1.weight 0.00010159848025068641\n",
      "fc1.bias 0.0004100938327610493\n",
      "\n",
      "Test set: Average loss: 2.1457 \n",
      "Accuracy: 7367/10000 (73.67%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00042805876582860944\n",
      "conv1.bias 0.001478702761232853\n",
      "conv2.weight 8.76187440007925e-05\n",
      "conv2.bias 0.0007096471963450313\n",
      "fc1.weight 0.00027662580832839013\n",
      "fc1.bias 0.0004739678930491209\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00042805876582860944\n",
      "conv1.bias 0.001478702761232853\n",
      "conv2.weight 8.76187440007925e-05\n",
      "conv2.bias 0.0007096471963450313\n",
      "fc1.weight 0.00027662580832839013\n",
      "fc1.bias 0.0004739678930491209\n",
      "\n",
      "Test set: Average loss: 2.1284 \n",
      "Accuracy: 8352/10000 (83.52%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00029874317348003385\n",
      "conv1.bias 0.0016588004073128104\n",
      "conv2.weight 7.31062050908804e-05\n",
      "conv2.bias 0.0007141228998079896\n",
      "fc1.weight 0.00021218443289399146\n",
      "fc1.bias 0.0007581321522593498\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00029874317348003385\n",
      "conv1.bias 0.0016588004073128104\n",
      "conv2.weight 7.31062050908804e-05\n",
      "conv2.bias 0.0007141228998079896\n",
      "fc1.weight 0.00021218443289399146\n",
      "fc1.bias 0.0007581321522593498\n",
      "\n",
      "Test set: Average loss: 2.1271 \n",
      "Accuracy: 8368/10000 (83.68%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00020238067954778672\n",
      "conv1.bias 0.002027053851634264\n",
      "conv2.weight 8.198587223887444e-05\n",
      "conv2.bias 0.0009392423671670258\n",
      "fc1.weight 0.00013566607376560568\n",
      "fc1.bias 0.0005182397551834583\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00020238067954778672\n",
      "conv1.bias 0.002027053851634264\n",
      "conv2.weight 8.198587223887444e-05\n",
      "conv2.bias 0.0009392423671670258\n",
      "fc1.weight 0.00013566607376560568\n",
      "fc1.bias 0.0005182397551834583\n",
      "\n",
      "Test set: Average loss: 2.1786 \n",
      "Accuracy: 5549/10000 (55.49%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00031042948365211486\n",
      "conv1.bias 0.0022288234904408455\n",
      "conv2.weight 7.106490898877383e-05\n",
      "conv2.bias 0.00069884501863271\n",
      "fc1.weight 0.0002766784746199846\n",
      "fc1.bias 0.00028887055814266207\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00031042948365211486\n",
      "conv1.bias 0.0022288234904408455\n",
      "conv2.weight 7.106490898877383e-05\n",
      "conv2.bias 0.00069884501863271\n",
      "fc1.weight 0.0002766784746199846\n",
      "fc1.bias 0.00028887055814266207\n",
      "\n",
      "Test set: Average loss: 2.1369 \n",
      "Accuracy: 7510/10000 (75.10%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00038784019649028777\n",
      "conv1.bias 0.001976016443222761\n",
      "conv2.weight 6.288353353738784e-05\n",
      "conv2.bias 0.0006692006136290729\n",
      "fc1.weight 0.000252999272197485\n",
      "fc1.bias 0.0004569528624415398\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00038784019649028777\n",
      "conv1.bias 0.001976016443222761\n",
      "conv2.weight 6.288353353738784e-05\n",
      "conv2.bias 0.0006692006136290729\n",
      "fc1.weight 0.000252999272197485\n",
      "fc1.bias 0.0004569528624415398\n",
      "\n",
      "Test set: Average loss: 2.1188 \n",
      "Accuracy: 7089/10000 (70.89%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00042820993810892104\n",
      "conv1.bias 0.0012572186533361673\n",
      "conv2.weight 7.390340790152549e-05\n",
      "conv2.bias 0.0007070258725434542\n",
      "fc1.weight 0.00016751993680372835\n",
      "fc1.bias 0.0005545693915337324\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00042820993810892104\n",
      "conv1.bias 0.0012572186533361673\n",
      "conv2.weight 7.390340790152549e-05\n",
      "conv2.bias 0.0007070258725434542\n",
      "fc1.weight 0.00016751993680372835\n",
      "fc1.bias 0.0005545693915337324\n",
      "\n",
      "Test set: Average loss: 2.1368 \n",
      "Accuracy: 7571/10000 (75.71%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00044316817075014115\n",
      "conv1.bias 0.001475525088608265\n",
      "conv2.weight 0.0002150825783610344\n",
      "conv2.bias 0.0007579444209113717\n",
      "fc1.weight 0.00035457778722047806\n",
      "fc1.bias 0.0009942205622792244\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00044316817075014115\n",
      "conv1.bias 0.001475525088608265\n",
      "conv2.weight 0.0002150825783610344\n",
      "conv2.bias 0.0007579444209113717\n",
      "fc1.weight 0.00035457778722047806\n",
      "fc1.bias 0.0009942205622792244\n",
      "\n",
      "Test set: Average loss: 2.0304 \n",
      "Accuracy: 7629/10000 (76.29%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0003449926525354385\n",
      "conv1.bias 0.0036474918015301228\n",
      "conv2.weight 6.969695910811424e-05\n",
      "conv2.bias 0.0009902890305966139\n",
      "fc1.weight 0.00013113364111632108\n",
      "fc1.bias 0.0010422304272651671\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0003449926525354385\n",
      "conv1.bias 0.0036474918015301228\n",
      "conv2.weight 6.969695910811424e-05\n",
      "conv2.bias 0.0009902890305966139\n",
      "fc1.weight 0.00013113364111632108\n",
      "fc1.bias 0.0010422304272651671\n",
      "\n",
      "Test set: Average loss: 2.1390 \n",
      "Accuracy: 7497/10000 (74.97%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004071203619241714\n",
      "conv1.bias 0.0018308322178199887\n",
      "conv2.weight 0.00010492920875549317\n",
      "conv2.bias 0.0007616493385285139\n",
      "fc1.weight 0.00026776590384542943\n",
      "fc1.bias 0.0007841000333428383\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004071203619241714\n",
      "conv1.bias 0.0018308322178199887\n",
      "conv2.weight 0.00010492920875549317\n",
      "conv2.bias 0.0007616493385285139\n",
      "fc1.weight 0.00026776590384542943\n",
      "fc1.bias 0.0007841000333428383\n",
      "\n",
      "Test set: Average loss: 2.0992 \n",
      "Accuracy: 7655/10000 (76.55%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00034465886652469636\n",
      "conv1.bias 0.0025070030242204666\n",
      "conv2.weight 6.97821332141757e-05\n",
      "conv2.bias 0.0008156466064974666\n",
      "fc1.weight 9.826683672145008e-05\n",
      "fc1.bias 0.0006353063508868217\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00034465886652469636\n",
      "conv1.bias 0.0025070030242204666\n",
      "conv2.weight 6.97821332141757e-05\n",
      "conv2.bias 0.0008156466064974666\n",
      "fc1.weight 9.826683672145008e-05\n",
      "fc1.bias 0.0006353063508868217\n",
      "\n",
      "Test set: Average loss: 2.0058 \n",
      "Accuracy: 8884/10000 (88.84%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004266241192817688\n",
      "conv1.bias 0.0029703278560191393\n",
      "conv2.weight 5.106689874082804e-05\n",
      "conv2.bias 0.0006870642537251115\n",
      "fc1.weight 0.00018974174745380878\n",
      "fc1.bias 0.0008985192514955997\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004266241192817688\n",
      "conv1.bias 0.0029703278560191393\n",
      "conv2.weight 5.106689874082804e-05\n",
      "conv2.bias 0.0006870642537251115\n",
      "fc1.weight 0.00018974174745380878\n",
      "fc1.bias 0.0008985192514955997\n",
      "\n",
      "Test set: Average loss: 2.0332 \n",
      "Accuracy: 9195/10000 (91.95%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0003079490177333355\n",
      "conv1.bias 0.0035432588774710894\n",
      "conv2.weight 7.082147989422083e-05\n",
      "conv2.bias 0.0008597900159657001\n",
      "fc1.weight 0.00017758957110345364\n",
      "fc1.bias 0.000817338190972805\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0003079490177333355\n",
      "conv1.bias 0.0035432588774710894\n",
      "conv2.weight 7.082147989422083e-05\n",
      "conv2.bias 0.0008597900159657001\n",
      "fc1.weight 0.00017758957110345364\n",
      "fc1.bias 0.000817338190972805\n",
      "\n",
      "Test set: Average loss: 2.0619 \n",
      "Accuracy: 8241/10000 (82.41%)\n",
      "\n",
      "##########################################\n",
      "###### 2 -th Trial!! ###########\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 15000 \n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.012999497652053833\n",
      "conv1.bias 0.012423711828887463\n",
      "conv2.weight 0.00041817013174295426\n",
      "conv2.bias 0.00034533068537712097\n",
      "fc1.weight 0.0003250643610954285\n",
      "fc1.bias 0.00035714241676032543\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.012999497652053833\n",
      "conv1.bias 0.012423711828887463\n",
      "conv2.weight 0.00041817013174295426\n",
      "conv2.bias 0.00034533068537712097\n",
      "fc1.weight 0.0003250643610954285\n",
      "fc1.bias 0.00035714241676032543\n",
      "\n",
      "Test set: Average loss: 2.3025 \n",
      "Accuracy: 1294/10000 (12.94%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00026972524821758273\n",
      "conv1.bias 0.0019172318279743195\n",
      "conv2.weight 0.00021808551624417306\n",
      "conv2.bias 0.0005701365880668163\n",
      "fc1.weight 5.3700199350714685e-05\n",
      "fc1.bias 0.000261061848141253\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00026972524821758273\n",
      "conv1.bias 0.0019172318279743195\n",
      "conv2.weight 0.00021808551624417306\n",
      "conv2.bias 0.0005701365880668163\n",
      "fc1.weight 5.3700199350714685e-05\n",
      "fc1.bias 0.000261061848141253\n",
      "\n",
      "Test set: Average loss: 2.3024 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 9.074763394892216e-05\n",
      "conv1.bias 0.00284751714207232\n",
      "conv2.weight 7.29684578254819e-05\n",
      "conv2.bias 0.0011600533034652472\n",
      "fc1.weight 5.265551153570414e-05\n",
      "fc1.bias 0.00030571538954973223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected users: [0 1 2 3]\n",
      "conv1.weight 9.074763394892216e-05\n",
      "conv1.bias 0.00284751714207232\n",
      "conv2.weight 7.29684578254819e-05\n",
      "conv2.bias 0.0011600533034652472\n",
      "fc1.weight 5.265551153570414e-05\n",
      "fc1.bias 0.00030571538954973223\n",
      "\n",
      "Test set: Average loss: 2.2794 \n",
      "Accuracy: 4743/10000 (47.43%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0002837703749537468\n",
      "conv1.bias 0.001972801750525832\n",
      "conv2.weight 8.880853652954102e-05\n",
      "conv2.bias 0.0009823536965996027\n",
      "fc1.weight 0.00019832951948046685\n",
      "fc1.bias 0.0019401267170906066\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0002837703749537468\n",
      "conv1.bias 0.001972801750525832\n",
      "conv2.weight 8.880853652954102e-05\n",
      "conv2.bias 0.0009823536965996027\n",
      "fc1.weight 0.00019832951948046685\n",
      "fc1.bias 0.0019401267170906066\n",
      "\n",
      "Test set: Average loss: 2.3025 \n",
      "Accuracy: 1010/10000 (10.10%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0002830581180751324\n",
      "conv1.bias 0.0015623422805219889\n",
      "conv2.weight 8.922155946493149e-05\n",
      "conv2.bias 0.000802811176981777\n",
      "fc1.weight 0.00010547529673203826\n",
      "fc1.bias 0.0002675517927855253\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0002830581180751324\n",
      "conv1.bias 0.0015623422805219889\n",
      "conv2.weight 8.922155946493149e-05\n",
      "conv2.bias 0.000802811176981777\n",
      "fc1.weight 0.00010547529673203826\n",
      "fc1.bias 0.0002675517927855253\n",
      "\n",
      "Test set: Average loss: 2.3021 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00031648229807615283\n",
      "conv1.bias 0.0015256059123203158\n",
      "conv2.weight 8.675329387187958e-05\n",
      "conv2.bias 0.0013471274869516492\n",
      "fc1.weight 6.011997465975583e-05\n",
      "fc1.bias 0.00038644890300929544\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00031648229807615283\n",
      "conv1.bias 0.0015256059123203158\n",
      "conv2.weight 8.675329387187958e-05\n",
      "conv2.bias 0.0013471274869516492\n",
      "fc1.weight 6.011997465975583e-05\n",
      "fc1.bias 0.00038644890300929544\n",
      "\n",
      "Test set: Average loss: 2.2992 \n",
      "Accuracy: 1135/10000 (11.35%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004885729402303695\n",
      "conv1.bias 0.001029096427373588\n",
      "conv2.weight 5.6684711016714575e-05\n",
      "conv2.bias 0.0010200823890045285\n",
      "fc1.weight 0.00014745474327355624\n",
      "fc1.bias 0.001234294194728136\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004885729402303695\n",
      "conv1.bias 0.001029096427373588\n",
      "conv2.weight 5.6684711016714575e-05\n",
      "conv2.bias 0.0010200823890045285\n",
      "fc1.weight 0.00014745474327355624\n",
      "fc1.bias 0.001234294194728136\n",
      "\n",
      "Test set: Average loss: 2.3001 \n",
      "Accuracy: 1135/10000 (11.35%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00020477324724197388\n",
      "conv1.bias 0.002479125279933214\n",
      "conv2.weight 5.349830724298954e-05\n",
      "conv2.bias 0.0014391241129487753\n",
      "fc1.weight 8.177022682502866e-05\n",
      "fc1.bias 0.0008095525205135346\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00020477324724197388\n",
      "conv1.bias 0.002479125279933214\n",
      "conv2.weight 5.349830724298954e-05\n",
      "conv2.bias 0.0014391241129487753\n",
      "fc1.weight 8.177022682502866e-05\n",
      "fc1.bias 0.0008095525205135346\n",
      "\n",
      "Test set: Average loss: 2.3020 \n",
      "Accuracy: 1177/10000 (11.77%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00037333335727453233\n",
      "conv1.bias 0.0012049106881022453\n",
      "conv2.weight 5.341710522770882e-05\n",
      "conv2.bias 0.0009480686858296394\n",
      "fc1.weight 0.0001482761697843671\n",
      "fc1.bias 0.0004352562129497528\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00037333335727453233\n",
      "conv1.bias 0.0012049106881022453\n",
      "conv2.weight 5.341710522770882e-05\n",
      "conv2.bias 0.0009480686858296394\n",
      "fc1.weight 0.0001482761697843671\n",
      "fc1.bias 0.0004352562129497528\n",
      "\n",
      "Test set: Average loss: 2.2418 \n",
      "Accuracy: 6006/10000 (60.06%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00046479035168886187\n",
      "conv1.bias 0.0019741759169846773\n",
      "conv2.weight 7.389377802610397e-05\n",
      "conv2.bias 0.0009450605139136314\n",
      "fc1.weight 0.0001331271487288177\n",
      "fc1.bias 0.0025825595483183863\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00046479035168886187\n",
      "conv1.bias 0.0019741759169846773\n",
      "conv2.weight 7.389377802610397e-05\n",
      "conv2.bias 0.0009450605139136314\n",
      "fc1.weight 0.0001331271487288177\n",
      "fc1.bias 0.0025825595483183863\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 974/10000 (9.74%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00038835640996694567\n",
      "conv1.bias 0.0012103377375751734\n",
      "conv2.weight 0.00018116919323801995\n",
      "conv2.bias 0.0008084108121693134\n",
      "fc1.weight 0.0002269300166517496\n",
      "fc1.bias 0.00021211295388638972\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00038835640996694567\n",
      "conv1.bias 0.0012103377375751734\n",
      "conv2.weight 0.00018116919323801995\n",
      "conv2.bias 0.0008084108121693134\n",
      "fc1.weight 0.0002269300166517496\n",
      "fc1.bias 0.00021211295388638972\n",
      "\n",
      "Test set: Average loss: 2.3023 \n",
      "Accuracy: 1135/10000 (11.35%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0002236171066761017\n",
      "conv1.bias 0.0010134776821359992\n",
      "conv2.weight 5.6850924156606194e-05\n",
      "conv2.bias 0.0009032633388414979\n",
      "fc1.weight 0.00010329585056751966\n",
      "fc1.bias 0.00045866677537560464\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0002236171066761017\n",
      "conv1.bias 0.0010134776821359992\n",
      "conv2.weight 5.6850924156606194e-05\n",
      "conv2.bias 0.0009032633388414979\n",
      "fc1.weight 0.00010329585056751966\n",
      "fc1.bias 0.00045866677537560464\n",
      "\n",
      "Test set: Average loss: 2.3013 \n",
      "Accuracy: 1135/10000 (11.35%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004917604476213455\n",
      "conv1.bias 0.0003565993974916637\n",
      "conv2.weight 0.00014726923778653146\n",
      "conv2.bias 0.0012779667740687728\n",
      "fc1.weight 0.00010996650671586394\n",
      "fc1.bias 0.001756768673658371\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004917604476213455\n",
      "conv1.bias 0.0003565993974916637\n",
      "conv2.weight 0.00014726923778653146\n",
      "conv2.bias 0.0012779667740687728\n",
      "fc1.weight 0.00010996650671586394\n",
      "fc1.bias 0.001756768673658371\n",
      "\n",
      "Test set: Average loss: 2.2846 \n",
      "Accuracy: 4351/10000 (43.51%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0002506894990801811\n",
      "conv1.bias 0.001240312703885138\n",
      "conv2.weight 0.00010105258785188198\n",
      "conv2.bias 0.0011857454665005207\n",
      "fc1.weight 8.78221879247576e-05\n",
      "fc1.bias 0.0005843449849635363\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0002506894990801811\n",
      "conv1.bias 0.001240312703885138\n",
      "conv2.weight 0.00010105258785188198\n",
      "conv2.bias 0.0011857454665005207\n",
      "fc1.weight 8.78221879247576e-05\n",
      "fc1.bias 0.0005843449849635363\n",
      "\n",
      "Test set: Average loss: 2.2990 \n",
      "Accuracy: 2737/10000 (27.37%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00020094599574804306\n",
      "conv1.bias 0.0022067746613174677\n",
      "conv2.weight 8.72691348195076e-05\n",
      "conv2.bias 0.0012027944903820753\n",
      "fc1.weight 0.00013762168819084764\n",
      "fc1.bias 0.0003007768653333187\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00020094599574804306\n",
      "conv1.bias 0.0022067746613174677\n",
      "conv2.weight 8.72691348195076e-05\n",
      "conv2.bias 0.0012027944903820753\n",
      "fc1.weight 0.00013762168819084764\n",
      "fc1.bias 0.0003007768653333187\n",
      "\n",
      "Test set: Average loss: 2.2664 \n",
      "Accuracy: 4916/10000 (49.16%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0001559818536043167\n",
      "conv1.bias 0.0027030110359191895\n",
      "conv2.weight 7.671637926250697e-05\n",
      "conv2.bias 0.0012287815334275365\n",
      "fc1.weight 0.00014128285693004727\n",
      "fc1.bias 0.0009961687959730626\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0001559818536043167\n",
      "conv1.bias 0.0027030110359191895\n",
      "conv2.weight 7.671637926250697e-05\n",
      "conv2.bias 0.0012287815334275365\n",
      "fc1.weight 0.00014128285693004727\n",
      "fc1.bias 0.0009961687959730626\n",
      "\n",
      "Test set: Average loss: 2.2779 \n",
      "Accuracy: 3911/10000 (39.11%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00022772079333662986\n",
      "conv1.bias 0.0022855978459119797\n",
      "conv2.weight 4.220397211611271e-05\n",
      "conv2.bias 0.0008884557755663991\n",
      "fc1.weight 0.00013796078274026514\n",
      "fc1.bias 0.0007342686410993337\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00022772079333662986\n",
      "conv1.bias 0.0022855978459119797\n",
      "conv2.weight 4.220397211611271e-05\n",
      "conv2.bias 0.0008884557755663991\n",
      "fc1.weight 0.00013796078274026514\n",
      "fc1.bias 0.0007342686410993337\n",
      "\n",
      "Test set: Average loss: 2.2473 \n",
      "Accuracy: 5474/10000 (54.74%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00022977612912654876\n",
      "conv1.bias 0.003741465974599123\n",
      "conv2.weight 4.378959536552429e-05\n",
      "conv2.bias 0.0009315444622188807\n",
      "fc1.weight 9.488625219091772e-05\n",
      "fc1.bias 0.0006804557982832193\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00022977612912654876\n",
      "conv1.bias 0.003741465974599123\n",
      "conv2.weight 4.378959536552429e-05\n",
      "conv2.bias 0.0009315444622188807\n",
      "fc1.weight 9.488625219091772e-05\n",
      "fc1.bias 0.0006804557982832193\n",
      "\n",
      "Test set: Average loss: 2.2179 \n",
      "Accuracy: 4845/10000 (48.45%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00040684714913368224\n",
      "conv1.bias 0.0020707487128674984\n",
      "conv2.weight 4.72736731171608e-05\n",
      "conv2.bias 0.000713069224730134\n",
      "fc1.weight 0.00018668561242520809\n",
      "fc1.bias 0.0004959069192409515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00040684714913368224\n",
      "conv1.bias 0.0020707487128674984\n",
      "conv2.weight 4.72736731171608e-05\n",
      "conv2.bias 0.000713069224730134\n",
      "fc1.weight 0.00018668561242520809\n",
      "fc1.bias 0.0004959069192409515\n",
      "\n",
      "Test set: Average loss: 2.1904 \n",
      "Accuracy: 6412/10000 (64.12%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004480298608541489\n",
      "conv1.bias 0.0012656340841203928\n",
      "conv2.weight 4.6910615637898445e-05\n",
      "conv2.bias 0.0006499281735159457\n",
      "fc1.weight 0.00014593733940273523\n",
      "fc1.bias 0.0007721595000475645\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004480298608541489\n",
      "conv1.bias 0.0012656340841203928\n",
      "conv2.weight 4.6910615637898445e-05\n",
      "conv2.bias 0.0006499281735159457\n",
      "fc1.weight 0.00014593733940273523\n",
      "fc1.bias 0.0007721595000475645\n",
      "\n",
      "Test set: Average loss: 2.1818 \n",
      "Accuracy: 5879/10000 (58.79%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.000545414499938488\n",
      "conv1.bias 0.0010822537587955594\n",
      "conv2.weight 9.71519947052002e-05\n",
      "conv2.bias 0.0007561437087133527\n",
      "fc1.weight 0.00019846120849251748\n",
      "fc1.bias 0.0007336953189224005\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.000545414499938488\n",
      "conv1.bias 0.0010822537587955594\n",
      "conv2.weight 9.71519947052002e-05\n",
      "conv2.bias 0.0007561437087133527\n",
      "fc1.weight 0.00019846120849251748\n",
      "fc1.bias 0.0007336953189224005\n",
      "\n",
      "Test set: Average loss: 2.1047 \n",
      "Accuracy: 6502/10000 (65.02%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00034814391285181045\n",
      "conv1.bias 0.0012412345968186855\n",
      "conv2.weight 7.769371382892132e-05\n",
      "conv2.bias 0.00087515520863235\n",
      "fc1.weight 0.00014913841150701045\n",
      "fc1.bias 0.0012396439909934998\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00034814391285181045\n",
      "conv1.bias 0.0012412345968186855\n",
      "conv2.weight 7.769371382892132e-05\n",
      "conv2.bias 0.00087515520863235\n",
      "fc1.weight 0.00014913841150701045\n",
      "fc1.bias 0.0012396439909934998\n",
      "\n",
      "Test set: Average loss: 2.1435 \n",
      "Accuracy: 6183/10000 (61.83%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.000426887571811676\n",
      "conv1.bias 0.001747783157043159\n",
      "conv2.weight 5.4530194029212e-05\n",
      "conv2.bias 0.0007158697117120028\n",
      "fc1.weight 0.00036549628712236883\n",
      "fc1.bias 0.0009710630401968956\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.000426887571811676\n",
      "conv1.bias 0.001747783157043159\n",
      "conv2.weight 5.4530194029212e-05\n",
      "conv2.bias 0.0007158697117120028\n",
      "fc1.weight 0.00036549628712236883\n",
      "fc1.bias 0.0009710630401968956\n",
      "\n",
      "Test set: Average loss: 2.1546 \n",
      "Accuracy: 6219/10000 (62.19%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.000330575704574585\n",
      "conv1.bias 0.0021775399800390005\n",
      "conv2.weight 7.837100885808467e-05\n",
      "conv2.bias 0.0008137908298522234\n",
      "fc1.weight 0.00016577390488237143\n",
      "fc1.bias 0.0012538079172372818\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.000330575704574585\n",
      "conv1.bias 0.0021775399800390005\n",
      "conv2.weight 7.837100885808467e-05\n",
      "conv2.bias 0.0008137908298522234\n",
      "fc1.weight 0.00016577390488237143\n",
      "fc1.bias 0.0012538079172372818\n",
      "\n",
      "Test set: Average loss: 2.1925 \n",
      "Accuracy: 6043/10000 (60.43%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004094027727842331\n",
      "conv1.bias 0.0019237450323998928\n",
      "conv2.weight 4.807128570973873e-05\n",
      "conv2.bias 0.0007148070726543665\n",
      "fc1.weight 0.00015502632595598698\n",
      "fc1.bias 0.0004356883931905031\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004094027727842331\n",
      "conv1.bias 0.0019237450323998928\n",
      "conv2.weight 4.807128570973873e-05\n",
      "conv2.bias 0.0007148070726543665\n",
      "fc1.weight 0.00015502632595598698\n",
      "fc1.bias 0.0004356883931905031\n",
      "\n",
      "Test set: Average loss: 2.1817 \n",
      "Accuracy: 6577/10000 (65.77%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004816211760044098\n",
      "conv1.bias 0.0013420010218396783\n",
      "conv2.weight 0.00011173259466886521\n",
      "conv2.bias 0.0007966442499309778\n",
      "fc1.weight 0.00031987796537578104\n",
      "fc1.bias 0.000341532239690423\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004816211760044098\n",
      "conv1.bias 0.0013420010218396783\n",
      "conv2.weight 0.00011173259466886521\n",
      "conv2.bias 0.0007966442499309778\n",
      "fc1.weight 0.00031987796537578104\n",
      "fc1.bias 0.000341532239690423\n",
      "\n",
      "Test set: Average loss: 2.1863 \n",
      "Accuracy: 6201/10000 (62.01%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00034804854542016986\n",
      "conv1.bias 0.003425882663577795\n",
      "conv2.weight 6.523893214762211e-05\n",
      "conv2.bias 0.0008175664697773755\n",
      "fc1.weight 0.00012469002977013587\n",
      "fc1.bias 0.00035518882796168325\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00034804854542016986\n",
      "conv1.bias 0.003425882663577795\n",
      "conv2.weight 6.523893214762211e-05\n",
      "conv2.bias 0.0008175664697773755\n",
      "fc1.weight 0.00012469002977013587\n",
      "fc1.bias 0.00035518882796168325\n",
      "\n",
      "Test set: Average loss: 2.0961 \n",
      "Accuracy: 6550/10000 (65.50%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00041176069527864456\n",
      "conv1.bias 0.0038079668302088976\n",
      "conv2.weight 5.2287825383245945e-05\n",
      "conv2.bias 0.0006901713786646724\n",
      "fc1.weight 0.00017656509298831226\n",
      "fc1.bias 0.0008689301088452339\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00041176069527864456\n",
      "conv1.bias 0.0038079668302088976\n",
      "conv2.weight 5.2287825383245945e-05\n",
      "conv2.bias 0.0006901713786646724\n",
      "fc1.weight 0.00017656509298831226\n",
      "fc1.bias 0.0008689301088452339\n",
      "\n",
      "Test set: Average loss: 2.2033 \n",
      "Accuracy: 6141/10000 (61.41%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004330703616142273\n",
      "conv1.bias 0.0017936013173311949\n",
      "conv2.weight 0.00015252195298671723\n",
      "conv2.bias 0.0007770113879814744\n",
      "fc1.weight 0.00027618766762316227\n",
      "fc1.bias 0.000439858203753829\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004330703616142273\n",
      "conv1.bias 0.0017936013173311949\n",
      "conv2.weight 0.00015252195298671723\n",
      "conv2.bias 0.0007770113879814744\n",
      "fc1.weight 0.00027618766762316227\n",
      "fc1.bias 0.000439858203753829\n",
      "\n",
      "Test set: Average loss: 2.1536 \n",
      "Accuracy: 7091/10000 (70.91%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0003345909342169762\n",
      "conv1.bias 0.00216009677387774\n",
      "conv2.weight 5.35278907045722e-05\n",
      "conv2.bias 0.0006322344997897744\n",
      "fc1.weight 0.00017767931567505001\n",
      "fc1.bias 0.0014216991141438485\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0003345909342169762\n",
      "conv1.bias 0.00216009677387774\n",
      "conv2.weight 5.35278907045722e-05\n",
      "conv2.bias 0.0006322344997897744\n",
      "fc1.weight 0.00017767931567505001\n",
      "fc1.bias 0.0014216991141438485\n",
      "\n",
      "Test set: Average loss: 2.2966 \n",
      "Accuracy: 2718/10000 (27.18%)\n",
      "\n",
      "##########################################\n",
      "###### 3 -th Trial!! ###########\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 15000 \n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.013122451305389405\n",
      "conv1.bias 0.016529960557818413\n",
      "conv2.weight 0.0004180841147899628\n",
      "conv2.bias 0.0004152126202825457\n",
      "fc1.weight 0.0003261306788772345\n",
      "fc1.bias 0.00031485804356634617\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.013122451305389405\n",
      "conv1.bias 0.016529960557818413\n",
      "conv2.weight 0.0004180841147899628\n",
      "conv2.bias 0.0004152126202825457\n",
      "fc1.weight 0.0003261306788772345\n",
      "fc1.bias 0.00031485804356634617\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1006/10000 (10.06%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00048442866653203965\n",
      "conv1.bias 0.002308330498635769\n",
      "conv2.weight 0.00023771917447447776\n",
      "conv2.bias 0.0004475511668715626\n",
      "fc1.weight 5.019027157686651e-05\n",
      "fc1.bias 0.000257687084376812\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00048442866653203965\n",
      "conv1.bias 0.002308330498635769\n",
      "conv2.weight 0.00023771917447447776\n",
      "conv2.bias 0.0004475511668715626\n",
      "fc1.weight 5.019027157686651e-05\n",
      "fc1.bias 0.000257687084376812\n",
      "\n",
      "Test set: Average loss: 2.3025 \n",
      "Accuracy: 974/10000 (9.74%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 8.947260677814483e-05\n",
      "conv1.bias 0.0036823851987719536\n",
      "conv2.weight 6.207346916198731e-05\n",
      "conv2.bias 0.0011127074249088764\n",
      "fc1.weight 5.328261177055538e-05\n",
      "fc1.bias 0.0004545202013105154\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 8.947260677814483e-05\n",
      "conv1.bias 0.0036823851987719536\n",
      "conv2.weight 6.207346916198731e-05\n",
      "conv2.bias 0.0011127074249088764\n",
      "fc1.weight 5.328261177055538e-05\n",
      "fc1.bias 0.0004545202013105154\n",
      "\n",
      "Test set: Average loss: 2.3024 \n",
      "Accuracy: 1062/10000 (10.62%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0002949822321534157\n",
      "conv1.bias 0.001589766819961369\n",
      "conv2.weight 3.813883755356073e-05\n",
      "conv2.bias 0.0009597228490747511\n",
      "fc1.weight 0.0001258589094504714\n",
      "fc1.bias 0.000601136963814497\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0002949822321534157\n",
      "conv1.bias 0.001589766819961369\n",
      "conv2.weight 3.813883755356073e-05\n",
      "conv2.bias 0.0009597228490747511\n",
      "fc1.weight 0.0001258589094504714\n",
      "fc1.bias 0.000601136963814497\n",
      "\n",
      "Test set: Average loss: 2.2983 \n",
      "Accuracy: 1190/10000 (11.90%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00040186136960983275\n",
      "conv1.bias 0.0010782848112285137\n",
      "conv2.weight 8.636845275759697e-05\n",
      "conv2.bias 0.001217526732943952\n",
      "fc1.weight 0.00013164085103198886\n",
      "fc1.bias 0.0008925083093345165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00040186136960983275\n",
      "conv1.bias 0.0010782848112285137\n",
      "conv2.weight 8.636845275759697e-05\n",
      "conv2.bias 0.001217526732943952\n",
      "fc1.weight 0.00013164085103198886\n",
      "fc1.bias 0.0008925083093345165\n",
      "\n",
      "Test set: Average loss: 2.2887 \n",
      "Accuracy: 4099/10000 (40.99%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0002865252271294594\n",
      "conv1.bias 0.0008499526884406805\n",
      "conv2.weight 6.050701718777418e-05\n",
      "conv2.bias 0.0010518338531255722\n",
      "fc1.weight 0.00012061991728842258\n",
      "fc1.bias 0.0004345510620623827\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0002865252271294594\n",
      "conv1.bias 0.0008499526884406805\n",
      "conv2.weight 6.050701718777418e-05\n",
      "conv2.bias 0.0010518338531255722\n",
      "fc1.weight 0.00012061991728842258\n",
      "fc1.bias 0.0004345510620623827\n",
      "\n",
      "Test set: Average loss: 2.3024 \n",
      "Accuracy: 1414/10000 (14.14%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0002025272697210312\n",
      "conv1.bias 0.0008205886697396636\n",
      "conv2.weight 4.606478847563267e-05\n",
      "conv2.bias 0.0008253090782091022\n",
      "fc1.weight 0.00010860830079764127\n",
      "fc1.bias 0.0005397083703428506\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0002025272697210312\n",
      "conv1.bias 0.0008205886697396636\n",
      "conv2.weight 4.606478847563267e-05\n",
      "conv2.bias 0.0008253090782091022\n",
      "fc1.weight 0.00010860830079764127\n",
      "fc1.bias 0.0005397083703428506\n",
      "\n",
      "Test set: Average loss: 2.3019 \n",
      "Accuracy: 990/10000 (9.90%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00030092552304267884\n",
      "conv1.bias 0.000949578417930752\n",
      "conv2.weight 7.445248309522867e-05\n",
      "conv2.bias 0.0008011349127627909\n",
      "fc1.weight 0.00023225462064146996\n",
      "fc1.bias 0.00039971102960407735\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00030092552304267884\n",
      "conv1.bias 0.000949578417930752\n",
      "conv2.weight 7.445248309522867e-05\n",
      "conv2.bias 0.0008011349127627909\n",
      "fc1.weight 0.00023225462064146996\n",
      "fc1.bias 0.00039971102960407735\n",
      "\n",
      "Test set: Average loss: 2.3024 \n",
      "Accuracy: 1669/10000 (16.69%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00023452024906873704\n",
      "conv1.bias 0.0028247893787920475\n",
      "conv2.weight 7.260227110236883e-05\n",
      "conv2.bias 0.00120864761993289\n",
      "fc1.weight 8.562736911699176e-05\n",
      "fc1.bias 0.00048764129169285295\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00023452024906873704\n",
      "conv1.bias 0.0028247893787920475\n",
      "conv2.weight 7.260227110236883e-05\n",
      "conv2.bias 0.00120864761993289\n",
      "fc1.weight 8.562736911699176e-05\n",
      "fc1.bias 0.00048764129169285295\n",
      "\n",
      "Test set: Average loss: 2.2984 \n",
      "Accuracy: 2406/10000 (24.06%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00017630696296691895\n",
      "conv1.bias 0.0028197537176311016\n",
      "conv2.weight 6.734586320817471e-05\n",
      "conv2.bias 0.0014916870277374983\n",
      "fc1.weight 7.096056942828e-05\n",
      "fc1.bias 0.0003696243977174163\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00017630696296691895\n",
      "conv1.bias 0.0028197537176311016\n",
      "conv2.weight 6.734586320817471e-05\n",
      "conv2.bias 0.0014916870277374983\n",
      "fc1.weight 7.096056942828e-05\n",
      "fc1.bias 0.0003696243977174163\n",
      "\n",
      "Test set: Average loss: 2.3017 \n",
      "Accuracy: 2449/10000 (24.49%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0002613212540745735\n",
      "conv1.bias 0.0011139006819576025\n",
      "conv2.weight 2.5715806987136603e-05\n",
      "conv2.bias 0.0008108979091048241\n",
      "fc1.weight 0.00021890937350690364\n",
      "fc1.bias 0.00040709832683205605\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0002613212540745735\n",
      "conv1.bias 0.0011139006819576025\n",
      "conv2.weight 2.5715806987136603e-05\n",
      "conv2.bias 0.0008108979091048241\n",
      "fc1.weight 0.00021890937350690364\n",
      "fc1.bias 0.00040709832683205605\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00025537850335240366\n",
      "conv1.bias 0.0012048673816025257\n",
      "conv2.weight 6.087492685765028e-05\n",
      "conv2.bias 0.0008541865972802043\n",
      "fc1.weight 0.00017314461292698978\n",
      "fc1.bias 0.0003277734387665987\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00025537850335240366\n",
      "conv1.bias 0.0012048673816025257\n",
      "conv2.weight 6.087492685765028e-05\n",
      "conv2.bias 0.0008541865972802043\n",
      "fc1.weight 0.00017314461292698978\n",
      "fc1.bias 0.0003277734387665987\n",
      "\n",
      "Test set: Average loss: 2.3014 \n",
      "Accuracy: 2035/10000 (20.35%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00029240626841783526\n",
      "conv1.bias 0.0023011069279164076\n",
      "conv2.weight 8.897969499230385e-05\n",
      "conv2.bias 0.0014010246377438307\n",
      "fc1.weight 5.231153918430209e-05\n",
      "fc1.bias 0.0008031384088099003\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00029240626841783526\n",
      "conv1.bias 0.0023011069279164076\n",
      "conv2.weight 8.897969499230385e-05\n",
      "conv2.bias 0.0014010246377438307\n",
      "fc1.weight 5.231153918430209e-05\n",
      "fc1.bias 0.0008031384088099003\n",
      "\n",
      "Test set: Average loss: 2.2041 \n",
      "Accuracy: 4653/10000 (46.53%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00043802306056022643\n",
      "conv1.bias 0.0010985496919602156\n",
      "conv2.weight 5.5099111050367356e-05\n",
      "conv2.bias 0.0009557527955621481\n",
      "fc1.weight 0.00017148656770586966\n",
      "fc1.bias 0.0008679266087710857\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00043802306056022643\n",
      "conv1.bias 0.0010985496919602156\n",
      "conv2.weight 5.5099111050367356e-05\n",
      "conv2.bias 0.0009557527955621481\n",
      "fc1.weight 0.00017148656770586966\n",
      "fc1.bias 0.0008679266087710857\n",
      "\n",
      "Test set: Average loss: 2.2826 \n",
      "Accuracy: 3643/10000 (36.43%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004375925660133362\n",
      "conv1.bias 0.001109248143620789\n",
      "conv2.weight 0.00015662264078855514\n",
      "conv2.bias 0.0009460584260523319\n",
      "fc1.weight 0.00021961105521768333\n",
      "fc1.bias 0.0006322436034679413\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004375925660133362\n",
      "conv1.bias 0.001109248143620789\n",
      "conv2.weight 0.00015662264078855514\n",
      "conv2.bias 0.0009460584260523319\n",
      "fc1.weight 0.00021961105521768333\n",
      "fc1.bias 0.0006322436034679413\n",
      "\n",
      "Test set: Average loss: 2.2017 \n",
      "Accuracy: 4871/10000 (48.71%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0006097943335771561\n",
      "conv1.bias 0.0005584725295193493\n",
      "conv2.weight 6.190502550452948e-05\n",
      "conv2.bias 0.0008058967650867999\n",
      "fc1.weight 0.00017061042599380015\n",
      "fc1.bias 0.0022524485364556314\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0006097943335771561\n",
      "conv1.bias 0.0005584725295193493\n",
      "conv2.weight 6.190502550452948e-05\n",
      "conv2.bias 0.0008058967650867999\n",
      "fc1.weight 0.00017061042599380015\n",
      "fc1.bias 0.0022524485364556314\n",
      "\n",
      "Test set: Average loss: 2.1892 \n",
      "Accuracy: 5125/10000 (51.25%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00021877117455005647\n",
      "conv1.bias 0.0012425350723788142\n",
      "conv2.weight 9.554105810821056e-05\n",
      "conv2.bias 0.0008578563574701548\n",
      "fc1.weight 0.00014737034216523172\n",
      "fc1.bias 0.0018851250410079957\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00021877117455005647\n",
      "conv1.bias 0.0012425350723788142\n",
      "conv2.weight 9.554105810821056e-05\n",
      "conv2.bias 0.0008578563574701548\n",
      "fc1.weight 0.00014737034216523172\n",
      "fc1.bias 0.0018851250410079957\n",
      "\n",
      "Test set: Average loss: 2.2033 \n",
      "Accuracy: 6177/10000 (61.77%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0005530194565653801\n",
      "conv1.bias 0.001014841254800558\n",
      "conv2.weight 0.0003278069570660591\n",
      "conv2.bias 0.0008147421176545322\n",
      "fc1.weight 0.0003067198907956481\n",
      "fc1.bias 0.0012848377227783203\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0005530194565653801\n",
      "conv1.bias 0.001014841254800558\n",
      "conv2.weight 0.0003278069570660591\n",
      "conv2.bias 0.0008147421176545322\n",
      "fc1.weight 0.0003067198907956481\n",
      "fc1.bias 0.0012848377227783203\n",
      "\n",
      "Test set: Average loss: 2.0855 \n",
      "Accuracy: 6603/10000 (66.03%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0006627470999956131\n",
      "conv1.bias 0.0015538865700364113\n",
      "conv2.weight 5.021868739277124e-05\n",
      "conv2.bias 0.0007532369345426559\n",
      "fc1.weight 0.00011067488230764866\n",
      "fc1.bias 0.0011588625609874725\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0006627470999956131\n",
      "conv1.bias 0.0015538865700364113\n",
      "conv2.weight 5.021868739277124e-05\n",
      "conv2.bias 0.0007532369345426559\n",
      "fc1.weight 0.00011067488230764866\n",
      "fc1.bias 0.0011588625609874725\n",
      "\n",
      "Test set: Average loss: 2.0819 \n",
      "Accuracy: 7125/10000 (71.25%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0005045875534415245\n",
      "conv1.bias 0.0008385377004742622\n",
      "conv2.weight 4.343078937381506e-05\n",
      "conv2.bias 0.0007633586646988988\n",
      "fc1.weight 0.00017932861810550095\n",
      "fc1.bias 0.001427204068750143\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0005045875534415245\n",
      "conv1.bias 0.0008385377004742622\n",
      "conv2.weight 4.343078937381506e-05\n",
      "conv2.bias 0.0007633586646988988\n",
      "fc1.weight 0.00017932861810550095\n",
      "fc1.bias 0.001427204068750143\n",
      "\n",
      "Test set: Average loss: 2.1607 \n",
      "Accuracy: 5998/10000 (59.98%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00046668946743011475\n",
      "conv1.bias 0.0010414752177894115\n",
      "conv2.weight 3.473888384178281e-05\n",
      "conv2.bias 0.0007055531023070216\n",
      "fc1.weight 0.0002115583745762706\n",
      "fc1.bias 0.0005445910152047873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00046668946743011475\n",
      "conv1.bias 0.0010414752177894115\n",
      "conv2.weight 3.473888384178281e-05\n",
      "conv2.bias 0.0007055531023070216\n",
      "fc1.weight 0.0002115583745762706\n",
      "fc1.bias 0.0005445910152047873\n",
      "\n",
      "Test set: Average loss: 2.1324 \n",
      "Accuracy: 7122/10000 (71.22%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0005831336975097657\n",
      "conv1.bias 0.0010605223942548037\n",
      "conv2.weight 0.00012278560549020768\n",
      "conv2.bias 0.0007887311512604356\n",
      "fc1.weight 0.00019545427057892083\n",
      "fc1.bias 0.0016018573194742202\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0005831336975097657\n",
      "conv1.bias 0.0010605223942548037\n",
      "conv2.weight 0.00012278560549020768\n",
      "conv2.bias 0.0007887311512604356\n",
      "fc1.weight 0.00019545427057892083\n",
      "fc1.bias 0.0016018573194742202\n",
      "\n",
      "Test set: Average loss: 2.0884 \n",
      "Accuracy: 6728/10000 (67.28%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00043635033071041105\n",
      "conv1.bias 0.001111175399273634\n",
      "conv2.weight 0.00011325790546834469\n",
      "conv2.bias 0.0008736064191907644\n",
      "fc1.weight 0.00011075304355472327\n",
      "fc1.bias 0.0028608856722712518\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00043635033071041105\n",
      "conv1.bias 0.001111175399273634\n",
      "conv2.weight 0.00011325790546834469\n",
      "conv2.bias 0.0008736064191907644\n",
      "fc1.weight 0.00011075304355472327\n",
      "fc1.bias 0.0028608856722712518\n",
      "\n",
      "Test set: Average loss: 2.0337 \n",
      "Accuracy: 7584/10000 (75.84%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.000569823756814003\n",
      "conv1.bias 0.0015817297389730811\n",
      "conv2.weight 4.4229044578969476e-05\n",
      "conv2.bias 0.0007402188493870199\n",
      "fc1.weight 0.00012346857693046331\n",
      "fc1.bias 0.0032171737402677536\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.000569823756814003\n",
      "conv1.bias 0.0015817297389730811\n",
      "conv2.weight 4.4229044578969476e-05\n",
      "conv2.bias 0.0007402188493870199\n",
      "fc1.weight 0.00012346857693046331\n",
      "fc1.bias 0.0032171737402677536\n",
      "\n",
      "Test set: Average loss: 2.1176 \n",
      "Accuracy: 6259/10000 (62.59%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00045965872704982756\n",
      "conv1.bias 0.0011495155049487948\n",
      "conv2.weight 0.00011688188649713993\n",
      "conv2.bias 0.0007423920324072242\n",
      "fc1.weight 0.000364377535879612\n",
      "fc1.bias 0.0017776366323232651\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00045965872704982756\n",
      "conv1.bias 0.0011495155049487948\n",
      "conv2.weight 0.00011688188649713993\n",
      "conv2.bias 0.0007423920324072242\n",
      "fc1.weight 0.000364377535879612\n",
      "fc1.bias 0.0017776366323232651\n",
      "\n",
      "Test set: Average loss: 2.2045 \n",
      "Accuracy: 5489/10000 (54.89%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0003196987137198448\n",
      "conv1.bias 0.0013761225854977965\n",
      "conv2.weight 4.1638598777353767e-05\n",
      "conv2.bias 0.0005968068726360798\n",
      "fc1.weight 0.00022144985850900412\n",
      "fc1.bias 0.0007382049225270748\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0003196987137198448\n",
      "conv1.bias 0.0013761225854977965\n",
      "conv2.weight 4.1638598777353767e-05\n",
      "conv2.bias 0.0005968068726360798\n",
      "fc1.weight 0.00022144985850900412\n",
      "fc1.bias 0.0007382049225270748\n",
      "\n",
      "Test set: Average loss: 2.1648 \n",
      "Accuracy: 6457/10000 (64.57%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00045112494379281995\n",
      "conv1.bias 0.00125754508189857\n",
      "conv2.weight 9.00089181959629e-05\n",
      "conv2.bias 0.0007559718214906752\n",
      "fc1.weight 0.000395516213029623\n",
      "fc1.bias 0.0004976443946361542\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00045112494379281995\n",
      "conv1.bias 0.00125754508189857\n",
      "conv2.weight 9.00089181959629e-05\n",
      "conv2.bias 0.0007559718214906752\n",
      "fc1.weight 0.000395516213029623\n",
      "fc1.bias 0.0004976443946361542\n",
      "\n",
      "Test set: Average loss: 2.1611 \n",
      "Accuracy: 7390/10000 (73.90%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0003019580617547035\n",
      "conv1.bias 0.001789251109585166\n",
      "conv2.weight 7.000207901000977e-05\n",
      "conv2.bias 0.000757808331400156\n",
      "fc1.weight 0.00017817337065935136\n",
      "fc1.bias 0.0006307773292064667\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0003019580617547035\n",
      "conv1.bias 0.001789251109585166\n",
      "conv2.weight 7.000207901000977e-05\n",
      "conv2.bias 0.000757808331400156\n",
      "fc1.weight 0.00017817337065935136\n",
      "fc1.bias 0.0006307773292064667\n",
      "\n",
      "Test set: Average loss: 2.1534 \n",
      "Accuracy: 7418/10000 (74.18%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004046208038926125\n",
      "conv1.bias 0.001341058174148202\n",
      "conv2.weight 0.00012455420568585396\n",
      "conv2.bias 0.000789056473877281\n",
      "fc1.weight 0.00031494214199483394\n",
      "fc1.bias 0.0007350881118327379\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004046208038926125\n",
      "conv1.bias 0.001341058174148202\n",
      "conv2.weight 0.00012455420568585396\n",
      "conv2.bias 0.000789056473877281\n",
      "fc1.weight 0.00031494214199483394\n",
      "fc1.bias 0.0007350881118327379\n",
      "\n",
      "Test set: Average loss: 2.2164 \n",
      "Accuracy: 5934/10000 (59.34%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0003146767243742943\n",
      "conv1.bias 0.0018565950449556112\n",
      "conv2.weight 0.00010807797312736511\n",
      "conv2.bias 0.0010054555023089051\n",
      "fc1.weight 0.0001267126761376858\n",
      "fc1.bias 0.000727497274056077\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0003146767243742943\n",
      "conv1.bias 0.0018565950449556112\n",
      "conv2.weight 0.00010807797312736511\n",
      "conv2.bias 0.0010054555023089051\n",
      "fc1.weight 0.0001267126761376858\n",
      "fc1.bias 0.000727497274056077\n",
      "\n",
      "Test set: Average loss: 2.1198 \n",
      "Accuracy: 8008/10000 (80.08%)\n",
      "\n",
      "##########################################\n",
      "###### 4 -th Trial!! ###########\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 15000 \n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.013078957796096802\n",
      "conv1.bias 0.009199323132634163\n",
      "conv2.weight 0.00041712842881679534\n",
      "conv2.bias 0.00038718245923519135\n",
      "fc1.weight 0.0003180223749950528\n",
      "fc1.bias 0.00029939552769064903\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.013078957796096802\n",
      "conv1.bias 0.009199323132634163\n",
      "conv2.weight 0.00041712842881679534\n",
      "conv2.bias 0.00038718245923519135\n",
      "fc1.weight 0.0003180223749950528\n",
      "fc1.bias 0.00029939552769064903\n",
      "\n",
      "Test set: Average loss: 2.3023 \n",
      "Accuracy: 1780/10000 (17.80%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.000381874293088913\n",
      "conv1.bias 0.001230092952027917\n",
      "conv2.weight 0.00021917244419455528\n",
      "conv2.bias 0.0004048778209835291\n",
      "fc1.weight 3.770402981899679e-05\n",
      "fc1.bias 0.00024931319057941437\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.000381874293088913\n",
      "conv1.bias 0.001230092952027917\n",
      "conv2.weight 0.00021917244419455528\n",
      "conv2.bias 0.0004048778209835291\n",
      "fc1.weight 3.770402981899679e-05\n",
      "fc1.bias 0.00024931319057941437\n",
      "\n",
      "Test set: Average loss: 2.3024 \n",
      "Accuracy: 1010/10000 (10.10%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 9.378167800605297e-05\n",
      "conv1.bias 0.0024054041132330894\n",
      "conv2.weight 5.6718047708272935e-05\n",
      "conv2.bias 0.0009373673819936812\n",
      "fc1.weight 7.77680310420692e-05\n",
      "fc1.bias 0.0006354266311973334\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 9.378167800605297e-05\n",
      "conv1.bias 0.0024054041132330894\n",
      "conv2.weight 5.6718047708272935e-05\n",
      "conv2.bias 0.0009373673819936812\n",
      "fc1.weight 7.77680310420692e-05\n",
      "fc1.bias 0.0006354266311973334\n",
      "\n",
      "Test set: Average loss: 2.2992 \n",
      "Accuracy: 1157/10000 (11.57%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00027486581355333327\n",
      "conv1.bias 0.00192451779730618\n",
      "conv2.weight 6.305743008852004e-05\n",
      "conv2.bias 0.0009808840695768595\n",
      "fc1.weight 0.00012425958411768078\n",
      "fc1.bias 0.0009799540042877198\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00027486581355333327\n",
      "conv1.bias 0.00192451779730618\n",
      "conv2.weight 6.305743008852004e-05\n",
      "conv2.bias 0.0009808840695768595\n",
      "fc1.weight 0.00012425958411768078\n",
      "fc1.bias 0.0009799540042877198\n",
      "\n",
      "Test set: Average loss: 2.3022 \n",
      "Accuracy: 1135/10000 (11.35%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00041744589805603027\n",
      "conv1.bias 0.001674123341217637\n",
      "conv2.weight 6.650830619037151e-05\n",
      "conv2.bias 0.001031681546010077\n",
      "fc1.weight 8.854869520291686e-05\n",
      "fc1.bias 0.0005441565997898578\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00041744589805603027\n",
      "conv1.bias 0.001674123341217637\n",
      "conv2.weight 6.650830619037151e-05\n",
      "conv2.bias 0.001031681546010077\n",
      "fc1.weight 8.854869520291686e-05\n",
      "fc1.bias 0.0005441565997898578\n",
      "\n",
      "Test set: Average loss: 2.3005 \n",
      "Accuracy: 1282/10000 (12.82%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0001527292560786009\n",
      "conv1.bias 0.002250678138807416\n",
      "conv2.weight 6.621431093662977e-05\n",
      "conv2.bias 0.0013631231850013137\n",
      "fc1.weight 5.618064315058291e-05\n",
      "fc1.bias 0.0005011494271457195\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0001527292560786009\n",
      "conv1.bias 0.002250678138807416\n",
      "conv2.weight 6.621431093662977e-05\n",
      "conv2.bias 0.0013631231850013137\n",
      "fc1.weight 5.618064315058291e-05\n",
      "fc1.bias 0.0005011494271457195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.2972 \n",
      "Accuracy: 2103/10000 (21.03%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0003396070748567581\n",
      "conv1.bias 0.0011098458198830485\n",
      "conv2.weight 4.8916684463620185e-05\n",
      "conv2.bias 0.0008464760612696409\n",
      "fc1.weight 0.00013696710811927915\n",
      "fc1.bias 0.0004894556012004613\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0003396070748567581\n",
      "conv1.bias 0.0011098458198830485\n",
      "conv2.weight 4.8916684463620185e-05\n",
      "conv2.bias 0.0008464760612696409\n",
      "fc1.weight 0.00013696710811927915\n",
      "fc1.bias 0.0004894556012004613\n",
      "\n",
      "Test set: Average loss: 2.3018 \n",
      "Accuracy: 1135/10000 (11.35%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004962036013603211\n",
      "conv1.bias 0.0013333468232303858\n",
      "conv2.weight 0.00016947578638792037\n",
      "conv2.bias 0.000898800790309906\n",
      "fc1.weight 0.00016055409796535968\n",
      "fc1.bias 0.0007047831546515227\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004962036013603211\n",
      "conv1.bias 0.0013333468232303858\n",
      "conv2.weight 0.00016947578638792037\n",
      "conv2.bias 0.000898800790309906\n",
      "fc1.weight 0.00016055409796535968\n",
      "fc1.bias 0.0007047831546515227\n",
      "\n",
      "Test set: Average loss: 2.3020 \n",
      "Accuracy: 1135/10000 (11.35%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.000171104371547699\n",
      "conv1.bias 0.001829137559980154\n",
      "conv2.weight 8.28934833407402e-05\n",
      "conv2.bias 0.0014706626534461975\n",
      "fc1.weight 4.819131863769144e-05\n",
      "fc1.bias 0.00026338589377701284\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.000171104371547699\n",
      "conv1.bias 0.001829137559980154\n",
      "conv2.weight 8.28934833407402e-05\n",
      "conv2.bias 0.0014706626534461975\n",
      "fc1.weight 4.819131863769144e-05\n",
      "fc1.bias 0.00026338589377701284\n",
      "\n",
      "Test set: Average loss: 2.2868 \n",
      "Accuracy: 4661/10000 (46.61%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00019917160272598267\n",
      "conv1.bias 0.0012005340540781617\n",
      "conv2.weight 7.175127044320106e-05\n",
      "conv2.bias 0.0012957241851836443\n",
      "fc1.weight 0.0001881942735053599\n",
      "fc1.bias 0.00038801266346126794\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00019917160272598267\n",
      "conv1.bias 0.0012005340540781617\n",
      "conv2.weight 7.175127044320106e-05\n",
      "conv2.bias 0.0012957241851836443\n",
      "fc1.weight 0.0001881942735053599\n",
      "fc1.bias 0.00038801266346126794\n",
      "\n",
      "Test set: Average loss: 2.2619 \n",
      "Accuracy: 4631/10000 (46.31%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00019047347828745842\n",
      "conv1.bias 0.0023219105787575245\n",
      "conv2.weight 7.723099552094936e-05\n",
      "conv2.bias 0.00110107462387532\n",
      "fc1.weight 0.00017784403171390296\n",
      "fc1.bias 0.0006192763336002827\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00019047347828745842\n",
      "conv1.bias 0.0023219105787575245\n",
      "conv2.weight 7.723099552094936e-05\n",
      "conv2.bias 0.00110107462387532\n",
      "fc1.weight 0.00017784403171390296\n",
      "fc1.bias 0.0006192763336002827\n",
      "\n",
      "Test set: Average loss: 2.3012 \n",
      "Accuracy: 1939/10000 (19.39%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00036273788660764697\n",
      "conv1.bias 0.0012548267841339111\n",
      "conv2.weight 5.872649140655994e-05\n",
      "conv2.bias 0.000874615041539073\n",
      "fc1.weight 0.00013439919566735626\n",
      "fc1.bias 0.0001568192266859114\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00036273788660764697\n",
      "conv1.bias 0.0012548267841339111\n",
      "conv2.weight 5.872649140655994e-05\n",
      "conv2.bias 0.000874615041539073\n",
      "fc1.weight 0.00013439919566735626\n",
      "fc1.bias 0.0001568192266859114\n",
      "\n",
      "Test set: Average loss: 2.2577 \n",
      "Accuracy: 5172/10000 (51.72%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0006074167042970657\n",
      "conv1.bias 0.0013962291413918138\n",
      "conv2.weight 7.027363870292902e-05\n",
      "conv2.bias 0.0008704520296305418\n",
      "fc1.weight 0.0001233170391060412\n",
      "fc1.bias 0.00028124041855335233\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0006074167042970657\n",
      "conv1.bias 0.0013962291413918138\n",
      "conv2.weight 7.027363870292902e-05\n",
      "conv2.bias 0.0008704520296305418\n",
      "fc1.weight 0.0001233170391060412\n",
      "fc1.bias 0.00028124041855335233\n",
      "\n",
      "Test set: Average loss: 2.1956 \n",
      "Accuracy: 6542/10000 (65.42%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0003200861811637878\n",
      "conv1.bias 0.0011598055716603994\n",
      "conv2.weight 8.209125138819217e-05\n",
      "conv2.bias 0.0008624221663922071\n",
      "fc1.weight 0.00027246016543358564\n",
      "fc1.bias 0.00037598991766571996\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0003200861811637878\n",
      "conv1.bias 0.0011598055716603994\n",
      "conv2.weight 8.209125138819217e-05\n",
      "conv2.bias 0.0008624221663922071\n",
      "fc1.weight 0.00027246016543358564\n",
      "fc1.bias 0.00037598991766571996\n",
      "\n",
      "Test set: Average loss: 2.2008 \n",
      "Accuracy: 7076/10000 (70.76%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004963687807321549\n",
      "conv1.bias 0.0013255098601803184\n",
      "conv2.weight 8.285665884613991e-05\n",
      "conv2.bias 0.0007181151886470616\n",
      "fc1.weight 0.0001813377137295902\n",
      "fc1.bias 0.000770942261442542\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004963687807321549\n",
      "conv1.bias 0.0013255098601803184\n",
      "conv2.weight 8.285665884613991e-05\n",
      "conv2.bias 0.0007181151886470616\n",
      "fc1.weight 0.0001813377137295902\n",
      "fc1.bias 0.000770942261442542\n",
      "\n",
      "Test set: Average loss: 2.1332 \n",
      "Accuracy: 6557/10000 (65.57%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004959570616483688\n",
      "conv1.bias 0.0011633930262178183\n",
      "conv2.weight 0.00020246785134077073\n",
      "conv2.bias 0.0007773112738505006\n",
      "fc1.weight 0.0002613707445561886\n",
      "fc1.bias 0.0011006588116288184\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004959570616483688\n",
      "conv1.bias 0.0011633930262178183\n",
      "conv2.weight 0.00020246785134077073\n",
      "conv2.bias 0.0007773112738505006\n",
      "fc1.weight 0.0002613707445561886\n",
      "fc1.bias 0.0011006588116288184\n",
      "\n",
      "Test set: Average loss: 2.1860 \n",
      "Accuracy: 6215/10000 (62.15%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00027006177231669426\n",
      "conv1.bias 0.0030780541710555553\n",
      "conv2.weight 0.00012603070586919785\n",
      "conv2.bias 0.0011961734853684902\n",
      "fc1.weight 0.00014336538733914495\n",
      "fc1.bias 0.0007472210563719273\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00027006177231669426\n",
      "conv1.bias 0.0030780541710555553\n",
      "conv2.weight 0.00012603070586919785\n",
      "conv2.bias 0.0011961734853684902\n",
      "fc1.weight 0.00014336538733914495\n",
      "fc1.bias 0.0007472210563719273\n",
      "\n",
      "Test set: Average loss: 2.2016 \n",
      "Accuracy: 6359/10000 (63.59%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00029595982283353806\n",
      "conv1.bias 0.0020135962404310703\n",
      "conv2.weight 4.973690491169691e-05\n",
      "conv2.bias 0.000745657947845757\n",
      "fc1.weight 0.00014772546710446478\n",
      "fc1.bias 0.0005378988105803728\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00029595982283353806\n",
      "conv1.bias 0.0020135962404310703\n",
      "conv2.weight 4.973690491169691e-05\n",
      "conv2.bias 0.000745657947845757\n",
      "fc1.weight 0.00014772546710446478\n",
      "fc1.bias 0.0005378988105803728\n",
      "\n",
      "Test set: Average loss: 2.1737 \n",
      "Accuracy: 6565/10000 (65.65%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0005399152636528015\n",
      "conv1.bias 0.0013759966241195798\n",
      "conv2.weight 0.00010391771793365479\n",
      "conv2.bias 0.0007595203351229429\n",
      "fc1.weight 0.00038715177215635775\n",
      "fc1.bias 0.0019151313230395318\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0005399152636528015\n",
      "conv1.bias 0.0013759966241195798\n",
      "conv2.weight 0.00010391771793365479\n",
      "conv2.bias 0.0007595203351229429\n",
      "fc1.weight 0.00038715177215635775\n",
      "fc1.bias 0.0019151313230395318\n",
      "\n",
      "Test set: Average loss: 2.1622 \n",
      "Accuracy: 8226/10000 (82.26%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00032897502183914184\n",
      "conv1.bias 0.0012466959888115525\n",
      "conv2.weight 9.646549820899963e-05\n",
      "conv2.bias 0.0007236185483634472\n",
      "fc1.weight 0.0001677347579970956\n",
      "fc1.bias 0.0006407780107110739\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00032897502183914184\n",
      "conv1.bias 0.0012466959888115525\n",
      "conv2.weight 9.646549820899963e-05\n",
      "conv2.bias 0.0007236185483634472\n",
      "fc1.weight 0.0001677347579970956\n",
      "fc1.bias 0.0006407780107110739\n",
      "\n",
      "Test set: Average loss: 2.2014 \n",
      "Accuracy: 6637/10000 (66.37%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00045490488409996035\n",
      "conv1.bias 0.0012773312628269196\n",
      "conv2.weight 0.00016370773315429687\n",
      "conv2.bias 0.0007991609163582325\n",
      "fc1.weight 0.000192536530084908\n",
      "fc1.bias 0.0009437347762286663\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00045490488409996035\n",
      "conv1.bias 0.0012773312628269196\n",
      "conv2.weight 0.00016370773315429687\n",
      "conv2.bias 0.0007991609163582325\n",
      "fc1.weight 0.000192536530084908\n",
      "fc1.bias 0.0009437347762286663\n",
      "\n",
      "Test set: Average loss: 2.2623 \n",
      "Accuracy: 4685/10000 (46.85%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00035067036747932434\n",
      "conv1.bias 0.0014012521132826805\n",
      "conv2.weight 0.00018022028729319572\n",
      "conv2.bias 0.0009023048332892358\n",
      "fc1.weight 0.00010188356973230838\n",
      "fc1.bias 0.0006323049310594797\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00035067036747932434\n",
      "conv1.bias 0.0014012521132826805\n",
      "conv2.weight 0.00018022028729319572\n",
      "conv2.bias 0.0009023048332892358\n",
      "fc1.weight 0.00010188356973230838\n",
      "fc1.bias 0.0006323049310594797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.2319 \n",
      "Accuracy: 6488/10000 (64.88%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00032741058617830277\n",
      "conv1.bias 0.002288865391165018\n",
      "conv2.weight 5.06634172052145e-05\n",
      "conv2.bias 0.0007730834768153727\n",
      "fc1.weight 0.00012441494036465883\n",
      "fc1.bias 0.0005870156455785036\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00032741058617830277\n",
      "conv1.bias 0.002288865391165018\n",
      "conv2.weight 5.06634172052145e-05\n",
      "conv2.bias 0.0007730834768153727\n",
      "fc1.weight 0.00012441494036465883\n",
      "fc1.bias 0.0005870156455785036\n",
      "\n",
      "Test set: Average loss: 2.1904 \n",
      "Accuracy: 7865/10000 (78.65%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00038885027170181273\n",
      "conv1.bias 0.001367316348478198\n",
      "conv2.weight 6.147927604615688e-05\n",
      "conv2.bias 0.0007360736490227282\n",
      "fc1.weight 0.0002855450613424182\n",
      "fc1.bias 0.0004590250551700592\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00038885027170181273\n",
      "conv1.bias 0.001367316348478198\n",
      "conv2.weight 6.147927604615688e-05\n",
      "conv2.bias 0.0007360736490227282\n",
      "fc1.weight 0.0002855450613424182\n",
      "fc1.bias 0.0004590250551700592\n",
      "\n",
      "Test set: Average loss: 2.1790 \n",
      "Accuracy: 6928/10000 (69.28%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0002694728225469589\n",
      "conv1.bias 0.0024748537689447403\n",
      "conv2.weight 5.686243064701557e-05\n",
      "conv2.bias 0.000873731798492372\n",
      "fc1.weight 0.00014424065593630076\n",
      "fc1.bias 0.0005736658815294505\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0002694728225469589\n",
      "conv1.bias 0.0024748537689447403\n",
      "conv2.weight 5.686243064701557e-05\n",
      "conv2.bias 0.000873731798492372\n",
      "fc1.weight 0.00014424065593630076\n",
      "fc1.bias 0.0005736658815294505\n",
      "\n",
      "Test set: Average loss: 2.1353 \n",
      "Accuracy: 8195/10000 (81.95%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0003629366308450699\n",
      "conv1.bias 0.002129296073690057\n",
      "conv2.weight 8.1989960744977e-05\n",
      "conv2.bias 0.0007389927050098777\n",
      "fc1.weight 0.0004212421365082264\n",
      "fc1.bias 0.000636791717261076\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0003629366308450699\n",
      "conv1.bias 0.002129296073690057\n",
      "conv2.weight 8.1989960744977e-05\n",
      "conv2.bias 0.0007389927050098777\n",
      "fc1.weight 0.0004212421365082264\n",
      "fc1.bias 0.000636791717261076\n",
      "\n",
      "Test set: Average loss: 2.1597 \n",
      "Accuracy: 7393/10000 (73.93%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00028230447322130203\n",
      "conv1.bias 0.002605878747999668\n",
      "conv2.weight 5.844471976161003e-05\n",
      "conv2.bias 0.0008678840822540224\n",
      "fc1.weight 0.00013818119186908006\n",
      "fc1.bias 0.0008408522233366966\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00028230447322130203\n",
      "conv1.bias 0.002605878747999668\n",
      "conv2.weight 5.844471976161003e-05\n",
      "conv2.bias 0.0008678840822540224\n",
      "fc1.weight 0.00013818119186908006\n",
      "fc1.bias 0.0008408522233366966\n",
      "\n",
      "Test set: Average loss: 2.1779 \n",
      "Accuracy: 6252/10000 (62.52%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0003397900983691216\n",
      "conv1.bias 0.0018492930103093386\n",
      "conv2.weight 4.8374831676483156e-05\n",
      "conv2.bias 0.0006799963884986937\n",
      "fc1.weight 0.00019953143782913685\n",
      "fc1.bias 0.000732637057080865\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0003397900983691216\n",
      "conv1.bias 0.0018492930103093386\n",
      "conv2.weight 4.8374831676483156e-05\n",
      "conv2.bias 0.0006799963884986937\n",
      "fc1.weight 0.00019953143782913685\n",
      "fc1.bias 0.000732637057080865\n",
      "\n",
      "Test set: Average loss: 2.1352 \n",
      "Accuracy: 8316/10000 (83.16%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0003131759911775589\n",
      "conv1.bias 0.0015006602043285966\n",
      "conv2.weight 5.6293383240699766e-05\n",
      "conv2.bias 0.0007004075450822711\n",
      "fc1.weight 0.00032469837460666893\n",
      "fc1.bias 0.0009490537457168102\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0003131759911775589\n",
      "conv1.bias 0.0015006602043285966\n",
      "conv2.weight 5.6293383240699766e-05\n",
      "conv2.bias 0.0007004075450822711\n",
      "fc1.weight 0.00032469837460666893\n",
      "fc1.bias 0.0009490537457168102\n",
      "\n",
      "Test set: Average loss: 2.1045 \n",
      "Accuracy: 8361/10000 (83.61%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004165142029523849\n",
      "conv1.bias 0.0013508666306734085\n",
      "conv2.weight 8.603965863585473e-05\n",
      "conv2.bias 0.0007191476761363447\n",
      "fc1.weight 0.00015147813828662038\n",
      "fc1.bias 0.000683695264160633\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004165142029523849\n",
      "conv1.bias 0.0013508666306734085\n",
      "conv2.weight 8.603965863585473e-05\n",
      "conv2.bias 0.0007191476761363447\n",
      "fc1.weight 0.00015147813828662038\n",
      "fc1.bias 0.000683695264160633\n",
      "\n",
      "Test set: Average loss: 2.0856 \n",
      "Accuracy: 8408/10000 (84.08%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "N = 8\n",
    "K = 4\n",
    "G = 2\n",
    "\n",
    "N_i = int(N/G) # = 2\n",
    "K_i = int(K/G) # = 2\n",
    "T = 3\n",
    "sigma = 1\n",
    "Noise_Alloc = [0,2,4]\n",
    "m = N_i # number of selected workers (if there is no straggler, m=N_i)\n",
    "\n",
    "print(N_i,K_i,T)\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K_i+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K_i+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K_i+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "z_array = np.array([-0.81, -0.22, 0.22, 0.81])\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "N_trials = 5\n",
    "N_epochs = 30\n",
    "\n",
    "loss_test_arr_K4_G2_N8_v3 = np.zeros((N_trials,N_epochs))\n",
    "acc_test_arr_K4_G2_N8_v3  = np.zeros((N_trials,N_epochs))\n",
    "\n",
    "for trial_idx in range(N_trials):\n",
    "    \n",
    "    print('##########################################')\n",
    "    print('######',trial_idx,'-th Trial!! ###########')\n",
    "    \n",
    "    net_glob = CNNMnist2(args=args)\n",
    "    net_glob.cuda()\n",
    "    net_glob.train()\n",
    "    \n",
    "    # copy weights\n",
    "    w_glob = net_glob.state_dict()\n",
    "    \n",
    "    X_tilde = np.empty((N,Size_submatrices,28*28))\n",
    "    y_tilde = np.empty((N,Size_submatrices,10))\n",
    "    \n",
    "    for G_idx in range(G):\n",
    "        \n",
    "        _Noise_label = np.ones((size_per_group*T,10)) * 0.1\n",
    "        \n",
    "        X_tilde_tmp,a,b = BACC_Enc_Data_v3(X_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde_tmp,a,b = BACC_Enc_Data_v3(y_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "        \n",
    "        stt_pos = G_idx * N_i\n",
    "        end_pos = (G_idx+1) * N_i\n",
    "        \n",
    "        X_tilde[stt_pos:end_pos,:,:] = X_tilde_tmp\n",
    "        y_tilde[stt_pos:end_pos,:,:] = y_tilde_tmp\n",
    "        \n",
    "   \n",
    "\n",
    "    for iter in range(N_epochs): #args.epochs\n",
    "        \n",
    "        w_group_array = []\n",
    "        for G_idx in range(G):\n",
    "            w_locals, loss_locals = [], []\n",
    "            idxs_users = np.random.choice(range(N_i), m, replace=False)\n",
    "            idxs_users = np.sort(idxs_users)\n",
    "            print('selected users:',idxs_users)\n",
    "\n",
    "            coded_net = BACC_Enc_Model_withNoise_v4(net_glob.cuda(), N_i, K_i, T, 1, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "            dec_z_array = []\n",
    "            for idx in idxs_users: #for idx in range(N):\n",
    "        #         print(idx)\n",
    "                local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[G_idx*N_i+idx,:,:], label=y_tilde[G_idx*N_i+idx,:,:])\n",
    "                w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "#                 w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                w_locals.append(copy.deepcopy(w))\n",
    "                loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "            # update global weights\n",
    "            #w_glob = FedAvg(w_locals)\n",
    "            w_group = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "            \n",
    "            w_group_array.append(copy.deepcopy(w_group))\n",
    "        \n",
    "        w_glob = copy.deepcopy(w_group_array[0])\n",
    "        for k in w_glob.keys():\n",
    "            for G_idx in range(1,G):\n",
    "                w_glob[k] += w_group_array[G_idx][k]\n",
    "            w_glob[k] = torch.div(w_glob[k], len(w_group_array))\n",
    "        \n",
    "        # copy weight to net_glob\n",
    "        net_glob.load_state_dict(w_glob)\n",
    "\n",
    "        # print loss\n",
    "    #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "    #     loss_train_arr.append(loss_train)\n",
    "\n",
    "        acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "        acc_test_arr_K4_G2_N8_v3[trial_idx][iter] = acc_test\n",
    "        loss_test_arr_K4_G2_N8_v3[trial_idx][iter] = loss_test\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1 2\n",
      "##########################################\n",
      "###### 0 -th Trial!! ###########\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.012978872060775757\n",
      "conv1.bias 0.010206453502178192\n",
      "conv2.weight 0.00041851505637168884\n",
      "conv2.bias 0.0003615621826611459\n",
      "fc1.weight 0.0003236997639760375\n",
      "fc1.bias 0.00026975274085998534\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.012978872060775757\n",
      "conv1.bias 0.010206453502178192\n",
      "conv2.weight 0.00041851505637168884\n",
      "conv2.bias 0.0003615621826611459\n",
      "fc1.weight 0.0003236997639760375\n",
      "fc1.bias 0.00026975274085998534\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.012978872060775757\n",
      "conv1.bias 0.010206453502178192\n",
      "conv2.weight 0.00041851505637168884\n",
      "conv2.bias 0.0003615621826611459\n",
      "fc1.weight 0.0003236997639760375\n",
      "fc1.bias 0.00026975274085998534\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.012978872060775757\n",
      "conv1.bias 0.010206453502178192\n",
      "conv2.weight 0.00041851505637168884\n",
      "conv2.bias 0.0003615621826611459\n",
      "fc1.weight 0.0003236997639760375\n",
      "fc1.bias 0.00026975274085998534\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00036702606827020644\n",
      "conv1.bias 0.00158684013877064\n",
      "conv2.weight 0.00019669925794005394\n",
      "conv2.bias 0.0003867020132020116\n",
      "fc1.weight 4.620567488018423e-05\n",
      "fc1.bias 0.0003317996859550476\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00036702606827020644\n",
      "conv1.bias 0.00158684013877064\n",
      "conv2.weight 0.00019669925794005394\n",
      "conv2.bias 0.0003867020132020116\n",
      "fc1.weight 4.620567488018423e-05\n",
      "fc1.bias 0.0003317996859550476\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00036702606827020644\n",
      "conv1.bias 0.00158684013877064\n",
      "conv2.weight 0.00019669925794005394\n",
      "conv2.bias 0.0003867020132020116\n",
      "fc1.weight 4.620567488018423e-05\n",
      "fc1.bias 0.0003317996859550476\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00036702606827020644\n",
      "conv1.bias 0.00158684013877064\n",
      "conv2.weight 0.00019669925794005394\n",
      "conv2.bias 0.0003867020132020116\n",
      "fc1.weight 4.620567488018423e-05\n",
      "fc1.bias 0.0003317996859550476\n",
      "\n",
      "Test set: Average loss: 2.2930 \n",
      "Accuracy: 3967/10000 (39.67%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00015589586459100247\n",
      "conv1.bias 0.002734860172495246\n",
      "conv2.weight 9.722107090055943e-05\n",
      "conv2.bias 0.0013434852007776499\n",
      "fc1.weight 5.256560398265719e-05\n",
      "fc1.bias 0.00046970718540251255\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00015589586459100247\n",
      "conv1.bias 0.002734860172495246\n",
      "conv2.weight 9.722107090055943e-05\n",
      "conv2.bias 0.0013434852007776499\n",
      "fc1.weight 5.256560398265719e-05\n",
      "fc1.bias 0.00046970718540251255\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00015589586459100247\n",
      "conv1.bias 0.002734860172495246\n",
      "conv2.weight 9.722107090055943e-05\n",
      "conv2.bias 0.0013434852007776499\n",
      "fc1.weight 5.256560398265719e-05\n",
      "fc1.bias 0.00046970718540251255\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00015589586459100247\n",
      "conv1.bias 0.002734860172495246\n",
      "conv2.weight 9.722107090055943e-05\n",
      "conv2.bias 0.0013434852007776499\n",
      "fc1.weight 5.256560398265719e-05\n",
      "fc1.bias 0.00046970718540251255\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00021348360925912857\n",
      "conv1.bias 0.001924840034916997\n",
      "conv2.weight 2.335874130949378e-05\n",
      "conv2.bias 0.0008036515791900456\n",
      "fc1.weight 0.00021097301505506038\n",
      "fc1.bias 0.0003815454663708806\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00021348360925912857\n",
      "conv1.bias 0.001924840034916997\n",
      "conv2.weight 2.335874130949378e-05\n",
      "conv2.bias 0.0008036515791900456\n",
      "fc1.weight 0.00021097301505506038\n",
      "fc1.bias 0.0003815454663708806\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00021348360925912857\n",
      "conv1.bias 0.001924840034916997\n",
      "conv2.weight 2.335874130949378e-05\n",
      "conv2.bias 0.0008036515791900456\n",
      "fc1.weight 0.00021097301505506038\n",
      "fc1.bias 0.0003815454663708806\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00021348360925912857\n",
      "conv1.bias 0.001924840034916997\n",
      "conv2.weight 2.335874130949378e-05\n",
      "conv2.bias 0.0008036515791900456\n",
      "fc1.weight 0.00021097301505506038\n",
      "fc1.bias 0.0003815454663708806\n",
      "\n",
      "Test set: Average loss: 2.3023 \n",
      "Accuracy: 1161/10000 (11.61%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00017149388790130616\n",
      "conv1.bias 0.001191118499264121\n",
      "conv2.weight 3.0206760857254267e-05\n",
      "conv2.bias 0.0006243223906494677\n",
      "fc1.weight 5.701933405362069e-05\n",
      "fc1.bias 0.0003484975779429078\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00017149388790130616\n",
      "conv1.bias 0.001191118499264121\n",
      "conv2.weight 3.0206760857254267e-05\n",
      "conv2.bias 0.0006243223906494677\n",
      "fc1.weight 5.701933405362069e-05\n",
      "fc1.bias 0.0003484975779429078\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00017149388790130616\n",
      "conv1.bias 0.001191118499264121\n",
      "conv2.weight 3.0206760857254267e-05\n",
      "conv2.bias 0.0006243223906494677\n",
      "fc1.weight 5.701933405362069e-05\n",
      "fc1.bias 0.0003484975779429078\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00017149388790130616\n",
      "conv1.bias 0.001191118499264121\n",
      "conv2.weight 3.0206760857254267e-05\n",
      "conv2.bias 0.0006243223906494677\n",
      "fc1.weight 5.701933405362069e-05\n",
      "fc1.bias 0.0003484975779429078\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004302160441875458\n",
      "conv1.bias 0.0010183880804106593\n",
      "conv2.weight 5.4282676428556444e-05\n",
      "conv2.bias 0.000794491614215076\n",
      "fc1.weight 0.0002355487085878849\n",
      "fc1.bias 0.0003235162235796452\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004302160441875458\n",
      "conv1.bias 0.0010183880804106593\n",
      "conv2.weight 5.4282676428556444e-05\n",
      "conv2.bias 0.000794491614215076\n",
      "fc1.weight 0.0002355487085878849\n",
      "fc1.bias 0.0003235162235796452\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004302160441875458\n",
      "conv1.bias 0.0010183880804106593\n",
      "conv2.weight 5.4282676428556444e-05\n",
      "conv2.bias 0.000794491614215076\n",
      "fc1.weight 0.0002355487085878849\n",
      "fc1.bias 0.0003235162235796452\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004302160441875458\n",
      "conv1.bias 0.0010183880804106593\n",
      "conv2.weight 5.4282676428556444e-05\n",
      "conv2.bias 0.000794491614215076\n",
      "fc1.weight 0.0002355487085878849\n",
      "fc1.bias 0.0003235162235796452\n",
      "\n",
      "Test set: Average loss: 2.2942 \n",
      "Accuracy: 3033/10000 (30.33%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00017912838608026506\n",
      "conv1.bias 0.0015738618094474077\n",
      "conv2.weight 0.00011339008808135986\n",
      "conv2.bias 0.0016667572781443596\n",
      "fc1.weight 7.754699327051639e-05\n",
      "fc1.bias 0.0001793133094906807\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00017912838608026506\n",
      "conv1.bias 0.0015738618094474077\n",
      "conv2.weight 0.00011339008808135986\n",
      "conv2.bias 0.0016667572781443596\n",
      "fc1.weight 7.754699327051639e-05\n",
      "fc1.bias 0.0001793133094906807\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00017912838608026506\n",
      "conv1.bias 0.0015738618094474077\n",
      "conv2.weight 0.00011339008808135986\n",
      "conv2.bias 0.0016667572781443596\n",
      "fc1.weight 7.754699327051639e-05\n",
      "fc1.bias 0.0001793133094906807\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00017912838608026506\n",
      "conv1.bias 0.0015738618094474077\n",
      "conv2.weight 0.00011339008808135986\n",
      "conv2.bias 0.0016667572781443596\n",
      "fc1.weight 7.754699327051639e-05\n",
      "fc1.bias 0.0001793133094906807\n",
      "\n",
      "Test set: Average loss: 2.2863 \n",
      "Accuracy: 4059/10000 (40.59%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000122747914865613\n",
      "conv1.bias 0.0024250787682831287\n",
      "conv2.weight 5.967519246041775e-05\n",
      "conv2.bias 0.0012171854032203555\n",
      "fc1.weight 0.0001193815260194242\n",
      "fc1.bias 0.00042518475092947485\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000122747914865613\n",
      "conv1.bias 0.0024250787682831287\n",
      "conv2.weight 5.967519246041775e-05\n",
      "conv2.bias 0.0012171854032203555\n",
      "fc1.weight 0.0001193815260194242\n",
      "fc1.bias 0.00042518475092947485\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000122747914865613\n",
      "conv1.bias 0.0024250787682831287\n",
      "conv2.weight 5.967519246041775e-05\n",
      "conv2.bias 0.0012171854032203555\n",
      "fc1.weight 0.0001193815260194242\n",
      "fc1.bias 0.00042518475092947485\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000122747914865613\n",
      "conv1.bias 0.0024250787682831287\n",
      "conv2.weight 5.967519246041775e-05\n",
      "conv2.bias 0.0012171854032203555\n",
      "fc1.weight 0.0001193815260194242\n",
      "fc1.bias 0.00042518475092947485\n",
      "\n",
      "Test set: Average loss: 2.2095 \n",
      "Accuracy: 6303/10000 (63.03%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000266752690076828\n",
      "conv1.bias 0.001969715114682913\n",
      "conv2.weight 0.00010611657053232193\n",
      "conv2.bias 0.0012039672583341599\n",
      "fc1.weight 0.0001245224615558982\n",
      "fc1.bias 0.0007589585147798061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected users: [0 1]\n",
      "conv1.weight 0.000266752690076828\n",
      "conv1.bias 0.001969715114682913\n",
      "conv2.weight 0.00010611657053232193\n",
      "conv2.bias 0.0012039672583341599\n",
      "fc1.weight 0.0001245224615558982\n",
      "fc1.bias 0.0007589585147798061\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000266752690076828\n",
      "conv1.bias 0.001969715114682913\n",
      "conv2.weight 0.00010611657053232193\n",
      "conv2.bias 0.0012039672583341599\n",
      "fc1.weight 0.0001245224615558982\n",
      "fc1.bias 0.0007589585147798061\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000266752690076828\n",
      "conv1.bias 0.001969715114682913\n",
      "conv2.weight 0.00010611657053232193\n",
      "conv2.bias 0.0012039672583341599\n",
      "fc1.weight 0.0001245224615558982\n",
      "fc1.bias 0.0007589585147798061\n",
      "\n",
      "Test set: Average loss: 2.2943 \n",
      "Accuracy: 2793/10000 (27.93%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00020861158147454263\n",
      "conv1.bias 0.001806002575904131\n",
      "conv2.weight 0.00010279072448611259\n",
      "conv2.bias 0.000956387200858444\n",
      "fc1.weight 0.0001977271866053343\n",
      "fc1.bias 0.00019003257621079682\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00020861158147454263\n",
      "conv1.bias 0.001806002575904131\n",
      "conv2.weight 0.00010279072448611259\n",
      "conv2.bias 0.000956387200858444\n",
      "fc1.weight 0.0001977271866053343\n",
      "fc1.bias 0.00019003257621079682\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00020861158147454263\n",
      "conv1.bias 0.001806002575904131\n",
      "conv2.weight 0.00010279072448611259\n",
      "conv2.bias 0.000956387200858444\n",
      "fc1.weight 0.0001977271866053343\n",
      "fc1.bias 0.00019003257621079682\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00020861158147454263\n",
      "conv1.bias 0.001806002575904131\n",
      "conv2.weight 0.00010279072448611259\n",
      "conv2.bias 0.000956387200858444\n",
      "fc1.weight 0.0001977271866053343\n",
      "fc1.bias 0.00019003257621079682\n",
      "\n",
      "Test set: Average loss: 2.2281 \n",
      "Accuracy: 5623/10000 (56.23%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0002528778277337551\n",
      "conv1.bias 0.0022149994038045406\n",
      "conv2.weight 7.080203387886285e-05\n",
      "conv2.bias 0.0009853009833022952\n",
      "fc1.weight 0.00015558741288259625\n",
      "fc1.bias 0.00038952026516199113\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0002528778277337551\n",
      "conv1.bias 0.0022149994038045406\n",
      "conv2.weight 7.080203387886285e-05\n",
      "conv2.bias 0.0009853009833022952\n",
      "fc1.weight 0.00015558741288259625\n",
      "fc1.bias 0.00038952026516199113\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0002528778277337551\n",
      "conv1.bias 0.0022149994038045406\n",
      "conv2.weight 7.080203387886285e-05\n",
      "conv2.bias 0.0009853009833022952\n",
      "fc1.weight 0.00015558741288259625\n",
      "fc1.bias 0.00038952026516199113\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0002528778277337551\n",
      "conv1.bias 0.0022149994038045406\n",
      "conv2.weight 7.080203387886285e-05\n",
      "conv2.bias 0.0009853009833022952\n",
      "fc1.weight 0.00015558741288259625\n",
      "fc1.bias 0.00038952026516199113\n",
      "\n",
      "Test set: Average loss: 2.0260 \n",
      "Accuracy: 6802/10000 (68.02%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003942490369081497\n",
      "conv1.bias 0.0018640805501490831\n",
      "conv2.weight 6.810985039919615e-05\n",
      "conv2.bias 0.0007871055277064443\n",
      "fc1.weight 0.0002459913957864046\n",
      "fc1.bias 0.0007070522755384445\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003942490369081497\n",
      "conv1.bias 0.0018640805501490831\n",
      "conv2.weight 6.810985039919615e-05\n",
      "conv2.bias 0.0007871055277064443\n",
      "fc1.weight 0.0002459913957864046\n",
      "fc1.bias 0.0007070522755384445\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003942490369081497\n",
      "conv1.bias 0.0018640805501490831\n",
      "conv2.weight 6.810985039919615e-05\n",
      "conv2.bias 0.0007871055277064443\n",
      "fc1.weight 0.0002459913957864046\n",
      "fc1.bias 0.0007070522755384445\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003942490369081497\n",
      "conv1.bias 0.0018640805501490831\n",
      "conv2.weight 6.810985039919615e-05\n",
      "conv2.bias 0.0007871055277064443\n",
      "fc1.weight 0.0002459913957864046\n",
      "fc1.bias 0.0007070522755384445\n",
      "\n",
      "Test set: Average loss: 1.9618 \n",
      "Accuracy: 8017/10000 (80.17%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004538702964782715\n",
      "conv1.bias 0.003712682519108057\n",
      "conv2.weight 9.214933961629868e-05\n",
      "conv2.bias 0.0010184264974668622\n",
      "fc1.weight 0.0001732588396407664\n",
      "fc1.bias 0.0007460067979991436\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004538702964782715\n",
      "conv1.bias 0.003712682519108057\n",
      "conv2.weight 9.214933961629868e-05\n",
      "conv2.bias 0.0010184264974668622\n",
      "fc1.weight 0.0001732588396407664\n",
      "fc1.bias 0.0007460067979991436\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004538702964782715\n",
      "conv1.bias 0.003712682519108057\n",
      "conv2.weight 9.214933961629868e-05\n",
      "conv2.bias 0.0010184264974668622\n",
      "fc1.weight 0.0001732588396407664\n",
      "fc1.bias 0.0007460067979991436\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004538702964782715\n",
      "conv1.bias 0.003712682519108057\n",
      "conv2.weight 9.214933961629868e-05\n",
      "conv2.bias 0.0010184264974668622\n",
      "fc1.weight 0.0001732588396407664\n",
      "fc1.bias 0.0007460067979991436\n",
      "\n",
      "Test set: Average loss: 1.9972 \n",
      "Accuracy: 7230/10000 (72.30%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004977672919631005\n",
      "conv1.bias 0.004311266355216503\n",
      "conv2.weight 6.341656669974327e-05\n",
      "conv2.bias 0.0008484984864480793\n",
      "fc1.weight 0.00027080795262008906\n",
      "fc1.bias 0.0005829549394547939\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004977672919631005\n",
      "conv1.bias 0.004311266355216503\n",
      "conv2.weight 6.341656669974327e-05\n",
      "conv2.bias 0.0008484984864480793\n",
      "fc1.weight 0.00027080795262008906\n",
      "fc1.bias 0.0005829549394547939\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004977672919631005\n",
      "conv1.bias 0.004311266355216503\n",
      "conv2.weight 6.341656669974327e-05\n",
      "conv2.bias 0.0008484984864480793\n",
      "fc1.weight 0.00027080795262008906\n",
      "fc1.bias 0.0005829549394547939\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004977672919631005\n",
      "conv1.bias 0.004311266355216503\n",
      "conv2.weight 6.341656669974327e-05\n",
      "conv2.bias 0.0008484984864480793\n",
      "fc1.weight 0.00027080795262008906\n",
      "fc1.bias 0.0005829549394547939\n",
      "\n",
      "Test set: Average loss: 1.9010 \n",
      "Accuracy: 9302/10000 (93.02%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006354466080665588\n",
      "conv1.bias 0.0027990047819912434\n",
      "conv2.weight 7.745571900159121e-05\n",
      "conv2.bias 0.0007012764108367264\n",
      "fc1.weight 0.0001857903553172946\n",
      "fc1.bias 0.0013574711978435517\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006354466080665588\n",
      "conv1.bias 0.0027990047819912434\n",
      "conv2.weight 7.745571900159121e-05\n",
      "conv2.bias 0.0007012764108367264\n",
      "fc1.weight 0.0001857903553172946\n",
      "fc1.bias 0.0013574711978435517\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006354466080665588\n",
      "conv1.bias 0.0027990047819912434\n",
      "conv2.weight 7.745571900159121e-05\n",
      "conv2.bias 0.0007012764108367264\n",
      "fc1.weight 0.0001857903553172946\n",
      "fc1.bias 0.0013574711978435517\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006354466080665588\n",
      "conv1.bias 0.0027990047819912434\n",
      "conv2.weight 7.745571900159121e-05\n",
      "conv2.bias 0.0007012764108367264\n",
      "fc1.weight 0.0001857903553172946\n",
      "fc1.bias 0.0013574711978435517\n",
      "\n",
      "Test set: Average loss: 1.8682 \n",
      "Accuracy: 8639/10000 (86.39%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005958612263202667\n",
      "conv1.bias 0.002976929070428014\n",
      "conv2.weight 9.748584590852261e-05\n",
      "conv2.bias 0.00072664231993258\n",
      "fc1.weight 0.00019632414914667607\n",
      "fc1.bias 0.0011798524297773837\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005958612263202667\n",
      "conv1.bias 0.002976929070428014\n",
      "conv2.weight 9.748584590852261e-05\n",
      "conv2.bias 0.00072664231993258\n",
      "fc1.weight 0.00019632414914667607\n",
      "fc1.bias 0.0011798524297773837\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005958612263202667\n",
      "conv1.bias 0.002976929070428014\n",
      "conv2.weight 9.748584590852261e-05\n",
      "conv2.bias 0.00072664231993258\n",
      "fc1.weight 0.00019632414914667607\n",
      "fc1.bias 0.0011798524297773837\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005958612263202667\n",
      "conv1.bias 0.002976929070428014\n",
      "conv2.weight 9.748584590852261e-05\n",
      "conv2.bias 0.00072664231993258\n",
      "fc1.weight 0.00019632414914667607\n",
      "fc1.bias 0.0011798524297773837\n",
      "\n",
      "Test set: Average loss: 1.9099 \n",
      "Accuracy: 8663/10000 (86.63%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00041143450886011124\n",
      "conv1.bias 0.002999406075105071\n",
      "conv2.weight 0.00010191543959081173\n",
      "conv2.bias 0.0010451474227011204\n",
      "fc1.weight 0.0002160392701625824\n",
      "fc1.bias 0.0011191637255251407\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00041143450886011124\n",
      "conv1.bias 0.002999406075105071\n",
      "conv2.weight 0.00010191543959081173\n",
      "conv2.bias 0.0010451474227011204\n",
      "fc1.weight 0.0002160392701625824\n",
      "fc1.bias 0.0011191637255251407\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00041143450886011124\n",
      "conv1.bias 0.002999406075105071\n",
      "conv2.weight 0.00010191543959081173\n",
      "conv2.bias 0.0010451474227011204\n",
      "fc1.weight 0.0002160392701625824\n",
      "fc1.bias 0.0011191637255251407\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00041143450886011124\n",
      "conv1.bias 0.002999406075105071\n",
      "conv2.weight 0.00010191543959081173\n",
      "conv2.bias 0.0010451474227011204\n",
      "fc1.weight 0.0002160392701625824\n",
      "fc1.bias 0.0011191637255251407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.2303 \n",
      "Accuracy: 6008/10000 (60.08%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00037152431905269624\n",
      "conv1.bias 0.0017853290773928165\n",
      "conv2.weight 0.0002505203895270824\n",
      "conv2.bias 0.0011582764564082026\n",
      "fc1.weight 0.00027368681039661167\n",
      "fc1.bias 0.0007456464227288961\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00037152431905269624\n",
      "conv1.bias 0.0017853290773928165\n",
      "conv2.weight 0.0002505203895270824\n",
      "conv2.bias 0.0011582764564082026\n",
      "fc1.weight 0.00027368681039661167\n",
      "fc1.bias 0.0007456464227288961\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00037152431905269624\n",
      "conv1.bias 0.0017853290773928165\n",
      "conv2.weight 0.0002505203895270824\n",
      "conv2.bias 0.0011582764564082026\n",
      "fc1.weight 0.00027368681039661167\n",
      "fc1.bias 0.0007456464227288961\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00037152431905269624\n",
      "conv1.bias 0.0017853290773928165\n",
      "conv2.weight 0.0002505203895270824\n",
      "conv2.bias 0.0011582764564082026\n",
      "fc1.weight 0.00027368681039661167\n",
      "fc1.bias 0.0007456464227288961\n",
      "\n",
      "Test set: Average loss: 1.9588 \n",
      "Accuracy: 7651/10000 (76.51%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004338262975215912\n",
      "conv1.bias 0.0053630429320037365\n",
      "conv2.weight 0.0001376140583306551\n",
      "conv2.bias 0.0014655222184956074\n",
      "fc1.weight 0.0001398836844600737\n",
      "fc1.bias 0.0009799990803003311\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004338262975215912\n",
      "conv1.bias 0.0053630429320037365\n",
      "conv2.weight 0.0001376140583306551\n",
      "conv2.bias 0.0014655222184956074\n",
      "fc1.weight 0.0001398836844600737\n",
      "fc1.bias 0.0009799990803003311\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004338262975215912\n",
      "conv1.bias 0.0053630429320037365\n",
      "conv2.weight 0.0001376140583306551\n",
      "conv2.bias 0.0014655222184956074\n",
      "fc1.weight 0.0001398836844600737\n",
      "fc1.bias 0.0009799990803003311\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004338262975215912\n",
      "conv1.bias 0.0053630429320037365\n",
      "conv2.weight 0.0001376140583306551\n",
      "conv2.bias 0.0014655222184956074\n",
      "fc1.weight 0.0001398836844600737\n",
      "fc1.bias 0.0009799990803003311\n",
      "\n",
      "Test set: Average loss: 1.9299 \n",
      "Accuracy: 7724/10000 (77.24%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005733109265565873\n",
      "conv1.bias 0.0020545206498354673\n",
      "conv2.weight 0.00010798152536153793\n",
      "conv2.bias 0.0008351188153028488\n",
      "fc1.weight 0.00029186252504587173\n",
      "fc1.bias 0.0018780004233121873\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005733109265565873\n",
      "conv1.bias 0.0020545206498354673\n",
      "conv2.weight 0.00010798152536153793\n",
      "conv2.bias 0.0008351188153028488\n",
      "fc1.weight 0.00029186252504587173\n",
      "fc1.bias 0.0018780004233121873\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005733109265565873\n",
      "conv1.bias 0.0020545206498354673\n",
      "conv2.weight 0.00010798152536153793\n",
      "conv2.bias 0.0008351188153028488\n",
      "fc1.weight 0.00029186252504587173\n",
      "fc1.bias 0.0018780004233121873\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005733109265565873\n",
      "conv1.bias 0.0020545206498354673\n",
      "conv2.weight 0.00010798152536153793\n",
      "conv2.bias 0.0008351188153028488\n",
      "fc1.weight 0.00029186252504587173\n",
      "fc1.bias 0.0018780004233121873\n",
      "\n",
      "Test set: Average loss: 1.8498 \n",
      "Accuracy: 9453/10000 (94.53%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005168087035417557\n",
      "conv1.bias 0.0031619053333997726\n",
      "conv2.weight 0.00016592007130384446\n",
      "conv2.bias 0.0009280957165174186\n",
      "fc1.weight 0.00015546109061688186\n",
      "fc1.bias 0.0013261052779853344\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005168087035417557\n",
      "conv1.bias 0.0031619053333997726\n",
      "conv2.weight 0.00016592007130384446\n",
      "conv2.bias 0.0009280957165174186\n",
      "fc1.weight 0.00015546109061688186\n",
      "fc1.bias 0.0013261052779853344\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005168087035417557\n",
      "conv1.bias 0.0031619053333997726\n",
      "conv2.weight 0.00016592007130384446\n",
      "conv2.bias 0.0009280957165174186\n",
      "fc1.weight 0.00015546109061688186\n",
      "fc1.bias 0.0013261052779853344\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005168087035417557\n",
      "conv1.bias 0.0031619053333997726\n",
      "conv2.weight 0.00016592007130384446\n",
      "conv2.bias 0.0009280957165174186\n",
      "fc1.weight 0.00015546109061688186\n",
      "fc1.bias 0.0013261052779853344\n",
      "\n",
      "Test set: Average loss: 1.9928 \n",
      "Accuracy: 8754/10000 (87.54%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003825576975941658\n",
      "conv1.bias 0.0016219888348132372\n",
      "conv2.weight 0.00026623880490660667\n",
      "conv2.bias 0.0008967980975285172\n",
      "fc1.weight 0.00029036193154752254\n",
      "fc1.bias 0.000947924330830574\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003825576975941658\n",
      "conv1.bias 0.0016219888348132372\n",
      "conv2.weight 0.00026623880490660667\n",
      "conv2.bias 0.0008967980975285172\n",
      "fc1.weight 0.00029036193154752254\n",
      "fc1.bias 0.000947924330830574\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003825576975941658\n",
      "conv1.bias 0.0016219888348132372\n",
      "conv2.weight 0.00026623880490660667\n",
      "conv2.bias 0.0008967980975285172\n",
      "fc1.weight 0.00029036193154752254\n",
      "fc1.bias 0.000947924330830574\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003825576975941658\n",
      "conv1.bias 0.0016219888348132372\n",
      "conv2.weight 0.00026623880490660667\n",
      "conv2.bias 0.0008967980975285172\n",
      "fc1.weight 0.00029036193154752254\n",
      "fc1.bias 0.000947924330830574\n",
      "\n",
      "Test set: Average loss: 1.9512 \n",
      "Accuracy: 9367/10000 (93.67%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00036070842295885083\n",
      "conv1.bias 0.0011173902312293649\n",
      "conv2.weight 0.0002976769581437111\n",
      "conv2.bias 0.0010899624321609735\n",
      "fc1.weight 0.00026823841035366057\n",
      "fc1.bias 0.0009968671016395091\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00036070842295885083\n",
      "conv1.bias 0.0011173902312293649\n",
      "conv2.weight 0.0002976769581437111\n",
      "conv2.bias 0.0010899624321609735\n",
      "fc1.weight 0.00026823841035366057\n",
      "fc1.bias 0.0009968671016395091\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00036070842295885083\n",
      "conv1.bias 0.0011173902312293649\n",
      "conv2.weight 0.0002976769581437111\n",
      "conv2.bias 0.0010899624321609735\n",
      "fc1.weight 0.00026823841035366057\n",
      "fc1.bias 0.0009968671016395091\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00036070842295885083\n",
      "conv1.bias 0.0011173902312293649\n",
      "conv2.weight 0.0002976769581437111\n",
      "conv2.bias 0.0010899624321609735\n",
      "fc1.weight 0.00026823841035366057\n",
      "fc1.bias 0.0009968671016395091\n",
      "\n",
      "Test set: Average loss: 1.9155 \n",
      "Accuracy: 9368/10000 (93.68%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0002718665823340416\n",
      "conv1.bias 0.0017395243048667908\n",
      "conv2.weight 0.00014877256006002427\n",
      "conv2.bias 0.0009267749264836311\n",
      "fc1.weight 0.00019529266282916068\n",
      "fc1.bias 0.0012721747159957885\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0002718665823340416\n",
      "conv1.bias 0.0017395243048667908\n",
      "conv2.weight 0.00014877256006002427\n",
      "conv2.bias 0.0009267749264836311\n",
      "fc1.weight 0.00019529266282916068\n",
      "fc1.bias 0.0012721747159957885\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0002718665823340416\n",
      "conv1.bias 0.0017395243048667908\n",
      "conv2.weight 0.00014877256006002427\n",
      "conv2.bias 0.0009267749264836311\n",
      "fc1.weight 0.00019529266282916068\n",
      "fc1.bias 0.0012721747159957885\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0002718665823340416\n",
      "conv1.bias 0.0017395243048667908\n",
      "conv2.weight 0.00014877256006002427\n",
      "conv2.bias 0.0009267749264836311\n",
      "fc1.weight 0.00019529266282916068\n",
      "fc1.bias 0.0012721747159957885\n",
      "\n",
      "Test set: Average loss: 1.8797 \n",
      "Accuracy: 8670/10000 (86.70%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005222296714782715\n",
      "conv1.bias 0.002926647663116455\n",
      "conv2.weight 9.09706950187683e-05\n",
      "conv2.bias 0.001046741963364184\n",
      "fc1.weight 0.00025224818382412194\n",
      "fc1.bias 0.0011909954249858857\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005222296714782715\n",
      "conv1.bias 0.002926647663116455\n",
      "conv2.weight 9.09706950187683e-05\n",
      "conv2.bias 0.001046741963364184\n",
      "fc1.weight 0.00025224818382412194\n",
      "fc1.bias 0.0011909954249858857\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005222296714782715\n",
      "conv1.bias 0.002926647663116455\n",
      "conv2.weight 9.09706950187683e-05\n",
      "conv2.bias 0.001046741963364184\n",
      "fc1.weight 0.00025224818382412194\n",
      "fc1.bias 0.0011909954249858857\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005222296714782715\n",
      "conv1.bias 0.002926647663116455\n",
      "conv2.weight 9.09706950187683e-05\n",
      "conv2.bias 0.001046741963364184\n",
      "fc1.weight 0.00025224818382412194\n",
      "fc1.bias 0.0011909954249858857\n",
      "\n",
      "Test set: Average loss: 1.9423 \n",
      "Accuracy: 8074/10000 (80.74%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00036436647176742555\n",
      "conv1.bias 0.0027667866088449955\n",
      "conv2.weight 0.00014547652564942838\n",
      "conv2.bias 0.0009369454346597195\n",
      "fc1.weight 0.0002697102259844542\n",
      "fc1.bias 0.0016165453940629958\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00036436647176742555\n",
      "conv1.bias 0.0027667866088449955\n",
      "conv2.weight 0.00014547652564942838\n",
      "conv2.bias 0.0009369454346597195\n",
      "fc1.weight 0.0002697102259844542\n",
      "fc1.bias 0.0016165453940629958\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00036436647176742555\n",
      "conv1.bias 0.0027667866088449955\n",
      "conv2.weight 0.00014547652564942838\n",
      "conv2.bias 0.0009369454346597195\n",
      "fc1.weight 0.0002697102259844542\n",
      "fc1.bias 0.0016165453940629958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected users: [0 1]\n",
      "conv1.weight 0.00036436647176742555\n",
      "conv1.bias 0.0027667866088449955\n",
      "conv2.weight 0.00014547652564942838\n",
      "conv2.bias 0.0009369454346597195\n",
      "fc1.weight 0.0002697102259844542\n",
      "fc1.bias 0.0016165453940629958\n",
      "\n",
      "Test set: Average loss: 1.9363 \n",
      "Accuracy: 7920/10000 (79.20%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00043885443359613417\n",
      "conv1.bias 0.0042341966181993484\n",
      "conv2.weight 9.77441668510437e-05\n",
      "conv2.bias 0.0009121769107878208\n",
      "fc1.weight 0.0002636796794831753\n",
      "fc1.bias 0.0013973048888146877\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00043885443359613417\n",
      "conv1.bias 0.0042341966181993484\n",
      "conv2.weight 9.77441668510437e-05\n",
      "conv2.bias 0.0009121769107878208\n",
      "fc1.weight 0.0002636796794831753\n",
      "fc1.bias 0.0013973048888146877\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00043885443359613417\n",
      "conv1.bias 0.0042341966181993484\n",
      "conv2.weight 9.77441668510437e-05\n",
      "conv2.bias 0.0009121769107878208\n",
      "fc1.weight 0.0002636796794831753\n",
      "fc1.bias 0.0013973048888146877\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00043885443359613417\n",
      "conv1.bias 0.0042341966181993484\n",
      "conv2.weight 9.77441668510437e-05\n",
      "conv2.bias 0.0009121769107878208\n",
      "fc1.weight 0.0002636796794831753\n",
      "fc1.bias 0.0013973048888146877\n",
      "\n",
      "Test set: Average loss: 1.8676 \n",
      "Accuracy: 8498/10000 (84.98%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004778711125254631\n",
      "conv1.bias 0.0049070194363594055\n",
      "conv2.weight 5.840410012751818e-05\n",
      "conv2.bias 0.0009064213372766972\n",
      "fc1.weight 0.00025699697434902193\n",
      "fc1.bias 0.0018497690558433532\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004778711125254631\n",
      "conv1.bias 0.0049070194363594055\n",
      "conv2.weight 5.840410012751818e-05\n",
      "conv2.bias 0.0009064213372766972\n",
      "fc1.weight 0.00025699697434902193\n",
      "fc1.bias 0.0018497690558433532\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004778711125254631\n",
      "conv1.bias 0.0049070194363594055\n",
      "conv2.weight 5.840410012751818e-05\n",
      "conv2.bias 0.0009064213372766972\n",
      "fc1.weight 0.00025699697434902193\n",
      "fc1.bias 0.0018497690558433532\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004778711125254631\n",
      "conv1.bias 0.0049070194363594055\n",
      "conv2.weight 5.840410012751818e-05\n",
      "conv2.bias 0.0009064213372766972\n",
      "fc1.weight 0.00025699697434902193\n",
      "fc1.bias 0.0018497690558433532\n",
      "\n",
      "Test set: Average loss: 1.8865 \n",
      "Accuracy: 9260/10000 (92.60%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00037529319524765015\n",
      "conv1.bias 0.003960632719099522\n",
      "conv2.weight 0.00013381136581301688\n",
      "conv2.bias 0.0009200159693136811\n",
      "fc1.weight 0.0002624620916321874\n",
      "fc1.bias 0.0018224487081170081\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00037529319524765015\n",
      "conv1.bias 0.003960632719099522\n",
      "conv2.weight 0.00013381136581301688\n",
      "conv2.bias 0.0009200159693136811\n",
      "fc1.weight 0.0002624620916321874\n",
      "fc1.bias 0.0018224487081170081\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00037529319524765015\n",
      "conv1.bias 0.003960632719099522\n",
      "conv2.weight 0.00013381136581301688\n",
      "conv2.bias 0.0009200159693136811\n",
      "fc1.weight 0.0002624620916321874\n",
      "fc1.bias 0.0018224487081170081\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00037529319524765015\n",
      "conv1.bias 0.003960632719099522\n",
      "conv2.weight 0.00013381136581301688\n",
      "conv2.bias 0.0009200159693136811\n",
      "fc1.weight 0.0002624620916321874\n",
      "fc1.bias 0.0018224487081170081\n",
      "\n",
      "Test set: Average loss: 1.8590 \n",
      "Accuracy: 9281/10000 (92.81%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000572364591062069\n",
      "conv1.bias 0.003907734993845224\n",
      "conv2.weight 7.803686894476413e-05\n",
      "conv2.bias 0.0008403909741900861\n",
      "fc1.weight 0.00020815741736441852\n",
      "fc1.bias 0.0017166955396533013\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000572364591062069\n",
      "conv1.bias 0.003907734993845224\n",
      "conv2.weight 7.803686894476413e-05\n",
      "conv2.bias 0.0008403909741900861\n",
      "fc1.weight 0.00020815741736441852\n",
      "fc1.bias 0.0017166955396533013\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000572364591062069\n",
      "conv1.bias 0.003907734993845224\n",
      "conv2.weight 7.803686894476413e-05\n",
      "conv2.bias 0.0008403909741900861\n",
      "fc1.weight 0.00020815741736441852\n",
      "fc1.bias 0.0017166955396533013\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000572364591062069\n",
      "conv1.bias 0.003907734993845224\n",
      "conv2.weight 7.803686894476413e-05\n",
      "conv2.bias 0.0008403909741900861\n",
      "fc1.weight 0.00020815741736441852\n",
      "fc1.bias 0.0017166955396533013\n",
      "\n",
      "Test set: Average loss: 1.9194 \n",
      "Accuracy: 8457/10000 (84.57%)\n",
      "\n",
      "##########################################\n",
      "###### 1 -th Trial!! ###########\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.01313120722770691\n",
      "conv1.bias 0.012159106321632862\n",
      "conv2.weight 0.0004161401093006134\n",
      "conv2.bias 0.0004336505080573261\n",
      "fc1.weight 0.0003195609897375107\n",
      "fc1.bias 0.0003141290275380015\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.01313120722770691\n",
      "conv1.bias 0.012159106321632862\n",
      "conv2.weight 0.0004161401093006134\n",
      "conv2.bias 0.0004336505080573261\n",
      "fc1.weight 0.0003195609897375107\n",
      "fc1.bias 0.0003141290275380015\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.01313120722770691\n",
      "conv1.bias 0.012159106321632862\n",
      "conv2.weight 0.0004161401093006134\n",
      "conv2.bias 0.0004336505080573261\n",
      "fc1.weight 0.0003195609897375107\n",
      "fc1.bias 0.0003141290275380015\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.01313120722770691\n",
      "conv1.bias 0.012159106321632862\n",
      "conv2.weight 0.0004161401093006134\n",
      "conv2.bias 0.0004336505080573261\n",
      "fc1.weight 0.0003195609897375107\n",
      "fc1.bias 0.0003141290275380015\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00041892874985933304\n",
      "conv1.bias 0.0013705200981348753\n",
      "conv2.weight 0.00035189878195524216\n",
      "conv2.bias 0.0005930309998802841\n",
      "fc1.weight 6.682680686935783e-05\n",
      "fc1.bias 0.00043851216323673723\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00041892874985933304\n",
      "conv1.bias 0.0013705200981348753\n",
      "conv2.weight 0.00035189878195524216\n",
      "conv2.bias 0.0005930309998802841\n",
      "fc1.weight 6.682680686935783e-05\n",
      "fc1.bias 0.00043851216323673723\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00041892874985933304\n",
      "conv1.bias 0.0013705200981348753\n",
      "conv2.weight 0.00035189878195524216\n",
      "conv2.bias 0.0005930309998802841\n",
      "fc1.weight 6.682680686935783e-05\n",
      "fc1.bias 0.00043851216323673723\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00041892874985933304\n",
      "conv1.bias 0.0013705200981348753\n",
      "conv2.weight 0.00035189878195524216\n",
      "conv2.bias 0.0005930309998802841\n",
      "fc1.weight 6.682680686935783e-05\n",
      "fc1.bias 0.00043851216323673723\n",
      "\n",
      "Test set: Average loss: 2.3025 \n",
      "Accuracy: 1010/10000 (10.10%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 8.672399446368217e-05\n",
      "conv1.bias 0.0015508891083300114\n",
      "conv2.weight 4.2872172780334947e-05\n",
      "conv2.bias 0.0010777218267321587\n",
      "fc1.weight 3.858247655443847e-05\n",
      "fc1.bias 0.0004331574309617281\n",
      "selected users: [0 1]\n",
      "conv1.weight 8.672399446368217e-05\n",
      "conv1.bias 0.0015508891083300114\n",
      "conv2.weight 4.2872172780334947e-05\n",
      "conv2.bias 0.0010777218267321587\n",
      "fc1.weight 3.858247655443847e-05\n",
      "fc1.bias 0.0004331574309617281\n",
      "selected users: [0 1]\n",
      "conv1.weight 8.672399446368217e-05\n",
      "conv1.bias 0.0015508891083300114\n",
      "conv2.weight 4.2872172780334947e-05\n",
      "conv2.bias 0.0010777218267321587\n",
      "fc1.weight 3.858247655443847e-05\n",
      "fc1.bias 0.0004331574309617281\n",
      "selected users: [0 1]\n",
      "conv1.weight 8.672399446368217e-05\n",
      "conv1.bias 0.0015508891083300114\n",
      "conv2.weight 4.2872172780334947e-05\n",
      "conv2.bias 0.0010777218267321587\n",
      "fc1.weight 3.858247655443847e-05\n",
      "fc1.bias 0.0004331574309617281\n",
      "\n",
      "Test set: Average loss: 2.2740 \n",
      "Accuracy: 3313/10000 (33.13%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0002333538606762886\n",
      "conv1.bias 0.0011147574987262487\n",
      "conv2.weight 8.670364506542683e-05\n",
      "conv2.bias 0.001224301173351705\n",
      "fc1.weight 0.00020580030977725982\n",
      "fc1.bias 0.0010206615552306175\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0002333538606762886\n",
      "conv1.bias 0.0011147574987262487\n",
      "conv2.weight 8.670364506542683e-05\n",
      "conv2.bias 0.001224301173351705\n",
      "fc1.weight 0.00020580030977725982\n",
      "fc1.bias 0.0010206615552306175\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0002333538606762886\n",
      "conv1.bias 0.0011147574987262487\n",
      "conv2.weight 8.670364506542683e-05\n",
      "conv2.bias 0.001224301173351705\n",
      "fc1.weight 0.00020580030977725982\n",
      "fc1.bias 0.0010206615552306175\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0002333538606762886\n",
      "conv1.bias 0.0011147574987262487\n",
      "conv2.weight 8.670364506542683e-05\n",
      "conv2.bias 0.001224301173351705\n",
      "fc1.weight 0.00020580030977725982\n",
      "fc1.bias 0.0010206615552306175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.1664 \n",
      "Accuracy: 6903/10000 (69.03%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004187897220253944\n",
      "conv1.bias 0.0012422810541465878\n",
      "conv2.weight 5.8550359681248666e-05\n",
      "conv2.bias 0.0009198588668368757\n",
      "fc1.weight 0.0002206700388342142\n",
      "fc1.bias 0.0007151366211473942\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004187897220253944\n",
      "conv1.bias 0.0012422810541465878\n",
      "conv2.weight 5.8550359681248666e-05\n",
      "conv2.bias 0.0009198588668368757\n",
      "fc1.weight 0.0002206700388342142\n",
      "fc1.bias 0.0007151366211473942\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004187897220253944\n",
      "conv1.bias 0.0012422810541465878\n",
      "conv2.weight 5.8550359681248666e-05\n",
      "conv2.bias 0.0009198588668368757\n",
      "fc1.weight 0.0002206700388342142\n",
      "fc1.bias 0.0007151366211473942\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004187897220253944\n",
      "conv1.bias 0.0012422810541465878\n",
      "conv2.weight 5.8550359681248666e-05\n",
      "conv2.bias 0.0009198588668368757\n",
      "fc1.weight 0.0002206700388342142\n",
      "fc1.bias 0.0007151366211473942\n",
      "\n",
      "Test set: Average loss: 2.1141 \n",
      "Accuracy: 5762/10000 (57.62%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00026463888585567477\n",
      "conv1.bias 0.0024447382893413305\n",
      "conv2.weight 0.00011615196242928505\n",
      "conv2.bias 0.0008982550934888422\n",
      "fc1.weight 0.00023218020796775818\n",
      "fc1.bias 0.0008198101073503494\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00026463888585567477\n",
      "conv1.bias 0.0024447382893413305\n",
      "conv2.weight 0.00011615196242928505\n",
      "conv2.bias 0.0008982550934888422\n",
      "fc1.weight 0.00023218020796775818\n",
      "fc1.bias 0.0008198101073503494\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00026463888585567477\n",
      "conv1.bias 0.0024447382893413305\n",
      "conv2.weight 0.00011615196242928505\n",
      "conv2.bias 0.0008982550934888422\n",
      "fc1.weight 0.00023218020796775818\n",
      "fc1.bias 0.0008198101073503494\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00026463888585567477\n",
      "conv1.bias 0.0024447382893413305\n",
      "conv2.weight 0.00011615196242928505\n",
      "conv2.bias 0.0008982550934888422\n",
      "fc1.weight 0.00023218020796775818\n",
      "fc1.bias 0.0008198101073503494\n",
      "\n",
      "Test set: Average loss: 1.9972 \n",
      "Accuracy: 7539/10000 (75.39%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004692240804433823\n",
      "conv1.bias 0.0033195088617503643\n",
      "conv2.weight 6.349682807922363e-05\n",
      "conv2.bias 0.0006788650061935186\n",
      "fc1.weight 0.00024864389561116693\n",
      "fc1.bias 0.0015029305592179298\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004692240804433823\n",
      "conv1.bias 0.0033195088617503643\n",
      "conv2.weight 6.349682807922363e-05\n",
      "conv2.bias 0.0006788650061935186\n",
      "fc1.weight 0.00024864389561116693\n",
      "fc1.bias 0.0015029305592179298\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004692240804433823\n",
      "conv1.bias 0.0033195088617503643\n",
      "conv2.weight 6.349682807922363e-05\n",
      "conv2.bias 0.0006788650061935186\n",
      "fc1.weight 0.00024864389561116693\n",
      "fc1.bias 0.0015029305592179298\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004692240804433823\n",
      "conv1.bias 0.0033195088617503643\n",
      "conv2.weight 6.349682807922363e-05\n",
      "conv2.bias 0.0006788650061935186\n",
      "fc1.weight 0.00024864389561116693\n",
      "fc1.bias 0.0015029305592179298\n",
      "\n",
      "Test set: Average loss: 2.0091 \n",
      "Accuracy: 6749/10000 (67.49%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005105114728212357\n",
      "conv1.bias 0.003905932419002056\n",
      "conv2.weight 6.537698209285736e-05\n",
      "conv2.bias 0.0006760015967302024\n",
      "fc1.weight 0.00019811317324638366\n",
      "fc1.bias 0.0016333270817995072\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005105114728212357\n",
      "conv1.bias 0.003905932419002056\n",
      "conv2.weight 6.537698209285736e-05\n",
      "conv2.bias 0.0006760015967302024\n",
      "fc1.weight 0.00019811317324638366\n",
      "fc1.bias 0.0016333270817995072\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005105114728212357\n",
      "conv1.bias 0.003905932419002056\n",
      "conv2.weight 6.537698209285736e-05\n",
      "conv2.bias 0.0006760015967302024\n",
      "fc1.weight 0.00019811317324638366\n",
      "fc1.bias 0.0016333270817995072\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005105114728212357\n",
      "conv1.bias 0.003905932419002056\n",
      "conv2.weight 6.537698209285736e-05\n",
      "conv2.bias 0.0006760015967302024\n",
      "fc1.weight 0.00019811317324638366\n",
      "fc1.bias 0.0016333270817995072\n",
      "\n",
      "Test set: Average loss: 1.9666 \n",
      "Accuracy: 8121/10000 (81.21%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00037638574838638305\n",
      "conv1.bias 0.0034513561986386776\n",
      "conv2.weight 0.00010740363970398903\n",
      "conv2.bias 0.0007437797030434012\n",
      "fc1.weight 0.0001842896337620914\n",
      "fc1.bias 0.0013545765541493893\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00037638574838638305\n",
      "conv1.bias 0.0034513561986386776\n",
      "conv2.weight 0.00010740363970398903\n",
      "conv2.bias 0.0007437797030434012\n",
      "fc1.weight 0.0001842896337620914\n",
      "fc1.bias 0.0013545765541493893\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00037638574838638305\n",
      "conv1.bias 0.0034513561986386776\n",
      "conv2.weight 0.00010740363970398903\n",
      "conv2.bias 0.0007437797030434012\n",
      "fc1.weight 0.0001842896337620914\n",
      "fc1.bias 0.0013545765541493893\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00037638574838638305\n",
      "conv1.bias 0.0034513561986386776\n",
      "conv2.weight 0.00010740363970398903\n",
      "conv2.bias 0.0007437797030434012\n",
      "fc1.weight 0.0001842896337620914\n",
      "fc1.bias 0.0013545765541493893\n",
      "\n",
      "Test set: Average loss: 1.9386 \n",
      "Accuracy: 8332/10000 (83.32%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00042732022702693937\n",
      "conv1.bias 0.004043449182063341\n",
      "conv2.weight 7.641761563718318e-05\n",
      "conv2.bias 0.000841156579554081\n",
      "fc1.weight 0.00030860607512295246\n",
      "fc1.bias 0.0016565574333071709\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00042732022702693937\n",
      "conv1.bias 0.004043449182063341\n",
      "conv2.weight 7.641761563718318e-05\n",
      "conv2.bias 0.000841156579554081\n",
      "fc1.weight 0.00030860607512295246\n",
      "fc1.bias 0.0016565574333071709\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00042732022702693937\n",
      "conv1.bias 0.004043449182063341\n",
      "conv2.weight 7.641761563718318e-05\n",
      "conv2.bias 0.000841156579554081\n",
      "fc1.weight 0.00030860607512295246\n",
      "fc1.bias 0.0016565574333071709\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00042732022702693937\n",
      "conv1.bias 0.004043449182063341\n",
      "conv2.weight 7.641761563718318e-05\n",
      "conv2.bias 0.000841156579554081\n",
      "fc1.weight 0.00030860607512295246\n",
      "fc1.bias 0.0016565574333071709\n",
      "\n",
      "Test set: Average loss: 1.9294 \n",
      "Accuracy: 8577/10000 (85.77%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005257937312126159\n",
      "conv1.bias 0.0033163842745125294\n",
      "conv2.weight 8.161050267517567e-05\n",
      "conv2.bias 0.0007313739042729139\n",
      "fc1.weight 0.0002362434519454837\n",
      "fc1.bias 0.0015180532820522786\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005257937312126159\n",
      "conv1.bias 0.0033163842745125294\n",
      "conv2.weight 8.161050267517567e-05\n",
      "conv2.bias 0.0007313739042729139\n",
      "fc1.weight 0.0002362434519454837\n",
      "fc1.bias 0.0015180532820522786\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005257937312126159\n",
      "conv1.bias 0.0033163842745125294\n",
      "conv2.weight 8.161050267517567e-05\n",
      "conv2.bias 0.0007313739042729139\n",
      "fc1.weight 0.0002362434519454837\n",
      "fc1.bias 0.0015180532820522786\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005257937312126159\n",
      "conv1.bias 0.0033163842745125294\n",
      "conv2.weight 8.161050267517567e-05\n",
      "conv2.bias 0.0007313739042729139\n",
      "fc1.weight 0.0002362434519454837\n",
      "fc1.bias 0.0015180532820522786\n",
      "\n",
      "Test set: Average loss: 1.8927 \n",
      "Accuracy: 9432/10000 (94.32%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005840563774108887\n",
      "conv1.bias 0.002398853190243244\n",
      "conv2.weight 0.00010492748580873012\n",
      "conv2.bias 0.0006927084177732468\n",
      "fc1.weight 0.00021681592334061862\n",
      "fc1.bias 0.0018263127654790877\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005840563774108887\n",
      "conv1.bias 0.002398853190243244\n",
      "conv2.weight 0.00010492748580873012\n",
      "conv2.bias 0.0006927084177732468\n",
      "fc1.weight 0.00021681592334061862\n",
      "fc1.bias 0.0018263127654790877\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005840563774108887\n",
      "conv1.bias 0.002398853190243244\n",
      "conv2.weight 0.00010492748580873012\n",
      "conv2.bias 0.0006927084177732468\n",
      "fc1.weight 0.00021681592334061862\n",
      "fc1.bias 0.0018263127654790877\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005840563774108887\n",
      "conv1.bias 0.002398853190243244\n",
      "conv2.weight 0.00010492748580873012\n",
      "conv2.bias 0.0006927084177732468\n",
      "fc1.weight 0.00021681592334061862\n",
      "fc1.bias 0.0018263127654790877\n",
      "\n",
      "Test set: Average loss: 1.9363 \n",
      "Accuracy: 9333/10000 (93.33%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003834984451532364\n",
      "conv1.bias 0.0017360993660986423\n",
      "conv2.weight 0.00026614345610141753\n",
      "conv2.bias 0.0009088668739423156\n",
      "fc1.weight 0.00017183301970362662\n",
      "fc1.bias 0.0013822613283991814\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003834984451532364\n",
      "conv1.bias 0.0017360993660986423\n",
      "conv2.weight 0.00026614345610141753\n",
      "conv2.bias 0.0009088668739423156\n",
      "fc1.weight 0.00017183301970362662\n",
      "fc1.bias 0.0013822613283991814\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003834984451532364\n",
      "conv1.bias 0.0017360993660986423\n",
      "conv2.weight 0.00026614345610141753\n",
      "conv2.bias 0.0009088668739423156\n",
      "fc1.weight 0.00017183301970362662\n",
      "fc1.bias 0.0013822613283991814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected users: [0 1]\n",
      "conv1.weight 0.0003834984451532364\n",
      "conv1.bias 0.0017360993660986423\n",
      "conv2.weight 0.00026614345610141753\n",
      "conv2.bias 0.0009088668739423156\n",
      "fc1.weight 0.00017183301970362662\n",
      "fc1.bias 0.0013822613283991814\n",
      "\n",
      "Test set: Average loss: 1.8937 \n",
      "Accuracy: 8878/10000 (88.78%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006156282126903534\n",
      "conv1.bias 0.003108806209638715\n",
      "conv2.weight 8.914894424378872e-05\n",
      "conv2.bias 0.0008367979316972196\n",
      "fc1.weight 0.00015966342762112616\n",
      "fc1.bias 0.0014409984461963177\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006156282126903534\n",
      "conv1.bias 0.003108806209638715\n",
      "conv2.weight 8.914894424378872e-05\n",
      "conv2.bias 0.0008367979316972196\n",
      "fc1.weight 0.00015966342762112616\n",
      "fc1.bias 0.0014409984461963177\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006156282126903534\n",
      "conv1.bias 0.003108806209638715\n",
      "conv2.weight 8.914894424378872e-05\n",
      "conv2.bias 0.0008367979316972196\n",
      "fc1.weight 0.00015966342762112616\n",
      "fc1.bias 0.0014409984461963177\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006156282126903534\n",
      "conv1.bias 0.003108806209638715\n",
      "conv2.weight 8.914894424378872e-05\n",
      "conv2.bias 0.0008367979316972196\n",
      "fc1.weight 0.00015966342762112616\n",
      "fc1.bias 0.0014409984461963177\n",
      "\n",
      "Test set: Average loss: 1.9655 \n",
      "Accuracy: 8129/10000 (81.29%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004971569776535034\n",
      "conv1.bias 0.0031097896862775087\n",
      "conv2.weight 8.936983533203602e-05\n",
      "conv2.bias 0.0007523061940446496\n",
      "fc1.weight 0.0003422827459871769\n",
      "fc1.bias 0.0010460934601724148\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004971569776535034\n",
      "conv1.bias 0.0031097896862775087\n",
      "conv2.weight 8.936983533203602e-05\n",
      "conv2.bias 0.0007523061940446496\n",
      "fc1.weight 0.0003422827459871769\n",
      "fc1.bias 0.0010460934601724148\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004971569776535034\n",
      "conv1.bias 0.0031097896862775087\n",
      "conv2.weight 8.936983533203602e-05\n",
      "conv2.bias 0.0007523061940446496\n",
      "fc1.weight 0.0003422827459871769\n",
      "fc1.bias 0.0010460934601724148\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004971569776535034\n",
      "conv1.bias 0.0031097896862775087\n",
      "conv2.weight 8.936983533203602e-05\n",
      "conv2.bias 0.0007523061940446496\n",
      "fc1.weight 0.0003422827459871769\n",
      "fc1.bias 0.0010460934601724148\n",
      "\n",
      "Test set: Average loss: 1.8968 \n",
      "Accuracy: 9248/10000 (92.48%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003281940147280693\n",
      "conv1.bias 0.0032963056582957506\n",
      "conv2.weight 0.00015704911202192308\n",
      "conv2.bias 0.001047304249368608\n",
      "fc1.weight 0.00020125405862927436\n",
      "fc1.bias 0.0014446954242885112\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003281940147280693\n",
      "conv1.bias 0.0032963056582957506\n",
      "conv2.weight 0.00015704911202192308\n",
      "conv2.bias 0.001047304249368608\n",
      "fc1.weight 0.00020125405862927436\n",
      "fc1.bias 0.0014446954242885112\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003281940147280693\n",
      "conv1.bias 0.0032963056582957506\n",
      "conv2.weight 0.00015704911202192308\n",
      "conv2.bias 0.001047304249368608\n",
      "fc1.weight 0.00020125405862927436\n",
      "fc1.bias 0.0014446954242885112\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003281940147280693\n",
      "conv1.bias 0.0032963056582957506\n",
      "conv2.weight 0.00015704911202192308\n",
      "conv2.bias 0.001047304249368608\n",
      "fc1.weight 0.00020125405862927436\n",
      "fc1.bias 0.0014446954242885112\n",
      "\n",
      "Test set: Average loss: 1.9557 \n",
      "Accuracy: 8602/10000 (86.02%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003532516583800316\n",
      "conv1.bias 0.003537689568474889\n",
      "conv2.weight 0.00010950958356261254\n",
      "conv2.bias 0.0008520150440745056\n",
      "fc1.weight 0.00021432670764625071\n",
      "fc1.bias 0.0011363737285137177\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003532516583800316\n",
      "conv1.bias 0.003537689568474889\n",
      "conv2.weight 0.00010950958356261254\n",
      "conv2.bias 0.0008520150440745056\n",
      "fc1.weight 0.00021432670764625071\n",
      "fc1.bias 0.0011363737285137177\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003532516583800316\n",
      "conv1.bias 0.003537689568474889\n",
      "conv2.weight 0.00010950958356261254\n",
      "conv2.bias 0.0008520150440745056\n",
      "fc1.weight 0.00021432670764625071\n",
      "fc1.bias 0.0011363737285137177\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003532516583800316\n",
      "conv1.bias 0.003537689568474889\n",
      "conv2.weight 0.00010950958356261254\n",
      "conv2.bias 0.0008520150440745056\n",
      "fc1.weight 0.00021432670764625071\n",
      "fc1.bias 0.0011363737285137177\n",
      "\n",
      "Test set: Average loss: 1.8894 \n",
      "Accuracy: 8595/10000 (85.95%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005483068525791168\n",
      "conv1.bias 0.003274529241025448\n",
      "conv2.weight 8.239713497459888e-05\n",
      "conv2.bias 0.000764074211474508\n",
      "fc1.weight 0.0003121321788057685\n",
      "fc1.bias 0.0012662624940276146\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005483068525791168\n",
      "conv1.bias 0.003274529241025448\n",
      "conv2.weight 8.239713497459888e-05\n",
      "conv2.bias 0.000764074211474508\n",
      "fc1.weight 0.0003121321788057685\n",
      "fc1.bias 0.0012662624940276146\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005483068525791168\n",
      "conv1.bias 0.003274529241025448\n",
      "conv2.weight 8.239713497459888e-05\n",
      "conv2.bias 0.000764074211474508\n",
      "fc1.weight 0.0003121321788057685\n",
      "fc1.bias 0.0012662624940276146\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005483068525791168\n",
      "conv1.bias 0.003274529241025448\n",
      "conv2.weight 8.239713497459888e-05\n",
      "conv2.bias 0.000764074211474508\n",
      "fc1.weight 0.0003121321788057685\n",
      "fc1.bias 0.0012662624940276146\n",
      "\n",
      "Test set: Average loss: 1.8948 \n",
      "Accuracy: 8631/10000 (86.31%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00039193667471408843\n",
      "conv1.bias 0.004615697078406811\n",
      "conv2.weight 0.00011649509891867638\n",
      "conv2.bias 0.0009621424833312631\n",
      "fc1.weight 0.0001753749093040824\n",
      "fc1.bias 0.0011951666325330733\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00039193667471408843\n",
      "conv1.bias 0.004615697078406811\n",
      "conv2.weight 0.00011649509891867638\n",
      "conv2.bias 0.0009621424833312631\n",
      "fc1.weight 0.0001753749093040824\n",
      "fc1.bias 0.0011951666325330733\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00039193667471408843\n",
      "conv1.bias 0.004615697078406811\n",
      "conv2.weight 0.00011649509891867638\n",
      "conv2.bias 0.0009621424833312631\n",
      "fc1.weight 0.0001753749093040824\n",
      "fc1.bias 0.0011951666325330733\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00039193667471408843\n",
      "conv1.bias 0.004615697078406811\n",
      "conv2.weight 0.00011649509891867638\n",
      "conv2.bias 0.0009621424833312631\n",
      "fc1.weight 0.0001753749093040824\n",
      "fc1.bias 0.0011951666325330733\n",
      "\n",
      "Test set: Average loss: 1.9099 \n",
      "Accuracy: 8619/10000 (86.19%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005523834377527237\n",
      "conv1.bias 0.003994946368038654\n",
      "conv2.weight 7.580073084682226e-05\n",
      "conv2.bias 0.0007481795037165284\n",
      "fc1.weight 0.00024499776773154733\n",
      "fc1.bias 0.0011872490867972374\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005523834377527237\n",
      "conv1.bias 0.003994946368038654\n",
      "conv2.weight 7.580073084682226e-05\n",
      "conv2.bias 0.0007481795037165284\n",
      "fc1.weight 0.00024499776773154733\n",
      "fc1.bias 0.0011872490867972374\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005523834377527237\n",
      "conv1.bias 0.003994946368038654\n",
      "conv2.weight 7.580073084682226e-05\n",
      "conv2.bias 0.0007481795037165284\n",
      "fc1.weight 0.00024499776773154733\n",
      "fc1.bias 0.0011872490867972374\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005523834377527237\n",
      "conv1.bias 0.003994946368038654\n",
      "conv2.weight 7.580073084682226e-05\n",
      "conv2.bias 0.0007481795037165284\n",
      "fc1.weight 0.00024499776773154733\n",
      "fc1.bias 0.0011872490867972374\n",
      "\n",
      "Test set: Average loss: 1.9816 \n",
      "Accuracy: 8610/10000 (86.10%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00030433535575866697\n",
      "conv1.bias 0.0033291312865912914\n",
      "conv2.weight 0.00015937289223074913\n",
      "conv2.bias 0.000965372659265995\n",
      "fc1.weight 0.00022205084096640348\n",
      "fc1.bias 0.0013995864428579807\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00030433535575866697\n",
      "conv1.bias 0.0033291312865912914\n",
      "conv2.weight 0.00015937289223074913\n",
      "conv2.bias 0.000965372659265995\n",
      "fc1.weight 0.00022205084096640348\n",
      "fc1.bias 0.0013995864428579807\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00030433535575866697\n",
      "conv1.bias 0.0033291312865912914\n",
      "conv2.weight 0.00015937289223074913\n",
      "conv2.bias 0.000965372659265995\n",
      "fc1.weight 0.00022205084096640348\n",
      "fc1.bias 0.0013995864428579807\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00030433535575866697\n",
      "conv1.bias 0.0033291312865912914\n",
      "conv2.weight 0.00015937289223074913\n",
      "conv2.bias 0.000965372659265995\n",
      "fc1.weight 0.00022205084096640348\n",
      "fc1.bias 0.0013995864428579807\n",
      "\n",
      "Test set: Average loss: 1.8410 \n",
      "Accuracy: 9388/10000 (93.88%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0007135285437107086\n",
      "conv1.bias 0.0032291412353515625\n",
      "conv2.weight 6.942345295101405e-05\n",
      "conv2.bias 0.0007319310680031776\n",
      "fc1.weight 0.00019547506235539913\n",
      "fc1.bias 0.0018359996378421784\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0007135285437107086\n",
      "conv1.bias 0.0032291412353515625\n",
      "conv2.weight 6.942345295101405e-05\n",
      "conv2.bias 0.0007319310680031776\n",
      "fc1.weight 0.00019547506235539913\n",
      "fc1.bias 0.0018359996378421784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected users: [0 1]\n",
      "conv1.weight 0.0007135285437107086\n",
      "conv1.bias 0.0032291412353515625\n",
      "conv2.weight 6.942345295101405e-05\n",
      "conv2.bias 0.0007319310680031776\n",
      "fc1.weight 0.00019547506235539913\n",
      "fc1.bias 0.0018359996378421784\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0007135285437107086\n",
      "conv1.bias 0.0032291412353515625\n",
      "conv2.weight 6.942345295101405e-05\n",
      "conv2.bias 0.0007319310680031776\n",
      "fc1.weight 0.00019547506235539913\n",
      "fc1.bias 0.0018359996378421784\n",
      "\n",
      "Test set: Average loss: 1.9497 \n",
      "Accuracy: 8407/10000 (84.07%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006416166573762894\n",
      "conv1.bias 0.0029078922234475613\n",
      "conv2.weight 0.00011374538764357566\n",
      "conv2.bias 0.0007573042530566454\n",
      "fc1.weight 0.00032149199396371844\n",
      "fc1.bias 0.001053486578166485\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006416166573762894\n",
      "conv1.bias 0.0029078922234475613\n",
      "conv2.weight 0.00011374538764357566\n",
      "conv2.bias 0.0007573042530566454\n",
      "fc1.weight 0.00032149199396371844\n",
      "fc1.bias 0.001053486578166485\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006416166573762894\n",
      "conv1.bias 0.0029078922234475613\n",
      "conv2.weight 0.00011374538764357566\n",
      "conv2.bias 0.0007573042530566454\n",
      "fc1.weight 0.00032149199396371844\n",
      "fc1.bias 0.001053486578166485\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006416166573762894\n",
      "conv1.bias 0.0029078922234475613\n",
      "conv2.weight 0.00011374538764357566\n",
      "conv2.bias 0.0007573042530566454\n",
      "fc1.weight 0.00032149199396371844\n",
      "fc1.bias 0.001053486578166485\n",
      "\n",
      "Test set: Average loss: 1.9179 \n",
      "Accuracy: 9211/10000 (92.11%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00027791198343038556\n",
      "conv1.bias 0.0026392959989607334\n",
      "conv2.weight 0.0002365901879966259\n",
      "conv2.bias 0.0011560367420315742\n",
      "fc1.weight 0.0002164410427212715\n",
      "fc1.bias 0.0015232597477734088\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00027791198343038556\n",
      "conv1.bias 0.0026392959989607334\n",
      "conv2.weight 0.0002365901879966259\n",
      "conv2.bias 0.0011560367420315742\n",
      "fc1.weight 0.0002164410427212715\n",
      "fc1.bias 0.0015232597477734088\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00027791198343038556\n",
      "conv1.bias 0.0026392959989607334\n",
      "conv2.weight 0.0002365901879966259\n",
      "conv2.bias 0.0011560367420315742\n",
      "fc1.weight 0.0002164410427212715\n",
      "fc1.bias 0.0015232597477734088\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00027791198343038556\n",
      "conv1.bias 0.0026392959989607334\n",
      "conv2.weight 0.0002365901879966259\n",
      "conv2.bias 0.0011560367420315742\n",
      "fc1.weight 0.0002164410427212715\n",
      "fc1.bias 0.0015232597477734088\n",
      "\n",
      "Test set: Average loss: 1.8593 \n",
      "Accuracy: 9395/10000 (93.95%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003419051319360733\n",
      "conv1.bias 0.0026893778704106808\n",
      "conv2.weight 0.00014690709300339223\n",
      "conv2.bias 0.0009991077240556479\n",
      "fc1.weight 0.0002584606176242232\n",
      "fc1.bias 0.0011513696052134037\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003419051319360733\n",
      "conv1.bias 0.0026893778704106808\n",
      "conv2.weight 0.00014690709300339223\n",
      "conv2.bias 0.0009991077240556479\n",
      "fc1.weight 0.0002584606176242232\n",
      "fc1.bias 0.0011513696052134037\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003419051319360733\n",
      "conv1.bias 0.0026893778704106808\n",
      "conv2.weight 0.00014690709300339223\n",
      "conv2.bias 0.0009991077240556479\n",
      "fc1.weight 0.0002584606176242232\n",
      "fc1.bias 0.0011513696052134037\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003419051319360733\n",
      "conv1.bias 0.0026893778704106808\n",
      "conv2.weight 0.00014690709300339223\n",
      "conv2.bias 0.0009991077240556479\n",
      "fc1.weight 0.0002584606176242232\n",
      "fc1.bias 0.0011513696052134037\n",
      "\n",
      "Test set: Average loss: 2.0157 \n",
      "Accuracy: 8821/10000 (88.21%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0002735299617052078\n",
      "conv1.bias 0.0020054103806614876\n",
      "conv2.weight 0.00018570663407444955\n",
      "conv2.bias 0.0010594513732939959\n",
      "fc1.weight 0.0001989952754229307\n",
      "fc1.bias 0.0011501271277666093\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0002735299617052078\n",
      "conv1.bias 0.0020054103806614876\n",
      "conv2.weight 0.00018570663407444955\n",
      "conv2.bias 0.0010594513732939959\n",
      "fc1.weight 0.0001989952754229307\n",
      "fc1.bias 0.0011501271277666093\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0002735299617052078\n",
      "conv1.bias 0.0020054103806614876\n",
      "conv2.weight 0.00018570663407444955\n",
      "conv2.bias 0.0010594513732939959\n",
      "fc1.weight 0.0001989952754229307\n",
      "fc1.bias 0.0011501271277666093\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0002735299617052078\n",
      "conv1.bias 0.0020054103806614876\n",
      "conv2.weight 0.00018570663407444955\n",
      "conv2.bias 0.0010594513732939959\n",
      "fc1.weight 0.0001989952754229307\n",
      "fc1.bias 0.0011501271277666093\n",
      "\n",
      "Test set: Average loss: 1.8272 \n",
      "Accuracy: 9444/10000 (94.44%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004772905260324478\n",
      "conv1.bias 0.0032640015706419945\n",
      "conv2.weight 9.902318008244038e-05\n",
      "conv2.bias 0.0010244050063192844\n",
      "fc1.weight 0.00017495753709226847\n",
      "fc1.bias 0.0014092027209699153\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004772905260324478\n",
      "conv1.bias 0.0032640015706419945\n",
      "conv2.weight 9.902318008244038e-05\n",
      "conv2.bias 0.0010244050063192844\n",
      "fc1.weight 0.00017495753709226847\n",
      "fc1.bias 0.0014092027209699153\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004772905260324478\n",
      "conv1.bias 0.0032640015706419945\n",
      "conv2.weight 9.902318008244038e-05\n",
      "conv2.bias 0.0010244050063192844\n",
      "fc1.weight 0.00017495753709226847\n",
      "fc1.bias 0.0014092027209699153\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004772905260324478\n",
      "conv1.bias 0.0032640015706419945\n",
      "conv2.weight 9.902318008244038e-05\n",
      "conv2.bias 0.0010244050063192844\n",
      "fc1.weight 0.00017495753709226847\n",
      "fc1.bias 0.0014092027209699153\n",
      "\n",
      "Test set: Average loss: 1.8799 \n",
      "Accuracy: 9367/10000 (93.67%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00031136831268668174\n",
      "conv1.bias 0.0032651678193360567\n",
      "conv2.weight 0.00010908078402280808\n",
      "conv2.bias 0.0010238984832540154\n",
      "fc1.weight 0.0002675898605957627\n",
      "fc1.bias 0.0010044170543551446\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00031136831268668174\n",
      "conv1.bias 0.0032651678193360567\n",
      "conv2.weight 0.00010908078402280808\n",
      "conv2.bias 0.0010238984832540154\n",
      "fc1.weight 0.0002675898605957627\n",
      "fc1.bias 0.0010044170543551446\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00031136831268668174\n",
      "conv1.bias 0.0032651678193360567\n",
      "conv2.weight 0.00010908078402280808\n",
      "conv2.bias 0.0010238984832540154\n",
      "fc1.weight 0.0002675898605957627\n",
      "fc1.bias 0.0010044170543551446\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00031136831268668174\n",
      "conv1.bias 0.0032651678193360567\n",
      "conv2.weight 0.00010908078402280808\n",
      "conv2.bias 0.0010238984832540154\n",
      "fc1.weight 0.0002675898605957627\n",
      "fc1.bias 0.0010044170543551446\n",
      "\n",
      "Test set: Average loss: 1.9540 \n",
      "Accuracy: 9329/10000 (93.29%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0002682149037718773\n",
      "conv1.bias 0.0025732875801622868\n",
      "conv2.weight 0.00015457506291568279\n",
      "conv2.bias 0.001025776844471693\n",
      "fc1.weight 0.0002590687945485115\n",
      "fc1.bias 0.0009579159319400788\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0002682149037718773\n",
      "conv1.bias 0.0025732875801622868\n",
      "conv2.weight 0.00015457506291568279\n",
      "conv2.bias 0.001025776844471693\n",
      "fc1.weight 0.0002590687945485115\n",
      "fc1.bias 0.0009579159319400788\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0002682149037718773\n",
      "conv1.bias 0.0025732875801622868\n",
      "conv2.weight 0.00015457506291568279\n",
      "conv2.bias 0.001025776844471693\n",
      "fc1.weight 0.0002590687945485115\n",
      "fc1.bias 0.0009579159319400788\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0002682149037718773\n",
      "conv1.bias 0.0025732875801622868\n",
      "conv2.weight 0.00015457506291568279\n",
      "conv2.bias 0.001025776844471693\n",
      "fc1.weight 0.0002590687945485115\n",
      "fc1.bias 0.0009579159319400788\n",
      "\n",
      "Test set: Average loss: 1.9683 \n",
      "Accuracy: 9376/10000 (93.76%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00031362708657979967\n",
      "conv1.bias 0.004515509121119976\n",
      "conv2.weight 0.00010814109817147255\n",
      "conv2.bias 0.0010832122061401606\n",
      "fc1.weight 0.00019205603748559952\n",
      "fc1.bias 0.000659114308655262\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00031362708657979967\n",
      "conv1.bias 0.004515509121119976\n",
      "conv2.weight 0.00010814109817147255\n",
      "conv2.bias 0.0010832122061401606\n",
      "fc1.weight 0.00019205603748559952\n",
      "fc1.bias 0.000659114308655262\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00031362708657979967\n",
      "conv1.bias 0.004515509121119976\n",
      "conv2.weight 0.00010814109817147255\n",
      "conv2.bias 0.0010832122061401606\n",
      "fc1.weight 0.00019205603748559952\n",
      "fc1.bias 0.000659114308655262\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00031362708657979967\n",
      "conv1.bias 0.004515509121119976\n",
      "conv2.weight 0.00010814109817147255\n",
      "conv2.bias 0.0010832122061401606\n",
      "fc1.weight 0.00019205603748559952\n",
      "fc1.bias 0.000659114308655262\n",
      "\n",
      "Test set: Average loss: 1.8773 \n",
      "Accuracy: 9370/10000 (93.70%)\n",
      "\n",
      "##########################################\n",
      "###### 2 -th Trial!! ###########\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.01281104326248169\n",
      "conv1.bias 0.011860298924148083\n",
      "conv2.weight 0.00041620325297117236\n",
      "conv2.bias 0.00043111899867653847\n",
      "fc1.weight 0.0003296931041404605\n",
      "fc1.bias 0.0003290711436420679\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.01281104326248169\n",
      "conv1.bias 0.011860298924148083\n",
      "conv2.weight 0.00041620325297117236\n",
      "conv2.bias 0.00043111899867653847\n",
      "fc1.weight 0.0003296931041404605\n",
      "fc1.bias 0.0003290711436420679\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.01281104326248169\n",
      "conv1.bias 0.011860298924148083\n",
      "conv2.weight 0.00041620325297117236\n",
      "conv2.bias 0.00043111899867653847\n",
      "fc1.weight 0.0003296931041404605\n",
      "fc1.bias 0.0003290711436420679\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.01281104326248169\n",
      "conv1.bias 0.011860298924148083\n",
      "conv2.weight 0.00041620325297117236\n",
      "conv2.bias 0.00043111899867653847\n",
      "fc1.weight 0.0003296931041404605\n",
      "fc1.bias 0.0003290711436420679\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004153008386492729\n",
      "conv1.bias 0.0016916211461648345\n",
      "conv2.weight 0.0002369132824242115\n",
      "conv2.bias 0.0004198955721221864\n",
      "fc1.weight 4.914297023788095e-05\n",
      "fc1.bias 0.00023292014375329018\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004153008386492729\n",
      "conv1.bias 0.0016916211461648345\n",
      "conv2.weight 0.0002369132824242115\n",
      "conv2.bias 0.0004198955721221864\n",
      "fc1.weight 4.914297023788095e-05\n",
      "fc1.bias 0.00023292014375329018\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004153008386492729\n",
      "conv1.bias 0.0016916211461648345\n",
      "conv2.weight 0.0002369132824242115\n",
      "conv2.bias 0.0004198955721221864\n",
      "fc1.weight 4.914297023788095e-05\n",
      "fc1.bias 0.00023292014375329018\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004153008386492729\n",
      "conv1.bias 0.0016916211461648345\n",
      "conv2.weight 0.0002369132824242115\n",
      "conv2.bias 0.0004198955721221864\n",
      "fc1.weight 4.914297023788095e-05\n",
      "fc1.bias 0.00023292014375329018\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1832/10000 (18.32%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 8.107580244541168e-05\n",
      "conv1.bias 0.003542958991602063\n",
      "conv2.weight 4.800538066774607e-05\n",
      "conv2.bias 0.0013817471917718649\n",
      "fc1.weight 3.2195815583691e-05\n",
      "fc1.bias 0.000467495433986187\n",
      "selected users: [0 1]\n",
      "conv1.weight 8.107580244541168e-05\n",
      "conv1.bias 0.003542958991602063\n",
      "conv2.weight 4.800538066774607e-05\n",
      "conv2.bias 0.0013817471917718649\n",
      "fc1.weight 3.2195815583691e-05\n",
      "fc1.bias 0.000467495433986187\n",
      "selected users: [0 1]\n",
      "conv1.weight 8.107580244541168e-05\n",
      "conv1.bias 0.003542958991602063\n",
      "conv2.weight 4.800538066774607e-05\n",
      "conv2.bias 0.0013817471917718649\n",
      "fc1.weight 3.2195815583691e-05\n",
      "fc1.bias 0.000467495433986187\n",
      "selected users: [0 1]\n",
      "conv1.weight 8.107580244541168e-05\n",
      "conv1.bias 0.003542958991602063\n",
      "conv2.weight 4.800538066774607e-05\n",
      "conv2.bias 0.0013817471917718649\n",
      "fc1.weight 3.2195815583691e-05\n",
      "fc1.bias 0.000467495433986187\n",
      "\n",
      "Test set: Average loss: 2.2871 \n",
      "Accuracy: 4286/10000 (42.86%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0001711485907435417\n",
      "conv1.bias 0.0026164816226810217\n",
      "conv2.weight 6.82000070810318e-05\n",
      "conv2.bias 0.0013798500876873732\n",
      "fc1.weight 0.0001527558546513319\n",
      "fc1.bias 0.0005332008004188538\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0001711485907435417\n",
      "conv1.bias 0.0026164816226810217\n",
      "conv2.weight 6.82000070810318e-05\n",
      "conv2.bias 0.0013798500876873732\n",
      "fc1.weight 0.0001527558546513319\n",
      "fc1.bias 0.0005332008004188538\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0001711485907435417\n",
      "conv1.bias 0.0026164816226810217\n",
      "conv2.weight 6.82000070810318e-05\n",
      "conv2.bias 0.0013798500876873732\n",
      "fc1.weight 0.0001527558546513319\n",
      "fc1.bias 0.0005332008004188538\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0001711485907435417\n",
      "conv1.bias 0.0026164816226810217\n",
      "conv2.weight 6.82000070810318e-05\n",
      "conv2.bias 0.0013798500876873732\n",
      "fc1.weight 0.0001527558546513319\n",
      "fc1.bias 0.0005332008004188538\n",
      "\n",
      "Test set: Average loss: 2.2812 \n",
      "Accuracy: 4356/10000 (43.56%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00020960506051778793\n",
      "conv1.bias 0.0019997803028672934\n",
      "conv2.weight 4.933140706270933e-05\n",
      "conv2.bias 0.0008874821942299604\n",
      "fc1.weight 0.00014993904624134303\n",
      "fc1.bias 0.0007015001960098743\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00020960506051778793\n",
      "conv1.bias 0.0019997803028672934\n",
      "conv2.weight 4.933140706270933e-05\n",
      "conv2.bias 0.0008874821942299604\n",
      "fc1.weight 0.00014993904624134303\n",
      "fc1.bias 0.0007015001960098743\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00020960506051778793\n",
      "conv1.bias 0.0019997803028672934\n",
      "conv2.weight 4.933140706270933e-05\n",
      "conv2.bias 0.0008874821942299604\n",
      "fc1.weight 0.00014993904624134303\n",
      "fc1.bias 0.0007015001960098743\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00020960506051778793\n",
      "conv1.bias 0.0019997803028672934\n",
      "conv2.weight 4.933140706270933e-05\n",
      "conv2.bias 0.0008874821942299604\n",
      "fc1.weight 0.00014993904624134303\n",
      "fc1.bias 0.0007015001960098743\n",
      "\n",
      "Test set: Average loss: 2.1824 \n",
      "Accuracy: 4978/10000 (49.78%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0007036937773227691\n",
      "conv1.bias 0.0009381314739584923\n",
      "conv2.weight 0.0001852164790034294\n",
      "conv2.bias 0.0009013002272695303\n",
      "fc1.weight 0.0003274166490882635\n",
      "fc1.bias 0.002086150459945202\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0007036937773227691\n",
      "conv1.bias 0.0009381314739584923\n",
      "conv2.weight 0.0001852164790034294\n",
      "conv2.bias 0.0009013002272695303\n",
      "fc1.weight 0.0003274166490882635\n",
      "fc1.bias 0.002086150459945202\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0007036937773227691\n",
      "conv1.bias 0.0009381314739584923\n",
      "conv2.weight 0.0001852164790034294\n",
      "conv2.bias 0.0009013002272695303\n",
      "fc1.weight 0.0003274166490882635\n",
      "fc1.bias 0.002086150459945202\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0007036937773227691\n",
      "conv1.bias 0.0009381314739584923\n",
      "conv2.weight 0.0001852164790034294\n",
      "conv2.bias 0.0009013002272695303\n",
      "fc1.weight 0.0003274166490882635\n",
      "fc1.bias 0.002086150459945202\n",
      "\n",
      "Test set: Average loss: 2.0892 \n",
      "Accuracy: 6615/10000 (66.15%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004316818341612816\n",
      "conv1.bias 0.002723958808928728\n",
      "conv2.weight 8.766606450080872e-05\n",
      "conv2.bias 0.0011462996480986476\n",
      "fc1.weight 0.00014539452968165278\n",
      "fc1.bias 0.003071093559265137\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004316818341612816\n",
      "conv1.bias 0.002723958808928728\n",
      "conv2.weight 8.766606450080872e-05\n",
      "conv2.bias 0.0011462996480986476\n",
      "fc1.weight 0.00014539452968165278\n",
      "fc1.bias 0.003071093559265137\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004316818341612816\n",
      "conv1.bias 0.002723958808928728\n",
      "conv2.weight 8.766606450080872e-05\n",
      "conv2.bias 0.0011462996480986476\n",
      "fc1.weight 0.00014539452968165278\n",
      "fc1.bias 0.003071093559265137\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004316818341612816\n",
      "conv1.bias 0.002723958808928728\n",
      "conv2.weight 8.766606450080872e-05\n",
      "conv2.bias 0.0011462996480986476\n",
      "fc1.weight 0.00014539452968165278\n",
      "fc1.bias 0.003071093559265137\n",
      "\n",
      "Test set: Average loss: 2.1094 \n",
      "Accuracy: 6522/10000 (65.22%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005329415947198868\n",
      "conv1.bias 0.0034638098441064358\n",
      "conv2.weight 4.853057209402323e-05\n",
      "conv2.bias 0.0008306216914206743\n",
      "fc1.weight 0.00025555775500833987\n",
      "fc1.bias 0.0023148754611611367\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005329415947198868\n",
      "conv1.bias 0.0034638098441064358\n",
      "conv2.weight 4.853057209402323e-05\n",
      "conv2.bias 0.0008306216914206743\n",
      "fc1.weight 0.00025555775500833987\n",
      "fc1.bias 0.0023148754611611367\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005329415947198868\n",
      "conv1.bias 0.0034638098441064358\n",
      "conv2.weight 4.853057209402323e-05\n",
      "conv2.bias 0.0008306216914206743\n",
      "fc1.weight 0.00025555775500833987\n",
      "fc1.bias 0.0023148754611611367\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005329415947198868\n",
      "conv1.bias 0.0034638098441064358\n",
      "conv2.weight 4.853057209402323e-05\n",
      "conv2.bias 0.0008306216914206743\n",
      "fc1.weight 0.00025555775500833987\n",
      "fc1.bias 0.0023148754611611367\n",
      "\n",
      "Test set: Average loss: 2.1289 \n",
      "Accuracy: 6769/10000 (67.69%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00030313294380903246\n",
      "conv1.bias 0.0038589020259678364\n",
      "conv2.weight 9.752039797604084e-05\n",
      "conv2.bias 0.0008739977492950857\n",
      "fc1.weight 0.00013089206768199802\n",
      "fc1.bias 0.0028346523642539976\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00030313294380903246\n",
      "conv1.bias 0.0038589020259678364\n",
      "conv2.weight 9.752039797604084e-05\n",
      "conv2.bias 0.0008739977492950857\n",
      "fc1.weight 0.00013089206768199802\n",
      "fc1.bias 0.0028346523642539976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected users: [0 1]\n",
      "conv1.weight 0.00030313294380903246\n",
      "conv1.bias 0.0038589020259678364\n",
      "conv2.weight 9.752039797604084e-05\n",
      "conv2.bias 0.0008739977492950857\n",
      "fc1.weight 0.00013089206768199802\n",
      "fc1.bias 0.0028346523642539976\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00030313294380903246\n",
      "conv1.bias 0.0038589020259678364\n",
      "conv2.weight 9.752039797604084e-05\n",
      "conv2.bias 0.0008739977492950857\n",
      "fc1.weight 0.00013089206768199802\n",
      "fc1.bias 0.0028346523642539976\n",
      "\n",
      "Test set: Average loss: 2.1191 \n",
      "Accuracy: 6176/10000 (61.76%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006282670050859452\n",
      "conv1.bias 0.002524810377508402\n",
      "conv2.weight 6.010021548718214e-05\n",
      "conv2.bias 0.0006855765241198242\n",
      "fc1.weight 0.0002732135821133852\n",
      "fc1.bias 0.00215422622859478\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006282670050859452\n",
      "conv1.bias 0.002524810377508402\n",
      "conv2.weight 6.010021548718214e-05\n",
      "conv2.bias 0.0006855765241198242\n",
      "fc1.weight 0.0002732135821133852\n",
      "fc1.bias 0.00215422622859478\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006282670050859452\n",
      "conv1.bias 0.002524810377508402\n",
      "conv2.weight 6.010021548718214e-05\n",
      "conv2.bias 0.0006855765241198242\n",
      "fc1.weight 0.0002732135821133852\n",
      "fc1.bias 0.00215422622859478\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006282670050859452\n",
      "conv1.bias 0.002524810377508402\n",
      "conv2.weight 6.010021548718214e-05\n",
      "conv2.bias 0.0006855765241198242\n",
      "fc1.weight 0.0002732135821133852\n",
      "fc1.bias 0.00215422622859478\n",
      "\n",
      "Test set: Average loss: 2.0259 \n",
      "Accuracy: 6536/10000 (65.36%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005184540152549744\n",
      "conv1.bias 0.00432219635695219\n",
      "conv2.weight 6.890194490551948e-05\n",
      "conv2.bias 0.0005542488652281463\n",
      "fc1.weight 0.0002016399521380663\n",
      "fc1.bias 0.001659025251865387\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005184540152549744\n",
      "conv1.bias 0.00432219635695219\n",
      "conv2.weight 6.890194490551948e-05\n",
      "conv2.bias 0.0005542488652281463\n",
      "fc1.weight 0.0002016399521380663\n",
      "fc1.bias 0.001659025251865387\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005184540152549744\n",
      "conv1.bias 0.00432219635695219\n",
      "conv2.weight 6.890194490551948e-05\n",
      "conv2.bias 0.0005542488652281463\n",
      "fc1.weight 0.0002016399521380663\n",
      "fc1.bias 0.001659025251865387\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005184540152549744\n",
      "conv1.bias 0.00432219635695219\n",
      "conv2.weight 6.890194490551948e-05\n",
      "conv2.bias 0.0005542488652281463\n",
      "fc1.weight 0.0002016399521380663\n",
      "fc1.bias 0.001659025251865387\n",
      "\n",
      "Test set: Average loss: 2.0508 \n",
      "Accuracy: 7300/10000 (73.00%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00029552353546023367\n",
      "conv1.bias 0.003124777227640152\n",
      "conv2.weight 0.00013296294026076793\n",
      "conv2.bias 0.0008217329741455615\n",
      "fc1.weight 0.00020642599556595088\n",
      "fc1.bias 0.0008294827304780484\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00029552353546023367\n",
      "conv1.bias 0.003124777227640152\n",
      "conv2.weight 0.00013296294026076793\n",
      "conv2.bias 0.0008217329741455615\n",
      "fc1.weight 0.00020642599556595088\n",
      "fc1.bias 0.0008294827304780484\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00029552353546023367\n",
      "conv1.bias 0.003124777227640152\n",
      "conv2.weight 0.00013296294026076793\n",
      "conv2.bias 0.0008217329741455615\n",
      "fc1.weight 0.00020642599556595088\n",
      "fc1.bias 0.0008294827304780484\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00029552353546023367\n",
      "conv1.bias 0.003124777227640152\n",
      "conv2.weight 0.00013296294026076793\n",
      "conv2.bias 0.0008217329741455615\n",
      "fc1.weight 0.00020642599556595088\n",
      "fc1.bias 0.0008294827304780484\n",
      "\n",
      "Test set: Average loss: 1.9814 \n",
      "Accuracy: 6674/10000 (66.74%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005117223039269447\n",
      "conv1.bias 0.002492801286280155\n",
      "conv2.weight 8.013869635760785e-05\n",
      "conv2.bias 0.0007095102337189019\n",
      "fc1.weight 0.00026477775536477567\n",
      "fc1.bias 0.0011450721882283687\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005117223039269447\n",
      "conv1.bias 0.002492801286280155\n",
      "conv2.weight 8.013869635760785e-05\n",
      "conv2.bias 0.0007095102337189019\n",
      "fc1.weight 0.00026477775536477567\n",
      "fc1.bias 0.0011450721882283687\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005117223039269447\n",
      "conv1.bias 0.002492801286280155\n",
      "conv2.weight 8.013869635760785e-05\n",
      "conv2.bias 0.0007095102337189019\n",
      "fc1.weight 0.00026477775536477567\n",
      "fc1.bias 0.0011450721882283687\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005117223039269447\n",
      "conv1.bias 0.002492801286280155\n",
      "conv2.weight 8.013869635760785e-05\n",
      "conv2.bias 0.0007095102337189019\n",
      "fc1.weight 0.00026477775536477567\n",
      "fc1.bias 0.0011450721882283687\n",
      "\n",
      "Test set: Average loss: 1.9371 \n",
      "Accuracy: 8453/10000 (84.53%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004197514802217483\n",
      "conv1.bias 0.004678669385612011\n",
      "conv2.weight 0.00011118086986243725\n",
      "conv2.bias 0.0008746501989662647\n",
      "fc1.weight 0.00019294766243547202\n",
      "fc1.bias 0.0011794266290962696\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004197514802217483\n",
      "conv1.bias 0.004678669385612011\n",
      "conv2.weight 0.00011118086986243725\n",
      "conv2.bias 0.0008746501989662647\n",
      "fc1.weight 0.00019294766243547202\n",
      "fc1.bias 0.0011794266290962696\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004197514802217483\n",
      "conv1.bias 0.004678669385612011\n",
      "conv2.weight 0.00011118086986243725\n",
      "conv2.bias 0.0008746501989662647\n",
      "fc1.weight 0.00019294766243547202\n",
      "fc1.bias 0.0011794266290962696\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004197514802217483\n",
      "conv1.bias 0.004678669385612011\n",
      "conv2.weight 0.00011118086986243725\n",
      "conv2.bias 0.0008746501989662647\n",
      "fc1.weight 0.00019294766243547202\n",
      "fc1.bias 0.0011794266290962696\n",
      "\n",
      "Test set: Average loss: 1.9154 \n",
      "Accuracy: 8100/10000 (81.00%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005722340568900108\n",
      "conv1.bias 0.003472115145996213\n",
      "conv2.weight 8.086327463388443e-05\n",
      "conv2.bias 0.0006651969742961228\n",
      "fc1.weight 0.00019007682567462325\n",
      "fc1.bias 0.001541224867105484\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005722340568900108\n",
      "conv1.bias 0.003472115145996213\n",
      "conv2.weight 8.086327463388443e-05\n",
      "conv2.bias 0.0006651969742961228\n",
      "fc1.weight 0.00019007682567462325\n",
      "fc1.bias 0.001541224867105484\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005722340568900108\n",
      "conv1.bias 0.003472115145996213\n",
      "conv2.weight 8.086327463388443e-05\n",
      "conv2.bias 0.0006651969742961228\n",
      "fc1.weight 0.00019007682567462325\n",
      "fc1.bias 0.001541224867105484\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005722340568900108\n",
      "conv1.bias 0.003472115145996213\n",
      "conv2.weight 8.086327463388443e-05\n",
      "conv2.bias 0.0006651969742961228\n",
      "fc1.weight 0.00019007682567462325\n",
      "fc1.bias 0.001541224867105484\n",
      "\n",
      "Test set: Average loss: 1.8649 \n",
      "Accuracy: 8673/10000 (86.73%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006002507358789443\n",
      "conv1.bias 0.004702304489910603\n",
      "conv2.weight 0.00011791044846177102\n",
      "conv2.bias 0.0007981512462720275\n",
      "fc1.weight 0.00022206036373972893\n",
      "fc1.bias 0.0015255363658070564\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006002507358789443\n",
      "conv1.bias 0.004702304489910603\n",
      "conv2.weight 0.00011791044846177102\n",
      "conv2.bias 0.0007981512462720275\n",
      "fc1.weight 0.00022206036373972893\n",
      "fc1.bias 0.0015255363658070564\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006002507358789443\n",
      "conv1.bias 0.004702304489910603\n",
      "conv2.weight 0.00011791044846177102\n",
      "conv2.bias 0.0007981512462720275\n",
      "fc1.weight 0.00022206036373972893\n",
      "fc1.bias 0.0015255363658070564\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006002507358789443\n",
      "conv1.bias 0.004702304489910603\n",
      "conv2.weight 0.00011791044846177102\n",
      "conv2.bias 0.0007981512462720275\n",
      "fc1.weight 0.00022206036373972893\n",
      "fc1.bias 0.0015255363658070564\n",
      "\n",
      "Test set: Average loss: 1.8661 \n",
      "Accuracy: 8688/10000 (86.88%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00048296395689249037\n",
      "conv1.bias 0.004910466261208057\n",
      "conv2.weight 0.00011356544680893421\n",
      "conv2.bias 0.0010011291597038507\n",
      "fc1.weight 0.00020277074072510003\n",
      "fc1.bias 0.0013455458916723727\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00048296395689249037\n",
      "conv1.bias 0.004910466261208057\n",
      "conv2.weight 0.00011356544680893421\n",
      "conv2.bias 0.0010011291597038507\n",
      "fc1.weight 0.00020277074072510003\n",
      "fc1.bias 0.0013455458916723727\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00048296395689249037\n",
      "conv1.bias 0.004910466261208057\n",
      "conv2.weight 0.00011356544680893421\n",
      "conv2.bias 0.0010011291597038507\n",
      "fc1.weight 0.00020277074072510003\n",
      "fc1.bias 0.0013455458916723727\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00048296395689249037\n",
      "conv1.bias 0.004910466261208057\n",
      "conv2.weight 0.00011356544680893421\n",
      "conv2.bias 0.0010011291597038507\n",
      "fc1.weight 0.00020277074072510003\n",
      "fc1.bias 0.0013455458916723727\n",
      "\n",
      "Test set: Average loss: 1.9844 \n",
      "Accuracy: 8713/10000 (87.13%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0002589999884366989\n",
      "conv1.bias 0.002978913951665163\n",
      "conv2.weight 0.00018400469794869424\n",
      "conv2.bias 0.0009497908758930862\n",
      "fc1.weight 0.00024617533199489117\n",
      "fc1.bias 0.0012106029316782951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected users: [0 1]\n",
      "conv1.weight 0.0002589999884366989\n",
      "conv1.bias 0.002978913951665163\n",
      "conv2.weight 0.00018400469794869424\n",
      "conv2.bias 0.0009497908758930862\n",
      "fc1.weight 0.00024617533199489117\n",
      "fc1.bias 0.0012106029316782951\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0002589999884366989\n",
      "conv1.bias 0.002978913951665163\n",
      "conv2.weight 0.00018400469794869424\n",
      "conv2.bias 0.0009497908758930862\n",
      "fc1.weight 0.00024617533199489117\n",
      "fc1.bias 0.0012106029316782951\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0002589999884366989\n",
      "conv1.bias 0.002978913951665163\n",
      "conv2.weight 0.00018400469794869424\n",
      "conv2.bias 0.0009497908758930862\n",
      "fc1.weight 0.00024617533199489117\n",
      "fc1.bias 0.0012106029316782951\n",
      "\n",
      "Test set: Average loss: 1.8625 \n",
      "Accuracy: 8668/10000 (86.68%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005687893182039261\n",
      "conv1.bias 0.004458390176296234\n",
      "conv2.weight 9.095124900341034e-05\n",
      "conv2.bias 0.0007891629356890917\n",
      "fc1.weight 0.00021207972895354033\n",
      "fc1.bias 0.0011698957532644272\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005687893182039261\n",
      "conv1.bias 0.004458390176296234\n",
      "conv2.weight 9.095124900341034e-05\n",
      "conv2.bias 0.0007891629356890917\n",
      "fc1.weight 0.00021207972895354033\n",
      "fc1.bias 0.0011698957532644272\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005687893182039261\n",
      "conv1.bias 0.004458390176296234\n",
      "conv2.weight 9.095124900341034e-05\n",
      "conv2.bias 0.0007891629356890917\n",
      "fc1.weight 0.00021207972895354033\n",
      "fc1.bias 0.0011698957532644272\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005687893182039261\n",
      "conv1.bias 0.004458390176296234\n",
      "conv2.weight 9.095124900341034e-05\n",
      "conv2.bias 0.0007891629356890917\n",
      "fc1.weight 0.00021207972895354033\n",
      "fc1.bias 0.0011698957532644272\n",
      "\n",
      "Test set: Average loss: 2.0074 \n",
      "Accuracy: 7428/10000 (74.28%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003508192300796509\n",
      "conv1.bias 0.004146657884120941\n",
      "conv2.weight 0.00011233530938625335\n",
      "conv2.bias 0.0007964060641825199\n",
      "fc1.weight 0.00021975445561110972\n",
      "fc1.bias 0.0005891304463148117\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003508192300796509\n",
      "conv1.bias 0.004146657884120941\n",
      "conv2.weight 0.00011233530938625335\n",
      "conv2.bias 0.0007964060641825199\n",
      "fc1.weight 0.00021975445561110972\n",
      "fc1.bias 0.0005891304463148117\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003508192300796509\n",
      "conv1.bias 0.004146657884120941\n",
      "conv2.weight 0.00011233530938625335\n",
      "conv2.bias 0.0007964060641825199\n",
      "fc1.weight 0.00021975445561110972\n",
      "fc1.bias 0.0005891304463148117\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003508192300796509\n",
      "conv1.bias 0.004146657884120941\n",
      "conv2.weight 0.00011233530938625335\n",
      "conv2.bias 0.0007964060641825199\n",
      "fc1.weight 0.00021975445561110972\n",
      "fc1.bias 0.0005891304463148117\n",
      "\n",
      "Test set: Average loss: 1.8378 \n",
      "Accuracy: 8713/10000 (87.13%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004947420954704285\n",
      "conv1.bias 0.004610104486346245\n",
      "conv2.weight 8.50437767803669e-05\n",
      "conv2.bias 0.0006942487088963389\n",
      "fc1.weight 0.00018601473420858382\n",
      "fc1.bias 0.0014653108082711697\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004947420954704285\n",
      "conv1.bias 0.004610104486346245\n",
      "conv2.weight 8.50437767803669e-05\n",
      "conv2.bias 0.0006942487088963389\n",
      "fc1.weight 0.00018601473420858382\n",
      "fc1.bias 0.0014653108082711697\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004947420954704285\n",
      "conv1.bias 0.004610104486346245\n",
      "conv2.weight 8.50437767803669e-05\n",
      "conv2.bias 0.0006942487088963389\n",
      "fc1.weight 0.00018601473420858382\n",
      "fc1.bias 0.0014653108082711697\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004947420954704285\n",
      "conv1.bias 0.004610104486346245\n",
      "conv2.weight 8.50437767803669e-05\n",
      "conv2.bias 0.0006942487088963389\n",
      "fc1.weight 0.00018601473420858382\n",
      "fc1.bias 0.0014653108082711697\n",
      "\n",
      "Test set: Average loss: 2.0086 \n",
      "Accuracy: 9201/10000 (92.01%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0002646557614207268\n",
      "conv1.bias 0.003314192872494459\n",
      "conv2.weight 0.00018834199756383895\n",
      "conv2.bias 0.0009711094899103045\n",
      "fc1.weight 0.0002661898732185364\n",
      "fc1.bias 0.0017448330298066139\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0002646557614207268\n",
      "conv1.bias 0.003314192872494459\n",
      "conv2.weight 0.00018834199756383895\n",
      "conv2.bias 0.0009711094899103045\n",
      "fc1.weight 0.0002661898732185364\n",
      "fc1.bias 0.0017448330298066139\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0002646557614207268\n",
      "conv1.bias 0.003314192872494459\n",
      "conv2.weight 0.00018834199756383895\n",
      "conv2.bias 0.0009711094899103045\n",
      "fc1.weight 0.0002661898732185364\n",
      "fc1.bias 0.0017448330298066139\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0002646557614207268\n",
      "conv1.bias 0.003314192872494459\n",
      "conv2.weight 0.00018834199756383895\n",
      "conv2.bias 0.0009711094899103045\n",
      "fc1.weight 0.0002661898732185364\n",
      "fc1.bias 0.0017448330298066139\n",
      "\n",
      "Test set: Average loss: 1.9423 \n",
      "Accuracy: 8199/10000 (81.99%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00033488571643829345\n",
      "conv1.bias 0.003130946308374405\n",
      "conv2.weight 0.0001341610588133335\n",
      "conv2.bias 0.001052126637659967\n",
      "fc1.weight 0.00022851759567856788\n",
      "fc1.bias 0.00109278354793787\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00033488571643829345\n",
      "conv1.bias 0.003130946308374405\n",
      "conv2.weight 0.0001341610588133335\n",
      "conv2.bias 0.001052126637659967\n",
      "fc1.weight 0.00022851759567856788\n",
      "fc1.bias 0.00109278354793787\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00033488571643829345\n",
      "conv1.bias 0.003130946308374405\n",
      "conv2.weight 0.0001341610588133335\n",
      "conv2.bias 0.001052126637659967\n",
      "fc1.weight 0.00022851759567856788\n",
      "fc1.bias 0.00109278354793787\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00033488571643829345\n",
      "conv1.bias 0.003130946308374405\n",
      "conv2.weight 0.0001341610588133335\n",
      "conv2.bias 0.001052126637659967\n",
      "fc1.weight 0.00022851759567856788\n",
      "fc1.bias 0.00109278354793787\n",
      "\n",
      "Test set: Average loss: 1.9376 \n",
      "Accuracy: 8623/10000 (86.23%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00033064723014831544\n",
      "conv1.bias 0.002673574723303318\n",
      "conv2.weight 0.00015103699639439582\n",
      "conv2.bias 0.0009880370926111937\n",
      "fc1.weight 0.00026616030372679235\n",
      "fc1.bias 0.0011638523079454898\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00033064723014831544\n",
      "conv1.bias 0.002673574723303318\n",
      "conv2.weight 0.00015103699639439582\n",
      "conv2.bias 0.0009880370926111937\n",
      "fc1.weight 0.00026616030372679235\n",
      "fc1.bias 0.0011638523079454898\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00033064723014831544\n",
      "conv1.bias 0.002673574723303318\n",
      "conv2.weight 0.00015103699639439582\n",
      "conv2.bias 0.0009880370926111937\n",
      "fc1.weight 0.00026616030372679235\n",
      "fc1.bias 0.0011638523079454898\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00033064723014831544\n",
      "conv1.bias 0.002673574723303318\n",
      "conv2.weight 0.00015103699639439582\n",
      "conv2.bias 0.0009880370926111937\n",
      "fc1.weight 0.00026616030372679235\n",
      "fc1.bias 0.0011638523079454898\n",
      "\n",
      "Test set: Average loss: 2.0350 \n",
      "Accuracy: 8153/10000 (81.53%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003174810856580734\n",
      "conv1.bias 0.0018525480991229415\n",
      "conv2.weight 0.00021510768681764602\n",
      "conv2.bias 0.0010657984530553222\n",
      "fc1.weight 0.0002256263978779316\n",
      "fc1.bias 0.0005342063028365374\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003174810856580734\n",
      "conv1.bias 0.0018525480991229415\n",
      "conv2.weight 0.00021510768681764602\n",
      "conv2.bias 0.0010657984530553222\n",
      "fc1.weight 0.0002256263978779316\n",
      "fc1.bias 0.0005342063028365374\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003174810856580734\n",
      "conv1.bias 0.0018525480991229415\n",
      "conv2.weight 0.00021510768681764602\n",
      "conv2.bias 0.0010657984530553222\n",
      "fc1.weight 0.0002256263978779316\n",
      "fc1.bias 0.0005342063028365374\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003174810856580734\n",
      "conv1.bias 0.0018525480991229415\n",
      "conv2.weight 0.00021510768681764602\n",
      "conv2.bias 0.0010657984530553222\n",
      "fc1.weight 0.0002256263978779316\n",
      "fc1.bias 0.0005342063028365374\n",
      "\n",
      "Test set: Average loss: 2.1213 \n",
      "Accuracy: 8537/10000 (85.37%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00033548437058925626\n",
      "conv1.bias 0.0016666933661326766\n",
      "conv2.weight 0.00020146256312727928\n",
      "conv2.bias 0.0011476108338683844\n",
      "fc1.weight 0.00018191401613876224\n",
      "fc1.bias 0.0013208784162998199\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00033548437058925626\n",
      "conv1.bias 0.0016666933661326766\n",
      "conv2.weight 0.00020146256312727928\n",
      "conv2.bias 0.0011476108338683844\n",
      "fc1.weight 0.00018191401613876224\n",
      "fc1.bias 0.0013208784162998199\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00033548437058925626\n",
      "conv1.bias 0.0016666933661326766\n",
      "conv2.weight 0.00020146256312727928\n",
      "conv2.bias 0.0011476108338683844\n",
      "fc1.weight 0.00018191401613876224\n",
      "fc1.bias 0.0013208784162998199\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00033548437058925626\n",
      "conv1.bias 0.0016666933661326766\n",
      "conv2.weight 0.00020146256312727928\n",
      "conv2.bias 0.0011476108338683844\n",
      "fc1.weight 0.00018191401613876224\n",
      "fc1.bias 0.0013208784162998199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.2423 \n",
      "Accuracy: 6452/10000 (64.52%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003347790986299515\n",
      "conv1.bias 0.0013663419522345066\n",
      "conv2.weight 0.00034193646162748335\n",
      "conv2.bias 0.0011341015342622995\n",
      "fc1.weight 0.00025046926457434894\n",
      "fc1.bias 0.0015469086356461048\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003347790986299515\n",
      "conv1.bias 0.0013663419522345066\n",
      "conv2.weight 0.00034193646162748335\n",
      "conv2.bias 0.0011341015342622995\n",
      "fc1.weight 0.00025046926457434894\n",
      "fc1.bias 0.0015469086356461048\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003347790986299515\n",
      "conv1.bias 0.0013663419522345066\n",
      "conv2.weight 0.00034193646162748335\n",
      "conv2.bias 0.0011341015342622995\n",
      "fc1.weight 0.00025046926457434894\n",
      "fc1.bias 0.0015469086356461048\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003347790986299515\n",
      "conv1.bias 0.0013663419522345066\n",
      "conv2.weight 0.00034193646162748335\n",
      "conv2.bias 0.0011341015342622995\n",
      "fc1.weight 0.00025046926457434894\n",
      "fc1.bias 0.0015469086356461048\n",
      "\n",
      "Test set: Average loss: 2.2149 \n",
      "Accuracy: 6957/10000 (69.57%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00028392495587468145\n",
      "conv1.bias 0.0022757069673389196\n",
      "conv2.weight 0.00010245516896247863\n",
      "conv2.bias 0.0010079265339300036\n",
      "fc1.weight 8.771845605224371e-05\n",
      "fc1.bias 0.001540719997137785\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00028392495587468145\n",
      "conv1.bias 0.0022757069673389196\n",
      "conv2.weight 0.00010245516896247863\n",
      "conv2.bias 0.0010079265339300036\n",
      "fc1.weight 8.771845605224371e-05\n",
      "fc1.bias 0.001540719997137785\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00028392495587468145\n",
      "conv1.bias 0.0022757069673389196\n",
      "conv2.weight 0.00010245516896247863\n",
      "conv2.bias 0.0010079265339300036\n",
      "fc1.weight 8.771845605224371e-05\n",
      "fc1.bias 0.001540719997137785\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00028392495587468145\n",
      "conv1.bias 0.0022757069673389196\n",
      "conv2.weight 0.00010245516896247863\n",
      "conv2.bias 0.0010079265339300036\n",
      "fc1.weight 8.771845605224371e-05\n",
      "fc1.bias 0.001540719997137785\n",
      "\n",
      "Test set: Average loss: 2.1463 \n",
      "Accuracy: 7200/10000 (72.00%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00027250256389379504\n",
      "conv1.bias 0.0022150094155222178\n",
      "conv2.weight 5.434444639831781e-05\n",
      "conv2.bias 0.0008535934030078351\n",
      "fc1.weight 0.0002131534507498145\n",
      "fc1.bias 0.0005795377772301435\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00027250256389379504\n",
      "conv1.bias 0.0022150094155222178\n",
      "conv2.weight 5.434444639831781e-05\n",
      "conv2.bias 0.0008535934030078351\n",
      "fc1.weight 0.0002131534507498145\n",
      "fc1.bias 0.0005795377772301435\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00027250256389379504\n",
      "conv1.bias 0.0022150094155222178\n",
      "conv2.weight 5.434444639831781e-05\n",
      "conv2.bias 0.0008535934030078351\n",
      "fc1.weight 0.0002131534507498145\n",
      "fc1.bias 0.0005795377772301435\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00027250256389379504\n",
      "conv1.bias 0.0022150094155222178\n",
      "conv2.weight 5.434444639831781e-05\n",
      "conv2.bias 0.0008535934030078351\n",
      "fc1.weight 0.0002131534507498145\n",
      "fc1.bias 0.0005795377772301435\n",
      "\n",
      "Test set: Average loss: 1.8868 \n",
      "Accuracy: 8591/10000 (85.91%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000591280497610569\n",
      "conv1.bias 0.0025419150479137897\n",
      "conv2.weight 9.921848773956298e-05\n",
      "conv2.bias 0.0009209388517774642\n",
      "fc1.weight 0.00014806800754740834\n",
      "fc1.bias 0.001963856630027294\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000591280497610569\n",
      "conv1.bias 0.0025419150479137897\n",
      "conv2.weight 9.921848773956298e-05\n",
      "conv2.bias 0.0009209388517774642\n",
      "fc1.weight 0.00014806800754740834\n",
      "fc1.bias 0.001963856630027294\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000591280497610569\n",
      "conv1.bias 0.0025419150479137897\n",
      "conv2.weight 9.921848773956298e-05\n",
      "conv2.bias 0.0009209388517774642\n",
      "fc1.weight 0.00014806800754740834\n",
      "fc1.bias 0.001963856630027294\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000591280497610569\n",
      "conv1.bias 0.0025419150479137897\n",
      "conv2.weight 9.921848773956298e-05\n",
      "conv2.bias 0.0009209388517774642\n",
      "fc1.weight 0.00014806800754740834\n",
      "fc1.bias 0.001963856630027294\n",
      "\n",
      "Test set: Average loss: 1.9354 \n",
      "Accuracy: 7539/10000 (75.39%)\n",
      "\n",
      "##########################################\n",
      "###### 3 -th Trial!! ###########\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.013698414564132691\n",
      "conv1.bias 0.010051611810922623\n",
      "conv2.weight 0.0004178867116570473\n",
      "conv2.bias 0.0005071382620371878\n",
      "fc1.weight 0.00032915116753429174\n",
      "fc1.bias 0.0003597036702558398\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.013698414564132691\n",
      "conv1.bias 0.010051611810922623\n",
      "conv2.weight 0.0004178867116570473\n",
      "conv2.bias 0.0005071382620371878\n",
      "fc1.weight 0.00032915116753429174\n",
      "fc1.bias 0.0003597036702558398\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.013698414564132691\n",
      "conv1.bias 0.010051611810922623\n",
      "conv2.weight 0.0004178867116570473\n",
      "conv2.bias 0.0005071382620371878\n",
      "fc1.weight 0.00032915116753429174\n",
      "fc1.bias 0.0003597036702558398\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.013698414564132691\n",
      "conv1.bias 0.010051611810922623\n",
      "conv2.weight 0.0004178867116570473\n",
      "conv2.bias 0.0005071382620371878\n",
      "fc1.weight 0.00032915116753429174\n",
      "fc1.bias 0.0003597036702558398\n",
      "\n",
      "Test set: Average loss: 2.3025 \n",
      "Accuracy: 1010/10000 (10.10%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003082704357802868\n",
      "conv1.bias 0.0015930626541376114\n",
      "conv2.weight 0.00024295862764120102\n",
      "conv2.bias 0.00043911999091506004\n",
      "fc1.weight 4.611013573594391e-05\n",
      "fc1.bias 0.0001832683221437037\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003082704357802868\n",
      "conv1.bias 0.0015930626541376114\n",
      "conv2.weight 0.00024295862764120102\n",
      "conv2.bias 0.00043911999091506004\n",
      "fc1.weight 4.611013573594391e-05\n",
      "fc1.bias 0.0001832683221437037\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003082704357802868\n",
      "conv1.bias 0.0015930626541376114\n",
      "conv2.weight 0.00024295862764120102\n",
      "conv2.bias 0.00043911999091506004\n",
      "fc1.weight 4.611013573594391e-05\n",
      "fc1.bias 0.0001832683221437037\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003082704357802868\n",
      "conv1.bias 0.0015930626541376114\n",
      "conv2.weight 0.00024295862764120102\n",
      "conv2.bias 0.00043911999091506004\n",
      "fc1.weight 4.611013573594391e-05\n",
      "fc1.bias 0.0001832683221437037\n",
      "\n",
      "Test set: Average loss: 2.2990 \n",
      "Accuracy: 2163/10000 (21.63%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 8.39180126786232e-05\n",
      "conv1.bias 0.0029305508360266685\n",
      "conv2.weight 7.187733426690101e-05\n",
      "conv2.bias 0.0013409454841166735\n",
      "fc1.weight 3.890347143169493e-05\n",
      "fc1.bias 0.0004984102211892605\n",
      "selected users: [0 1]\n",
      "conv1.weight 8.39180126786232e-05\n",
      "conv1.bias 0.0029305508360266685\n",
      "conv2.weight 7.187733426690101e-05\n",
      "conv2.bias 0.0013409454841166735\n",
      "fc1.weight 3.890347143169493e-05\n",
      "fc1.bias 0.0004984102211892605\n",
      "selected users: [0 1]\n",
      "conv1.weight 8.39180126786232e-05\n",
      "conv1.bias 0.0029305508360266685\n",
      "conv2.weight 7.187733426690101e-05\n",
      "conv2.bias 0.0013409454841166735\n",
      "fc1.weight 3.890347143169493e-05\n",
      "fc1.bias 0.0004984102211892605\n",
      "selected users: [0 1]\n",
      "conv1.weight 8.39180126786232e-05\n",
      "conv1.bias 0.0029305508360266685\n",
      "conv2.weight 7.187733426690101e-05\n",
      "conv2.bias 0.0013409454841166735\n",
      "fc1.weight 3.890347143169493e-05\n",
      "fc1.bias 0.0004984102211892605\n",
      "\n",
      "Test set: Average loss: 2.3014 \n",
      "Accuracy: 1011/10000 (10.11%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00024376362562179567\n",
      "conv1.bias 0.0019116923213005066\n",
      "conv2.weight 1.4208611100912093e-05\n",
      "conv2.bias 0.0009134610299952328\n",
      "fc1.weight 0.00015143121127039194\n",
      "fc1.bias 0.0006116706877946854\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00024376362562179567\n",
      "conv1.bias 0.0019116923213005066\n",
      "conv2.weight 1.4208611100912093e-05\n",
      "conv2.bias 0.0009134610299952328\n",
      "fc1.weight 0.00015143121127039194\n",
      "fc1.bias 0.0006116706877946854\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00024376362562179567\n",
      "conv1.bias 0.0019116923213005066\n",
      "conv2.weight 1.4208611100912093e-05\n",
      "conv2.bias 0.0009134610299952328\n",
      "fc1.weight 0.00015143121127039194\n",
      "fc1.bias 0.0006116706877946854\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00024376362562179567\n",
      "conv1.bias 0.0019116923213005066\n",
      "conv2.weight 1.4208611100912093e-05\n",
      "conv2.bias 0.0009134610299952328\n",
      "fc1.weight 0.00015143121127039194\n",
      "fc1.bias 0.0006116706877946854\n",
      "\n",
      "Test set: Average loss: 2.3010 \n",
      "Accuracy: 1511/10000 (15.11%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0001311427354812622\n",
      "conv1.bias 0.0029433926101773977\n",
      "conv2.weight 5.172346718609333e-05\n",
      "conv2.bias 0.001066623255610466\n",
      "fc1.weight 5.4730335250496864e-05\n",
      "fc1.bias 0.00034317048266530037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected users: [0 1]\n",
      "conv1.weight 0.0001311427354812622\n",
      "conv1.bias 0.0029433926101773977\n",
      "conv2.weight 5.172346718609333e-05\n",
      "conv2.bias 0.001066623255610466\n",
      "fc1.weight 5.4730335250496864e-05\n",
      "fc1.bias 0.00034317048266530037\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0001311427354812622\n",
      "conv1.bias 0.0029433926101773977\n",
      "conv2.weight 5.172346718609333e-05\n",
      "conv2.bias 0.001066623255610466\n",
      "fc1.weight 5.4730335250496864e-05\n",
      "fc1.bias 0.00034317048266530037\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0001311427354812622\n",
      "conv1.bias 0.0029433926101773977\n",
      "conv2.weight 5.172346718609333e-05\n",
      "conv2.bias 0.001066623255610466\n",
      "fc1.weight 5.4730335250496864e-05\n",
      "fc1.bias 0.00034317048266530037\n",
      "\n",
      "Test set: Average loss: 2.2973 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000128854401409626\n",
      "conv1.bias 0.005274810828268528\n",
      "conv2.weight 4.950044676661491e-05\n",
      "conv2.bias 0.0014323629438877106\n",
      "fc1.weight 9.186190436594188e-05\n",
      "fc1.bias 0.0006635484285652637\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000128854401409626\n",
      "conv1.bias 0.005274810828268528\n",
      "conv2.weight 4.950044676661491e-05\n",
      "conv2.bias 0.0014323629438877106\n",
      "fc1.weight 9.186190436594188e-05\n",
      "fc1.bias 0.0006635484285652637\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000128854401409626\n",
      "conv1.bias 0.005274810828268528\n",
      "conv2.weight 4.950044676661491e-05\n",
      "conv2.bias 0.0014323629438877106\n",
      "fc1.weight 9.186190436594188e-05\n",
      "fc1.bias 0.0006635484285652637\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000128854401409626\n",
      "conv1.bias 0.005274810828268528\n",
      "conv2.weight 4.950044676661491e-05\n",
      "conv2.bias 0.0014323629438877106\n",
      "fc1.weight 9.186190436594188e-05\n",
      "fc1.bias 0.0006635484285652637\n",
      "\n",
      "Test set: Average loss: 2.3020 \n",
      "Accuracy: 1260/10000 (12.60%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004764948785305023\n",
      "conv1.bias 0.0015006097964942455\n",
      "conv2.weight 3.750578965991736e-05\n",
      "conv2.bias 0.0008518650429323316\n",
      "fc1.weight 0.00018225240055471658\n",
      "fc1.bias 0.0004890623968094587\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004764948785305023\n",
      "conv1.bias 0.0015006097964942455\n",
      "conv2.weight 3.750578965991736e-05\n",
      "conv2.bias 0.0008518650429323316\n",
      "fc1.weight 0.00018225240055471658\n",
      "fc1.bias 0.0004890623968094587\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004764948785305023\n",
      "conv1.bias 0.0015006097964942455\n",
      "conv2.weight 3.750578965991736e-05\n",
      "conv2.bias 0.0008518650429323316\n",
      "fc1.weight 0.00018225240055471658\n",
      "fc1.bias 0.0004890623968094587\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004764948785305023\n",
      "conv1.bias 0.0015006097964942455\n",
      "conv2.weight 3.750578965991736e-05\n",
      "conv2.bias 0.0008518650429323316\n",
      "fc1.weight 0.00018225240055471658\n",
      "fc1.bias 0.0004890623968094587\n",
      "\n",
      "Test set: Average loss: 2.3022 \n",
      "Accuracy: 1135/10000 (11.35%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00025474276393651964\n",
      "conv1.bias 0.0019576502963900566\n",
      "conv2.weight 9.71724558621645e-05\n",
      "conv2.bias 0.0012588633690029383\n",
      "fc1.weight 4.220074624754489e-05\n",
      "fc1.bias 0.00037652235478162764\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00025474276393651964\n",
      "conv1.bias 0.0019576502963900566\n",
      "conv2.weight 9.71724558621645e-05\n",
      "conv2.bias 0.0012588633690029383\n",
      "fc1.weight 4.220074624754489e-05\n",
      "fc1.bias 0.00037652235478162764\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00025474276393651964\n",
      "conv1.bias 0.0019576502963900566\n",
      "conv2.weight 9.71724558621645e-05\n",
      "conv2.bias 0.0012588633690029383\n",
      "fc1.weight 4.220074624754489e-05\n",
      "fc1.bias 0.00037652235478162764\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00025474276393651964\n",
      "conv1.bias 0.0019576502963900566\n",
      "conv2.weight 9.71724558621645e-05\n",
      "conv2.bias 0.0012588633690029383\n",
      "fc1.weight 4.220074624754489e-05\n",
      "fc1.bias 0.00037652235478162764\n",
      "\n",
      "Test set: Average loss: 2.2827 \n",
      "Accuracy: 4704/10000 (47.04%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00017030037939548494\n",
      "conv1.bias 0.0015959609299898148\n",
      "conv2.weight 8.157590404152871e-05\n",
      "conv2.bias 0.0014327671378850937\n",
      "fc1.weight 8.482331177219748e-05\n",
      "fc1.bias 0.0003325598547235131\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00017030037939548494\n",
      "conv1.bias 0.0015959609299898148\n",
      "conv2.weight 8.157590404152871e-05\n",
      "conv2.bias 0.0014327671378850937\n",
      "fc1.weight 8.482331177219748e-05\n",
      "fc1.bias 0.0003325598547235131\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00017030037939548494\n",
      "conv1.bias 0.0015959609299898148\n",
      "conv2.weight 8.157590404152871e-05\n",
      "conv2.bias 0.0014327671378850937\n",
      "fc1.weight 8.482331177219748e-05\n",
      "fc1.bias 0.0003325598547235131\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00017030037939548494\n",
      "conv1.bias 0.0015959609299898148\n",
      "conv2.weight 8.157590404152871e-05\n",
      "conv2.bias 0.0014327671378850937\n",
      "fc1.weight 8.482331177219748e-05\n",
      "fc1.bias 0.0003325598547235131\n",
      "\n",
      "Test set: Average loss: 2.2300 \n",
      "Accuracy: 3715/10000 (37.15%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00030628278851509096\n",
      "conv1.bias 0.0020165550522506237\n",
      "conv2.weight 5.775705445557833e-05\n",
      "conv2.bias 0.000985058257356286\n",
      "fc1.weight 0.0002579384716227651\n",
      "fc1.bias 0.0004394452087581158\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00030628278851509096\n",
      "conv1.bias 0.0020165550522506237\n",
      "conv2.weight 5.775705445557833e-05\n",
      "conv2.bias 0.000985058257356286\n",
      "fc1.weight 0.0002579384716227651\n",
      "fc1.bias 0.0004394452087581158\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00030628278851509096\n",
      "conv1.bias 0.0020165550522506237\n",
      "conv2.weight 5.775705445557833e-05\n",
      "conv2.bias 0.000985058257356286\n",
      "fc1.weight 0.0002579384716227651\n",
      "fc1.bias 0.0004394452087581158\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00030628278851509096\n",
      "conv1.bias 0.0020165550522506237\n",
      "conv2.weight 5.775705445557833e-05\n",
      "conv2.bias 0.000985058257356286\n",
      "fc1.weight 0.0002579384716227651\n",
      "fc1.bias 0.0004394452087581158\n",
      "\n",
      "Test set: Average loss: 2.1028 \n",
      "Accuracy: 6699/10000 (66.99%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003287939727306366\n",
      "conv1.bias 0.002101124031469226\n",
      "conv2.weight 6.215563509613275e-05\n",
      "conv2.bias 0.0007996584754437208\n",
      "fc1.weight 0.00020732260309159757\n",
      "fc1.bias 0.0007926728576421738\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003287939727306366\n",
      "conv1.bias 0.002101124031469226\n",
      "conv2.weight 6.215563509613275e-05\n",
      "conv2.bias 0.0007996584754437208\n",
      "fc1.weight 0.00020732260309159757\n",
      "fc1.bias 0.0007926728576421738\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003287939727306366\n",
      "conv1.bias 0.002101124031469226\n",
      "conv2.weight 6.215563509613275e-05\n",
      "conv2.bias 0.0007996584754437208\n",
      "fc1.weight 0.00020732260309159757\n",
      "fc1.bias 0.0007926728576421738\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003287939727306366\n",
      "conv1.bias 0.002101124031469226\n",
      "conv2.weight 6.215563509613275e-05\n",
      "conv2.bias 0.0007996584754437208\n",
      "fc1.weight 0.00020732260309159757\n",
      "fc1.bias 0.0007926728576421738\n",
      "\n",
      "Test set: Average loss: 1.9467 \n",
      "Accuracy: 8690/10000 (86.90%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004488292336463928\n",
      "conv1.bias 0.0033211354166269302\n",
      "conv2.weight 6.940372753888368e-05\n",
      "conv2.bias 0.0007670280756428838\n",
      "fc1.weight 0.0001925670076161623\n",
      "fc1.bias 0.0013830585405230523\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004488292336463928\n",
      "conv1.bias 0.0033211354166269302\n",
      "conv2.weight 6.940372753888368e-05\n",
      "conv2.bias 0.0007670280756428838\n",
      "fc1.weight 0.0001925670076161623\n",
      "fc1.bias 0.0013830585405230523\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004488292336463928\n",
      "conv1.bias 0.0033211354166269302\n",
      "conv2.weight 6.940372753888368e-05\n",
      "conv2.bias 0.0007670280756428838\n",
      "fc1.weight 0.0001925670076161623\n",
      "fc1.bias 0.0013830585405230523\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004488292336463928\n",
      "conv1.bias 0.0033211354166269302\n",
      "conv2.weight 6.940372753888368e-05\n",
      "conv2.bias 0.0007670280756428838\n",
      "fc1.weight 0.0001925670076161623\n",
      "fc1.bias 0.0013830585405230523\n",
      "\n",
      "Test set: Average loss: 1.9454 \n",
      "Accuracy: 8188/10000 (81.88%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003834470361471176\n",
      "conv1.bias 0.0024675598833709955\n",
      "conv2.weight 9.040429256856441e-05\n",
      "conv2.bias 0.0007489502313546836\n",
      "fc1.weight 0.0001816108589991927\n",
      "fc1.bias 0.0016260605305433273\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003834470361471176\n",
      "conv1.bias 0.0024675598833709955\n",
      "conv2.weight 9.040429256856441e-05\n",
      "conv2.bias 0.0007489502313546836\n",
      "fc1.weight 0.0001816108589991927\n",
      "fc1.bias 0.0016260605305433273\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003834470361471176\n",
      "conv1.bias 0.0024675598833709955\n",
      "conv2.weight 9.040429256856441e-05\n",
      "conv2.bias 0.0007489502313546836\n",
      "fc1.weight 0.0001816108589991927\n",
      "fc1.bias 0.0016260605305433273\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003834470361471176\n",
      "conv1.bias 0.0024675598833709955\n",
      "conv2.weight 9.040429256856441e-05\n",
      "conv2.bias 0.0007489502313546836\n",
      "fc1.weight 0.0001816108589991927\n",
      "fc1.bias 0.0016260605305433273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.8868 \n",
      "Accuracy: 9378/10000 (93.78%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00041841469705104827\n",
      "conv1.bias 0.002029910683631897\n",
      "conv2.weight 0.00015872979536652566\n",
      "conv2.bias 0.0009406895260326564\n",
      "fc1.weight 0.00015268716961145402\n",
      "fc1.bias 0.0016428548842668533\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00041841469705104827\n",
      "conv1.bias 0.002029910683631897\n",
      "conv2.weight 0.00015872979536652566\n",
      "conv2.bias 0.0009406895260326564\n",
      "fc1.weight 0.00015268716961145402\n",
      "fc1.bias 0.0016428548842668533\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00041841469705104827\n",
      "conv1.bias 0.002029910683631897\n",
      "conv2.weight 0.00015872979536652566\n",
      "conv2.bias 0.0009406895260326564\n",
      "fc1.weight 0.00015268716961145402\n",
      "fc1.bias 0.0016428548842668533\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00041841469705104827\n",
      "conv1.bias 0.002029910683631897\n",
      "conv2.weight 0.00015872979536652566\n",
      "conv2.bias 0.0009406895260326564\n",
      "fc1.weight 0.00015268716961145402\n",
      "fc1.bias 0.0016428548842668533\n",
      "\n",
      "Test set: Average loss: 1.8881 \n",
      "Accuracy: 8712/10000 (87.12%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006007156521081925\n",
      "conv1.bias 0.003040028503164649\n",
      "conv2.weight 8.268987759947777e-05\n",
      "conv2.bias 0.0008037009392865002\n",
      "fc1.weight 0.00024100528098642827\n",
      "fc1.bias 0.0013599753379821776\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006007156521081925\n",
      "conv1.bias 0.003040028503164649\n",
      "conv2.weight 8.268987759947777e-05\n",
      "conv2.bias 0.0008037009392865002\n",
      "fc1.weight 0.00024100528098642827\n",
      "fc1.bias 0.0013599753379821776\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006007156521081925\n",
      "conv1.bias 0.003040028503164649\n",
      "conv2.weight 8.268987759947777e-05\n",
      "conv2.bias 0.0008037009392865002\n",
      "fc1.weight 0.00024100528098642827\n",
      "fc1.bias 0.0013599753379821776\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006007156521081925\n",
      "conv1.bias 0.003040028503164649\n",
      "conv2.weight 8.268987759947777e-05\n",
      "conv2.bias 0.0008037009392865002\n",
      "fc1.weight 0.00024100528098642827\n",
      "fc1.bias 0.0013599753379821776\n",
      "\n",
      "Test set: Average loss: 1.8773 \n",
      "Accuracy: 9379/10000 (93.79%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004495350271463394\n",
      "conv1.bias 0.0030244006775319576\n",
      "conv2.weight 9.239883162081242e-05\n",
      "conv2.bias 0.0008313945727422833\n",
      "fc1.weight 0.0002192555693909526\n",
      "fc1.bias 0.0017233151942491532\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004495350271463394\n",
      "conv1.bias 0.0030244006775319576\n",
      "conv2.weight 9.239883162081242e-05\n",
      "conv2.bias 0.0008313945727422833\n",
      "fc1.weight 0.0002192555693909526\n",
      "fc1.bias 0.0017233151942491532\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004495350271463394\n",
      "conv1.bias 0.0030244006775319576\n",
      "conv2.weight 9.239883162081242e-05\n",
      "conv2.bias 0.0008313945727422833\n",
      "fc1.weight 0.0002192555693909526\n",
      "fc1.bias 0.0017233151942491532\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004495350271463394\n",
      "conv1.bias 0.0030244006775319576\n",
      "conv2.weight 9.239883162081242e-05\n",
      "conv2.bias 0.0008313945727422833\n",
      "fc1.weight 0.0002192555693909526\n",
      "fc1.bias 0.0017233151942491532\n",
      "\n",
      "Test set: Average loss: 1.8453 \n",
      "Accuracy: 9435/10000 (94.35%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00044877655804157255\n",
      "conv1.bias 0.004468532279133797\n",
      "conv2.weight 8.792373351752758e-05\n",
      "conv2.bias 0.0009317353251390159\n",
      "fc1.weight 0.0002509254962205887\n",
      "fc1.bias 0.0018061432987451554\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00044877655804157255\n",
      "conv1.bias 0.004468532279133797\n",
      "conv2.weight 8.792373351752758e-05\n",
      "conv2.bias 0.0009317353251390159\n",
      "fc1.weight 0.0002509254962205887\n",
      "fc1.bias 0.0018061432987451554\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00044877655804157255\n",
      "conv1.bias 0.004468532279133797\n",
      "conv2.weight 8.792373351752758e-05\n",
      "conv2.bias 0.0009317353251390159\n",
      "fc1.weight 0.0002509254962205887\n",
      "fc1.bias 0.0018061432987451554\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00044877655804157255\n",
      "conv1.bias 0.004468532279133797\n",
      "conv2.weight 8.792373351752758e-05\n",
      "conv2.bias 0.0009317353251390159\n",
      "fc1.weight 0.0002509254962205887\n",
      "fc1.bias 0.0018061432987451554\n",
      "\n",
      "Test set: Average loss: 1.9430 \n",
      "Accuracy: 7647/10000 (76.47%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004775737971067429\n",
      "conv1.bias 0.002849777229130268\n",
      "conv2.weight 0.00011324048042297363\n",
      "conv2.bias 0.0008166418992914259\n",
      "fc1.weight 0.0003040226176381111\n",
      "fc1.bias 0.0012785781174898148\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004775737971067429\n",
      "conv1.bias 0.002849777229130268\n",
      "conv2.weight 0.00011324048042297363\n",
      "conv2.bias 0.0008166418992914259\n",
      "fc1.weight 0.0003040226176381111\n",
      "fc1.bias 0.0012785781174898148\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004775737971067429\n",
      "conv1.bias 0.002849777229130268\n",
      "conv2.weight 0.00011324048042297363\n",
      "conv2.bias 0.0008166418992914259\n",
      "fc1.weight 0.0003040226176381111\n",
      "fc1.bias 0.0012785781174898148\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004775737971067429\n",
      "conv1.bias 0.002849777229130268\n",
      "conv2.weight 0.00011324048042297363\n",
      "conv2.bias 0.0008166418992914259\n",
      "fc1.weight 0.0003040226176381111\n",
      "fc1.bias 0.0012785781174898148\n",
      "\n",
      "Test set: Average loss: 1.9047 \n",
      "Accuracy: 9087/10000 (90.87%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006359488517045975\n",
      "conv1.bias 0.002510039135813713\n",
      "conv2.weight 8.879000321030616e-05\n",
      "conv2.bias 0.0007049591513350606\n",
      "fc1.weight 0.00026479579973965884\n",
      "fc1.bias 0.001579071953892708\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006359488517045975\n",
      "conv1.bias 0.002510039135813713\n",
      "conv2.weight 8.879000321030616e-05\n",
      "conv2.bias 0.0007049591513350606\n",
      "fc1.weight 0.00026479579973965884\n",
      "fc1.bias 0.001579071953892708\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006359488517045975\n",
      "conv1.bias 0.002510039135813713\n",
      "conv2.weight 8.879000321030616e-05\n",
      "conv2.bias 0.0007049591513350606\n",
      "fc1.weight 0.00026479579973965884\n",
      "fc1.bias 0.001579071953892708\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006359488517045975\n",
      "conv1.bias 0.002510039135813713\n",
      "conv2.weight 8.879000321030616e-05\n",
      "conv2.bias 0.0007049591513350606\n",
      "fc1.weight 0.00026479579973965884\n",
      "fc1.bias 0.001579071953892708\n",
      "\n",
      "Test set: Average loss: 1.8907 \n",
      "Accuracy: 8767/10000 (87.67%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00041117951273918153\n",
      "conv1.bias 0.003133766818791628\n",
      "conv2.weight 0.00014137374237179756\n",
      "conv2.bias 0.000921816797927022\n",
      "fc1.weight 0.00017737022135406734\n",
      "fc1.bias 0.0016609344631433487\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00041117951273918153\n",
      "conv1.bias 0.003133766818791628\n",
      "conv2.weight 0.00014137374237179756\n",
      "conv2.bias 0.000921816797927022\n",
      "fc1.weight 0.00017737022135406734\n",
      "fc1.bias 0.0016609344631433487\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00041117951273918153\n",
      "conv1.bias 0.003133766818791628\n",
      "conv2.weight 0.00014137374237179756\n",
      "conv2.bias 0.000921816797927022\n",
      "fc1.weight 0.00017737022135406734\n",
      "fc1.bias 0.0016609344631433487\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00041117951273918153\n",
      "conv1.bias 0.003133766818791628\n",
      "conv2.weight 0.00014137374237179756\n",
      "conv2.bias 0.000921816797927022\n",
      "fc1.weight 0.00017737022135406734\n",
      "fc1.bias 0.0016609344631433487\n",
      "\n",
      "Test set: Average loss: 1.9037 \n",
      "Accuracy: 8528/10000 (85.28%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005662151798605919\n",
      "conv1.bias 0.0021719331853091717\n",
      "conv2.weight 0.00011778158135712146\n",
      "conv2.bias 0.0007281425059773028\n",
      "fc1.weight 0.0003130874363705516\n",
      "fc1.bias 0.0015196629799902439\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005662151798605919\n",
      "conv1.bias 0.0021719331853091717\n",
      "conv2.weight 0.00011778158135712146\n",
      "conv2.bias 0.0007281425059773028\n",
      "fc1.weight 0.0003130874363705516\n",
      "fc1.bias 0.0015196629799902439\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005662151798605919\n",
      "conv1.bias 0.0021719331853091717\n",
      "conv2.weight 0.00011778158135712146\n",
      "conv2.bias 0.0007281425059773028\n",
      "fc1.weight 0.0003130874363705516\n",
      "fc1.bias 0.0015196629799902439\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005662151798605919\n",
      "conv1.bias 0.0021719331853091717\n",
      "conv2.weight 0.00011778158135712146\n",
      "conv2.bias 0.0007281425059773028\n",
      "fc1.weight 0.0003130874363705516\n",
      "fc1.bias 0.0015196629799902439\n",
      "\n",
      "Test set: Average loss: 1.8439 \n",
      "Accuracy: 9457/10000 (94.57%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00044880721718072893\n",
      "conv1.bias 0.002922002226114273\n",
      "conv2.weight 0.00015153491869568825\n",
      "conv2.bias 0.0009305431740358472\n",
      "fc1.weight 0.0001389874960295856\n",
      "fc1.bias 0.0018260741606354713\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00044880721718072893\n",
      "conv1.bias 0.002922002226114273\n",
      "conv2.weight 0.00015153491869568825\n",
      "conv2.bias 0.0009305431740358472\n",
      "fc1.weight 0.0001389874960295856\n",
      "fc1.bias 0.0018260741606354713\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00044880721718072893\n",
      "conv1.bias 0.002922002226114273\n",
      "conv2.weight 0.00015153491869568825\n",
      "conv2.bias 0.0009305431740358472\n",
      "fc1.weight 0.0001389874960295856\n",
      "fc1.bias 0.0018260741606354713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected users: [0 1]\n",
      "conv1.weight 0.00044880721718072893\n",
      "conv1.bias 0.002922002226114273\n",
      "conv2.weight 0.00015153491869568825\n",
      "conv2.bias 0.0009305431740358472\n",
      "fc1.weight 0.0001389874960295856\n",
      "fc1.bias 0.0018260741606354713\n",
      "\n",
      "Test set: Average loss: 1.9203 \n",
      "Accuracy: 8585/10000 (85.85%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003576018288731575\n",
      "conv1.bias 0.0032367040403187275\n",
      "conv2.weight 0.00011017287150025368\n",
      "conv2.bias 0.0008429470472037792\n",
      "fc1.weight 0.00026007399428635837\n",
      "fc1.bias 0.0013396444730460644\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003576018288731575\n",
      "conv1.bias 0.0032367040403187275\n",
      "conv2.weight 0.00011017287150025368\n",
      "conv2.bias 0.0008429470472037792\n",
      "fc1.weight 0.00026007399428635837\n",
      "fc1.bias 0.0013396444730460644\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003576018288731575\n",
      "conv1.bias 0.0032367040403187275\n",
      "conv2.weight 0.00011017287150025368\n",
      "conv2.bias 0.0008429470472037792\n",
      "fc1.weight 0.00026007399428635837\n",
      "fc1.bias 0.0013396444730460644\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003576018288731575\n",
      "conv1.bias 0.0032367040403187275\n",
      "conv2.weight 0.00011017287150025368\n",
      "conv2.bias 0.0008429470472037792\n",
      "fc1.weight 0.00026007399428635837\n",
      "fc1.bias 0.0013396444730460644\n",
      "\n",
      "Test set: Average loss: 1.8271 \n",
      "Accuracy: 9448/10000 (94.48%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005570133402943611\n",
      "conv1.bias 0.004331977106630802\n",
      "conv2.weight 0.00010290207341313362\n",
      "conv2.bias 0.0010606516152620316\n",
      "fc1.weight 0.00016848979284986854\n",
      "fc1.bias 0.0016659818589687347\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005570133402943611\n",
      "conv1.bias 0.004331977106630802\n",
      "conv2.weight 0.00010290207341313362\n",
      "conv2.bias 0.0010606516152620316\n",
      "fc1.weight 0.00016848979284986854\n",
      "fc1.bias 0.0016659818589687347\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005570133402943611\n",
      "conv1.bias 0.004331977106630802\n",
      "conv2.weight 0.00010290207341313362\n",
      "conv2.bias 0.0010606516152620316\n",
      "fc1.weight 0.00016848979284986854\n",
      "fc1.bias 0.0016659818589687347\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005570133402943611\n",
      "conv1.bias 0.004331977106630802\n",
      "conv2.weight 0.00010290207341313362\n",
      "conv2.bias 0.0010606516152620316\n",
      "fc1.weight 0.00016848979284986854\n",
      "fc1.bias 0.0016659818589687347\n",
      "\n",
      "Test set: Average loss: 1.8961 \n",
      "Accuracy: 9368/10000 (93.68%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00029687128961086273\n",
      "conv1.bias 0.0027532947715371847\n",
      "conv2.weight 0.00018004026263952256\n",
      "conv2.bias 0.0009177104802802205\n",
      "fc1.weight 0.0002643611282110214\n",
      "fc1.bias 0.0015520723536610603\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00029687128961086273\n",
      "conv1.bias 0.0027532947715371847\n",
      "conv2.weight 0.00018004026263952256\n",
      "conv2.bias 0.0009177104802802205\n",
      "fc1.weight 0.0002643611282110214\n",
      "fc1.bias 0.0015520723536610603\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00029687128961086273\n",
      "conv1.bias 0.0027532947715371847\n",
      "conv2.weight 0.00018004026263952256\n",
      "conv2.bias 0.0009177104802802205\n",
      "fc1.weight 0.0002643611282110214\n",
      "fc1.bias 0.0015520723536610603\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00029687128961086273\n",
      "conv1.bias 0.0027532947715371847\n",
      "conv2.weight 0.00018004026263952256\n",
      "conv2.bias 0.0009177104802802205\n",
      "fc1.weight 0.0002643611282110214\n",
      "fc1.bias 0.0015520723536610603\n",
      "\n",
      "Test set: Average loss: 1.8678 \n",
      "Accuracy: 9411/10000 (94.11%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00035143818706274033\n",
      "conv1.bias 0.0031085568480193615\n",
      "conv2.weight 0.00013611864298582077\n",
      "conv2.bias 0.0009307946311309934\n",
      "fc1.weight 0.00019640035461634397\n",
      "fc1.bias 0.0012380045838654042\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00035143818706274033\n",
      "conv1.bias 0.0031085568480193615\n",
      "conv2.weight 0.00013611864298582077\n",
      "conv2.bias 0.0009307946311309934\n",
      "fc1.weight 0.00019640035461634397\n",
      "fc1.bias 0.0012380045838654042\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00035143818706274033\n",
      "conv1.bias 0.0031085568480193615\n",
      "conv2.weight 0.00013611864298582077\n",
      "conv2.bias 0.0009307946311309934\n",
      "fc1.weight 0.00019640035461634397\n",
      "fc1.bias 0.0012380045838654042\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00035143818706274033\n",
      "conv1.bias 0.0031085568480193615\n",
      "conv2.weight 0.00013611864298582077\n",
      "conv2.bias 0.0009307946311309934\n",
      "fc1.weight 0.00019640035461634397\n",
      "fc1.bias 0.0012380045838654042\n",
      "\n",
      "Test set: Average loss: 2.0784 \n",
      "Accuracy: 7830/10000 (78.30%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00029844000935554506\n",
      "conv1.bias 0.0017796583706513047\n",
      "conv2.weight 0.00020497165620326997\n",
      "conv2.bias 0.000898041354957968\n",
      "fc1.weight 0.00027083815075457096\n",
      "fc1.bias 0.0007644000928848982\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00029844000935554506\n",
      "conv1.bias 0.0017796583706513047\n",
      "conv2.weight 0.00020497165620326997\n",
      "conv2.bias 0.000898041354957968\n",
      "fc1.weight 0.00027083815075457096\n",
      "fc1.bias 0.0007644000928848982\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00029844000935554506\n",
      "conv1.bias 0.0017796583706513047\n",
      "conv2.weight 0.00020497165620326997\n",
      "conv2.bias 0.000898041354957968\n",
      "fc1.weight 0.00027083815075457096\n",
      "fc1.bias 0.0007644000928848982\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00029844000935554506\n",
      "conv1.bias 0.0017796583706513047\n",
      "conv2.weight 0.00020497165620326997\n",
      "conv2.bias 0.000898041354957968\n",
      "fc1.weight 0.00027083815075457096\n",
      "fc1.bias 0.0007644000928848982\n",
      "\n",
      "Test set: Average loss: 1.9665 \n",
      "Accuracy: 7641/10000 (76.41%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000548638179898262\n",
      "conv1.bias 0.0036767085548490286\n",
      "conv2.weight 0.00011170375160872937\n",
      "conv2.bias 0.001049933722242713\n",
      "fc1.weight 0.00016996769700199365\n",
      "fc1.bias 0.0009460912086069583\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000548638179898262\n",
      "conv1.bias 0.0036767085548490286\n",
      "conv2.weight 0.00011170375160872937\n",
      "conv2.bias 0.001049933722242713\n",
      "fc1.weight 0.00016996769700199365\n",
      "fc1.bias 0.0009460912086069583\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000548638179898262\n",
      "conv1.bias 0.0036767085548490286\n",
      "conv2.weight 0.00011170375160872937\n",
      "conv2.bias 0.001049933722242713\n",
      "fc1.weight 0.00016996769700199365\n",
      "fc1.bias 0.0009460912086069583\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000548638179898262\n",
      "conv1.bias 0.0036767085548490286\n",
      "conv2.weight 0.00011170375160872937\n",
      "conv2.bias 0.001049933722242713\n",
      "fc1.weight 0.00016996769700199365\n",
      "fc1.bias 0.0009460912086069583\n",
      "\n",
      "Test set: Average loss: 2.1974 \n",
      "Accuracy: 7620/10000 (76.20%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00031433410942554473\n",
      "conv1.bias 0.00213475595228374\n",
      "conv2.weight 0.0002197176218032837\n",
      "conv2.bias 0.0010378392180427909\n",
      "fc1.weight 0.0002971595618873835\n",
      "fc1.bias 0.0005990198347717524\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00031433410942554473\n",
      "conv1.bias 0.00213475595228374\n",
      "conv2.weight 0.0002197176218032837\n",
      "conv2.bias 0.0010378392180427909\n",
      "fc1.weight 0.0002971595618873835\n",
      "fc1.bias 0.0005990198347717524\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00031433410942554473\n",
      "conv1.bias 0.00213475595228374\n",
      "conv2.weight 0.0002197176218032837\n",
      "conv2.bias 0.0010378392180427909\n",
      "fc1.weight 0.0002971595618873835\n",
      "fc1.bias 0.0005990198347717524\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00031433410942554473\n",
      "conv1.bias 0.00213475595228374\n",
      "conv2.weight 0.0002197176218032837\n",
      "conv2.bias 0.0010378392180427909\n",
      "fc1.weight 0.0002971595618873835\n",
      "fc1.bias 0.0005990198347717524\n",
      "\n",
      "Test set: Average loss: 1.9171 \n",
      "Accuracy: 8513/10000 (85.13%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005115517228841781\n",
      "conv1.bias 0.002162983175367117\n",
      "conv2.weight 7.163940463215113e-05\n",
      "conv2.bias 0.0007879913318902254\n",
      "fc1.weight 0.00016061024507507682\n",
      "fc1.bias 0.0014574972912669182\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005115517228841781\n",
      "conv1.bias 0.002162983175367117\n",
      "conv2.weight 7.163940463215113e-05\n",
      "conv2.bias 0.0007879913318902254\n",
      "fc1.weight 0.00016061024507507682\n",
      "fc1.bias 0.0014574972912669182\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005115517228841781\n",
      "conv1.bias 0.002162983175367117\n",
      "conv2.weight 7.163940463215113e-05\n",
      "conv2.bias 0.0007879913318902254\n",
      "fc1.weight 0.00016061024507507682\n",
      "fc1.bias 0.0014574972912669182\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005115517228841781\n",
      "conv1.bias 0.002162983175367117\n",
      "conv2.weight 7.163940463215113e-05\n",
      "conv2.bias 0.0007879913318902254\n",
      "fc1.weight 0.00016061024507507682\n",
      "fc1.bias 0.0014574972912669182\n",
      "\n",
      "Test set: Average loss: 1.9621 \n",
      "Accuracy: 9419/10000 (94.19%)\n",
      "\n",
      "##########################################\n",
      "###### 4 -th Trial!! ###########\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 1 2 15000 \n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.014086833000183105\n",
      "conv1.bias 0.017035653814673424\n",
      "conv2.weight 0.0004176490381360054\n",
      "conv2.bias 0.0003641850780695677\n",
      "fc1.weight 0.00032827453687787054\n",
      "fc1.bias 0.0002711811335757375\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.014086833000183105\n",
      "conv1.bias 0.017035653814673424\n",
      "conv2.weight 0.0004176490381360054\n",
      "conv2.bias 0.0003641850780695677\n",
      "fc1.weight 0.00032827453687787054\n",
      "fc1.bias 0.0002711811335757375\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.014086833000183105\n",
      "conv1.bias 0.017035653814673424\n",
      "conv2.weight 0.0004176490381360054\n",
      "conv2.bias 0.0003641850780695677\n",
      "fc1.weight 0.00032827453687787054\n",
      "fc1.bias 0.0002711811335757375\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.014086833000183105\n",
      "conv1.bias 0.017035653814673424\n",
      "conv2.weight 0.0004176490381360054\n",
      "conv2.bias 0.0003641850780695677\n",
      "fc1.weight 0.00032827453687787054\n",
      "fc1.bias 0.0002711811335757375\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003375473991036415\n",
      "conv1.bias 0.0020306967198848724\n",
      "conv2.weight 0.0002042088471353054\n",
      "conv2.bias 0.00036237877793610096\n",
      "fc1.weight 3.7181779043748973e-05\n",
      "fc1.bias 0.00044721313752233983\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003375473991036415\n",
      "conv1.bias 0.0020306967198848724\n",
      "conv2.weight 0.0002042088471353054\n",
      "conv2.bias 0.00036237877793610096\n",
      "fc1.weight 3.7181779043748973e-05\n",
      "fc1.bias 0.00044721313752233983\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003375473991036415\n",
      "conv1.bias 0.0020306967198848724\n",
      "conv2.weight 0.0002042088471353054\n",
      "conv2.bias 0.00036237877793610096\n",
      "fc1.weight 3.7181779043748973e-05\n",
      "fc1.bias 0.00044721313752233983\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003375473991036415\n",
      "conv1.bias 0.0020306967198848724\n",
      "conv2.weight 0.0002042088471353054\n",
      "conv2.bias 0.00036237877793610096\n",
      "fc1.weight 3.7181779043748973e-05\n",
      "fc1.bias 0.00044721313752233983\n",
      "\n",
      "Test set: Average loss: 2.3000 \n",
      "Accuracy: 2743/10000 (27.43%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 7.395678665488958e-05\n",
      "conv1.bias 0.003521224018186331\n",
      "conv2.weight 5.135142244398594e-05\n",
      "conv2.bias 0.0012486042687669396\n",
      "fc1.weight 4.746237536892295e-05\n",
      "fc1.bias 0.0002741271164268255\n",
      "selected users: [0 1]\n",
      "conv1.weight 7.395678665488958e-05\n",
      "conv1.bias 0.003521224018186331\n",
      "conv2.weight 5.135142244398594e-05\n",
      "conv2.bias 0.0012486042687669396\n",
      "fc1.weight 4.746237536892295e-05\n",
      "fc1.bias 0.0002741271164268255\n",
      "selected users: [0 1]\n",
      "conv1.weight 7.395678665488958e-05\n",
      "conv1.bias 0.003521224018186331\n",
      "conv2.weight 5.135142244398594e-05\n",
      "conv2.bias 0.0012486042687669396\n",
      "fc1.weight 4.746237536892295e-05\n",
      "fc1.bias 0.0002741271164268255\n",
      "selected users: [0 1]\n",
      "conv1.weight 7.395678665488958e-05\n",
      "conv1.bias 0.003521224018186331\n",
      "conv2.weight 5.135142244398594e-05\n",
      "conv2.bias 0.0012486042687669396\n",
      "fc1.weight 4.746237536892295e-05\n",
      "fc1.bias 0.0002741271164268255\n",
      "\n",
      "Test set: Average loss: 2.2962 \n",
      "Accuracy: 2440/10000 (24.40%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00019039247184991835\n",
      "conv1.bias 0.003158950712531805\n",
      "conv2.weight 6.626949179917574e-05\n",
      "conv2.bias 0.0013054120354354382\n",
      "fc1.weight 0.00017077664379030466\n",
      "fc1.bias 0.0006348120979964734\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00019039247184991835\n",
      "conv1.bias 0.003158950712531805\n",
      "conv2.weight 6.626949179917574e-05\n",
      "conv2.bias 0.0013054120354354382\n",
      "fc1.weight 0.00017077664379030466\n",
      "fc1.bias 0.0006348120979964734\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00019039247184991835\n",
      "conv1.bias 0.003158950712531805\n",
      "conv2.weight 6.626949179917574e-05\n",
      "conv2.bias 0.0013054120354354382\n",
      "fc1.weight 0.00017077664379030466\n",
      "fc1.bias 0.0006348120979964734\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00019039247184991835\n",
      "conv1.bias 0.003158950712531805\n",
      "conv2.weight 6.626949179917574e-05\n",
      "conv2.bias 0.0013054120354354382\n",
      "fc1.weight 0.00017077664379030466\n",
      "fc1.bias 0.0006348120979964734\n",
      "\n",
      "Test set: Average loss: 2.2606 \n",
      "Accuracy: 5003/10000 (50.03%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016641087830066682\n",
      "conv1.bias 0.0033626812510192394\n",
      "conv2.weight 4.6716169454157355e-05\n",
      "conv2.bias 0.0009132276754826307\n",
      "fc1.weight 0.00012953728437423707\n",
      "fc1.bias 0.0006384746637195348\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016641087830066682\n",
      "conv1.bias 0.0033626812510192394\n",
      "conv2.weight 4.6716169454157355e-05\n",
      "conv2.bias 0.0009132276754826307\n",
      "fc1.weight 0.00012953728437423707\n",
      "fc1.bias 0.0006384746637195348\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016641087830066682\n",
      "conv1.bias 0.0033626812510192394\n",
      "conv2.weight 4.6716169454157355e-05\n",
      "conv2.bias 0.0009132276754826307\n",
      "fc1.weight 0.00012953728437423707\n",
      "fc1.bias 0.0006384746637195348\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016641087830066682\n",
      "conv1.bias 0.0033626812510192394\n",
      "conv2.weight 4.6716169454157355e-05\n",
      "conv2.bias 0.0009132276754826307\n",
      "fc1.weight 0.00012953728437423707\n",
      "fc1.bias 0.0006384746637195348\n",
      "\n",
      "Test set: Average loss: 2.1351 \n",
      "Accuracy: 5634/10000 (56.34%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003300837054848671\n",
      "conv1.bias 0.00308581767603755\n",
      "conv2.weight 4.959648475050926e-05\n",
      "conv2.bias 0.0007701654685661197\n",
      "fc1.weight 0.00019750322680920363\n",
      "fc1.bias 0.0007047931663691998\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003300837054848671\n",
      "conv1.bias 0.00308581767603755\n",
      "conv2.weight 4.959648475050926e-05\n",
      "conv2.bias 0.0007701654685661197\n",
      "fc1.weight 0.00019750322680920363\n",
      "fc1.bias 0.0007047931663691998\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003300837054848671\n",
      "conv1.bias 0.00308581767603755\n",
      "conv2.weight 4.959648475050926e-05\n",
      "conv2.bias 0.0007701654685661197\n",
      "fc1.weight 0.00019750322680920363\n",
      "fc1.bias 0.0007047931663691998\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003300837054848671\n",
      "conv1.bias 0.00308581767603755\n",
      "conv2.weight 4.959648475050926e-05\n",
      "conv2.bias 0.0007701654685661197\n",
      "fc1.weight 0.00019750322680920363\n",
      "fc1.bias 0.0007047931663691998\n",
      "\n",
      "Test set: Average loss: 1.9864 \n",
      "Accuracy: 7672/10000 (76.72%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00029599934816360474\n",
      "conv1.bias 0.005304697901010513\n",
      "conv2.weight 7.526816334575414e-05\n",
      "conv2.bias 0.0008651112439110875\n",
      "fc1.weight 0.00024612373672425745\n",
      "fc1.bias 0.0013208783231675625\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00029599934816360474\n",
      "conv1.bias 0.005304697901010513\n",
      "conv2.weight 7.526816334575414e-05\n",
      "conv2.bias 0.0008651112439110875\n",
      "fc1.weight 0.00024612373672425745\n",
      "fc1.bias 0.0013208783231675625\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00029599934816360474\n",
      "conv1.bias 0.005304697901010513\n",
      "conv2.weight 7.526816334575414e-05\n",
      "conv2.bias 0.0008651112439110875\n",
      "fc1.weight 0.00024612373672425745\n",
      "fc1.bias 0.0013208783231675625\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00029599934816360474\n",
      "conv1.bias 0.005304697901010513\n",
      "conv2.weight 7.526816334575414e-05\n",
      "conv2.bias 0.0008651112439110875\n",
      "fc1.weight 0.00024612373672425745\n",
      "fc1.bias 0.0013208783231675625\n",
      "\n",
      "Test set: Average loss: 1.9891 \n",
      "Accuracy: 7119/10000 (71.19%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003259675949811935\n",
      "conv1.bias 0.004835922736674547\n",
      "conv2.weight 6.197700276970864e-05\n",
      "conv2.bias 0.0007315663387998939\n",
      "fc1.weight 0.00029776918236166237\n",
      "fc1.bias 0.0008175321854650974\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003259675949811935\n",
      "conv1.bias 0.004835922736674547\n",
      "conv2.weight 6.197700276970864e-05\n",
      "conv2.bias 0.0007315663387998939\n",
      "fc1.weight 0.00029776918236166237\n",
      "fc1.bias 0.0008175321854650974\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003259675949811935\n",
      "conv1.bias 0.004835922736674547\n",
      "conv2.weight 6.197700276970864e-05\n",
      "conv2.bias 0.0007315663387998939\n",
      "fc1.weight 0.00029776918236166237\n",
      "fc1.bias 0.0008175321854650974\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003259675949811935\n",
      "conv1.bias 0.004835922736674547\n",
      "conv2.weight 6.197700276970864e-05\n",
      "conv2.bias 0.0007315663387998939\n",
      "fc1.weight 0.00029776918236166237\n",
      "fc1.bias 0.0008175321854650974\n",
      "\n",
      "Test set: Average loss: 1.9843 \n",
      "Accuracy: 7356/10000 (73.56%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003671360015869141\n",
      "conv1.bias 0.003531698603183031\n",
      "conv2.weight 7.622914854437113e-05\n",
      "conv2.bias 0.0007235364173538983\n",
      "fc1.weight 0.0003096994711086154\n",
      "fc1.bias 0.0010508310981094838\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003671360015869141\n",
      "conv1.bias 0.003531698603183031\n",
      "conv2.weight 7.622914854437113e-05\n",
      "conv2.bias 0.0007235364173538983\n",
      "fc1.weight 0.0003096994711086154\n",
      "fc1.bias 0.0010508310981094838\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003671360015869141\n",
      "conv1.bias 0.003531698603183031\n",
      "conv2.weight 7.622914854437113e-05\n",
      "conv2.bias 0.0007235364173538983\n",
      "fc1.weight 0.0003096994711086154\n",
      "fc1.bias 0.0010508310981094838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected users: [0 1]\n",
      "conv1.weight 0.0003671360015869141\n",
      "conv1.bias 0.003531698603183031\n",
      "conv2.weight 7.622914854437113e-05\n",
      "conv2.bias 0.0007235364173538983\n",
      "fc1.weight 0.0003096994711086154\n",
      "fc1.bias 0.0010508310981094838\n",
      "\n",
      "Test set: Average loss: 1.9156 \n",
      "Accuracy: 8028/10000 (80.28%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004318598285317421\n",
      "conv1.bias 0.00454168114811182\n",
      "conv2.weight 0.0001045920792967081\n",
      "conv2.bias 0.0008151166257448494\n",
      "fc1.weight 0.0001903501804918051\n",
      "fc1.bias 0.0010524818673729897\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004318598285317421\n",
      "conv1.bias 0.00454168114811182\n",
      "conv2.weight 0.0001045920792967081\n",
      "conv2.bias 0.0008151166257448494\n",
      "fc1.weight 0.0001903501804918051\n",
      "fc1.bias 0.0010524818673729897\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004318598285317421\n",
      "conv1.bias 0.00454168114811182\n",
      "conv2.weight 0.0001045920792967081\n",
      "conv2.bias 0.0008151166257448494\n",
      "fc1.weight 0.0001903501804918051\n",
      "fc1.bias 0.0010524818673729897\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004318598285317421\n",
      "conv1.bias 0.00454168114811182\n",
      "conv2.weight 0.0001045920792967081\n",
      "conv2.bias 0.0008151166257448494\n",
      "fc1.weight 0.0001903501804918051\n",
      "fc1.bias 0.0010524818673729897\n",
      "\n",
      "Test set: Average loss: 1.9486 \n",
      "Accuracy: 8317/10000 (83.17%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00026287825778126717\n",
      "conv1.bias 0.0035679698921740055\n",
      "conv2.weight 0.00015007455833256243\n",
      "conv2.bias 0.000884376117028296\n",
      "fc1.weight 0.00036389895249158146\n",
      "fc1.bias 0.0010422212071716785\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00026287825778126717\n",
      "conv1.bias 0.0035679698921740055\n",
      "conv2.weight 0.00015007455833256243\n",
      "conv2.bias 0.000884376117028296\n",
      "fc1.weight 0.00036389895249158146\n",
      "fc1.bias 0.0010422212071716785\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00026287825778126717\n",
      "conv1.bias 0.0035679698921740055\n",
      "conv2.weight 0.00015007455833256243\n",
      "conv2.bias 0.000884376117028296\n",
      "fc1.weight 0.00036389895249158146\n",
      "fc1.bias 0.0010422212071716785\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00026287825778126717\n",
      "conv1.bias 0.0035679698921740055\n",
      "conv2.weight 0.00015007455833256243\n",
      "conv2.bias 0.000884376117028296\n",
      "fc1.weight 0.00036389895249158146\n",
      "fc1.bias 0.0010422212071716785\n",
      "\n",
      "Test set: Average loss: 2.0756 \n",
      "Accuracy: 8363/10000 (83.63%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00023644546046853066\n",
      "conv1.bias 0.0022580227814614773\n",
      "conv2.weight 0.00024227147921919822\n",
      "conv2.bias 0.0010900967754423618\n",
      "fc1.weight 0.00027877825777977705\n",
      "fc1.bias 0.000622004782781005\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00023644546046853066\n",
      "conv1.bias 0.0022580227814614773\n",
      "conv2.weight 0.00024227147921919822\n",
      "conv2.bias 0.0010900967754423618\n",
      "fc1.weight 0.00027877825777977705\n",
      "fc1.bias 0.000622004782781005\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00023644546046853066\n",
      "conv1.bias 0.0022580227814614773\n",
      "conv2.weight 0.00024227147921919822\n",
      "conv2.bias 0.0010900967754423618\n",
      "fc1.weight 0.00027877825777977705\n",
      "fc1.bias 0.000622004782781005\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00023644546046853066\n",
      "conv1.bias 0.0022580227814614773\n",
      "conv2.weight 0.00024227147921919822\n",
      "conv2.bias 0.0010900967754423618\n",
      "fc1.weight 0.00027877825777977705\n",
      "fc1.bias 0.000622004782781005\n",
      "\n",
      "Test set: Average loss: 1.9260 \n",
      "Accuracy: 9260/10000 (92.60%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00039589695632457733\n",
      "conv1.bias 0.0033024242147803307\n",
      "conv2.weight 0.00011298680678009987\n",
      "conv2.bias 0.0011352724395692348\n",
      "fc1.weight 0.0001818593591451645\n",
      "fc1.bias 0.001704862155020237\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00039589695632457733\n",
      "conv1.bias 0.0033024242147803307\n",
      "conv2.weight 0.00011298680678009987\n",
      "conv2.bias 0.0011352724395692348\n",
      "fc1.weight 0.0001818593591451645\n",
      "fc1.bias 0.001704862155020237\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00039589695632457733\n",
      "conv1.bias 0.0033024242147803307\n",
      "conv2.weight 0.00011298680678009987\n",
      "conv2.bias 0.0011352724395692348\n",
      "fc1.weight 0.0001818593591451645\n",
      "fc1.bias 0.001704862155020237\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00039589695632457733\n",
      "conv1.bias 0.0033024242147803307\n",
      "conv2.weight 0.00011298680678009987\n",
      "conv2.bias 0.0011352724395692348\n",
      "fc1.weight 0.0001818593591451645\n",
      "fc1.bias 0.001704862155020237\n",
      "\n",
      "Test set: Average loss: 1.8954 \n",
      "Accuracy: 9210/10000 (92.10%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004368696361780167\n",
      "conv1.bias 0.0041921320371329784\n",
      "conv2.weight 9.377687238156795e-05\n",
      "conv2.bias 0.0009631860302761197\n",
      "fc1.weight 0.00028440775349736214\n",
      "fc1.bias 0.0016922583803534509\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004368696361780167\n",
      "conv1.bias 0.0041921320371329784\n",
      "conv2.weight 9.377687238156795e-05\n",
      "conv2.bias 0.0009631860302761197\n",
      "fc1.weight 0.00028440775349736214\n",
      "fc1.bias 0.0016922583803534509\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004368696361780167\n",
      "conv1.bias 0.0041921320371329784\n",
      "conv2.weight 9.377687238156795e-05\n",
      "conv2.bias 0.0009631860302761197\n",
      "fc1.weight 0.00028440775349736214\n",
      "fc1.bias 0.0016922583803534509\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004368696361780167\n",
      "conv1.bias 0.0041921320371329784\n",
      "conv2.weight 9.377687238156795e-05\n",
      "conv2.bias 0.0009631860302761197\n",
      "fc1.weight 0.00028440775349736214\n",
      "fc1.bias 0.0016922583803534509\n",
      "\n",
      "Test set: Average loss: 1.9237 \n",
      "Accuracy: 7826/10000 (78.26%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004266040027141571\n",
      "conv1.bias 0.0029574998188763857\n",
      "conv2.weight 8.143715560436248e-05\n",
      "conv2.bias 0.0007660199189558625\n",
      "fc1.weight 0.00025736945681273935\n",
      "fc1.bias 0.002061320096254349\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004266040027141571\n",
      "conv1.bias 0.0029574998188763857\n",
      "conv2.weight 8.143715560436248e-05\n",
      "conv2.bias 0.0007660199189558625\n",
      "fc1.weight 0.00025736945681273935\n",
      "fc1.bias 0.002061320096254349\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004266040027141571\n",
      "conv1.bias 0.0029574998188763857\n",
      "conv2.weight 8.143715560436248e-05\n",
      "conv2.bias 0.0007660199189558625\n",
      "fc1.weight 0.00025736945681273935\n",
      "fc1.bias 0.002061320096254349\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0004266040027141571\n",
      "conv1.bias 0.0029574998188763857\n",
      "conv2.weight 8.143715560436248e-05\n",
      "conv2.bias 0.0007660199189558625\n",
      "fc1.weight 0.00025736945681273935\n",
      "fc1.bias 0.002061320096254349\n",
      "\n",
      "Test set: Average loss: 1.9675 \n",
      "Accuracy: 8905/10000 (89.05%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003337294608354569\n",
      "conv1.bias 0.0021170172840356827\n",
      "conv2.weight 0.00018353167921304704\n",
      "conv2.bias 0.0009270054288208485\n",
      "fc1.weight 0.00016476474702358245\n",
      "fc1.bias 0.001676691509783268\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003337294608354569\n",
      "conv1.bias 0.0021170172840356827\n",
      "conv2.weight 0.00018353167921304704\n",
      "conv2.bias 0.0009270054288208485\n",
      "fc1.weight 0.00016476474702358245\n",
      "fc1.bias 0.001676691509783268\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003337294608354569\n",
      "conv1.bias 0.0021170172840356827\n",
      "conv2.weight 0.00018353167921304704\n",
      "conv2.bias 0.0009270054288208485\n",
      "fc1.weight 0.00016476474702358245\n",
      "fc1.bias 0.001676691509783268\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003337294608354569\n",
      "conv1.bias 0.0021170172840356827\n",
      "conv2.weight 0.00018353167921304704\n",
      "conv2.bias 0.0009270054288208485\n",
      "fc1.weight 0.00016476474702358245\n",
      "fc1.bias 0.001676691509783268\n",
      "\n",
      "Test set: Average loss: 1.9325 \n",
      "Accuracy: 7800/10000 (78.00%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005426491796970367\n",
      "conv1.bias 0.002185732126235962\n",
      "conv2.weight 8.749115280807019e-05\n",
      "conv2.bias 0.0007076846668496728\n",
      "fc1.weight 0.0002848342061042786\n",
      "fc1.bias 0.0013526964001357555\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005426491796970367\n",
      "conv1.bias 0.002185732126235962\n",
      "conv2.weight 8.749115280807019e-05\n",
      "conv2.bias 0.0007076846668496728\n",
      "fc1.weight 0.0002848342061042786\n",
      "fc1.bias 0.0013526964001357555\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005426491796970367\n",
      "conv1.bias 0.002185732126235962\n",
      "conv2.weight 8.749115280807019e-05\n",
      "conv2.bias 0.0007076846668496728\n",
      "fc1.weight 0.0002848342061042786\n",
      "fc1.bias 0.0013526964001357555\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005426491796970367\n",
      "conv1.bias 0.002185732126235962\n",
      "conv2.weight 8.749115280807019e-05\n",
      "conv2.bias 0.0007076846668496728\n",
      "fc1.weight 0.0002848342061042786\n",
      "fc1.bias 0.0013526964001357555\n",
      "\n",
      "Test set: Average loss: 1.8850 \n",
      "Accuracy: 8400/10000 (84.00%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005714534968137741\n",
      "conv1.bias 0.0030278575140982866\n",
      "conv2.weight 9.069159626960754e-05\n",
      "conv2.bias 0.0006717967335134745\n",
      "fc1.weight 0.0002154617104679346\n",
      "fc1.bias 0.001953108236193657\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005714534968137741\n",
      "conv1.bias 0.0030278575140982866\n",
      "conv2.weight 9.069159626960754e-05\n",
      "conv2.bias 0.0006717967335134745\n",
      "fc1.weight 0.0002154617104679346\n",
      "fc1.bias 0.001953108236193657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected users: [0 1]\n",
      "conv1.weight 0.0005714534968137741\n",
      "conv1.bias 0.0030278575140982866\n",
      "conv2.weight 9.069159626960754e-05\n",
      "conv2.bias 0.0006717967335134745\n",
      "fc1.weight 0.0002154617104679346\n",
      "fc1.bias 0.001953108236193657\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005714534968137741\n",
      "conv1.bias 0.0030278575140982866\n",
      "conv2.weight 9.069159626960754e-05\n",
      "conv2.bias 0.0006717967335134745\n",
      "fc1.weight 0.0002154617104679346\n",
      "fc1.bias 0.001953108236193657\n",
      "\n",
      "Test set: Average loss: 1.8993 \n",
      "Accuracy: 8592/10000 (85.92%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000490482747554779\n",
      "conv1.bias 0.002764808014035225\n",
      "conv2.weight 7.401576265692712e-05\n",
      "conv2.bias 0.0007964908145368099\n",
      "fc1.weight 0.00024193464778363705\n",
      "fc1.bias 0.0021743200719356536\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000490482747554779\n",
      "conv1.bias 0.002764808014035225\n",
      "conv2.weight 7.401576265692712e-05\n",
      "conv2.bias 0.0007964908145368099\n",
      "fc1.weight 0.00024193464778363705\n",
      "fc1.bias 0.0021743200719356536\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000490482747554779\n",
      "conv1.bias 0.002764808014035225\n",
      "conv2.weight 7.401576265692712e-05\n",
      "conv2.bias 0.0007964908145368099\n",
      "fc1.weight 0.00024193464778363705\n",
      "fc1.bias 0.0021743200719356536\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000490482747554779\n",
      "conv1.bias 0.002764808014035225\n",
      "conv2.weight 7.401576265692712e-05\n",
      "conv2.bias 0.0007964908145368099\n",
      "fc1.weight 0.00024193464778363705\n",
      "fc1.bias 0.0021743200719356536\n",
      "\n",
      "Test set: Average loss: 2.2406 \n",
      "Accuracy: 4853/10000 (48.53%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0002128542959690094\n",
      "conv1.bias 0.002365074586123228\n",
      "conv2.weight 0.0002296573668718338\n",
      "conv2.bias 0.0010565612465143204\n",
      "fc1.weight 0.00020412649028003215\n",
      "fc1.bias 0.0005343294236809015\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0002128542959690094\n",
      "conv1.bias 0.002365074586123228\n",
      "conv2.weight 0.0002296573668718338\n",
      "conv2.bias 0.0010565612465143204\n",
      "fc1.weight 0.00020412649028003215\n",
      "fc1.bias 0.0005343294236809015\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0002128542959690094\n",
      "conv1.bias 0.002365074586123228\n",
      "conv2.weight 0.0002296573668718338\n",
      "conv2.bias 0.0010565612465143204\n",
      "fc1.weight 0.00020412649028003215\n",
      "fc1.bias 0.0005343294236809015\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0002128542959690094\n",
      "conv1.bias 0.002365074586123228\n",
      "conv2.weight 0.0002296573668718338\n",
      "conv2.bias 0.0010565612465143204\n",
      "fc1.weight 0.00020412649028003215\n",
      "fc1.bias 0.0005343294236809015\n",
      "\n",
      "Test set: Average loss: 2.1454 \n",
      "Accuracy: 4724/10000 (47.24%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00038582608103752134\n",
      "conv1.bias 0.0046871621161699295\n",
      "conv2.weight 0.00010960894636809826\n",
      "conv2.bias 0.001246924977749586\n",
      "fc1.weight 0.00011301597114652396\n",
      "fc1.bias 0.0010564571246504783\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00038582608103752134\n",
      "conv1.bias 0.0046871621161699295\n",
      "conv2.weight 0.00010960894636809826\n",
      "conv2.bias 0.001246924977749586\n",
      "fc1.weight 0.00011301597114652396\n",
      "fc1.bias 0.0010564571246504783\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00038582608103752134\n",
      "conv1.bias 0.0046871621161699295\n",
      "conv2.weight 0.00010960894636809826\n",
      "conv2.bias 0.001246924977749586\n",
      "fc1.weight 0.00011301597114652396\n",
      "fc1.bias 0.0010564571246504783\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00038582608103752134\n",
      "conv1.bias 0.0046871621161699295\n",
      "conv2.weight 0.00010960894636809826\n",
      "conv2.bias 0.001246924977749586\n",
      "fc1.weight 0.00011301597114652396\n",
      "fc1.bias 0.0010564571246504783\n",
      "\n",
      "Test set: Average loss: 1.9929 \n",
      "Accuracy: 7436/10000 (74.36%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005298235267400742\n",
      "conv1.bias 0.0022879140451550484\n",
      "conv2.weight 6.357842590659857e-05\n",
      "conv2.bias 0.0007455847226083279\n",
      "fc1.weight 0.00023402890656143427\n",
      "fc1.bias 0.0014491743408143521\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005298235267400742\n",
      "conv1.bias 0.0022879140451550484\n",
      "conv2.weight 6.357842590659857e-05\n",
      "conv2.bias 0.0007455847226083279\n",
      "fc1.weight 0.00023402890656143427\n",
      "fc1.bias 0.0014491743408143521\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005298235267400742\n",
      "conv1.bias 0.0022879140451550484\n",
      "conv2.weight 6.357842590659857e-05\n",
      "conv2.bias 0.0007455847226083279\n",
      "fc1.weight 0.00023402890656143427\n",
      "fc1.bias 0.0014491743408143521\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005298235267400742\n",
      "conv1.bias 0.0022879140451550484\n",
      "conv2.weight 6.357842590659857e-05\n",
      "conv2.bias 0.0007455847226083279\n",
      "fc1.weight 0.00023402890656143427\n",
      "fc1.bias 0.0014491743408143521\n",
      "\n",
      "Test set: Average loss: 2.1696 \n",
      "Accuracy: 7462/10000 (74.62%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00028389446437358855\n",
      "conv1.bias 0.0015426690224558115\n",
      "conv2.weight 0.0002177099511027336\n",
      "conv2.bias 0.0009126710356213152\n",
      "fc1.weight 0.00020463610999286175\n",
      "fc1.bias 0.0004442998673766851\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00028389446437358855\n",
      "conv1.bias 0.0015426690224558115\n",
      "conv2.weight 0.0002177099511027336\n",
      "conv2.bias 0.0009126710356213152\n",
      "fc1.weight 0.00020463610999286175\n",
      "fc1.bias 0.0004442998673766851\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00028389446437358855\n",
      "conv1.bias 0.0015426690224558115\n",
      "conv2.weight 0.0002177099511027336\n",
      "conv2.bias 0.0009126710356213152\n",
      "fc1.weight 0.00020463610999286175\n",
      "fc1.bias 0.0004442998673766851\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00028389446437358855\n",
      "conv1.bias 0.0015426690224558115\n",
      "conv2.weight 0.0002177099511027336\n",
      "conv2.bias 0.0009126710356213152\n",
      "fc1.weight 0.00020463610999286175\n",
      "fc1.bias 0.0004442998673766851\n",
      "\n",
      "Test set: Average loss: 1.9043 \n",
      "Accuracy: 8367/10000 (83.67%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000697704628109932\n",
      "conv1.bias 0.002155567053705454\n",
      "conv2.weight 6.353392731398344e-05\n",
      "conv2.bias 0.0007684303564019501\n",
      "fc1.weight 0.00016730609349906443\n",
      "fc1.bias 0.00128436591476202\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000697704628109932\n",
      "conv1.bias 0.002155567053705454\n",
      "conv2.weight 6.353392731398344e-05\n",
      "conv2.bias 0.0007684303564019501\n",
      "fc1.weight 0.00016730609349906443\n",
      "fc1.bias 0.00128436591476202\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000697704628109932\n",
      "conv1.bias 0.002155567053705454\n",
      "conv2.weight 6.353392731398344e-05\n",
      "conv2.bias 0.0007684303564019501\n",
      "fc1.weight 0.00016730609349906443\n",
      "fc1.bias 0.00128436591476202\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000697704628109932\n",
      "conv1.bias 0.002155567053705454\n",
      "conv2.weight 6.353392731398344e-05\n",
      "conv2.bias 0.0007684303564019501\n",
      "fc1.weight 0.00016730609349906443\n",
      "fc1.bias 0.00128436591476202\n",
      "\n",
      "Test set: Average loss: 1.9238 \n",
      "Accuracy: 8068/10000 (80.68%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006887290626764297\n",
      "conv1.bias 0.0031900862231850624\n",
      "conv2.weight 6.477429065853358e-05\n",
      "conv2.bias 0.0007115878397598863\n",
      "fc1.weight 0.000279553746804595\n",
      "fc1.bias 0.0011424751952290534\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006887290626764297\n",
      "conv1.bias 0.0031900862231850624\n",
      "conv2.weight 6.477429065853358e-05\n",
      "conv2.bias 0.0007115878397598863\n",
      "fc1.weight 0.000279553746804595\n",
      "fc1.bias 0.0011424751952290534\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006887290626764297\n",
      "conv1.bias 0.0031900862231850624\n",
      "conv2.weight 6.477429065853358e-05\n",
      "conv2.bias 0.0007115878397598863\n",
      "fc1.weight 0.000279553746804595\n",
      "fc1.bias 0.0011424751952290534\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006887290626764297\n",
      "conv1.bias 0.0031900862231850624\n",
      "conv2.weight 6.477429065853358e-05\n",
      "conv2.bias 0.0007115878397598863\n",
      "fc1.weight 0.000279553746804595\n",
      "fc1.bias 0.0011424751952290534\n",
      "\n",
      "Test set: Average loss: 2.0256 \n",
      "Accuracy: 8121/10000 (81.21%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003807590901851654\n",
      "conv1.bias 0.001800802070647478\n",
      "conv2.weight 0.00015754494816064835\n",
      "conv2.bias 0.0009233183227479458\n",
      "fc1.weight 0.0001704503083601594\n",
      "fc1.bias 0.0013071922585368156\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003807590901851654\n",
      "conv1.bias 0.001800802070647478\n",
      "conv2.weight 0.00015754494816064835\n",
      "conv2.bias 0.0009233183227479458\n",
      "fc1.weight 0.0001704503083601594\n",
      "fc1.bias 0.0013071922585368156\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003807590901851654\n",
      "conv1.bias 0.001800802070647478\n",
      "conv2.weight 0.00015754494816064835\n",
      "conv2.bias 0.0009233183227479458\n",
      "fc1.weight 0.0001704503083601594\n",
      "fc1.bias 0.0013071922585368156\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0003807590901851654\n",
      "conv1.bias 0.001800802070647478\n",
      "conv2.weight 0.00015754494816064835\n",
      "conv2.bias 0.0009233183227479458\n",
      "fc1.weight 0.0001704503083601594\n",
      "fc1.bias 0.0013071922585368156\n",
      "\n",
      "Test set: Average loss: 1.9031 \n",
      "Accuracy: 8438/10000 (84.38%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005064823105931282\n",
      "conv1.bias 0.002381072612479329\n",
      "conv2.weight 0.00013109263963997364\n",
      "conv2.bias 0.0011221294989809394\n",
      "fc1.weight 0.00024645542725920677\n",
      "fc1.bias 0.001628502458333969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected users: [0 1]\n",
      "conv1.weight 0.0005064823105931282\n",
      "conv1.bias 0.002381072612479329\n",
      "conv2.weight 0.00013109263963997364\n",
      "conv2.bias 0.0011221294989809394\n",
      "fc1.weight 0.00024645542725920677\n",
      "fc1.bias 0.001628502458333969\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005064823105931282\n",
      "conv1.bias 0.002381072612479329\n",
      "conv2.weight 0.00013109263963997364\n",
      "conv2.bias 0.0011221294989809394\n",
      "fc1.weight 0.00024645542725920677\n",
      "fc1.bias 0.001628502458333969\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005064823105931282\n",
      "conv1.bias 0.002381072612479329\n",
      "conv2.weight 0.00013109263963997364\n",
      "conv2.bias 0.0011221294989809394\n",
      "fc1.weight 0.00024645542725920677\n",
      "fc1.bias 0.001628502458333969\n",
      "\n",
      "Test set: Average loss: 2.0016 \n",
      "Accuracy: 8365/10000 (83.65%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000428488664329052\n",
      "conv1.bias 0.0015776964137330651\n",
      "conv2.weight 0.00017354713752865792\n",
      "conv2.bias 0.0009931749664247036\n",
      "fc1.weight 0.0002467575948685408\n",
      "fc1.bias 0.0022003265097737313\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000428488664329052\n",
      "conv1.bias 0.0015776964137330651\n",
      "conv2.weight 0.00017354713752865792\n",
      "conv2.bias 0.0009931749664247036\n",
      "fc1.weight 0.0002467575948685408\n",
      "fc1.bias 0.0022003265097737313\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000428488664329052\n",
      "conv1.bias 0.0015776964137330651\n",
      "conv2.weight 0.00017354713752865792\n",
      "conv2.bias 0.0009931749664247036\n",
      "fc1.weight 0.0002467575948685408\n",
      "fc1.bias 0.0022003265097737313\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000428488664329052\n",
      "conv1.bias 0.0015776964137330651\n",
      "conv2.weight 0.00017354713752865792\n",
      "conv2.bias 0.0009931749664247036\n",
      "fc1.weight 0.0002467575948685408\n",
      "fc1.bias 0.0022003265097737313\n",
      "\n",
      "Test set: Average loss: 1.9118 \n",
      "Accuracy: 7744/10000 (77.44%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006744354963302612\n",
      "conv1.bias 0.0010360238375142217\n",
      "conv2.weight 7.792744785547257e-05\n",
      "conv2.bias 0.0007654554792679846\n",
      "fc1.weight 0.00019959802739322186\n",
      "fc1.bias 0.0023810403421521188\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006744354963302612\n",
      "conv1.bias 0.0010360238375142217\n",
      "conv2.weight 7.792744785547257e-05\n",
      "conv2.bias 0.0007654554792679846\n",
      "fc1.weight 0.00019959802739322186\n",
      "fc1.bias 0.0023810403421521188\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006744354963302612\n",
      "conv1.bias 0.0010360238375142217\n",
      "conv2.weight 7.792744785547257e-05\n",
      "conv2.bias 0.0007654554792679846\n",
      "fc1.weight 0.00019959802739322186\n",
      "fc1.bias 0.0023810403421521188\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006744354963302612\n",
      "conv1.bias 0.0010360238375142217\n",
      "conv2.weight 7.792744785547257e-05\n",
      "conv2.bias 0.0007654554792679846\n",
      "fc1.weight 0.00019959802739322186\n",
      "fc1.bias 0.0023810403421521188\n",
      "\n",
      "Test set: Average loss: 2.1094 \n",
      "Accuracy: 7041/10000 (70.41%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00047544091939926146\n",
      "conv1.bias 0.0010028420947492123\n",
      "conv2.weight 0.0003385312482714653\n",
      "conv2.bias 0.001036330359056592\n",
      "fc1.weight 0.00021146887447685004\n",
      "fc1.bias 0.001817685179412365\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00047544091939926146\n",
      "conv1.bias 0.0010028420947492123\n",
      "conv2.weight 0.0003385312482714653\n",
      "conv2.bias 0.001036330359056592\n",
      "fc1.weight 0.00021146887447685004\n",
      "fc1.bias 0.001817685179412365\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00047544091939926146\n",
      "conv1.bias 0.0010028420947492123\n",
      "conv2.weight 0.0003385312482714653\n",
      "conv2.bias 0.001036330359056592\n",
      "fc1.weight 0.00021146887447685004\n",
      "fc1.bias 0.001817685179412365\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00047544091939926146\n",
      "conv1.bias 0.0010028420947492123\n",
      "conv2.weight 0.0003385312482714653\n",
      "conv2.bias 0.001036330359056592\n",
      "fc1.weight 0.00021146887447685004\n",
      "fc1.bias 0.001817685179412365\n",
      "\n",
      "Test set: Average loss: 1.9124 \n",
      "Accuracy: 8639/10000 (86.39%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "N = 8\n",
    "K = 4\n",
    "G = 4\n",
    "\n",
    "size_per_group = int(60000/G)\n",
    "\n",
    "X_group = np.reshape(encoding_input_array_np, (G,size_per_group,28*28))\n",
    "y_group = np.reshape(encoding_label_array_np, (G,size_per_group,args.num_classes)) \n",
    "\n",
    "\n",
    "N_i = int(N/G) # = 2\n",
    "K_i = int(K/G) # = 2\n",
    "T = 2\n",
    "sigma = 1\n",
    "Noise_Alloc = [0,2]\n",
    "m = N_i # number of selected workers (if there is no straggler, m=N_i)\n",
    "\n",
    "print(N_i,K_i,T)\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K_i+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K_i+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K_i+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "z_array = np.array([-0.521, 0.521])\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "N_trials = 5\n",
    "N_epochs = 30\n",
    "\n",
    "loss_test_arr_K4_G4_N8_v3 = np.zeros((N_trials,N_epochs))\n",
    "acc_test_arr_K4_G4_N8_v3  = np.zeros((N_trials,N_epochs))\n",
    "\n",
    "for trial_idx in range(N_trials):\n",
    "    \n",
    "    print('##########################################')\n",
    "    print('######',trial_idx,'-th Trial!! ###########')\n",
    "    \n",
    "    net_glob = CNNMnist2(args=args)\n",
    "    net_glob.cuda()\n",
    "    net_glob.train()\n",
    "    \n",
    "    # copy weights\n",
    "    w_glob = net_glob.state_dict()\n",
    "    \n",
    "    X_tilde = np.empty((N,Size_submatrices,28*28))\n",
    "    y_tilde = np.empty((N,Size_submatrices,10))\n",
    "    \n",
    "    for G_idx in range(G):\n",
    "        \n",
    "        _Noise_label = np.ones((size_per_group*T,10)) * 0.1\n",
    "        \n",
    "        X_tilde_tmp,a,b = BACC_Enc_Data_v3(X_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde_tmp,a,b = BACC_Enc_Data_v3(y_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "        \n",
    "        stt_pos = G_idx * N_i\n",
    "        end_pos = (G_idx+1) * N_i\n",
    "        \n",
    "        X_tilde[stt_pos:end_pos,:,:] = X_tilde_tmp\n",
    "        y_tilde[stt_pos:end_pos,:,:] = y_tilde_tmp\n",
    "        \n",
    "   \n",
    "\n",
    "    for iter in range(N_epochs): #args.epochs\n",
    "        \n",
    "        w_group_array = []\n",
    "        for G_idx in range(G):\n",
    "            w_locals, loss_locals = [], []\n",
    "            idxs_users = np.random.choice(range(N_i), m, replace=False)\n",
    "            idxs_users = np.sort(idxs_users)\n",
    "            print('selected users:',idxs_users)\n",
    "\n",
    "            coded_net = BACC_Enc_Model_withNoise_v4(net_glob.cuda(), N_i, K_i, T, 1, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "            dec_z_array = []\n",
    "            for idx in idxs_users: #for idx in range(N):\n",
    "        #         print(idx)\n",
    "                local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[G_idx*N_i+idx,:,:], label=y_tilde[G_idx*N_i+idx,:,:])\n",
    "                w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "#                 w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                w_locals.append(copy.deepcopy(w))\n",
    "                loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "            # update global weights\n",
    "            #w_glob = FedAvg(w_locals)\n",
    "            w_group = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "            \n",
    "            w_group_array.append(copy.deepcopy(w_group))\n",
    "        \n",
    "        w_glob = copy.deepcopy(w_group_array[0])\n",
    "        for k in w_glob.keys():\n",
    "            for G_idx in range(1,G):\n",
    "                w_glob[k] += w_group_array[G_idx][k]\n",
    "            w_glob[k] = torch.div(w_glob[k], len(w_group_array))\n",
    "        \n",
    "        # copy weight to net_glob\n",
    "        net_glob.load_state_dict(w_glob)\n",
    "\n",
    "        # print loss\n",
    "    #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "    #     loss_train_arr.append(loss_train)\n",
    "\n",
    "        acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "        acc_test_arr_K4_G4_N8_v3[trial_idx][iter] = acc_test\n",
    "        loss_test_arr_K4_G4_N8_v3[trial_idx][iter] = loss_test\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "B_sel = 0\n",
    "\n",
    "# plt.plot(acc_test_arr_v1[0,B_sel,0,0:30],label='N=2, B='+str(B_array[B_sel]))\n",
    "plt.plot(acc_test_arr_K4_G1_v3[2,B_sel,0,0:30],label='w/o Grouping, K=4, N=4, G=1' )\n",
    "plt.plot(acc_test_arr_K4_G1_v3[3,B_sel,0,0:30],label='w/o Grouping, K=4, N=8, G=1' )\n",
    "# plt.plot(acc_test_arr_v1[2,B_sel,0,0:30],label='N=6, B='+str(B_array[B_sel]))\n",
    "# plt.plot(acc_test_arr_v1[3,B_sel,0,0:30],label='N=8, B='+str(B_array[B_sel]))\n",
    "plt.plot(acc_test_arr_G2_N4_v3[0,0:30],label='w/ Grouping,   K=4, N=4, G=2')\n",
    "plt.plot(acc_test_arr_K4_G2_N8_v3[0,0:30],label='w/ Grouping,   K=4, N=8, G=2')\n",
    "plt.plot(acc_test_arr_K4_G4_N8_v3[0,0:30],label='w/ Grouping,   K=4, N=8, G=4')\n",
    "# plt.plot(acc_test_arr_G3[0,0:30],label='Grouping, N=6, G=3, B='+str(B_array[B_sel]))\n",
    "\n",
    "# plt.plot(acc_test_arr_v1[4,B_sel,0,0:30],label='N=7, B='+str(B_array[B_sel]))\n",
    "# plt.plot(acc_test_arr_v1[5,B_sel,0,0:30],label='N=8, B='+str(B_array[B_sel]))\n",
    "\n",
    "# plt.plot(plot_acc_v2[2,sigma_sel,:],label='T=10')\n",
    "# plt.plot(plot_acc_v2[3,sigma_sel,:],label='T=11')\n",
    "\n",
    "# plt.plot(acc_test_arr_uncoded,'r',label='uncoded')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "# plt.title(title_name)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Test accuracy(%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
