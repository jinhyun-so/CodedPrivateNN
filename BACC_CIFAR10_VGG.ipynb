{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import basic libraries\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Import PyTorch\n",
    "import torch # import main library\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn # import modules\n",
    "from torch.autograd import Function # import Function to create custom activations\n",
    "from torch.nn.parameter import Parameter # import Parameter to create custom activations with learnable parameters\n",
    "from torch import optim # import optimizers for demonstrations\n",
    "import torch.nn.functional as F # import torch functions\n",
    "from torchvision import datasets, transforms # import transformations to use for demo\n",
    "from torch.nn.parameter import Parameter # import Parameter to create custom activations with learnable parameters\n",
    "\n",
    "from models.activ_func import *\n",
    "from models.Nets import *\n",
    "from models.test import test_img\n",
    "from models.Update import *\n",
    "\n",
    "\n",
    "from utils.sampling import mnist_iid, mnist_noniid, cifar_iid\n",
    "from utils.options import args_parser\n",
    "from models.Update import LocalUpdate\n",
    "from models.Nets import MLP, CNNMnist, CNNCifar, LeNet, CNNMnist2, CNNMnist3\n",
    "from models.Fed import FedAvg\n",
    "from models.Fed import FedQAvg, Quantization, Quantization_Finite, my_score, my_score_Finite\n",
    "from models.test import test_img\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from utils.functions import *\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "class my_argument:    \n",
    "    epochs    = 16    #\"rounds of training\"\n",
    "    num_users = 4  # \"number of users: K\"\n",
    "    frac      = 0.5 #\"the fraction of clients: C\"\n",
    "    local_ep  = 1 #\"the number of local epochs: E\"\n",
    "    local_bs  = 50  #\"local batch size: B\"\n",
    "    bs        = 50 #\"test batch size\"\n",
    "    lr        = 0.001 #\"learning rate\"\n",
    "    momentum  = 0.5 # \"SGD momentum (default: 0.5)\"\n",
    "    weight_decay = 5e-4\n",
    "    split     = 'user' # \"train-test split type, user or sample\"\n",
    "    opt='ADAM'\n",
    "    loss='Custom' # 'Custom' or 'Default'\n",
    "\n",
    "    # model arguments\n",
    "    model = 'cnn'\n",
    "    kernel_num=9 #, help='number of each kind of kernel')\n",
    "    kernel_sizes='3,4,5' #  help='comma-separated kernel size to use for convolution')\n",
    "    norm='batch_norm' #, help=\"batch_norm, layer_norm, or None\")\n",
    "    num_filters=32 #, help=\"number of filters for conv nets\")\n",
    "    max_pool='True' #help=\"Whether use max pooling rather than strided convolutions\")\n",
    "\n",
    "    # other arguments\n",
    "    dataset='cifar' #, help=\"name of dataset\")\n",
    "    iid=1\n",
    "    num_classes=10#, help=\"number of classes\")\n",
    "    num_channels=1#, help=\"number of channels of imges\")\n",
    "    gpu=1#, help=\"GPU ID, -1 for CPU\")\n",
    "    stopping_rounds=10#, help='rounds of early stopping')\n",
    "    verbose='False'#, help='verbose print')\n",
    "    seed=1#, help='random seed (default: 1)')\n",
    "    \n",
    "args = my_argument()\n",
    "\n",
    "args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)\n",
    "args.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(args.device)\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Normalize the test set same as training set without augmentation\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# trainset = torchvision.datasets.CIFAR10(\n",
    "#     root=\"./data/cifar\", train=True, download=True, transform=transform_train)\n",
    "# trainloader = torch.utils.data.DataLoader(\n",
    "#     trainset, batch_size=args.bs, shuffle=True, num_workers=2)\n",
    "\n",
    "# testset = torchvision.datasets.CIFAR10(\n",
    "#     root=\"./data/cifar\", train=False, download=True, transform=transform_test)\n",
    "# testloader = torch.utils.data.DataLoader(\n",
    "#     testset, batch_size=args.bs, shuffle=False, num_workers=2)\n",
    "\n",
    "dataset_train = datasets.CIFAR10('../data/cifar', train=True, download=True, transform=transform_train)\n",
    "dataset_test = datasets.CIFAR10('../data/cifar', train=False, download=True, transform=transform_test)\n",
    "if args.iid:\n",
    "    dict_users = cifar_iid(dataset_train, args.num_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, testloader, criterion):\n",
    "    test_loss = 0\n",
    "    accuracy = 0\n",
    "    for images, labels in testloader:\n",
    "\n",
    "#         images.resize_(images.shape[0], 3*32*32)\n",
    "\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        output = model.forward(images)\n",
    "        test_loss += criterion(output, labels).item()\n",
    "\n",
    "        ps = torch.exp(output)\n",
    "        equality = (labels.data == ps.max(dim=1)[1])\n",
    "        accuracy += equality.type(torch.FloatTensor).mean()\n",
    "\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0. Plain, VGG11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.2768 \n",
      "Accuracy: 1022/10000 (10.22%)\n",
      "\n",
      "Round   0, Average loss 1.207\n",
      "\n",
      "Test set: Average loss: 0.6481 \n",
      "Accuracy: 7813/10000 (78.13%)\n",
      "\n",
      "Round   1, Average loss 0.815\n",
      "\n",
      "Test set: Average loss: 0.5360 \n",
      "Accuracy: 8243/10000 (82.43%)\n",
      "\n",
      "Round   2, Average loss 0.619\n",
      "\n",
      "Test set: Average loss: 0.4601 \n",
      "Accuracy: 8466/10000 (84.66%)\n",
      "\n",
      "Round   3, Average loss 0.508\n",
      "\n",
      "Test set: Average loss: 0.4605 \n",
      "Accuracy: 8488/10000 (84.88%)\n",
      "\n",
      "Round   4, Average loss 0.430\n",
      "\n",
      "Test set: Average loss: 0.4270 \n",
      "Accuracy: 8643/10000 (86.43%)\n",
      "\n",
      "Round   5, Average loss 0.376\n",
      "\n",
      "Test set: Average loss: 0.4115 \n",
      "Accuracy: 8693/10000 (86.93%)\n",
      "\n",
      "Round   6, Average loss 0.333\n",
      "\n",
      "Test set: Average loss: 0.4152 \n",
      "Accuracy: 8727/10000 (87.27%)\n",
      "\n",
      "Round   7, Average loss 0.301\n",
      "\n",
      "Test set: Average loss: 0.4171 \n",
      "Accuracy: 8731/10000 (87.31%)\n",
      "\n",
      "Round   8, Average loss 0.274\n",
      "\n",
      "Test set: Average loss: 0.4011 \n",
      "Accuracy: 8767/10000 (87.67%)\n",
      "\n",
      "Round   9, Average loss 0.257\n",
      "\n",
      "Test set: Average loss: 0.3961 \n",
      "Accuracy: 8838/10000 (88.38%)\n",
      "\n",
      "Round  10, Average loss 0.240\n",
      "\n",
      "Test set: Average loss: 0.4068 \n",
      "Accuracy: 8793/10000 (87.93%)\n",
      "\n",
      "Round  11, Average loss 0.229\n",
      "\n",
      "Test set: Average loss: 0.4257 \n",
      "Accuracy: 8747/10000 (87.47%)\n",
      "\n",
      "Round  12, Average loss 0.218\n",
      "\n",
      "Test set: Average loss: 0.3974 \n",
      "Accuracy: 8817/10000 (88.17%)\n",
      "\n",
      "Round  13, Average loss 0.210\n",
      "\n",
      "Test set: Average loss: 0.4130 \n",
      "Accuracy: 8774/10000 (87.74%)\n",
      "\n",
      "Round  14, Average loss 0.200\n",
      "\n",
      "Test set: Average loss: 0.4098 \n",
      "Accuracy: 8758/10000 (87.58%)\n",
      "\n",
      "Round  15, Average loss 0.195\n",
      "\n",
      "Test set: Average loss: 0.4150 \n",
      "Accuracy: 8796/10000 (87.96%)\n",
      "\n",
      "Round  16, Average loss 0.188\n",
      "\n",
      "Test set: Average loss: 0.3960 \n",
      "Accuracy: 8839/10000 (88.39%)\n",
      "\n",
      "Round  17, Average loss 0.185\n",
      "\n",
      "Test set: Average loss: 0.4061 \n",
      "Accuracy: 8839/10000 (88.39%)\n",
      "\n",
      "Round  18, Average loss 0.181\n",
      "\n",
      "Test set: Average loss: 0.4082 \n",
      "Accuracy: 8817/10000 (88.17%)\n",
      "\n",
      "Round  19, Average loss 0.177\n",
      "\n",
      "Test set: Average loss: 0.4342 \n",
      "Accuracy: 8783/10000 (87.83%)\n",
      "\n",
      "Round  20, Average loss 0.174\n",
      "\n",
      "Test set: Average loss: 0.4300 \n",
      "Accuracy: 8792/10000 (87.92%)\n",
      "\n",
      "Round  21, Average loss 0.169\n",
      "\n",
      "Test set: Average loss: 0.4034 \n",
      "Accuracy: 8824/10000 (88.24%)\n",
      "\n",
      "Round  22, Average loss 0.167\n",
      "\n",
      "Test set: Average loss: 0.4014 \n",
      "Accuracy: 8835/10000 (88.35%)\n",
      "\n",
      "Round  23, Average loss 0.165\n",
      "\n",
      "Test set: Average loss: 0.4069 \n",
      "Accuracy: 8830/10000 (88.30%)\n",
      "\n",
      "Round  24, Average loss 0.163\n",
      "\n",
      "Test set: Average loss: 0.4136 \n",
      "Accuracy: 8818/10000 (88.18%)\n",
      "\n",
      "Round  25, Average loss 0.159\n",
      "\n",
      "Test set: Average loss: 0.4003 \n",
      "Accuracy: 8852/10000 (88.52%)\n",
      "\n",
      "Round  26, Average loss 0.158\n",
      "\n",
      "Test set: Average loss: 0.4006 \n",
      "Accuracy: 8840/10000 (88.40%)\n",
      "\n",
      "Round  27, Average loss 0.156\n",
      "\n",
      "Test set: Average loss: 0.3986 \n",
      "Accuracy: 8853/10000 (88.53%)\n",
      "\n",
      "Round  28, Average loss 0.156\n",
      "\n",
      "Test set: Average loss: 0.3903 \n",
      "Accuracy: 8933/10000 (89.33%)\n",
      "\n",
      "Round  29, Average loss 0.154\n",
      "\n",
      "Test set: Average loss: 0.4024 \n",
      "Accuracy: 8838/10000 (88.38%)\n",
      "\n",
      "Round  30, Average loss 0.152\n",
      "\n",
      "Test set: Average loss: 0.4072 \n",
      "Accuracy: 8847/10000 (88.47%)\n",
      "\n",
      "Round  31, Average loss 0.149\n",
      "\n",
      "Test set: Average loss: 0.4201 \n",
      "Accuracy: 8787/10000 (87.87%)\n",
      "\n",
      "Round  32, Average loss 0.149\n",
      "\n",
      "Test set: Average loss: 0.4190 \n",
      "Accuracy: 8818/10000 (88.18%)\n",
      "\n",
      "Round  33, Average loss 0.149\n",
      "\n",
      "Test set: Average loss: 0.4249 \n",
      "Accuracy: 8798/10000 (87.98%)\n",
      "\n",
      "Round  34, Average loss 0.147\n",
      "\n",
      "Test set: Average loss: 0.4049 \n",
      "Accuracy: 8878/10000 (88.78%)\n",
      "\n",
      "Round  35, Average loss 0.146\n",
      "\n",
      "Test set: Average loss: 0.4043 \n",
      "Accuracy: 8878/10000 (88.78%)\n",
      "\n",
      "Round  36, Average loss 0.146\n",
      "\n",
      "Test set: Average loss: 0.4167 \n",
      "Accuracy: 8838/10000 (88.38%)\n",
      "\n",
      "Round  37, Average loss 0.145\n",
      "\n",
      "Test set: Average loss: 0.4058 \n",
      "Accuracy: 8876/10000 (88.76%)\n",
      "\n",
      "Round  38, Average loss 0.143\n",
      "\n",
      "Test set: Average loss: 0.4209 \n",
      "Accuracy: 8849/10000 (88.49%)\n",
      "\n",
      "Round  39, Average loss 0.143\n",
      "\n",
      "Test set: Average loss: 0.3929 \n",
      "Accuracy: 8887/10000 (88.87%)\n",
      "\n",
      "Round  40, Average loss 0.142\n",
      "\n",
      "Test set: Average loss: 0.4113 \n",
      "Accuracy: 8862/10000 (88.62%)\n",
      "\n",
      "Round  41, Average loss 0.142\n",
      "\n",
      "Test set: Average loss: 0.4132 \n",
      "Accuracy: 8847/10000 (88.47%)\n",
      "\n",
      "Round  42, Average loss 0.139\n",
      "\n",
      "Test set: Average loss: 0.4128 \n",
      "Accuracy: 8849/10000 (88.49%)\n",
      "\n",
      "Round  43, Average loss 0.138\n",
      "\n",
      "Test set: Average loss: 0.4075 \n",
      "Accuracy: 8875/10000 (88.75%)\n",
      "\n",
      "Round  44, Average loss 0.139\n",
      "\n",
      "Test set: Average loss: 0.4083 \n",
      "Accuracy: 8865/10000 (88.65%)\n",
      "\n",
      "Round  45, Average loss 0.139\n",
      "\n",
      "Test set: Average loss: 0.3942 \n",
      "Accuracy: 8904/10000 (89.04%)\n",
      "\n",
      "Round  46, Average loss 0.139\n",
      "\n",
      "Test set: Average loss: 0.4000 \n",
      "Accuracy: 8884/10000 (88.84%)\n",
      "\n",
      "Round  47, Average loss 0.137\n",
      "\n",
      "Test set: Average loss: 0.3996 \n",
      "Accuracy: 8885/10000 (88.85%)\n",
      "\n",
      "Round  48, Average loss 0.137\n",
      "\n",
      "Test set: Average loss: 0.4138 \n",
      "Accuracy: 8856/10000 (88.56%)\n",
      "\n",
      "Round  49, Average loss 0.135\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "from models.vgg import *\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "N_epochs = 50\n",
    "\n",
    "net_glob = VGG('VGG11')\n",
    "net_glob.cuda()\n",
    "\n",
    "acc_test_VGG11_FedAvg = np.empty(N_epochs)\n",
    "loss_test_VGG11_FedAvg = np.empty(N_epochs)\n",
    "\n",
    "for iter in range(N_epochs):\n",
    "    w_locals, loss_locals = [], []\n",
    "\n",
    "    m = args.num_users\n",
    "    \n",
    "    idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "    \n",
    "    for idx in idxs_users:\n",
    "        local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])\n",
    "        w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))\n",
    "        w_locals.append(copy.deepcopy(w))\n",
    "        loss_locals.append(copy.deepcopy(loss))\n",
    "        \n",
    "    # update global weights\n",
    "    w_glob = FedAvg(w_locals)\n",
    "\n",
    "    # copy weight to net_glob\n",
    "    net_glob.load_state_dict(w_glob)\n",
    "    \n",
    "    acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "\n",
    "    # print loss\n",
    "    loss_avg = sum(loss_locals) / len(loss_locals)\n",
    "    print('Round {:3d}, Average loss {:.3f}'.format(iter, loss_avg))\n",
    "#     loss_train.append(loss_avg)\n",
    "    \n",
    "    acc_test_VGG11_FedAvg[iter] = acc_test\n",
    "    loss_test_VGG11_FedAvg[iter] = loss_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1. BACC, G=1, K=2, N=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "class my_argument:    \n",
    "    epochs    = 16    #\"rounds of training\"\n",
    "    num_users = 2  # \"number of users: K\"\n",
    "    frac      = 0.5 #\"the fraction of clients: C\"\n",
    "    local_ep  = 5 #\"the number of local epochs: E\"\n",
    "    local_bs  = 50  #\"local batch size: B\"\n",
    "    bs        = 50 #\"test batch size\"\n",
    "    lr        = 0.001 #\"learning rate\"\n",
    "    momentum  = 0.5 # \"SGD momentum (default: 0.5)\"\n",
    "    weight_decay = 5e-4\n",
    "    split     = 'user' # \"train-test split type, user or sample\"\n",
    "    opt='ADAM'\n",
    "    loss='Custom' # 'Custom' or 'Default'\n",
    "\n",
    "    # model arguments\n",
    "    model = 'cnn'\n",
    "    kernel_num=9 #, help='number of each kind of kernel')\n",
    "    kernel_sizes='3,4,5' #  help='comma-separated kernel size to use for convolution')\n",
    "    norm='batch_norm' #, help=\"batch_norm, layer_norm, or None\")\n",
    "    num_filters=32 #, help=\"number of filters for conv nets\")\n",
    "    max_pool='True' #help=\"Whether use max pooling rather than strided convolutions\")\n",
    "\n",
    "    # other arguments\n",
    "    dataset='cifar' #, help=\"name of dataset\")\n",
    "    iid=1\n",
    "    num_classes=10#, help=\"number of classes\")\n",
    "    num_channels=1#, help=\"number of channels of imges\")\n",
    "    gpu=1#, help=\"GPU ID, -1 for CPU\")\n",
    "    stopping_rounds=10#, help='rounds of early stopping')\n",
    "    verbose='False'#, help='verbose print')\n",
    "    seed=1#, help='random seed (default: 1)')\n",
    "    \n",
    "args = my_argument()\n",
    "\n",
    "args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)\n",
    "args.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(args.device)\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Normalize the test set same as training set without augmentation\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# trainset = torchvision.datasets.CIFAR10(\n",
    "#     root=\"./data/cifar\", train=True, download=True, transform=transform_train)\n",
    "# trainloader = torch.utils.data.DataLoader(\n",
    "#     trainset, batch_size=args.bs, shuffle=True, num_workers=2)\n",
    "\n",
    "# testset = torchvision.datasets.CIFAR10(\n",
    "#     root=\"./data/cifar\", train=False, download=True, transform=transform_test)\n",
    "# testloader = torch.utils.data.DataLoader(\n",
    "#     testset, batch_size=args.bs, shuffle=False, num_workers=2)\n",
    "\n",
    "dataset_train = datasets.CIFAR10('../data/cifar', train=True, download=True, transform=transform_train)\n",
    "dataset_test = datasets.CIFAR10('../data/cifar', train=False, download=True, transform=transform_test)\n",
    "if args.iid:\n",
    "    dict_users = cifar_iid(dataset_train, args.num_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of X: (50000, 3072)\n",
      "size of Y: (50000, 10)\n",
      "(50000, 10)\n",
      "2.072988976516284\n"
     ]
    }
   ],
   "source": [
    "encoding_input_array_np = np.empty((len(dataset_train),32*32*3))\n",
    "encoding_label_array_np = np.empty((len(dataset_train),args.num_classes))\n",
    "print(\"size of X:\" ,encoding_input_array_np.shape)\n",
    "print(\"size of Y:\" ,encoding_label_array_np.shape)\n",
    "\n",
    "Size_submatrices = int(50000/args.num_users)\n",
    "\n",
    "\n",
    "for i in range(args.num_users):\n",
    "    \n",
    "    stt_pos = i*Size_submatrices\n",
    "    end_pos = (i+1)*Size_submatrices\n",
    "#     print(i,stt_pos,end_pos)\n",
    "    Temp_train = DataLoader(DatasetSplit(dataset_train, dict_users[i]), batch_size=Size_submatrices, shuffle=True)\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(Temp_train):\n",
    "        \n",
    "        images_np = images.detach().cpu().numpy()\n",
    "        \n",
    "#         print(np.size(images_np))\n",
    "        encoding_input_array_np[stt_pos:end_pos,:] = np.reshape(images_np, (Size_submatrices,32*32*3))\n",
    "#         print(encoding_input_array_np[stt_pos:end_pos,:].shape)\n",
    "\n",
    "        onehot_labels = torch.nn.functional.one_hot(labels,num_classes=args.num_classes)\n",
    "        labels_np = onehot_labels.detach().cpu().numpy()\n",
    "#         print(labels_np.shape)\n",
    "        encoding_label_array_np[stt_pos:end_pos,:] = labels_np\n",
    "    \n",
    "print(np.shape(encoding_label_array_np))\n",
    "\n",
    "avg_power = np.sum(encoding_input_array_np*encoding_input_array_np, axis=1)/np.shape(encoding_input_array_np)[1]\n",
    "avg_power = np.sum(avg_power)/np.shape(encoding_input_array_np)[0]\n",
    "print(avg_power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "sigma = 0.1\n",
      "\n",
      "\n",
      "\n",
      "Learning Rate = 0.001\n",
      "\n",
      "\n",
      "\n",
      "z_array: [-0.81  0.81]\n",
      "2.3175440334168256\n",
      "2.317544033416827\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 25000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 25000 \n",
      "\n",
      "(T, sigma)= 3 0.1 )  0 -th Trial!!\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3042 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round   0, Average loss 2.304 Test accuracy 10.000\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5417 \n",
      "Accuracy: 6173/10000 (61.73%)\n",
      "\n",
      "Round   1, Average loss 1.542 Test accuracy 61.730\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4107 \n",
      "Accuracy: 6443/10000 (64.43%)\n",
      "\n",
      "Round   2, Average loss 1.411 Test accuracy 64.430\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4290 \n",
      "Accuracy: 6282/10000 (62.82%)\n",
      "\n",
      "Round   3, Average loss 1.429 Test accuracy 62.820\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.3121 \n",
      "Accuracy: 6878/10000 (68.78%)\n",
      "\n",
      "Round   4, Average loss 1.312 Test accuracy 68.780\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.3189 \n",
      "Accuracy: 6881/10000 (68.81%)\n",
      "\n",
      "Round   5, Average loss 1.319 Test accuracy 68.810\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.3216 \n",
      "Accuracy: 6811/10000 (68.11%)\n",
      "\n",
      "Round   6, Average loss 1.322 Test accuracy 68.110\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.3169 \n",
      "Accuracy: 6853/10000 (68.53%)\n",
      "\n",
      "Round   7, Average loss 1.317 Test accuracy 68.530\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2892 \n",
      "Accuracy: 7013/10000 (70.13%)\n",
      "\n",
      "Round   8, Average loss 1.289 Test accuracy 70.130\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.3181 \n",
      "Accuracy: 6823/10000 (68.23%)\n",
      "\n",
      "Round   9, Average loss 1.318 Test accuracy 68.230\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.3661 \n",
      "Accuracy: 6580/10000 (65.80%)\n",
      "\n",
      "Round  10, Average loss 1.366 Test accuracy 65.800\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2958 \n",
      "Accuracy: 6976/10000 (69.76%)\n",
      "\n",
      "Round  11, Average loss 1.296 Test accuracy 69.760\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.3332 \n",
      "Accuracy: 6789/10000 (67.89%)\n",
      "\n",
      "Round  12, Average loss 1.333 Test accuracy 67.890\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.3735 \n",
      "Accuracy: 6599/10000 (65.99%)\n",
      "\n",
      "Round  13, Average loss 1.373 Test accuracy 65.990\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.3065 \n",
      "Accuracy: 6940/10000 (69.40%)\n",
      "\n",
      "Round  14, Average loss 1.306 Test accuracy 69.400\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.3084 \n",
      "Accuracy: 6949/10000 (69.49%)\n",
      "\n",
      "Round  15, Average loss 1.308 Test accuracy 69.490\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.3011 \n",
      "Accuracy: 7007/10000 (70.07%)\n",
      "\n",
      "Round  16, Average loss 1.301 Test accuracy 70.070\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.3165 \n",
      "Accuracy: 6917/10000 (69.17%)\n",
      "\n",
      "Round  17, Average loss 1.317 Test accuracy 69.170\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.3041 \n",
      "Accuracy: 6974/10000 (69.74%)\n",
      "\n",
      "Round  18, Average loss 1.304 Test accuracy 69.740\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.3091 \n",
      "Accuracy: 6928/10000 (69.28%)\n",
      "\n",
      "Round  19, Average loss 1.309 Test accuracy 69.280\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2946 \n",
      "Accuracy: 7011/10000 (70.11%)\n",
      "\n",
      "Round  20, Average loss 1.295 Test accuracy 70.110\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2779 \n",
      "Accuracy: 7153/10000 (71.53%)\n",
      "\n",
      "Round  21, Average loss 1.278 Test accuracy 71.530\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2965 \n",
      "Accuracy: 6963/10000 (69.63%)\n",
      "\n",
      "Round  22, Average loss 1.296 Test accuracy 69.630\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2734 \n",
      "Accuracy: 7143/10000 (71.43%)\n",
      "\n",
      "Round  23, Average loss 1.273 Test accuracy 71.430\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2843 \n",
      "Accuracy: 7033/10000 (70.33%)\n",
      "\n",
      "Round  24, Average loss 1.284 Test accuracy 70.330\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2740 \n",
      "Accuracy: 7124/10000 (71.24%)\n",
      "\n",
      "Round  25, Average loss 1.274 Test accuracy 71.240\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2580 \n",
      "Accuracy: 7169/10000 (71.69%)\n",
      "\n",
      "Round  26, Average loss 1.258 Test accuracy 71.690\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.3347 \n",
      "Accuracy: 6814/10000 (68.14%)\n",
      "\n",
      "Round  27, Average loss 1.335 Test accuracy 68.140\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.3228 \n",
      "Accuracy: 6811/10000 (68.11%)\n",
      "\n",
      "Round  28, Average loss 1.323 Test accuracy 68.110\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.3049 \n",
      "Accuracy: 6938/10000 (69.38%)\n",
      "\n",
      "Round  29, Average loss 1.305 Test accuracy 69.380\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2609 \n",
      "Accuracy: 7130/10000 (71.30%)\n",
      "\n",
      "Round  30, Average loss 1.261 Test accuracy 71.300\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2472 \n",
      "Accuracy: 7244/10000 (72.44%)\n",
      "\n",
      "Round  31, Average loss 1.247 Test accuracy 72.440\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2810 \n",
      "Accuracy: 7025/10000 (70.25%)\n",
      "\n",
      "Round  32, Average loss 1.281 Test accuracy 70.250\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2694 \n",
      "Accuracy: 7160/10000 (71.60%)\n",
      "\n",
      "Round  33, Average loss 1.269 Test accuracy 71.600\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2292 \n",
      "Accuracy: 7313/10000 (73.13%)\n",
      "\n",
      "Round  34, Average loss 1.229 Test accuracy 73.130\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2801 \n",
      "Accuracy: 7134/10000 (71.34%)\n",
      "\n",
      "Round  35, Average loss 1.280 Test accuracy 71.340\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2757 \n",
      "Accuracy: 7109/10000 (71.09%)\n",
      "\n",
      "Round  36, Average loss 1.276 Test accuracy 71.090\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2762 \n",
      "Accuracy: 7128/10000 (71.28%)\n",
      "\n",
      "Round  37, Average loss 1.276 Test accuracy 71.280\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2605 \n",
      "Accuracy: 7166/10000 (71.66%)\n",
      "\n",
      "Round  38, Average loss 1.260 Test accuracy 71.660\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2810 \n",
      "Accuracy: 7045/10000 (70.45%)\n",
      "\n",
      "Round  39, Average loss 1.281 Test accuracy 70.450\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.3120 \n",
      "Accuracy: 6909/10000 (69.09%)\n",
      "\n",
      "Round  40, Average loss 1.312 Test accuracy 69.090\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2949 \n",
      "Accuracy: 7041/10000 (70.41%)\n",
      "\n",
      "Round  41, Average loss 1.295 Test accuracy 70.410\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.3020 \n",
      "Accuracy: 6984/10000 (69.84%)\n",
      "\n",
      "Round  42, Average loss 1.302 Test accuracy 69.840\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2659 \n",
      "Accuracy: 7149/10000 (71.49%)\n",
      "\n",
      "Round  43, Average loss 1.266 Test accuracy 71.490\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2661 \n",
      "Accuracy: 7165/10000 (71.65%)\n",
      "\n",
      "Round  44, Average loss 1.266 Test accuracy 71.650\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2514 \n",
      "Accuracy: 7274/10000 (72.74%)\n",
      "\n",
      "Round  45, Average loss 1.251 Test accuracy 72.740\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2773 \n",
      "Accuracy: 7078/10000 (70.78%)\n",
      "\n",
      "Round  46, Average loss 1.277 Test accuracy 70.780\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2344 \n",
      "Accuracy: 7322/10000 (73.22%)\n",
      "\n",
      "Round  47, Average loss 1.234 Test accuracy 73.220\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2845 \n",
      "Accuracy: 7043/10000 (70.43%)\n",
      "\n",
      "Round  48, Average loss 1.285 Test accuracy 70.430\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2881 \n",
      "Accuracy: 6993/10000 (69.93%)\n",
      "\n",
      "Round  49, Average loss 1.288 Test accuracy 69.930\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "K = 2\n",
    "T = 3\n",
    "sigma = 0.1\n",
    "Noise_Alloc = [0,2,4]\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "\n",
    "N_array = [2]\n",
    "B_array = [0.5]\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 50\n",
    "\n",
    "lr_array = [0.001] # 0.001 is the bset\n",
    "\n",
    "sigma_array = [0.1]\n",
    "# lr_array = [0.1, 0.01, 0.005, 0.001,0.0005]\n",
    "\n",
    "\n",
    "loss_test_arr_K2_G1_v3 = np.zeros((len(sigma_array),len(lr_array),N_trials,N_epochs))\n",
    "acc_test_arr_K2_G1_v3  = np.zeros((len(sigma_array),len(lr_array),N_trials,N_epochs))\n",
    "\n",
    "N = 2\n",
    "for sigma_idx in range(len(sigma_array)):\n",
    "    \n",
    "    sigma = sigma_array[sigma_idx]\n",
    "    \n",
    "    print('\\n\\n')\n",
    "    print('sigma =',sigma)\n",
    "    print('\\n\\n')\n",
    "           \n",
    "        \n",
    "    # print(\"alpha_array: \",alpha_array,'\\n')\n",
    "    \n",
    "    \n",
    "    for lr_idx in range(len(lr_array)):\n",
    "        \n",
    "        args.lr = lr_array[lr_idx]\n",
    "        \n",
    "        print('Learning Rate =',args.lr)\n",
    "        print('\\n\\n')\n",
    "#         while(len(z_array)<N):\n",
    "#             z_tmp = np.random.uniform(-1,1,1)\n",
    "#             MIS_tmp = MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_tmp], 1,sigma)\n",
    "#             if MIS_tmp < B and MIS_tmp > 0.1:\n",
    "#                 z_array.append(z_tmp[0])\n",
    "#         \n",
    "#         z_array = np.sort(z_array)\n",
    "#         print(N_idx,'!!!')\n",
    "#         if N_idx==0:\n",
    "# #             z_array = np.array([-0.94,-0.534,0.534, 0.94])\n",
    "# #         elif N_idx==1:\n",
    "# #             z_array = np.array([-0.94, -0.73, 0.73, 0.94])\n",
    "#         elif N_idx==1:\n",
    "#             z_array = np.array([-0.94, -0.125, 0.125, 0.94])\n",
    "#         else:\n",
    "# #             z_array = np.array([-0.94, -0.73, -0.534, -0.125, 0.125, 0.534, 0.73, 0.94])\n",
    "# #             z_array = np.array([-0.9, -0.81, -0.22, -0.20, 0.20, 0.22, 0.81, 0.9])\n",
    "        \n",
    "        z_array = np.array([-0.81, 0.81])\n",
    "        \n",
    "        print('z_array:',z_array)\n",
    "        if sigma != 0:\n",
    "            for j in range(len(z_array)):\n",
    "                print(MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_array[j]], 1,sigma))\n",
    "\n",
    "        \n",
    "        _Noise_label = np.ones((25000*T,10)) * 0.1\n",
    "\n",
    "        X_tilde,a,b = BACC_Enc_Data_v3(encoding_input_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde,a,b = BACC_Enc_Data_v3(encoding_label_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "\n",
    "        m = N # m is the number of received result @ master\n",
    "    #     print('number of results:',m)\n",
    "\n",
    "        for trial_idx in range(N_trials):\n",
    "            print('(T, sigma)=',T,sigma,') ',trial_idx,'-th Trial!!')\n",
    "\n",
    "            net_glob = VGG('VGG11')\n",
    "            net_glob.cuda()\n",
    "            net_glob.train()\n",
    "\n",
    "            # copy weights\n",
    "            w_glob = net_glob.state_dict()\n",
    "\n",
    "            for iter in range(N_epochs): #args.epochs\n",
    "                w_locals, loss_locals = [], []\n",
    "                idxs_users = np.random.choice(range(N), m, replace=False)\n",
    "                idxs_users = np.sort(idxs_users)\n",
    "                print('selected users:',idxs_users)\n",
    "                \n",
    "                coded_net = BACC_Enc_Model_withNoise_v4(net_glob.cuda(), N, K, T, sigma, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "                dec_z_array = []\n",
    "                for idx in idxs_users: #for idx in range(N):\n",
    "            #         print(idx)\n",
    "                    local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[idx,:,:], label=y_tilde[idx,:,:])\n",
    "                    w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "#                     w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                    w_locals.append(copy.deepcopy(w))\n",
    "                    loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                    dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "                # update global weights\n",
    "                #w_glob = FedAvg(w_locals)\n",
    "                w_glob = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "\n",
    "\n",
    "                # copy weight to net_glob\n",
    "                net_glob.load_state_dict(w_glob)\n",
    "\n",
    "                # print loss\n",
    "            #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "            #     loss_train_arr.append(loss_train)\n",
    "\n",
    "                acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "                acc_test_arr_K2_G1_v3[sigma_idx][lr_idx][trial_idx][iter] = acc_test\n",
    "                loss_test_arr_K2_G1_v3[sigma_idx][lr_idx][trial_idx][iter] = loss_test\n",
    "                \n",
    "#                 PATH = \"./save_models/CIFAR10_LeNet_BACC_woPowerAlign_K2_sigma_E50_iter\"+str(iter)+\".pt\"\n",
    "#                 torch.save(net_glob.state_dict(), PATH)\n",
    "                \n",
    "                if iter % 1 ==0:\n",
    "                    print('Round {:3d}, Average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_test, acc_test))\n",
    "                #print(loss_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2. Power Normalization of encoded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "sigma = 0.1\n",
      "\n",
      "\n",
      "\n",
      "Learning Rate = 0.001\n",
      "\n",
      "\n",
      "\n",
      "z_array: [-0.81  0.81]\n",
      "2.3175440334168256\n",
      "2.317544033416827\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 25000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 25000 \n",
      "\n",
      "Adjust the power of X_tilde\n",
      "0 0.41032127848630773\n",
      "power after adjusting = 2.0729889765162834\n",
      "1 0.41200328927527813\n",
      "power after adjusting = 2.072988976516283\n",
      "\n",
      "(T, sigma)= 3 0.1 )  0 -th Trial!!\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3117 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round   0, Average loss 2.312 Test accuracy 10.000\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4833 \n",
      "Accuracy: 6417/10000 (64.17%)\n",
      "\n",
      "Round   1, Average loss 1.483 Test accuracy 64.170\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2451 \n",
      "Accuracy: 7369/10000 (73.69%)\n",
      "\n",
      "Round   2, Average loss 1.245 Test accuracy 73.690\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2395 \n",
      "Accuracy: 7308/10000 (73.08%)\n",
      "\n",
      "Round   3, Average loss 1.240 Test accuracy 73.080\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2101 \n",
      "Accuracy: 7457/10000 (74.57%)\n",
      "\n",
      "Round   4, Average loss 1.210 Test accuracy 74.570\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.1769 \n",
      "Accuracy: 7639/10000 (76.39%)\n",
      "\n",
      "Round   5, Average loss 1.177 Test accuracy 76.390\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.1673 \n",
      "Accuracy: 7657/10000 (76.57%)\n",
      "\n",
      "Round   6, Average loss 1.167 Test accuracy 76.570\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.1876 \n",
      "Accuracy: 7566/10000 (75.66%)\n",
      "\n",
      "Round   7, Average loss 1.188 Test accuracy 75.660\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.1720 \n",
      "Accuracy: 7619/10000 (76.19%)\n",
      "\n",
      "Round   8, Average loss 1.172 Test accuracy 76.190\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.1749 \n",
      "Accuracy: 7663/10000 (76.63%)\n",
      "\n",
      "Round   9, Average loss 1.175 Test accuracy 76.630\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.1922 \n",
      "Accuracy: 7556/10000 (75.56%)\n",
      "\n",
      "Round  10, Average loss 1.192 Test accuracy 75.560\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.1621 \n",
      "Accuracy: 7661/10000 (76.61%)\n",
      "\n",
      "Round  11, Average loss 1.162 Test accuracy 76.610\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.1870 \n",
      "Accuracy: 7537/10000 (75.37%)\n",
      "\n",
      "Round  12, Average loss 1.187 Test accuracy 75.370\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.1551 \n",
      "Accuracy: 7727/10000 (77.27%)\n",
      "\n",
      "Round  13, Average loss 1.155 Test accuracy 77.270\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.1544 \n",
      "Accuracy: 7712/10000 (77.12%)\n",
      "\n",
      "Round  14, Average loss 1.154 Test accuracy 77.120\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.1508 \n",
      "Accuracy: 7759/10000 (77.59%)\n",
      "\n",
      "Round  15, Average loss 1.151 Test accuracy 77.590\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.1597 \n",
      "Accuracy: 7682/10000 (76.82%)\n",
      "\n",
      "Round  16, Average loss 1.160 Test accuracy 76.820\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.1556 \n",
      "Accuracy: 7705/10000 (77.05%)\n",
      "\n",
      "Round  17, Average loss 1.156 Test accuracy 77.050\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.1827 \n",
      "Accuracy: 7559/10000 (75.59%)\n",
      "\n",
      "Round  18, Average loss 1.183 Test accuracy 75.590\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.1724 \n",
      "Accuracy: 7650/10000 (76.50%)\n",
      "\n",
      "Round  19, Average loss 1.172 Test accuracy 76.500\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.1969 \n",
      "Accuracy: 7498/10000 (74.98%)\n",
      "\n",
      "Round  20, Average loss 1.197 Test accuracy 74.980\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.1614 \n",
      "Accuracy: 7640/10000 (76.40%)\n",
      "\n",
      "Round  21, Average loss 1.161 Test accuracy 76.400\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.1857 \n",
      "Accuracy: 7608/10000 (76.08%)\n",
      "\n",
      "Round  22, Average loss 1.186 Test accuracy 76.080\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.1433 \n",
      "Accuracy: 7757/10000 (77.57%)\n",
      "\n",
      "Round  23, Average loss 1.143 Test accuracy 77.570\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.1610 \n",
      "Accuracy: 7676/10000 (76.76%)\n",
      "\n",
      "Round  24, Average loss 1.161 Test accuracy 76.760\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.1770 \n",
      "Accuracy: 7564/10000 (75.64%)\n",
      "\n",
      "Round  25, Average loss 1.177 Test accuracy 75.640\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.1719 \n",
      "Accuracy: 7582/10000 (75.82%)\n",
      "\n",
      "Round  26, Average loss 1.172 Test accuracy 75.820\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.1612 \n",
      "Accuracy: 7619/10000 (76.19%)\n",
      "\n",
      "Round  27, Average loss 1.161 Test accuracy 76.190\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.1512 \n",
      "Accuracy: 7777/10000 (77.77%)\n",
      "\n",
      "Round  28, Average loss 1.151 Test accuracy 77.770\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.1668 \n",
      "Accuracy: 7663/10000 (76.63%)\n",
      "\n",
      "Round  29, Average loss 1.167 Test accuracy 76.630\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.1714 \n",
      "Accuracy: 7684/10000 (76.84%)\n",
      "\n",
      "Round  30, Average loss 1.171 Test accuracy 76.840\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.1610 \n",
      "Accuracy: 7775/10000 (77.75%)\n",
      "\n",
      "Round  31, Average loss 1.161 Test accuracy 77.750\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.1691 \n",
      "Accuracy: 7642/10000 (76.42%)\n",
      "\n",
      "Round  32, Average loss 1.169 Test accuracy 76.420\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.1609 \n",
      "Accuracy: 7641/10000 (76.41%)\n",
      "\n",
      "Round  33, Average loss 1.161 Test accuracy 76.410\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.1640 \n",
      "Accuracy: 7638/10000 (76.38%)\n",
      "\n",
      "Round  34, Average loss 1.164 Test accuracy 76.380\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.1711 \n",
      "Accuracy: 7663/10000 (76.63%)\n",
      "\n",
      "Round  35, Average loss 1.171 Test accuracy 76.630\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.1761 \n",
      "Accuracy: 7597/10000 (75.97%)\n",
      "\n",
      "Round  36, Average loss 1.176 Test accuracy 75.970\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.1559 \n",
      "Accuracy: 7696/10000 (76.96%)\n",
      "\n",
      "Round  37, Average loss 1.156 Test accuracy 76.960\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.1818 \n",
      "Accuracy: 7618/10000 (76.18%)\n",
      "\n",
      "Round  38, Average loss 1.182 Test accuracy 76.180\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.1694 \n",
      "Accuracy: 7648/10000 (76.48%)\n",
      "\n",
      "Round  39, Average loss 1.169 Test accuracy 76.480\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.1826 \n",
      "Accuracy: 7532/10000 (75.32%)\n",
      "\n",
      "Round  40, Average loss 1.183 Test accuracy 75.320\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2023 \n",
      "Accuracy: 7516/10000 (75.16%)\n",
      "\n",
      "Round  41, Average loss 1.202 Test accuracy 75.160\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.1578 \n",
      "Accuracy: 7649/10000 (76.49%)\n",
      "\n",
      "Round  42, Average loss 1.158 Test accuracy 76.490\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.1621 \n",
      "Accuracy: 7671/10000 (76.71%)\n",
      "\n",
      "Round  43, Average loss 1.162 Test accuracy 76.710\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.1766 \n",
      "Accuracy: 7665/10000 (76.65%)\n",
      "\n",
      "Round  44, Average loss 1.177 Test accuracy 76.650\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.1772 \n",
      "Accuracy: 7607/10000 (76.07%)\n",
      "\n",
      "Round  45, Average loss 1.177 Test accuracy 76.070\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.1550 \n",
      "Accuracy: 7718/10000 (77.18%)\n",
      "\n",
      "Round  46, Average loss 1.155 Test accuracy 77.180\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.1458 \n",
      "Accuracy: 7773/10000 (77.73%)\n",
      "\n",
      "Round  47, Average loss 1.146 Test accuracy 77.730\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.1496 \n",
      "Accuracy: 7755/10000 (77.55%)\n",
      "\n",
      "Round  48, Average loss 1.150 Test accuracy 77.550\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.1925 \n",
      "Accuracy: 7485/10000 (74.85%)\n",
      "\n",
      "Round  49, Average loss 1.193 Test accuracy 74.850\n",
      "\n",
      "\n",
      "\n",
      "sigma = 0.3\n",
      "\n",
      "\n",
      "\n",
      "Learning Rate = 0.001\n",
      "\n",
      "\n",
      "\n",
      "z_array: [-0.81  0.81]\n",
      "1.2192623088642704\n",
      "1.2192623088642716\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 25000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 25000 \n",
      "\n",
      "Adjust the power of X_tilde\n",
      "0 0.44813736838324136\n",
      "power after adjusting = 2.0729889765162834\n",
      "1 0.4498306225704006\n",
      "power after adjusting = 2.0729889765162843\n",
      "\n",
      "(T, sigma)= 3 0.3 )  0 -th Trial!!\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3051 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round   0, Average loss 2.305 Test accuracy 10.000\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5284 \n",
      "Accuracy: 6177/10000 (61.77%)\n",
      "\n",
      "Round   1, Average loss 1.528 Test accuracy 61.770\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.3367 \n",
      "Accuracy: 6842/10000 (68.42%)\n",
      "\n",
      "Round   2, Average loss 1.337 Test accuracy 68.420\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2817 \n",
      "Accuracy: 7088/10000 (70.88%)\n",
      "\n",
      "Round   3, Average loss 1.282 Test accuracy 70.880\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2973 \n",
      "Accuracy: 6975/10000 (69.75%)\n",
      "\n",
      "Round   4, Average loss 1.297 Test accuracy 69.750\n",
      "selected users: [0 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.2768 \n",
      "Accuracy: 7042/10000 (70.42%)\n",
      "\n",
      "Round   5, Average loss 1.277 Test accuracy 70.420\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2836 \n",
      "Accuracy: 7015/10000 (70.15%)\n",
      "\n",
      "Round   6, Average loss 1.284 Test accuracy 70.150\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.3067 \n",
      "Accuracy: 6961/10000 (69.61%)\n",
      "\n",
      "Round   7, Average loss 1.307 Test accuracy 69.610\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2745 \n",
      "Accuracy: 7107/10000 (71.07%)\n",
      "\n",
      "Round   8, Average loss 1.274 Test accuracy 71.070\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2320 \n",
      "Accuracy: 7362/10000 (73.62%)\n",
      "\n",
      "Round   9, Average loss 1.232 Test accuracy 73.620\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2480 \n",
      "Accuracy: 7162/10000 (71.62%)\n",
      "\n",
      "Round  10, Average loss 1.248 Test accuracy 71.620\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2217 \n",
      "Accuracy: 7263/10000 (72.63%)\n",
      "\n",
      "Round  11, Average loss 1.222 Test accuracy 72.630\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2387 \n",
      "Accuracy: 7283/10000 (72.83%)\n",
      "\n",
      "Round  12, Average loss 1.239 Test accuracy 72.830\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2280 \n",
      "Accuracy: 7316/10000 (73.16%)\n",
      "\n",
      "Round  13, Average loss 1.228 Test accuracy 73.160\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2146 \n",
      "Accuracy: 7331/10000 (73.31%)\n",
      "\n",
      "Round  14, Average loss 1.215 Test accuracy 73.310\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2321 \n",
      "Accuracy: 7284/10000 (72.84%)\n",
      "\n",
      "Round  15, Average loss 1.232 Test accuracy 72.840\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2292 \n",
      "Accuracy: 7325/10000 (73.25%)\n",
      "\n",
      "Round  16, Average loss 1.229 Test accuracy 73.250\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2264 \n",
      "Accuracy: 7319/10000 (73.19%)\n",
      "\n",
      "Round  17, Average loss 1.226 Test accuracy 73.190\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2110 \n",
      "Accuracy: 7436/10000 (74.36%)\n",
      "\n",
      "Round  18, Average loss 1.211 Test accuracy 74.360\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2154 \n",
      "Accuracy: 7372/10000 (73.72%)\n",
      "\n",
      "Round  19, Average loss 1.215 Test accuracy 73.720\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2062 \n",
      "Accuracy: 7407/10000 (74.07%)\n",
      "\n",
      "Round  20, Average loss 1.206 Test accuracy 74.070\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2487 \n",
      "Accuracy: 7239/10000 (72.39%)\n",
      "\n",
      "Round  21, Average loss 1.249 Test accuracy 72.390\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2149 \n",
      "Accuracy: 7418/10000 (74.18%)\n",
      "\n",
      "Round  22, Average loss 1.215 Test accuracy 74.180\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2373 \n",
      "Accuracy: 7254/10000 (72.54%)\n",
      "\n",
      "Round  23, Average loss 1.237 Test accuracy 72.540\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2055 \n",
      "Accuracy: 7433/10000 (74.33%)\n",
      "\n",
      "Round  24, Average loss 1.205 Test accuracy 74.330\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2322 \n",
      "Accuracy: 7302/10000 (73.02%)\n",
      "\n",
      "Round  25, Average loss 1.232 Test accuracy 73.020\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2480 \n",
      "Accuracy: 7170/10000 (71.70%)\n",
      "\n",
      "Round  26, Average loss 1.248 Test accuracy 71.700\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2408 \n",
      "Accuracy: 7280/10000 (72.80%)\n",
      "\n",
      "Round  27, Average loss 1.241 Test accuracy 72.800\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2636 \n",
      "Accuracy: 7108/10000 (71.08%)\n",
      "\n",
      "Round  28, Average loss 1.264 Test accuracy 71.080\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2383 \n",
      "Accuracy: 7281/10000 (72.81%)\n",
      "\n",
      "Round  29, Average loss 1.238 Test accuracy 72.810\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2286 \n",
      "Accuracy: 7317/10000 (73.17%)\n",
      "\n",
      "Round  30, Average loss 1.229 Test accuracy 73.170\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2077 \n",
      "Accuracy: 7443/10000 (74.43%)\n",
      "\n",
      "Round  31, Average loss 1.208 Test accuracy 74.430\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2427 \n",
      "Accuracy: 7340/10000 (73.40%)\n",
      "\n",
      "Round  32, Average loss 1.243 Test accuracy 73.400\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2437 \n",
      "Accuracy: 7217/10000 (72.17%)\n",
      "\n",
      "Round  33, Average loss 1.244 Test accuracy 72.170\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2567 \n",
      "Accuracy: 7176/10000 (71.76%)\n",
      "\n",
      "Round  34, Average loss 1.257 Test accuracy 71.760\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2431 \n",
      "Accuracy: 7187/10000 (71.87%)\n",
      "\n",
      "Round  35, Average loss 1.243 Test accuracy 71.870\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2328 \n",
      "Accuracy: 7237/10000 (72.37%)\n",
      "\n",
      "Round  36, Average loss 1.233 Test accuracy 72.370\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2183 \n",
      "Accuracy: 7389/10000 (73.89%)\n",
      "\n",
      "Round  37, Average loss 1.218 Test accuracy 73.890\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2184 \n",
      "Accuracy: 7374/10000 (73.74%)\n",
      "\n",
      "Round  38, Average loss 1.218 Test accuracy 73.740\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2365 \n",
      "Accuracy: 7272/10000 (72.72%)\n",
      "\n",
      "Round  39, Average loss 1.236 Test accuracy 72.720\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2305 \n",
      "Accuracy: 7285/10000 (72.85%)\n",
      "\n",
      "Round  40, Average loss 1.231 Test accuracy 72.850\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2202 \n",
      "Accuracy: 7286/10000 (72.86%)\n",
      "\n",
      "Round  41, Average loss 1.220 Test accuracy 72.860\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2352 \n",
      "Accuracy: 7187/10000 (71.87%)\n",
      "\n",
      "Round  42, Average loss 1.235 Test accuracy 71.870\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2453 \n",
      "Accuracy: 7248/10000 (72.48%)\n",
      "\n",
      "Round  43, Average loss 1.245 Test accuracy 72.480\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2103 \n",
      "Accuracy: 7418/10000 (74.18%)\n",
      "\n",
      "Round  44, Average loss 1.210 Test accuracy 74.180\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2237 \n",
      "Accuracy: 7361/10000 (73.61%)\n",
      "\n",
      "Round  45, Average loss 1.224 Test accuracy 73.610\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2172 \n",
      "Accuracy: 7345/10000 (73.45%)\n",
      "\n",
      "Round  46, Average loss 1.217 Test accuracy 73.450\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2197 \n",
      "Accuracy: 7323/10000 (73.23%)\n",
      "\n",
      "Round  47, Average loss 1.220 Test accuracy 73.230\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2252 \n",
      "Accuracy: 7292/10000 (72.92%)\n",
      "\n",
      "Round  48, Average loss 1.225 Test accuracy 72.920\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2016 \n",
      "Accuracy: 7423/10000 (74.23%)\n",
      "\n",
      "Round  49, Average loss 1.202 Test accuracy 74.230\n",
      "\n",
      "\n",
      "\n",
      "sigma = 1\n",
      "\n",
      "\n",
      "\n",
      "Learning Rate = 0.001\n",
      "\n",
      "\n",
      "\n",
      "z_array: [-0.81  0.81]\n",
      "0.4838626198316927\n",
      "0.48386261983169315\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 25000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 25000 \n",
      "\n",
      "Adjust the power of X_tilde\n",
      "0 0.8780508346560063\n",
      "power after adjusting = 2.072988976516284\n",
      "1 0.8795096603838369\n",
      "power after adjusting = 2.0729889765162843\n",
      "\n",
      "(T, sigma)= 3 1 )  0 -th Trial!!\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3061 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round   0, Average loss 2.306 Test accuracy 10.000\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.8147 \n",
      "Accuracy: 4211/10000 (42.11%)\n",
      "\n",
      "Round   1, Average loss 1.815 Test accuracy 42.110\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6689 \n",
      "Accuracy: 4870/10000 (48.70%)\n",
      "\n",
      "Round   2, Average loss 1.669 Test accuracy 48.700\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6042 \n",
      "Accuracy: 5296/10000 (52.96%)\n",
      "\n",
      "Round   3, Average loss 1.604 Test accuracy 52.960\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5624 \n",
      "Accuracy: 5461/10000 (54.61%)\n",
      "\n",
      "Round   4, Average loss 1.562 Test accuracy 54.610\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5713 \n",
      "Accuracy: 5322/10000 (53.22%)\n",
      "\n",
      "Round   5, Average loss 1.571 Test accuracy 53.220\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5081 \n",
      "Accuracy: 5716/10000 (57.16%)\n",
      "\n",
      "Round   6, Average loss 1.508 Test accuracy 57.160\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5435 \n",
      "Accuracy: 5502/10000 (55.02%)\n",
      "\n",
      "Round   7, Average loss 1.543 Test accuracy 55.020\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5523 \n",
      "Accuracy: 5438/10000 (54.38%)\n",
      "\n",
      "Round   8, Average loss 1.552 Test accuracy 54.380\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4910 \n",
      "Accuracy: 5773/10000 (57.73%)\n",
      "\n",
      "Round   9, Average loss 1.491 Test accuracy 57.730\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4958 \n",
      "Accuracy: 5817/10000 (58.17%)\n",
      "\n",
      "Round  10, Average loss 1.496 Test accuracy 58.170\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5097 \n",
      "Accuracy: 5738/10000 (57.38%)\n",
      "\n",
      "Round  11, Average loss 1.510 Test accuracy 57.380\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4715 \n",
      "Accuracy: 5915/10000 (59.15%)\n",
      "\n",
      "Round  12, Average loss 1.471 Test accuracy 59.150\n",
      "selected users: [0 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.4874 \n",
      "Accuracy: 5918/10000 (59.18%)\n",
      "\n",
      "Round  13, Average loss 1.487 Test accuracy 59.180\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5097 \n",
      "Accuracy: 5735/10000 (57.35%)\n",
      "\n",
      "Round  14, Average loss 1.510 Test accuracy 57.350\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5024 \n",
      "Accuracy: 5662/10000 (56.62%)\n",
      "\n",
      "Round  15, Average loss 1.502 Test accuracy 56.620\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5065 \n",
      "Accuracy: 5704/10000 (57.04%)\n",
      "\n",
      "Round  16, Average loss 1.507 Test accuracy 57.040\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5012 \n",
      "Accuracy: 5704/10000 (57.04%)\n",
      "\n",
      "Round  17, Average loss 1.501 Test accuracy 57.040\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4820 \n",
      "Accuracy: 5812/10000 (58.12%)\n",
      "\n",
      "Round  18, Average loss 1.482 Test accuracy 58.120\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4794 \n",
      "Accuracy: 5850/10000 (58.50%)\n",
      "\n",
      "Round  19, Average loss 1.479 Test accuracy 58.500\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5092 \n",
      "Accuracy: 5709/10000 (57.09%)\n",
      "\n",
      "Round  20, Average loss 1.509 Test accuracy 57.090\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4968 \n",
      "Accuracy: 5799/10000 (57.99%)\n",
      "\n",
      "Round  21, Average loss 1.497 Test accuracy 57.990\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4596 \n",
      "Accuracy: 5968/10000 (59.68%)\n",
      "\n",
      "Round  22, Average loss 1.460 Test accuracy 59.680\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4897 \n",
      "Accuracy: 5917/10000 (59.17%)\n",
      "\n",
      "Round  23, Average loss 1.490 Test accuracy 59.170\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5038 \n",
      "Accuracy: 5802/10000 (58.02%)\n",
      "\n",
      "Round  24, Average loss 1.504 Test accuracy 58.020\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5071 \n",
      "Accuracy: 5737/10000 (57.37%)\n",
      "\n",
      "Round  25, Average loss 1.507 Test accuracy 57.370\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4779 \n",
      "Accuracy: 5926/10000 (59.26%)\n",
      "\n",
      "Round  26, Average loss 1.478 Test accuracy 59.260\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4663 \n",
      "Accuracy: 5979/10000 (59.79%)\n",
      "\n",
      "Round  27, Average loss 1.466 Test accuracy 59.790\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5190 \n",
      "Accuracy: 5731/10000 (57.31%)\n",
      "\n",
      "Round  28, Average loss 1.519 Test accuracy 57.310\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5012 \n",
      "Accuracy: 5790/10000 (57.90%)\n",
      "\n",
      "Round  29, Average loss 1.501 Test accuracy 57.900\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5174 \n",
      "Accuracy: 5673/10000 (56.73%)\n",
      "\n",
      "Round  30, Average loss 1.517 Test accuracy 56.730\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5252 \n",
      "Accuracy: 5626/10000 (56.26%)\n",
      "\n",
      "Round  31, Average loss 1.525 Test accuracy 56.260\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5195 \n",
      "Accuracy: 5691/10000 (56.91%)\n",
      "\n",
      "Round  32, Average loss 1.520 Test accuracy 56.910\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5122 \n",
      "Accuracy: 5695/10000 (56.95%)\n",
      "\n",
      "Round  33, Average loss 1.512 Test accuracy 56.950\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5078 \n",
      "Accuracy: 5763/10000 (57.63%)\n",
      "\n",
      "Round  34, Average loss 1.508 Test accuracy 57.630\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5081 \n",
      "Accuracy: 5722/10000 (57.22%)\n",
      "\n",
      "Round  35, Average loss 1.508 Test accuracy 57.220\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5113 \n",
      "Accuracy: 5771/10000 (57.71%)\n",
      "\n",
      "Round  36, Average loss 1.511 Test accuracy 57.710\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4811 \n",
      "Accuracy: 5847/10000 (58.47%)\n",
      "\n",
      "Round  37, Average loss 1.481 Test accuracy 58.470\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5252 \n",
      "Accuracy: 5663/10000 (56.63%)\n",
      "\n",
      "Round  38, Average loss 1.525 Test accuracy 56.630\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4626 \n",
      "Accuracy: 5935/10000 (59.35%)\n",
      "\n",
      "Round  39, Average loss 1.463 Test accuracy 59.350\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5125 \n",
      "Accuracy: 5682/10000 (56.82%)\n",
      "\n",
      "Round  40, Average loss 1.512 Test accuracy 56.820\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4828 \n",
      "Accuracy: 5828/10000 (58.28%)\n",
      "\n",
      "Round  41, Average loss 1.483 Test accuracy 58.280\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5085 \n",
      "Accuracy: 5752/10000 (57.52%)\n",
      "\n",
      "Round  42, Average loss 1.508 Test accuracy 57.520\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5187 \n",
      "Accuracy: 5666/10000 (56.66%)\n",
      "\n",
      "Round  43, Average loss 1.519 Test accuracy 56.660\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5432 \n",
      "Accuracy: 5511/10000 (55.11%)\n",
      "\n",
      "Round  44, Average loss 1.543 Test accuracy 55.110\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5060 \n",
      "Accuracy: 5822/10000 (58.22%)\n",
      "\n",
      "Round  45, Average loss 1.506 Test accuracy 58.220\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5111 \n",
      "Accuracy: 5787/10000 (57.87%)\n",
      "\n",
      "Round  46, Average loss 1.511 Test accuracy 57.870\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4962 \n",
      "Accuracy: 5761/10000 (57.61%)\n",
      "\n",
      "Round  47, Average loss 1.496 Test accuracy 57.610\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5268 \n",
      "Accuracy: 5571/10000 (55.71%)\n",
      "\n",
      "Round  48, Average loss 1.527 Test accuracy 55.710\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5232 \n",
      "Accuracy: 5579/10000 (55.79%)\n",
      "\n",
      "Round  49, Average loss 1.523 Test accuracy 55.790\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "K = 2\n",
    "T = 3\n",
    "sigma = 0.1\n",
    "Noise_Alloc = [0,2,4]\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "\n",
    "N_array = [2]\n",
    "B_array = [0.5]\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 50\n",
    "\n",
    "lr_array = [0.001] # 0.001 is the bset\n",
    "\n",
    "sigma_array = [0.1,0.3,1]\n",
    "# lr_array = [0.1, 0.01, 0.005, 0.001,0.0005]\n",
    "\n",
    "\n",
    "loss_test_arr_K2_G1_powerAlign = np.zeros((len(sigma_array),len(lr_array),N_trials,N_epochs))\n",
    "acc_test_arr_K2_G1_powerAlign  = np.zeros((len(sigma_array),len(lr_array),N_trials,N_epochs))\n",
    "\n",
    "N = 2\n",
    "for sigma_idx in range(len(sigma_array)):\n",
    "    \n",
    "    sigma = sigma_array[sigma_idx]\n",
    "    \n",
    "    print('\\n\\n')\n",
    "    print('sigma =',sigma)\n",
    "    print('\\n\\n')\n",
    "           \n",
    "        \n",
    "    # print(\"alpha_array: \",alpha_array,'\\n')\n",
    "    \n",
    "    \n",
    "    for lr_idx in range(len(lr_array)):\n",
    "        \n",
    "        args.lr = lr_array[lr_idx]\n",
    "        \n",
    "        print('Learning Rate =',args.lr)\n",
    "        print('\\n\\n')\n",
    "#         while(len(z_array)<N):\n",
    "#             z_tmp = np.random.uniform(-1,1,1)\n",
    "#             MIS_tmp = MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_tmp], 1,sigma)\n",
    "#             if MIS_tmp < B and MIS_tmp > 0.1:\n",
    "#                 z_array.append(z_tmp[0])\n",
    "#         \n",
    "#         z_array = np.sort(z_array)\n",
    "#         print(N_idx,'!!!')\n",
    "#         if N_idx==0:\n",
    "# #             z_array = np.array([-0.94,-0.534,0.534, 0.94])\n",
    "# #         elif N_idx==1:\n",
    "# #             z_array = np.array([-0.94, -0.73, 0.73, 0.94])\n",
    "#         elif N_idx==1:\n",
    "#             z_array = np.array([-0.94, -0.125, 0.125, 0.94])\n",
    "#         else:\n",
    "# #             z_array = np.array([-0.94, -0.73, -0.534, -0.125, 0.125, 0.534, 0.73, 0.94])\n",
    "# #             z_array = np.array([-0.9, -0.81, -0.22, -0.20, 0.20, 0.22, 0.81, 0.9])\n",
    "        \n",
    "        z_array = np.array([-0.81, 0.81])\n",
    "        \n",
    "        print('z_array:',z_array)\n",
    "        if sigma != 0:\n",
    "            for j in range(len(z_array)):\n",
    "                print(MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_array[j]], 1,sigma))\n",
    "\n",
    "        \n",
    "        _Noise_label = np.ones((25000*T,10)) * 0.1\n",
    "\n",
    "        X_tilde,a,b = BACC_Enc_Data_v3(encoding_input_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde,a,b = BACC_Enc_Data_v3(encoding_label_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "        \n",
    "        print(\"Adjust the power of X_tilde\")\n",
    "        \n",
    "        for p_idx in range(N):\n",
    "            tmp_power = np.sum(X_tilde[p_idx,:,:] * X_tilde[p_idx,:,:], axis=1)/np.shape(X_tilde)[2]\n",
    "            tmp_power = np.sum(tmp_power)/np.shape(X_tilde)[1]\n",
    "            print(p_idx, tmp_power)\n",
    "            \n",
    "            X_tilde[p_idx,:,:] = X_tilde[p_idx,:,:] * np.sqrt(avg_power/tmp_power)\n",
    "            tmp_power = np.sum(X_tilde[p_idx,:,:] * X_tilde[p_idx,:,:], axis=1)/np.shape(X_tilde)[2]\n",
    "            tmp_power = np.sum(tmp_power)/np.shape(X_tilde)[1]\n",
    "            print('power after adjusting =',tmp_power)\n",
    "        print()\n",
    "        \n",
    "        m = N # m is the number of received result @ master\n",
    "    #     print('number of results:',m)\n",
    "\n",
    "        for trial_idx in range(N_trials):\n",
    "            print('(T, sigma)=',T,sigma,') ',trial_idx,'-th Trial!!')\n",
    "\n",
    "            net_glob = VGG('VGG11')\n",
    "            net_glob.cuda()\n",
    "            net_glob.train()\n",
    "\n",
    "            # copy weights\n",
    "            w_glob = net_glob.state_dict()\n",
    "\n",
    "            for iter in range(N_epochs): #args.epochs\n",
    "                w_locals, loss_locals = [], []\n",
    "                idxs_users = np.random.choice(range(N), m, replace=False)\n",
    "                idxs_users = np.sort(idxs_users)\n",
    "                print('selected users:',idxs_users)\n",
    "                \n",
    "                coded_net = BACC_Enc_Model_withNoise_v4(net_glob.cuda(), N, K, T, sigma, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "                dec_z_array = []\n",
    "                for idx in idxs_users: #for idx in range(N):\n",
    "            #         print(idx)\n",
    "                    local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[idx,:,:], label=y_tilde[idx,:,:])\n",
    "                    w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "#                     w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                    w_locals.append(copy.deepcopy(w))\n",
    "                    loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                    dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "                # update global weights\n",
    "                #w_glob = FedAvg(w_locals)\n",
    "                w_glob = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "\n",
    "                # copy weight to net_glob\n",
    "                net_glob.load_state_dict(w_glob)\n",
    "\n",
    "                # print loss\n",
    "            #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "            #     loss_train_arr.append(loss_train)\n",
    "\n",
    "                acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "                acc_test_arr_K2_G1_powerAlign[sigma_idx][lr_idx][trial_idx][iter] = acc_test\n",
    "                loss_test_arr_K2_G1_powerAlign[sigma_idx][lr_idx][trial_idx][iter] = loss_test\n",
    "                \n",
    "#                 PATH = \"./save_models/CIFAR10_LeNet_BACC_PowerAlign_K2_sigma_E50_iter\"+str(iter)+\".pt\"\n",
    "#                 torch.save(net_glob.state_dict(), PATH)\n",
    "                \n",
    "                if iter % 1 ==0:\n",
    "                    print('Round {:3d}, Average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_test, acc_test))\n",
    "                #print(loss_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3. Differential Privacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of X: (50000, 3072)\n",
      "size of Y: (50000, 10)\n",
      "(50000, 10)\n",
      "2.071976396195241\n"
     ]
    }
   ],
   "source": [
    "encoding_input_array_np = np.empty((len(dataset_train),32*32*3))\n",
    "encoding_label_array_np = np.empty((len(dataset_train),args.num_classes))\n",
    "print(\"size of X:\" ,encoding_input_array_np.shape)\n",
    "print(\"size of Y:\" ,encoding_label_array_np.shape)\n",
    "\n",
    "Size_submatrices = int(50000/args.num_users)\n",
    "\n",
    "sigma = 0.1\n",
    "\n",
    "for i in range(args.num_users):\n",
    "    \n",
    "    stt_pos = i*Size_submatrices\n",
    "    end_pos = (i+1)*Size_submatrices\n",
    "#     print(i,stt_pos,end_pos)\n",
    "    Temp_train = DataLoader(DatasetSplit(dataset_train, dict_users[i]), batch_size=Size_submatrices, shuffle=True)\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(Temp_train):\n",
    "        \n",
    "        images_np = images.detach().cpu().numpy()\n",
    "        \n",
    "#         print(np.size(images_np))\n",
    "        encoding_input_array_np[stt_pos:end_pos,:] = np.reshape(images_np, (Size_submatrices,32*32*3)) \n",
    "#         encoding_input_array_np[stt_pos:end_pos,:] = np.reshape(images_np, (Size_submatrices,32*32*3)) + np.random.normal(0,sigma,size=(Size_submatrices,32*32*3))\n",
    "#         print(encoding_input_array_np[stt_pos:end_pos,:].shape)\n",
    "\n",
    "        onehot_labels = torch.nn.functional.one_hot(labels,num_classes=args.num_classes)\n",
    "        labels_np = onehot_labels.detach().cpu().numpy()\n",
    "#         print(labels_np.shape)\n",
    "        encoding_label_array_np[stt_pos:end_pos,:] = labels_np\n",
    "    \n",
    "print(np.shape(encoding_label_array_np))\n",
    "\n",
    "avg_power = np.sum(encoding_input_array_np*encoding_input_array_np, axis=1)/np.shape(encoding_input_array_np)[1]\n",
    "avg_power = np.sum(avg_power)/np.shape(encoding_input_array_np)[0]\n",
    "print(avg_power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "sigma = 0.1\n",
      "\n",
      "\n",
      "\n",
      "Learning Rate = 0.0005\n",
      "\n",
      "\n",
      "\n",
      "(sigma, lr)= 0.1 0.0005 )  0 -th Trial!!\n",
      "\n",
      "Test set: Average loss: 2.1124 \n",
      "Accuracy: 2259/10000 (22.59%)\n",
      "\n",
      "Round   0, Average loss 2.112 Test accuracy 22.590\n",
      "\n",
      "Test set: Average loss: 0.7450 \n",
      "Accuracy: 7442/10000 (74.42%)\n",
      "\n",
      "Round   1, Average loss 0.745 Test accuracy 74.420\n",
      "\n",
      "Test set: Average loss: 0.6536 \n",
      "Accuracy: 8003/10000 (80.03%)\n",
      "\n",
      "Round   2, Average loss 0.654 Test accuracy 80.030\n",
      "\n",
      "Test set: Average loss: 0.6762 \n",
      "Accuracy: 8228/10000 (82.28%)\n",
      "\n",
      "Round   3, Average loss 0.676 Test accuracy 82.280\n",
      "\n",
      "Test set: Average loss: 0.7081 \n",
      "Accuracy: 8210/10000 (82.10%)\n",
      "\n",
      "Round   4, Average loss 0.708 Test accuracy 82.100\n",
      "\n",
      "Test set: Average loss: 0.7251 \n",
      "Accuracy: 8228/10000 (82.28%)\n",
      "\n",
      "Round   5, Average loss 0.725 Test accuracy 82.280\n",
      "\n",
      "Test set: Average loss: 0.7019 \n",
      "Accuracy: 8283/10000 (82.83%)\n",
      "\n",
      "Round   6, Average loss 0.702 Test accuracy 82.830\n",
      "\n",
      "Test set: Average loss: 0.7191 \n",
      "Accuracy: 8275/10000 (82.75%)\n",
      "\n",
      "Round   7, Average loss 0.719 Test accuracy 82.750\n",
      "\n",
      "Test set: Average loss: 0.6948 \n",
      "Accuracy: 8318/10000 (83.18%)\n",
      "\n",
      "Round   8, Average loss 0.695 Test accuracy 83.180\n",
      "\n",
      "Test set: Average loss: 0.7093 \n",
      "Accuracy: 8281/10000 (82.81%)\n",
      "\n",
      "Round   9, Average loss 0.709 Test accuracy 82.810\n",
      "\n",
      "Test set: Average loss: 0.7228 \n",
      "Accuracy: 8291/10000 (82.91%)\n",
      "\n",
      "Round  10, Average loss 0.723 Test accuracy 82.910\n",
      "\n",
      "Test set: Average loss: 0.7253 \n",
      "Accuracy: 8305/10000 (83.05%)\n",
      "\n",
      "Round  11, Average loss 0.725 Test accuracy 83.050\n",
      "\n",
      "Test set: Average loss: 0.7239 \n",
      "Accuracy: 8271/10000 (82.71%)\n",
      "\n",
      "Round  12, Average loss 0.724 Test accuracy 82.710\n",
      "\n",
      "Test set: Average loss: 0.7414 \n",
      "Accuracy: 8292/10000 (82.92%)\n",
      "\n",
      "Round  13, Average loss 0.741 Test accuracy 82.920\n",
      "\n",
      "Test set: Average loss: 0.7200 \n",
      "Accuracy: 8259/10000 (82.59%)\n",
      "\n",
      "Round  14, Average loss 0.720 Test accuracy 82.590\n",
      "\n",
      "Test set: Average loss: 0.7407 \n",
      "Accuracy: 8277/10000 (82.77%)\n",
      "\n",
      "Round  15, Average loss 0.741 Test accuracy 82.770\n",
      "\n",
      "Test set: Average loss: 0.7195 \n",
      "Accuracy: 8306/10000 (83.06%)\n",
      "\n",
      "Round  16, Average loss 0.719 Test accuracy 83.060\n",
      "\n",
      "Test set: Average loss: 0.7296 \n",
      "Accuracy: 8247/10000 (82.47%)\n",
      "\n",
      "Round  17, Average loss 0.730 Test accuracy 82.470\n",
      "\n",
      "Test set: Average loss: 0.7413 \n",
      "Accuracy: 8266/10000 (82.66%)\n",
      "\n",
      "Round  18, Average loss 0.741 Test accuracy 82.660\n",
      "\n",
      "Test set: Average loss: 0.7403 \n",
      "Accuracy: 8271/10000 (82.71%)\n",
      "\n",
      "Round  19, Average loss 0.740 Test accuracy 82.710\n",
      "\n",
      "Test set: Average loss: 0.7731 \n",
      "Accuracy: 8181/10000 (81.81%)\n",
      "\n",
      "Round  20, Average loss 0.773 Test accuracy 81.810\n",
      "\n",
      "Test set: Average loss: 0.7854 \n",
      "Accuracy: 8225/10000 (82.25%)\n",
      "\n",
      "Round  21, Average loss 0.785 Test accuracy 82.250\n",
      "\n",
      "Test set: Average loss: 0.7996 \n",
      "Accuracy: 8170/10000 (81.70%)\n",
      "\n",
      "Round  22, Average loss 0.800 Test accuracy 81.700\n",
      "\n",
      "Test set: Average loss: 0.7704 \n",
      "Accuracy: 8240/10000 (82.40%)\n",
      "\n",
      "Round  23, Average loss 0.770 Test accuracy 82.400\n",
      "\n",
      "Test set: Average loss: 0.7678 \n",
      "Accuracy: 8193/10000 (81.93%)\n",
      "\n",
      "Round  24, Average loss 0.768 Test accuracy 81.930\n",
      "\n",
      "Test set: Average loss: 0.7701 \n",
      "Accuracy: 8231/10000 (82.31%)\n",
      "\n",
      "Round  25, Average loss 0.770 Test accuracy 82.310\n",
      "\n",
      "Test set: Average loss: 0.7585 \n",
      "Accuracy: 8234/10000 (82.34%)\n",
      "\n",
      "Round  26, Average loss 0.758 Test accuracy 82.340\n",
      "\n",
      "Test set: Average loss: 0.7542 \n",
      "Accuracy: 8228/10000 (82.28%)\n",
      "\n",
      "Round  27, Average loss 0.754 Test accuracy 82.280\n",
      "\n",
      "Test set: Average loss: 0.7543 \n",
      "Accuracy: 8267/10000 (82.67%)\n",
      "\n",
      "Round  28, Average loss 0.754 Test accuracy 82.670\n",
      "\n",
      "Test set: Average loss: 0.7847 \n",
      "Accuracy: 8209/10000 (82.09%)\n",
      "\n",
      "Round  29, Average loss 0.785 Test accuracy 82.090\n",
      "\n",
      "Test set: Average loss: 0.7643 \n",
      "Accuracy: 8259/10000 (82.59%)\n",
      "\n",
      "Round  30, Average loss 0.764 Test accuracy 82.590\n",
      "\n",
      "Test set: Average loss: 0.7808 \n",
      "Accuracy: 8235/10000 (82.35%)\n",
      "\n",
      "Round  31, Average loss 0.781 Test accuracy 82.350\n",
      "\n",
      "Test set: Average loss: 0.7880 \n",
      "Accuracy: 8194/10000 (81.94%)\n",
      "\n",
      "Round  32, Average loss 0.788 Test accuracy 81.940\n",
      "\n",
      "Test set: Average loss: 0.7772 \n",
      "Accuracy: 8258/10000 (82.58%)\n",
      "\n",
      "Round  33, Average loss 0.777 Test accuracy 82.580\n",
      "\n",
      "Test set: Average loss: 0.7876 \n",
      "Accuracy: 8212/10000 (82.12%)\n",
      "\n",
      "Round  34, Average loss 0.788 Test accuracy 82.120\n",
      "\n",
      "Test set: Average loss: 0.7854 \n",
      "Accuracy: 8191/10000 (81.91%)\n",
      "\n",
      "Round  35, Average loss 0.785 Test accuracy 81.910\n",
      "\n",
      "Test set: Average loss: 0.7917 \n",
      "Accuracy: 8215/10000 (82.15%)\n",
      "\n",
      "Round  36, Average loss 0.792 Test accuracy 82.150\n",
      "\n",
      "Test set: Average loss: 0.7960 \n",
      "Accuracy: 8233/10000 (82.33%)\n",
      "\n",
      "Round  37, Average loss 0.796 Test accuracy 82.330\n",
      "\n",
      "Test set: Average loss: 0.7863 \n",
      "Accuracy: 8237/10000 (82.37%)\n",
      "\n",
      "Round  38, Average loss 0.786 Test accuracy 82.370\n",
      "\n",
      "Test set: Average loss: 0.7970 \n",
      "Accuracy: 8202/10000 (82.02%)\n",
      "\n",
      "Round  39, Average loss 0.797 Test accuracy 82.020\n",
      "\n",
      "Test set: Average loss: 0.7923 \n",
      "Accuracy: 8206/10000 (82.06%)\n",
      "\n",
      "Round  40, Average loss 0.792 Test accuracy 82.060\n",
      "\n",
      "Test set: Average loss: 0.7957 \n",
      "Accuracy: 8237/10000 (82.37%)\n",
      "\n",
      "Round  41, Average loss 0.796 Test accuracy 82.370\n",
      "\n",
      "Test set: Average loss: 0.7940 \n",
      "Accuracy: 8197/10000 (81.97%)\n",
      "\n",
      "Round  42, Average loss 0.794 Test accuracy 81.970\n",
      "\n",
      "Test set: Average loss: 0.7860 \n",
      "Accuracy: 8277/10000 (82.77%)\n",
      "\n",
      "Round  43, Average loss 0.786 Test accuracy 82.770\n",
      "\n",
      "Test set: Average loss: 0.7987 \n",
      "Accuracy: 8223/10000 (82.23%)\n",
      "\n",
      "Round  44, Average loss 0.799 Test accuracy 82.230\n",
      "\n",
      "Test set: Average loss: 0.7997 \n",
      "Accuracy: 8187/10000 (81.87%)\n",
      "\n",
      "Round  45, Average loss 0.800 Test accuracy 81.870\n",
      "\n",
      "Test set: Average loss: 0.8018 \n",
      "Accuracy: 8202/10000 (82.02%)\n",
      "\n",
      "Round  46, Average loss 0.802 Test accuracy 82.020\n",
      "\n",
      "Test set: Average loss: 0.7720 \n",
      "Accuracy: 8247/10000 (82.47%)\n",
      "\n",
      "Round  47, Average loss 0.772 Test accuracy 82.470\n",
      "\n",
      "Test set: Average loss: 0.7609 \n",
      "Accuracy: 8250/10000 (82.50%)\n",
      "\n",
      "Round  48, Average loss 0.761 Test accuracy 82.500\n",
      "\n",
      "Test set: Average loss: 0.8200 \n",
      "Accuracy: 8185/10000 (81.85%)\n",
      "\n",
      "Round  49, Average loss 0.820 Test accuracy 81.850\n",
      "\n",
      "\n",
      "\n",
      "sigma = 0.3\n",
      "\n",
      "\n",
      "\n",
      "Learning Rate = 0.0005\n",
      "\n",
      "\n",
      "\n",
      "(sigma, lr)= 0.3 0.0005 )  0 -th Trial!!\n",
      "\n",
      "Test set: Average loss: 2.5351 \n",
      "Accuracy: 1735/10000 (17.35%)\n",
      "\n",
      "Round   0, Average loss 2.535 Test accuracy 17.350\n",
      "\n",
      "Test set: Average loss: 1.1310 \n",
      "Accuracy: 6256/10000 (62.56%)\n",
      "\n",
      "Round   1, Average loss 1.131 Test accuracy 62.560\n",
      "\n",
      "Test set: Average loss: 0.8803 \n",
      "Accuracy: 7299/10000 (72.99%)\n",
      "\n",
      "Round   2, Average loss 0.880 Test accuracy 72.990\n",
      "\n",
      "Test set: Average loss: 0.9290 \n",
      "Accuracy: 7432/10000 (74.32%)\n",
      "\n",
      "Round   3, Average loss 0.929 Test accuracy 74.320\n",
      "\n",
      "Test set: Average loss: 1.0143 \n",
      "Accuracy: 7400/10000 (74.00%)\n",
      "\n",
      "Round   4, Average loss 1.014 Test accuracy 74.000\n",
      "\n",
      "Test set: Average loss: 1.0084 \n",
      "Accuracy: 7488/10000 (74.88%)\n",
      "\n",
      "Round   5, Average loss 1.008 Test accuracy 74.880\n",
      "\n",
      "Test set: Average loss: 0.9815 \n",
      "Accuracy: 7593/10000 (75.93%)\n",
      "\n",
      "Round   6, Average loss 0.982 Test accuracy 75.930\n",
      "\n",
      "Test set: Average loss: 1.0418 \n",
      "Accuracy: 7501/10000 (75.01%)\n",
      "\n",
      "Round   7, Average loss 1.042 Test accuracy 75.010\n",
      "\n",
      "Test set: Average loss: 1.0453 \n",
      "Accuracy: 7492/10000 (74.92%)\n",
      "\n",
      "Round   8, Average loss 1.045 Test accuracy 74.920\n",
      "\n",
      "Test set: Average loss: 0.9747 \n",
      "Accuracy: 7633/10000 (76.33%)\n",
      "\n",
      "Round   9, Average loss 0.975 Test accuracy 76.330\n",
      "\n",
      "Test set: Average loss: 0.9868 \n",
      "Accuracy: 7664/10000 (76.64%)\n",
      "\n",
      "Round  10, Average loss 0.987 Test accuracy 76.640\n",
      "\n",
      "Test set: Average loss: 1.0779 \n",
      "Accuracy: 7488/10000 (74.88%)\n",
      "\n",
      "Round  11, Average loss 1.078 Test accuracy 74.880\n",
      "\n",
      "Test set: Average loss: 1.0920 \n",
      "Accuracy: 7571/10000 (75.71%)\n",
      "\n",
      "Round  12, Average loss 1.092 Test accuracy 75.710\n",
      "\n",
      "Test set: Average loss: 1.0506 \n",
      "Accuracy: 7629/10000 (76.29%)\n",
      "\n",
      "Round  13, Average loss 1.051 Test accuracy 76.290\n",
      "\n",
      "Test set: Average loss: 1.1492 \n",
      "Accuracy: 7466/10000 (74.66%)\n",
      "\n",
      "Round  14, Average loss 1.149 Test accuracy 74.660\n",
      "\n",
      "Test set: Average loss: 1.0469 \n",
      "Accuracy: 7532/10000 (75.32%)\n",
      "\n",
      "Round  15, Average loss 1.047 Test accuracy 75.320\n",
      "\n",
      "Test set: Average loss: 1.1549 \n",
      "Accuracy: 7381/10000 (73.81%)\n",
      "\n",
      "Round  16, Average loss 1.155 Test accuracy 73.810\n",
      "\n",
      "Test set: Average loss: 1.0666 \n",
      "Accuracy: 7576/10000 (75.76%)\n",
      "\n",
      "Round  17, Average loss 1.067 Test accuracy 75.760\n",
      "\n",
      "Test set: Average loss: 1.0996 \n",
      "Accuracy: 7530/10000 (75.30%)\n",
      "\n",
      "Round  18, Average loss 1.100 Test accuracy 75.300\n",
      "\n",
      "Test set: Average loss: 1.0596 \n",
      "Accuracy: 7601/10000 (76.01%)\n",
      "\n",
      "Round  19, Average loss 1.060 Test accuracy 76.010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.0623 \n",
      "Accuracy: 7523/10000 (75.23%)\n",
      "\n",
      "Round  20, Average loss 1.062 Test accuracy 75.230\n",
      "\n",
      "Test set: Average loss: 1.0321 \n",
      "Accuracy: 7618/10000 (76.18%)\n",
      "\n",
      "Round  21, Average loss 1.032 Test accuracy 76.180\n",
      "\n",
      "Test set: Average loss: 1.0674 \n",
      "Accuracy: 7567/10000 (75.67%)\n",
      "\n",
      "Round  22, Average loss 1.067 Test accuracy 75.670\n",
      "\n",
      "Test set: Average loss: 1.1518 \n",
      "Accuracy: 7516/10000 (75.16%)\n",
      "\n",
      "Round  23, Average loss 1.152 Test accuracy 75.160\n",
      "\n",
      "Test set: Average loss: 1.0602 \n",
      "Accuracy: 7604/10000 (76.04%)\n",
      "\n",
      "Round  24, Average loss 1.060 Test accuracy 76.040\n",
      "\n",
      "Test set: Average loss: 1.0711 \n",
      "Accuracy: 7604/10000 (76.04%)\n",
      "\n",
      "Round  25, Average loss 1.071 Test accuracy 76.040\n",
      "\n",
      "Test set: Average loss: 1.0473 \n",
      "Accuracy: 7638/10000 (76.38%)\n",
      "\n",
      "Round  26, Average loss 1.047 Test accuracy 76.380\n",
      "\n",
      "Test set: Average loss: 0.9875 \n",
      "Accuracy: 7749/10000 (77.49%)\n",
      "\n",
      "Round  27, Average loss 0.988 Test accuracy 77.490\n",
      "\n",
      "Test set: Average loss: 1.0541 \n",
      "Accuracy: 7631/10000 (76.31%)\n",
      "\n",
      "Round  28, Average loss 1.054 Test accuracy 76.310\n",
      "\n",
      "Test set: Average loss: 1.1215 \n",
      "Accuracy: 7526/10000 (75.26%)\n",
      "\n",
      "Round  29, Average loss 1.122 Test accuracy 75.260\n",
      "\n",
      "Test set: Average loss: 1.0551 \n",
      "Accuracy: 7647/10000 (76.47%)\n",
      "\n",
      "Round  30, Average loss 1.055 Test accuracy 76.470\n",
      "\n",
      "Test set: Average loss: 1.0621 \n",
      "Accuracy: 7689/10000 (76.89%)\n",
      "\n",
      "Round  31, Average loss 1.062 Test accuracy 76.890\n",
      "\n",
      "Test set: Average loss: 1.0308 \n",
      "Accuracy: 7695/10000 (76.95%)\n",
      "\n",
      "Round  32, Average loss 1.031 Test accuracy 76.950\n",
      "\n",
      "Test set: Average loss: 1.0756 \n",
      "Accuracy: 7637/10000 (76.37%)\n",
      "\n",
      "Round  33, Average loss 1.076 Test accuracy 76.370\n",
      "\n",
      "Test set: Average loss: 1.0805 \n",
      "Accuracy: 7617/10000 (76.17%)\n",
      "\n",
      "Round  34, Average loss 1.080 Test accuracy 76.170\n",
      "\n",
      "Test set: Average loss: 1.0536 \n",
      "Accuracy: 7705/10000 (77.05%)\n",
      "\n",
      "Round  35, Average loss 1.054 Test accuracy 77.050\n",
      "\n",
      "Test set: Average loss: 1.0844 \n",
      "Accuracy: 7595/10000 (75.95%)\n",
      "\n",
      "Round  36, Average loss 1.084 Test accuracy 75.950\n",
      "\n",
      "Test set: Average loss: 1.0382 \n",
      "Accuracy: 7677/10000 (76.77%)\n",
      "\n",
      "Round  37, Average loss 1.038 Test accuracy 76.770\n",
      "\n",
      "Test set: Average loss: 1.0511 \n",
      "Accuracy: 7719/10000 (77.19%)\n",
      "\n",
      "Round  38, Average loss 1.051 Test accuracy 77.190\n",
      "\n",
      "Test set: Average loss: 1.0901 \n",
      "Accuracy: 7664/10000 (76.64%)\n",
      "\n",
      "Round  39, Average loss 1.090 Test accuracy 76.640\n",
      "\n",
      "Test set: Average loss: 1.0265 \n",
      "Accuracy: 7733/10000 (77.33%)\n",
      "\n",
      "Round  40, Average loss 1.026 Test accuracy 77.330\n",
      "\n",
      "Test set: Average loss: 1.0761 \n",
      "Accuracy: 7644/10000 (76.44%)\n",
      "\n",
      "Round  41, Average loss 1.076 Test accuracy 76.440\n",
      "\n",
      "Test set: Average loss: 1.0689 \n",
      "Accuracy: 7610/10000 (76.10%)\n",
      "\n",
      "Round  42, Average loss 1.069 Test accuracy 76.100\n",
      "\n",
      "Test set: Average loss: 1.0429 \n",
      "Accuracy: 7719/10000 (77.19%)\n",
      "\n",
      "Round  43, Average loss 1.043 Test accuracy 77.190\n",
      "\n",
      "Test set: Average loss: 1.0563 \n",
      "Accuracy: 7701/10000 (77.01%)\n",
      "\n",
      "Round  44, Average loss 1.056 Test accuracy 77.010\n",
      "\n",
      "Test set: Average loss: 1.0545 \n",
      "Accuracy: 7714/10000 (77.14%)\n",
      "\n",
      "Round  45, Average loss 1.054 Test accuracy 77.140\n",
      "\n",
      "Test set: Average loss: 1.0223 \n",
      "Accuracy: 7771/10000 (77.71%)\n",
      "\n",
      "Round  46, Average loss 1.022 Test accuracy 77.710\n",
      "\n",
      "Test set: Average loss: 1.1366 \n",
      "Accuracy: 7590/10000 (75.90%)\n",
      "\n",
      "Round  47, Average loss 1.137 Test accuracy 75.900\n",
      "\n",
      "Test set: Average loss: 1.0944 \n",
      "Accuracy: 7668/10000 (76.68%)\n",
      "\n",
      "Round  48, Average loss 1.094 Test accuracy 76.680\n",
      "\n",
      "Test set: Average loss: 1.0821 \n",
      "Accuracy: 7676/10000 (76.76%)\n",
      "\n",
      "Round  49, Average loss 1.082 Test accuracy 76.760\n",
      "\n",
      "\n",
      "\n",
      "sigma = 0.5\n",
      "\n",
      "\n",
      "\n",
      "Learning Rate = 0.0005\n",
      "\n",
      "\n",
      "\n",
      "(sigma, lr)= 0.5 0.0005 )  0 -th Trial!!\n",
      "\n",
      "Test set: Average loss: 2.7286 \n",
      "Accuracy: 1002/10000 (10.02%)\n",
      "\n",
      "Round   0, Average loss 2.729 Test accuracy 10.020\n",
      "\n",
      "Test set: Average loss: 1.3485 \n",
      "Accuracy: 5540/10000 (55.40%)\n",
      "\n",
      "Round   1, Average loss 1.348 Test accuracy 55.400\n",
      "\n",
      "Test set: Average loss: 1.2185 \n",
      "Accuracy: 6466/10000 (64.66%)\n",
      "\n",
      "Round   2, Average loss 1.218 Test accuracy 64.660\n",
      "\n",
      "Test set: Average loss: 1.3904 \n",
      "Accuracy: 6476/10000 (64.76%)\n",
      "\n",
      "Round   3, Average loss 1.390 Test accuracy 64.760\n",
      "\n",
      "Test set: Average loss: 1.6094 \n",
      "Accuracy: 6395/10000 (63.95%)\n",
      "\n",
      "Round   4, Average loss 1.609 Test accuracy 63.950\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "N = 2\n",
    "K = 2\n",
    "N_trials = 1\n",
    "N_epochs = 50\n",
    "\n",
    "lr_array = [0.0005] # [0.001, 0.0005, 0.0001]\n",
    "\n",
    "sigma_array = [0.1, 0.3, 0.5]\n",
    "# lr_array = [0.1, 0.01, 0.005, 0.001,0.0005]\n",
    "\n",
    "\n",
    "loss_test_arr_K2_DP_v2 = np.zeros((len(sigma_array),len(lr_array),N_trials,N_epochs))\n",
    "acc_test_arr_K2_DP_v2  = np.zeros((len(sigma_array),len(lr_array),N_trials,N_epochs))\n",
    "\n",
    "\n",
    "for sigma_idx in range(len(sigma_array)):\n",
    "    \n",
    "    sigma = sigma_array[sigma_idx]\n",
    "    \n",
    "    print('\\n\\n')\n",
    "    print('sigma =',sigma)\n",
    "    print('\\n\\n')\n",
    "           \n",
    "        \n",
    "    # print(\"alpha_array: \",alpha_array,'\\n')\n",
    "    \n",
    "    \n",
    "    for lr_idx in range(len(lr_array)):\n",
    "        \n",
    "        args.lr = lr_array[lr_idx]\n",
    "        \n",
    "        print('Learning Rate =',args.lr)\n",
    "        print('\\n\\n')\n",
    "        \n",
    "        X_tilde = np.reshape(encoding_input_array_np, (N,Size_submatrices, 32*32*3)) + np.random.normal(0,sigma,size=(N,Size_submatrices, 32*32*3))\n",
    "        y_tilde = np.reshape(encoding_label_array_np, (N,Size_submatrices, 10))\n",
    "\n",
    "#         X_tilde,a,b = BACC_Enc_Data_v3(encoding_input_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "#         y_tilde,a,b = BACC_Enc_Data_v3(encoding_label_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "        \n",
    "#         print(\"Adjust the power of X_tilde\")\n",
    "        \n",
    "#         for p_idx in range(N):\n",
    "#             tmp_power = np.sum(X_tilde[p_idx,:,:] * X_tilde[p_idx,:,:], axis=1)/np.shape(X_tilde)[2]\n",
    "#             tmp_power = np.sum(tmp_power)/np.shape(X_tilde)[1]\n",
    "#             print(p_idx, tmp_power)\n",
    "            \n",
    "#             X_tilde[p_idx,:,:] = X_tilde[p_idx,:,:] * np.sqrt(avg_power/tmp_power)\n",
    "#             tmp_power = np.sum(X_tilde[p_idx,:,:] * X_tilde[p_idx,:,:], axis=1)/np.shape(X_tilde)[2]\n",
    "#             tmp_power = np.sum(tmp_power)/np.shape(X_tilde)[1]\n",
    "#             print('power after adjusting =',tmp_power)\n",
    "#         print()\n",
    "        \n",
    "        m = N # m is the number of received result @ master\n",
    "    #     print('number of results:',m)\n",
    "\n",
    "        for trial_idx in range(N_trials):\n",
    "            print('(sigma, lr)=',sigma,args.lr,') ',trial_idx,'-th Trial!!')\n",
    "\n",
    "            net_glob = VGG('VGG11')\n",
    "            net_glob.cuda()\n",
    "            net_glob.train()\n",
    "\n",
    "            # copy weights\n",
    "            w_glob = net_glob.state_dict()\n",
    "\n",
    "            for iter in range(N_epochs): #args.epochs\n",
    "                w_locals, loss_locals = [], []\n",
    "                idxs_users = np.random.choice(range(N), m, replace=False)\n",
    "                idxs_users = np.sort(idxs_users)\n",
    "                \n",
    "                dec_z_array = []\n",
    "                \n",
    "                coded_net = DP_Model(net_glob.cuda(), N, sigma)\n",
    "                \n",
    "                for idx in idxs_users: #for idx in range(N):\n",
    "            #         print(idx)\n",
    "                    local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[idx,:,:], label=y_tilde[idx,:,:])\n",
    "                    w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "#                     w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                    w_locals.append(copy.deepcopy(w))\n",
    "                    loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                    dec_z_array.append(z_array[idx])\n",
    "             \n",
    "                                \n",
    "                w_glob = copy.deepcopy(w_locals[0])\n",
    "                for k in w_glob.keys():\n",
    "                    for G_idx in range(1,N):\n",
    "                        w_glob[k] += w_locals[G_idx][k]\n",
    "                    w_glob[k] = torch.div(w_glob[k], N)\n",
    "\n",
    "                # copy weight to net_glob\n",
    "                net_glob.load_state_dict(w_glob)\n",
    "\n",
    "                # print loss\n",
    "            #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "            #     loss_train_arr.append(loss_train)\n",
    "\n",
    "                acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "                acc_test_arr_K2_DP_v2[sigma_idx][lr_idx][trial_idx][iter] = acc_test\n",
    "                loss_test_arr_K2_DP_v2[sigma_idx][lr_idx][trial_idx][iter] = loss_test\n",
    "                \n",
    "#                 PATH = \"./save_models/CIFAR10_LeNet_BACC_PowerAlign_K2_sigma_E50_iter\"+str(iter)+\".pt\"\n",
    "#                 torch.save(net_glob.state_dict(), PATH)\n",
    "                \n",
    "                if iter % 1 ==0:\n",
    "                    print('Round {:3d}, Average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_test, acc_test))\n",
    "                #print(loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lr_array = [0.1, 0.01, 0.005, 0.001,0.0005]\n",
    "\n",
    "plt.plot(acc_test_FedAvg, label='Plain, K=2, N=2')\n",
    "plt.plot(acc_test_arr_K2_G1_v3[0,0,0,0:50],label='BACC, w/o PowerAlign, K=2, N=2, sigma=0.1' )\n",
    "plt.plot(acc_test_arr_K2_G1_powerAlign[0,0,0,0:50],label='BACC, w/   PowerAlign, K=2, N=2, sigma=0.1' )\n",
    "plt.plot(acc_test_arr_K2_DP_v2[0,0,0,0:50],label='DP, K=2, N=2, sigma=0.1' )\n",
    "\n",
    "# plt.plot(acc_test_arr_K2_G1_powerAlign[0,1,0,0:50],label='w/o Grouping, K=2, N=2, sigma=0.1, PowerAlignment' )\n",
    "# plt.plot(acc_test_arr_K4_G1[0,3,0,0:30],label='w/o Grouping, K=4, N=4, G=1, lr=0.001' )\n",
    "# plt.plot(acc_test_arr_K4_G1[0,4,0,0:30],label='w/o Grouping, K=4, N=4, G=1, lr=0.0005' )\n",
    "\n",
    "\n",
    "# plt.plot(acc_test_arr_uncoded,'r',label='uncoded')\n",
    "plt.title('VGG11')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "# plt.title(title_name)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Test accuracy(%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 3.5231 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round   0, Average loss 1.505\n",
      "\n",
      "Test set: Average loss: 4.7447 \n",
      "Accuracy: 1695/10000 (16.95%)\n",
      "\n",
      "Round   1, Average loss 0.986\n",
      "\n",
      "Test set: Average loss: 0.5528 \n",
      "Accuracy: 8223/10000 (82.23%)\n",
      "\n",
      "Round   2, Average loss 0.688\n",
      "\n",
      "Test set: Average loss: 0.4657 \n",
      "Accuracy: 8553/10000 (85.53%)\n",
      "\n",
      "Round   3, Average loss 0.528\n",
      "\n",
      "Test set: Average loss: 0.4027 \n",
      "Accuracy: 8748/10000 (87.48%)\n",
      "\n",
      "Round   4, Average loss 0.432\n",
      "\n",
      "Test set: Average loss: 0.4261 \n",
      "Accuracy: 8692/10000 (86.92%)\n",
      "\n",
      "Round   5, Average loss 0.367\n",
      "\n",
      "Test set: Average loss: 0.3925 \n",
      "Accuracy: 8832/10000 (88.32%)\n",
      "\n",
      "Round   6, Average loss 0.323\n",
      "\n",
      "Test set: Average loss: 0.3624 \n",
      "Accuracy: 8924/10000 (89.24%)\n",
      "\n",
      "Round   7, Average loss 0.288\n",
      "\n",
      "Test set: Average loss: 0.3757 \n",
      "Accuracy: 8910/10000 (89.10%)\n",
      "\n",
      "Round   8, Average loss 0.265\n",
      "\n",
      "Test set: Average loss: 0.3900 \n",
      "Accuracy: 8870/10000 (88.70%)\n",
      "\n",
      "Round   9, Average loss 0.243\n",
      "\n",
      "Test set: Average loss: 0.3644 \n",
      "Accuracy: 8963/10000 (89.63%)\n",
      "\n",
      "Round  10, Average loss 0.229\n",
      "\n",
      "Test set: Average loss: 0.3710 \n",
      "Accuracy: 9008/10000 (90.08%)\n",
      "\n",
      "Round  11, Average loss 0.216\n",
      "\n",
      "Test set: Average loss: 0.3766 \n",
      "Accuracy: 8973/10000 (89.73%)\n",
      "\n",
      "Round  12, Average loss 0.205\n",
      "\n",
      "Test set: Average loss: 0.3625 \n",
      "Accuracy: 9016/10000 (90.16%)\n",
      "\n",
      "Round  13, Average loss 0.195\n",
      "\n",
      "Test set: Average loss: 0.3543 \n",
      "Accuracy: 9017/10000 (90.17%)\n",
      "\n",
      "Round  14, Average loss 0.189\n",
      "\n",
      "Test set: Average loss: 0.3700 \n",
      "Accuracy: 9011/10000 (90.11%)\n",
      "\n",
      "Round  15, Average loss 0.182\n",
      "\n",
      "Test set: Average loss: 0.3663 \n",
      "Accuracy: 9007/10000 (90.07%)\n",
      "\n",
      "Round  16, Average loss 0.179\n",
      "\n",
      "Test set: Average loss: 0.3568 \n",
      "Accuracy: 9039/10000 (90.39%)\n",
      "\n",
      "Round  17, Average loss 0.172\n",
      "\n",
      "Test set: Average loss: 0.3837 \n",
      "Accuracy: 8999/10000 (89.99%)\n",
      "\n",
      "Round  18, Average loss 0.169\n",
      "\n",
      "Test set: Average loss: 0.3556 \n",
      "Accuracy: 9053/10000 (90.53%)\n",
      "\n",
      "Round  19, Average loss 0.166\n",
      "\n",
      "Test set: Average loss: 0.3744 \n",
      "Accuracy: 9050/10000 (90.50%)\n",
      "\n",
      "Round  20, Average loss 0.163\n",
      "\n",
      "Test set: Average loss: 0.3476 \n",
      "Accuracy: 9095/10000 (90.95%)\n",
      "\n",
      "Round  21, Average loss 0.159\n",
      "\n",
      "Test set: Average loss: 0.3639 \n",
      "Accuracy: 9020/10000 (90.20%)\n",
      "\n",
      "Round  22, Average loss 0.156\n",
      "\n",
      "Test set: Average loss: 0.3454 \n",
      "Accuracy: 9065/10000 (90.65%)\n",
      "\n",
      "Round  23, Average loss 0.153\n",
      "\n",
      "Test set: Average loss: 0.3522 \n",
      "Accuracy: 9043/10000 (90.43%)\n",
      "\n",
      "Round  24, Average loss 0.151\n",
      "\n",
      "Test set: Average loss: 0.3501 \n",
      "Accuracy: 9090/10000 (90.90%)\n",
      "\n",
      "Round  25, Average loss 0.151\n",
      "\n",
      "Test set: Average loss: 0.3628 \n",
      "Accuracy: 9053/10000 (90.53%)\n",
      "\n",
      "Round  26, Average loss 0.148\n",
      "\n",
      "Test set: Average loss: 0.3676 \n",
      "Accuracy: 9040/10000 (90.40%)\n",
      "\n",
      "Round  27, Average loss 0.149\n",
      "\n",
      "Test set: Average loss: 0.3403 \n",
      "Accuracy: 9083/10000 (90.83%)\n",
      "\n",
      "Round  28, Average loss 0.145\n",
      "\n",
      "Test set: Average loss: 0.3476 \n",
      "Accuracy: 9089/10000 (90.89%)\n",
      "\n",
      "Round  29, Average loss 0.143\n",
      "\n",
      "Test set: Average loss: 0.3357 \n",
      "Accuracy: 9119/10000 (91.19%)\n",
      "\n",
      "Round  30, Average loss 0.142\n",
      "\n",
      "Test set: Average loss: 0.3405 \n",
      "Accuracy: 9100/10000 (91.00%)\n",
      "\n",
      "Round  31, Average loss 0.140\n",
      "\n",
      "Test set: Average loss: 0.3481 \n",
      "Accuracy: 9106/10000 (91.06%)\n",
      "\n",
      "Round  32, Average loss 0.139\n",
      "\n",
      "Test set: Average loss: 0.3559 \n",
      "Accuracy: 9055/10000 (90.55%)\n",
      "\n",
      "Round  33, Average loss 0.141\n",
      "\n",
      "Test set: Average loss: 0.3674 \n",
      "Accuracy: 9049/10000 (90.49%)\n",
      "\n",
      "Round  34, Average loss 0.138\n",
      "\n",
      "Test set: Average loss: 0.3559 \n",
      "Accuracy: 9074/10000 (90.74%)\n",
      "\n",
      "Round  35, Average loss 0.138\n",
      "\n",
      "Test set: Average loss: 0.3717 \n",
      "Accuracy: 9030/10000 (90.30%)\n",
      "\n",
      "Round  36, Average loss 0.135\n",
      "\n",
      "Test set: Average loss: 0.3449 \n",
      "Accuracy: 9078/10000 (90.78%)\n",
      "\n",
      "Round  37, Average loss 0.136\n",
      "\n",
      "Test set: Average loss: 0.3426 \n",
      "Accuracy: 9075/10000 (90.75%)\n",
      "\n",
      "Round  38, Average loss 0.134\n",
      "\n",
      "Test set: Average loss: 0.3731 \n",
      "Accuracy: 9044/10000 (90.44%)\n",
      "\n",
      "Round  39, Average loss 0.133\n",
      "\n",
      "Test set: Average loss: 0.3469 \n",
      "Accuracy: 9070/10000 (90.70%)\n",
      "\n",
      "Round  40, Average loss 0.133\n",
      "\n",
      "Test set: Average loss: 0.3479 \n",
      "Accuracy: 9070/10000 (90.70%)\n",
      "\n",
      "Round  41, Average loss 0.133\n",
      "\n",
      "Test set: Average loss: 0.3515 \n",
      "Accuracy: 9083/10000 (90.83%)\n",
      "\n",
      "Round  42, Average loss 0.133\n",
      "\n",
      "Test set: Average loss: 0.3460 \n",
      "Accuracy: 9090/10000 (90.90%)\n",
      "\n",
      "Round  43, Average loss 0.131\n",
      "\n",
      "Test set: Average loss: 0.3360 \n",
      "Accuracy: 9113/10000 (91.13%)\n",
      "\n",
      "Round  44, Average loss 0.131\n",
      "\n",
      "Test set: Average loss: 0.3305 \n",
      "Accuracy: 9101/10000 (91.01%)\n",
      "\n",
      "Round  45, Average loss 0.131\n",
      "\n",
      "Test set: Average loss: 0.3416 \n",
      "Accuracy: 9100/10000 (91.00%)\n",
      "\n",
      "Round  46, Average loss 0.128\n",
      "\n",
      "Test set: Average loss: 0.3397 \n",
      "Accuracy: 9109/10000 (91.09%)\n",
      "\n",
      "Round  47, Average loss 0.129\n",
      "\n",
      "Test set: Average loss: 0.3431 \n",
      "Accuracy: 9080/10000 (90.80%)\n",
      "\n",
      "Round  48, Average loss 0.130\n",
      "\n",
      "Test set: Average loss: 0.3318 \n",
      "Accuracy: 9093/10000 (90.93%)\n",
      "\n",
      "Round  49, Average loss 0.129\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "from models.vgg import *\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "N_epochs = 50\n",
    "\n",
    "net_glob = VGG('VGG19')\n",
    "net_glob.cuda()\n",
    "\n",
    "acc_test_FedAvg = np.empty(N_epochs)\n",
    "loss_test_FedAvg = np.empty(N_epochs)\n",
    "\n",
    "for iter in range(N_epochs):\n",
    "    w_locals, loss_locals = [], []\n",
    "\n",
    "    m = args.num_users\n",
    "    \n",
    "    idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "    \n",
    "    for idx in idxs_users:\n",
    "        local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])\n",
    "        w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))\n",
    "        w_locals.append(copy.deepcopy(w))\n",
    "        loss_locals.append(copy.deepcopy(loss))\n",
    "        \n",
    "    # update global weights\n",
    "    w_glob = FedAvg(w_locals)\n",
    "\n",
    "    # copy weight to net_glob\n",
    "    net_glob.load_state_dict(w_glob)\n",
    "    \n",
    "    acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "\n",
    "    # print loss\n",
    "    loss_avg = sum(loss_locals) / len(loss_locals)\n",
    "    print('Round {:3d}, Average loss {:.3f}'.format(iter, loss_avg))\n",
    "#     loss_train.append(loss_avg)\n",
    "    \n",
    "    acc_test_FedAvg[iter] = acc_test\n",
    "    loss_test_FedAvg[iter] = loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
