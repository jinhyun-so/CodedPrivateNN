{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Import basic libraries\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Import PyTorch\n",
    "import torch # import main library\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn # import modules\n",
    "from torch.autograd import Function # import Function to create custom activations\n",
    "from torch.nn.parameter import Parameter # import Parameter to create custom activations with learnable parameters\n",
    "from torch import optim # import optimizers for demonstrations\n",
    "import torch.nn.functional as F # import torch functions\n",
    "from torchvision import datasets, transforms # import transformations to use for demo\n",
    "from torch.nn.parameter import Parameter # import Parameter to create custom activations with learnable parameters\n",
    "\n",
    "from models.activ_func import *\n",
    "from models.Nets import *\n",
    "from models.test import test_img\n",
    "from models.Update import *\n",
    "\n",
    "\n",
    "from utils.sampling import mnist_iid, mnist_noniid, cifar_iid\n",
    "from utils.options import args_parser\n",
    "from models.Update import LocalUpdate\n",
    "from models.Nets import MLP, CNNMnist, CNNCifar, LeNet, CNNMnist2, CNNMnist3\n",
    "from models.Fed import FedAvg\n",
    "from models.Fed import FedQAvg, Quantization, Quantization_Finite, my_score, my_score_Finite\n",
    "from models.test import test_img\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from utils.functions import *\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "class my_argument:    \n",
    "    epochs    = 16    #\"rounds of training\"\n",
    "    num_users = 4  # \"number of users: K\"\n",
    "    frac      = 0.5 #\"the fraction of clients: C\"\n",
    "    local_ep  = 1 #\"the number of local epochs: E\"\n",
    "    local_bs  = 50  #\"local batch size: B\"\n",
    "    bs        = 50 #\"test batch size\"\n",
    "    lr        = 0.001 #\"learning rate\"\n",
    "    momentum  = 0.5 # \"SGD momentum (default: 0.5)\"\n",
    "    weight_decay = 5e-4\n",
    "    split     = 'user' # \"train-test split type, user or sample\"\n",
    "    opt='ADAM'\n",
    "    loss='Custom' # 'Custom' or 'Default'\n",
    "\n",
    "    # model arguments\n",
    "    model = 'cnn'\n",
    "    kernel_num=9 #, help='number of each kind of kernel')\n",
    "    kernel_sizes='3,4,5' #  help='comma-separated kernel size to use for convolution')\n",
    "    norm='batch_norm' #, help=\"batch_norm, layer_norm, or None\")\n",
    "    num_filters=32 #, help=\"number of filters for conv nets\")\n",
    "    max_pool='True' #help=\"Whether use max pooling rather than strided convolutions\")\n",
    "\n",
    "    # other arguments\n",
    "    dataset='cifar' #, help=\"name of dataset\")\n",
    "    iid=1\n",
    "    num_classes=10#, help=\"number of classes\")\n",
    "    num_channels=1#, help=\"number of channels of imges\")\n",
    "    gpu=1#, help=\"GPU ID, -1 for CPU\")\n",
    "    stopping_rounds=10#, help='rounds of early stopping')\n",
    "    verbose='False'#, help='verbose print')\n",
    "    seed=1#, help='random seed (default: 1)')\n",
    "    \n",
    "args = my_argument()\n",
    "\n",
    "args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)\n",
    "args.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(args.device)\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Normalize the test set same as training set without augmentation\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# trainset = torchvision.datasets.CIFAR10(\n",
    "#     root=\"./data/cifar\", train=True, download=True, transform=transform_train)\n",
    "# trainloader = torch.utils.data.DataLoader(\n",
    "#     trainset, batch_size=args.bs, shuffle=True, num_workers=2)\n",
    "\n",
    "# testset = torchvision.datasets.CIFAR10(\n",
    "#     root=\"./data/cifar\", train=False, download=True, transform=transform_test)\n",
    "# testloader = torch.utils.data.DataLoader(\n",
    "#     testset, batch_size=args.bs, shuffle=False, num_workers=2)\n",
    "\n",
    "dataset_train = datasets.CIFAR10('../data/cifar', train=True, download=True, transform=transform_train)\n",
    "dataset_test = datasets.CIFAR10('../data/cifar', train=False, download=True, transform=transform_test)\n",
    "if args.iid:\n",
    "    dict_users = cifar_iid(dataset_train, args.num_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, testloader, criterion):\n",
    "    test_loss = 0\n",
    "    accuracy = 0\n",
    "    for images, labels in testloader:\n",
    "\n",
    "#         images.resize_(images.shape[0], 3*32*32)\n",
    "\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        output = model.forward(images)\n",
    "        test_loss += criterion(output, labels).item()\n",
    "\n",
    "        ps = torch.exp(output)\n",
    "        equality = (labels.data == ps.max(dim=1)[1])\n",
    "        accuracy += equality.type(torch.FloatTensor).mean()\n",
    "\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Train LeNet with uncoded Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.9165 \n",
      "Accuracy: 3086/10000 (30.86%)\n",
      "\n",
      "Round   0, Average loss 2.052\n",
      "\n",
      "Test set: Average loss: 1.7166 \n",
      "Accuracy: 3844/10000 (38.44%)\n",
      "\n",
      "Round   1, Average loss 1.866\n",
      "\n",
      "Test set: Average loss: 1.6110 \n",
      "Accuracy: 4128/10000 (41.28%)\n",
      "\n",
      "Round   2, Average loss 1.749\n",
      "\n",
      "Test set: Average loss: 1.5467 \n",
      "Accuracy: 4440/10000 (44.40%)\n",
      "\n",
      "Round   3, Average loss 1.687\n",
      "\n",
      "Test set: Average loss: 1.4918 \n",
      "Accuracy: 4617/10000 (46.17%)\n",
      "\n",
      "Round   4, Average loss 1.633\n",
      "\n",
      "Test set: Average loss: 1.4462 \n",
      "Accuracy: 4749/10000 (47.49%)\n",
      "\n",
      "Round   5, Average loss 1.590\n",
      "\n",
      "Test set: Average loss: 1.4069 \n",
      "Accuracy: 4902/10000 (49.02%)\n",
      "\n",
      "Round   6, Average loss 1.555\n",
      "\n",
      "Test set: Average loss: 1.3776 \n",
      "Accuracy: 5043/10000 (50.43%)\n",
      "\n",
      "Round   7, Average loss 1.525\n",
      "\n",
      "Test set: Average loss: 1.3544 \n",
      "Accuracy: 5160/10000 (51.60%)\n",
      "\n",
      "Round   8, Average loss 1.495\n",
      "\n",
      "Test set: Average loss: 1.3273 \n",
      "Accuracy: 5259/10000 (52.59%)\n",
      "\n",
      "Round   9, Average loss 1.472\n",
      "\n",
      "Test set: Average loss: 1.3180 \n",
      "Accuracy: 5321/10000 (53.21%)\n",
      "\n",
      "Round  10, Average loss 1.455\n",
      "\n",
      "Test set: Average loss: 1.2940 \n",
      "Accuracy: 5439/10000 (54.39%)\n",
      "\n",
      "Round  11, Average loss 1.430\n",
      "\n",
      "Test set: Average loss: 1.2773 \n",
      "Accuracy: 5452/10000 (54.52%)\n",
      "\n",
      "Round  12, Average loss 1.413\n",
      "\n",
      "Test set: Average loss: 1.2589 \n",
      "Accuracy: 5518/10000 (55.18%)\n",
      "\n",
      "Round  13, Average loss 1.397\n",
      "\n",
      "Test set: Average loss: 1.2478 \n",
      "Accuracy: 5560/10000 (55.60%)\n",
      "\n",
      "Round  14, Average loss 1.387\n",
      "\n",
      "Test set: Average loss: 1.2330 \n",
      "Accuracy: 5619/10000 (56.19%)\n",
      "\n",
      "Round  15, Average loss 1.368\n",
      "\n",
      "Test set: Average loss: 1.2163 \n",
      "Accuracy: 5702/10000 (57.02%)\n",
      "\n",
      "Round  16, Average loss 1.353\n",
      "\n",
      "Test set: Average loss: 1.2089 \n",
      "Accuracy: 5701/10000 (57.01%)\n",
      "\n",
      "Round  17, Average loss 1.344\n",
      "\n",
      "Test set: Average loss: 1.2011 \n",
      "Accuracy: 5741/10000 (57.41%)\n",
      "\n",
      "Round  18, Average loss 1.330\n",
      "\n",
      "Test set: Average loss: 1.1903 \n",
      "Accuracy: 5750/10000 (57.50%)\n",
      "\n",
      "Round  19, Average loss 1.319\n",
      "\n",
      "Test set: Average loss: 1.1778 \n",
      "Accuracy: 5833/10000 (58.33%)\n",
      "\n",
      "Round  20, Average loss 1.305\n",
      "\n",
      "Test set: Average loss: 1.1689 \n",
      "Accuracy: 5838/10000 (58.38%)\n",
      "\n",
      "Round  21, Average loss 1.297\n",
      "\n",
      "Test set: Average loss: 1.1585 \n",
      "Accuracy: 5853/10000 (58.53%)\n",
      "\n",
      "Round  22, Average loss 1.287\n",
      "\n",
      "Test set: Average loss: 1.1536 \n",
      "Accuracy: 5880/10000 (58.80%)\n",
      "\n",
      "Round  23, Average loss 1.280\n",
      "\n",
      "Test set: Average loss: 1.1400 \n",
      "Accuracy: 5945/10000 (59.45%)\n",
      "\n",
      "Round  24, Average loss 1.271\n",
      "\n",
      "Test set: Average loss: 1.1354 \n",
      "Accuracy: 5923/10000 (59.23%)\n",
      "\n",
      "Round  25, Average loss 1.263\n",
      "\n",
      "Test set: Average loss: 1.1304 \n",
      "Accuracy: 5957/10000 (59.57%)\n",
      "\n",
      "Round  26, Average loss 1.251\n",
      "\n",
      "Test set: Average loss: 1.1270 \n",
      "Accuracy: 6017/10000 (60.17%)\n",
      "\n",
      "Round  27, Average loss 1.247\n",
      "\n",
      "Test set: Average loss: 1.1228 \n",
      "Accuracy: 6029/10000 (60.29%)\n",
      "\n",
      "Round  28, Average loss 1.237\n",
      "\n",
      "Test set: Average loss: 1.1083 \n",
      "Accuracy: 6099/10000 (60.99%)\n",
      "\n",
      "Round  29, Average loss 1.236\n",
      "\n",
      "Test set: Average loss: 1.1074 \n",
      "Accuracy: 6048/10000 (60.48%)\n",
      "\n",
      "Round  30, Average loss 1.226\n",
      "\n",
      "Test set: Average loss: 1.0931 \n",
      "Accuracy: 6126/10000 (61.26%)\n",
      "\n",
      "Round  31, Average loss 1.210\n",
      "\n",
      "Test set: Average loss: 1.0838 \n",
      "Accuracy: 6156/10000 (61.56%)\n",
      "\n",
      "Round  32, Average loss 1.212\n",
      "\n",
      "Test set: Average loss: 1.0819 \n",
      "Accuracy: 6156/10000 (61.56%)\n",
      "\n",
      "Round  33, Average loss 1.201\n",
      "\n",
      "Test set: Average loss: 1.0765 \n",
      "Accuracy: 6205/10000 (62.05%)\n",
      "\n",
      "Round  34, Average loss 1.198\n",
      "\n",
      "Test set: Average loss: 1.0676 \n",
      "Accuracy: 6209/10000 (62.09%)\n",
      "\n",
      "Round  35, Average loss 1.194\n",
      "\n",
      "Test set: Average loss: 1.0663 \n",
      "Accuracy: 6203/10000 (62.03%)\n",
      "\n",
      "Round  36, Average loss 1.186\n",
      "\n",
      "Test set: Average loss: 1.0665 \n",
      "Accuracy: 6216/10000 (62.16%)\n",
      "\n",
      "Round  37, Average loss 1.180\n",
      "\n",
      "Test set: Average loss: 1.0561 \n",
      "Accuracy: 6285/10000 (62.85%)\n",
      "\n",
      "Round  38, Average loss 1.178\n",
      "\n",
      "Test set: Average loss: 1.0548 \n",
      "Accuracy: 6287/10000 (62.87%)\n",
      "\n",
      "Round  39, Average loss 1.168\n",
      "\n",
      "Test set: Average loss: 1.0538 \n",
      "Accuracy: 6279/10000 (62.79%)\n",
      "\n",
      "Round  40, Average loss 1.171\n",
      "\n",
      "Test set: Average loss: 1.0471 \n",
      "Accuracy: 6294/10000 (62.94%)\n",
      "\n",
      "Round  41, Average loss 1.162\n",
      "\n",
      "Test set: Average loss: 1.0353 \n",
      "Accuracy: 6329/10000 (63.29%)\n",
      "\n",
      "Round  42, Average loss 1.153\n",
      "\n",
      "Test set: Average loss: 1.0290 \n",
      "Accuracy: 6359/10000 (63.59%)\n",
      "\n",
      "Round  43, Average loss 1.152\n",
      "\n",
      "Test set: Average loss: 1.0290 \n",
      "Accuracy: 6351/10000 (63.51%)\n",
      "\n",
      "Round  44, Average loss 1.142\n",
      "\n",
      "Test set: Average loss: 1.0384 \n",
      "Accuracy: 6280/10000 (62.80%)\n",
      "\n",
      "Round  45, Average loss 1.143\n",
      "\n",
      "Test set: Average loss: 1.0224 \n",
      "Accuracy: 6340/10000 (63.40%)\n",
      "\n",
      "Round  46, Average loss 1.132\n",
      "\n",
      "Test set: Average loss: 1.0188 \n",
      "Accuracy: 6385/10000 (63.85%)\n",
      "\n",
      "Round  47, Average loss 1.132\n",
      "\n",
      "Test set: Average loss: 1.0203 \n",
      "Accuracy: 6371/10000 (63.71%)\n",
      "\n",
      "Round  48, Average loss 1.127\n",
      "\n",
      "Test set: Average loss: 1.0202 \n",
      "Accuracy: 6355/10000 (63.55%)\n",
      "\n",
      "Round  49, Average loss 1.119\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "N_epochs = 50\n",
    "\n",
    "net_glob = CNNCifar(args=args)\n",
    "net_glob.cuda()\n",
    "\n",
    "acc_test_FedAvg = np.empty(N_epochs)\n",
    "loss_test_FedAvg = np.empty(N_epochs)\n",
    "\n",
    "for iter in range(N_epochs):\n",
    "    w_locals, loss_locals = [], []\n",
    "\n",
    "    m = args.num_users\n",
    "    \n",
    "    idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "    \n",
    "    for idx in idxs_users:\n",
    "        local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])\n",
    "        w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))\n",
    "        w_locals.append(copy.deepcopy(w))\n",
    "        loss_locals.append(copy.deepcopy(loss))\n",
    "        \n",
    "    # update global weights\n",
    "    w_glob = FedAvg(w_locals)\n",
    "\n",
    "    # copy weight to net_glob\n",
    "    net_glob.load_state_dict(w_glob)\n",
    "    \n",
    "    acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "\n",
    "    # print loss\n",
    "    loss_avg = sum(loss_locals) / len(loss_locals)\n",
    "    print('Round {:3d}, Average loss {:.3f}'.format(iter, loss_avg))\n",
    "#     loss_train.append(loss_avg)\n",
    "    \n",
    "    acc_test_FedAvg[iter] = acc_test\n",
    "    loss_test_FedAvg[iter] = loss_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. BACC without grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1. K=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "class my_argument:    \n",
    "    epochs    = 16    #\"rounds of training\"\n",
    "    num_users = 2  # \"number of users: K\"\n",
    "    frac      = 0.5 #\"the fraction of clients: C\"\n",
    "    local_ep  = 5 #\"the number of local epochs: E\"\n",
    "    local_bs  = 50  #\"local batch size: B\"\n",
    "    bs        = 50 #\"test batch size\"\n",
    "    lr        = 0.001 #\"learning rate\"\n",
    "    momentum  = 0.5 # \"SGD momentum (default: 0.5)\"\n",
    "    weight_decay = 5e-4\n",
    "    split     = 'user' # \"train-test split type, user or sample\"\n",
    "    opt='ADAM'\n",
    "    loss='Custom' # 'Custom' or 'Default'\n",
    "\n",
    "    # model arguments\n",
    "    model = 'cnn'\n",
    "    kernel_num=9 #, help='number of each kind of kernel')\n",
    "    kernel_sizes='3,4,5' #  help='comma-separated kernel size to use for convolution')\n",
    "    norm='batch_norm' #, help=\"batch_norm, layer_norm, or None\")\n",
    "    num_filters=32 #, help=\"number of filters for conv nets\")\n",
    "    max_pool='True' #help=\"Whether use max pooling rather than strided convolutions\")\n",
    "\n",
    "    # other arguments\n",
    "    dataset='cifar' #, help=\"name of dataset\")\n",
    "    iid=1\n",
    "    num_classes=10#, help=\"number of classes\")\n",
    "    num_channels=1#, help=\"number of channels of imges\")\n",
    "    gpu=1#, help=\"GPU ID, -1 for CPU\")\n",
    "    stopping_rounds=10#, help='rounds of early stopping')\n",
    "    verbose='False'#, help='verbose print')\n",
    "    seed=1#, help='random seed (default: 1)')\n",
    "    \n",
    "args = my_argument()\n",
    "\n",
    "args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)\n",
    "args.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(args.device)\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Normalize the test set same as training set without augmentation\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# trainset = torchvision.datasets.CIFAR10(\n",
    "#     root=\"./data/cifar\", train=True, download=True, transform=transform_train)\n",
    "# trainloader = torch.utils.data.DataLoader(\n",
    "#     trainset, batch_size=args.bs, shuffle=True, num_workers=2)\n",
    "\n",
    "# testset = torchvision.datasets.CIFAR10(\n",
    "#     root=\"./data/cifar\", train=False, download=True, transform=transform_test)\n",
    "# testloader = torch.utils.data.DataLoader(\n",
    "#     testset, batch_size=args.bs, shuffle=False, num_workers=2)\n",
    "\n",
    "dataset_train = datasets.CIFAR10('../data/cifar', train=True, download=True, transform=transform_train)\n",
    "dataset_test = datasets.CIFAR10('../data/cifar', train=False, download=True, transform=transform_test)\n",
    "if args.iid:\n",
    "    dict_users = cifar_iid(dataset_train, args.num_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of X: (50000, 3072)\n",
      "size of Y: (50000, 10)\n",
      "(50000, 10)\n"
     ]
    }
   ],
   "source": [
    "encoding_input_array_np = np.empty((len(dataset_train),32*32*3))\n",
    "encoding_label_array_np = np.empty((len(dataset_train),args.num_classes))\n",
    "print(\"size of X:\" ,encoding_input_array_np.shape)\n",
    "print(\"size of Y:\" ,encoding_label_array_np.shape)\n",
    "\n",
    "Size_submatrices = int(50000/args.num_users)\n",
    "\n",
    "\n",
    "for i in range(args.num_users):\n",
    "    \n",
    "    stt_pos = i*Size_submatrices\n",
    "    end_pos = (i+1)*Size_submatrices\n",
    "#     print(i,stt_pos,end_pos)\n",
    "    Temp_train = DataLoader(DatasetSplit(dataset_train, dict_users[i]), batch_size=Size_submatrices, shuffle=True)\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(Temp_train):\n",
    "        \n",
    "        images_np = images.detach().cpu().numpy()\n",
    "        \n",
    "#         print(np.size(images_np))\n",
    "        encoding_input_array_np[stt_pos:end_pos,:] = np.reshape(images_np, (Size_submatrices,32*32*3))\n",
    "#         print(encoding_input_array_np[stt_pos:end_pos,:].shape)\n",
    "\n",
    "        onehot_labels = torch.nn.functional.one_hot(labels,num_classes=args.num_classes)\n",
    "        labels_np = onehot_labels.detach().cpu().numpy()\n",
    "#         print(labels_np.shape)\n",
    "        encoding_label_array_np[stt_pos:end_pos,:] = labels_np\n",
    "    \n",
    "print(np.shape(encoding_label_array_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "sigma = 1\n",
      "\n",
      "\n",
      "\n",
      "Learning Rate = 0.0001\n",
      "\n",
      "\n",
      "\n",
      "z_array: [-0.81  0.81]\n",
      "0.4838626198316927\n",
      "0.48386261983169315\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 25000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 25000 \n",
      "\n",
      "(T, sigma)= 3 1 )  0 -th Trial!!\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.004243196381462945\n",
      "conv1.bias 0.003279557451605797\n",
      "conv2.weight 0.0022550747791926064\n",
      "conv2.bias 0.0024322920944541693\n",
      "fc1.weight 0.0008298005263010661\n",
      "fc1.bias 0.0007430403182903926\n",
      "fc2.weight 0.0027880922196403383\n",
      "fc2.bias 0.0023284006331648144\n",
      "fc3.weight 0.003929461467833746\n",
      "fc3.bias 0.002615336701273918\n",
      "\n",
      "Test set: Average loss: 1.9216 \n",
      "Accuracy: 2978/10000 (29.78%)\n",
      "\n",
      "Round   0, Average loss 1.922 Test accuracy 29.780\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002410018179151747\n",
      "conv1.bias 0.001994607038795948\n",
      "conv2.weight 0.0009603058298428853\n",
      "conv2.bias 0.0008245866047218442\n",
      "fc1.weight 0.00019571040074030559\n",
      "fc1.bias 0.00021543152009447417\n",
      "fc2.weight 0.0004269897937774658\n",
      "fc2.bias 0.0006866218256098884\n",
      "fc3.weight 0.0011192210373424348\n",
      "fc3.bias 0.0005373802036046982\n",
      "\n",
      "Test set: Average loss: 1.8852 \n",
      "Accuracy: 3013/10000 (30.13%)\n",
      "\n",
      "Round   1, Average loss 1.885 Test accuracy 30.130\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002057826519012451\n",
      "conv1.bias 0.0019301921129226685\n",
      "conv2.weight 0.0008246949315071105\n",
      "conv2.bias 0.0007187537848949432\n",
      "fc1.weight 0.00021159694592158\n",
      "fc1.bias 0.0003439569224913915\n",
      "fc2.weight 0.0004222593137196132\n",
      "fc2.bias 0.0009620336017438344\n",
      "fc3.weight 0.0008251999105725969\n",
      "fc3.bias 0.000463381502777338\n",
      "\n",
      "Test set: Average loss: 1.8771 \n",
      "Accuracy: 3054/10000 (30.54%)\n",
      "\n",
      "Round   2, Average loss 1.877 Test accuracy 30.540\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001777746147579617\n",
      "conv1.bias 0.0019060016299287479\n",
      "conv2.weight 0.0007813460131486257\n",
      "conv2.bias 0.0008057198720052838\n",
      "fc1.weight 0.0002241771618525187\n",
      "fc1.bias 0.0005142605863511562\n",
      "fc2.weight 0.0005143925784126161\n",
      "fc2.bias 0.0014252950038228715\n",
      "fc3.weight 0.0008044631708235967\n",
      "fc3.bias 0.0007745237555354834\n",
      "\n",
      "Test set: Average loss: 1.8668 \n",
      "Accuracy: 3100/10000 (31.00%)\n",
      "\n",
      "Round   3, Average loss 1.867 Test accuracy 31.000\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0015904712677001953\n",
      "conv1.bias 0.0018637152388691902\n",
      "conv2.weight 0.0007717736562093099\n",
      "conv2.bias 0.0008435070631094277\n",
      "fc1.weight 0.00023493536313374839\n",
      "fc1.bias 0.00063656202207009\n",
      "fc2.weight 0.0005903516496930804\n",
      "fc2.bias 0.001751107828957694\n",
      "fc3.weight 0.000850702751250494\n",
      "fc3.bias 0.0010535018518567085\n",
      "\n",
      "Test set: Average loss: 1.8582 \n",
      "Accuracy: 3137/10000 (31.37%)\n",
      "\n",
      "Round   4, Average loss 1.858 Test accuracy 31.370\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0014747717645433214\n",
      "conv1.bias 0.001811348833143711\n",
      "conv2.weight 0.000780187745889028\n",
      "conv2.bias 0.0008439936209470034\n",
      "fc1.weight 0.0002420695424079895\n",
      "fc1.bias 0.0007380722711483637\n",
      "fc2.weight 0.0006389253669314914\n",
      "fc2.bias 0.0019561279387701126\n",
      "fc3.weight 0.0009020540685880752\n",
      "fc3.bias 0.0012209149077534675\n",
      "\n",
      "Test set: Average loss: 1.8535 \n",
      "Accuracy: 3154/10000 (31.54%)\n",
      "\n",
      "Round   5, Average loss 1.853 Test accuracy 31.540\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0014071946673923068\n",
      "conv1.bias 0.0017663994804024696\n",
      "conv2.weight 0.0007952577869097391\n",
      "conv2.bias 0.0008341496577486396\n",
      "fc1.weight 0.0002470094164212545\n",
      "fc1.bias 0.0008285186563928922\n",
      "fc2.weight 0.0006700743758489215\n",
      "fc2.bias 0.0020660791723501114\n",
      "fc3.weight 0.0009436500923974174\n",
      "fc3.bias 0.0012779390439391137\n",
      "\n",
      "Test set: Average loss: 1.8493 \n",
      "Accuracy: 3190/10000 (31.90%)\n",
      "\n",
      "Round   6, Average loss 1.849 Test accuracy 31.900\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001370918353398641\n",
      "conv1.bias 0.0017439639195799828\n",
      "conv2.weight 0.0008089729646841685\n",
      "conv2.bias 0.0008296619635075331\n",
      "fc1.weight 0.0002510532935460409\n",
      "fc1.bias 0.0009190870448946952\n",
      "fc2.weight 0.0006912677060990106\n",
      "fc2.bias 0.0020969747787430173\n",
      "fc3.weight 0.0009740126984460014\n",
      "fc3.bias 0.0012737829238176346\n",
      "\n",
      "Test set: Average loss: 1.8460 \n",
      "Accuracy: 3223/10000 (32.23%)\n",
      "\n",
      "Round   7, Average loss 1.846 Test accuracy 32.230\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001352436145146688\n",
      "conv1.bias 0.0017231358215212822\n",
      "conv2.weight 0.0008200231691201528\n",
      "conv2.bias 0.0008307433454319835\n",
      "fc1.weight 0.00025495298703511555\n",
      "fc1.bias 0.0009788023307919502\n",
      "fc2.weight 0.0007097251831539094\n",
      "fc2.bias 0.002011320065884363\n",
      "fc3.weight 0.0010008740283194042\n",
      "fc3.bias 0.0011885358951985836\n",
      "\n",
      "Test set: Average loss: 1.8443 \n",
      "Accuracy: 3273/10000 (32.73%)\n",
      "\n",
      "Round   8, Average loss 1.844 Test accuracy 32.730\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001355250808927748\n",
      "conv1.bias 0.001729994701842467\n",
      "conv2.weight 0.0008245875438054402\n",
      "conv2.bias 0.0008329801494255662\n",
      "fc1.weight 0.000258890430132548\n",
      "fc1.bias 0.0009906173994143803\n",
      "fc2.weight 0.000724002860841297\n",
      "fc2.bias 0.0017884747967833565\n",
      "fc3.weight 0.0010274860830534072\n",
      "fc3.bias 0.0010874914936721326\n",
      "\n",
      "Test set: Average loss: 1.8390 \n",
      "Accuracy: 3314/10000 (33.14%)\n",
      "\n",
      "Round   9, Average loss 1.839 Test accuracy 33.140\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0013746155632866754\n",
      "conv1.bias 0.001781887064377467\n",
      "conv2.weight 0.0008264149228731792\n",
      "conv2.bias 0.000846310518682003\n",
      "fc1.weight 0.0002622519930203756\n",
      "fc1.bias 0.0009679468969504038\n",
      "fc2.weight 0.0007375593223269024\n",
      "fc2.bias 0.0015839944992746627\n",
      "fc3.weight 0.0010544105654671078\n",
      "fc3.bias 0.0009601738303899765\n",
      "\n",
      "Test set: Average loss: 1.8390 \n",
      "Accuracy: 3332/10000 (33.32%)\n",
      "\n",
      "Round  10, Average loss 1.839 Test accuracy 33.320\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0014181182119581434\n",
      "conv1.bias 0.0018114658693472545\n",
      "conv2.weight 0.0008285421629746755\n",
      "conv2.bias 0.0008645766647532582\n",
      "fc1.weight 0.00026514033476511637\n",
      "fc1.bias 0.0009330055365959803\n",
      "fc2.weight 0.0007487303207791041\n",
      "fc2.bias 0.001398573141722452\n",
      "fc3.weight 0.0010788051854996453\n",
      "fc3.bias 0.0008662034757435322\n",
      "\n",
      "Test set: Average loss: 1.8355 \n",
      "Accuracy: 3367/10000 (33.67%)\n",
      "\n",
      "Round  11, Average loss 1.835 Test accuracy 33.670\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0014761951234605578\n",
      "conv1.bias 0.0019226443643371265\n",
      "conv2.weight 0.000831643541653951\n",
      "conv2.bias 0.0008872076869010925\n",
      "fc1.weight 0.00026808299620946247\n",
      "fc1.bias 0.0008966421087582906\n",
      "fc2.weight 0.0007568763835089547\n",
      "fc2.bias 0.0012624668223517282\n",
      "fc3.weight 0.0010966048354194278\n",
      "fc3.bias 0.0007897268049418926\n",
      "\n",
      "Test set: Average loss: 1.8330 \n",
      "Accuracy: 3394/10000 (33.94%)\n",
      "\n",
      "Round  12, Average loss 1.833 Test accuracy 33.940\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0015317188368903266\n",
      "conv1.bias 0.00202162005007267\n",
      "conv2.weight 0.0008347315589586894\n",
      "conv2.bias 0.0008997920667752624\n",
      "fc1.weight 0.0002708925207455953\n",
      "fc1.bias 0.0008661553263664245\n",
      "fc2.weight 0.0007650326168726361\n",
      "fc2.bias 0.0011584395099253882\n",
      "fc3.weight 0.0011149234714962187\n",
      "fc3.bias 0.0007317436393350363\n",
      "\n",
      "Test set: Average loss: 1.8299 \n",
      "Accuracy: 3420/10000 (34.20%)\n",
      "\n",
      "Round  13, Average loss 1.830 Test accuracy 34.200\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0015907365745968288\n",
      "conv1.bias 0.0021400932843486467\n",
      "conv2.weight 0.0008405009905497233\n",
      "conv2.bias 0.0009396602399647236\n",
      "fc1.weight 0.0002735125223795573\n",
      "fc1.bias 0.0008436428382992744\n",
      "fc2.weight 0.0007689360588315934\n",
      "fc2.bias 0.0010866325880799974\n",
      "fc3.weight 0.001132716451372419\n",
      "fc3.bias 0.0006937162019312382\n",
      "\n",
      "Test set: Average loss: 1.8290 \n",
      "Accuracy: 3440/10000 (34.40%)\n",
      "\n",
      "Round  14, Average loss 1.829 Test accuracy 34.400\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0016338943110571968\n",
      "conv1.bias 0.002237113192677498\n",
      "conv2.weight 0.0008512753248214722\n",
      "conv2.bias 0.0009490362135693431\n",
      "fc1.weight 0.00027590433756510417\n",
      "fc1.bias 0.0008208690832058589\n",
      "fc2.weight 0.0007729059647000026\n",
      "fc2.bias 0.001022631125081153\n",
      "fc3.weight 0.0011423660176140922\n",
      "fc3.bias 0.0006593658123165369\n",
      "\n",
      "Test set: Average loss: 1.8266 \n",
      "Accuracy: 3447/10000 (34.47%)\n",
      "\n",
      "Round  15, Average loss 1.827 Test accuracy 34.470\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0016797945234510634\n",
      "conv1.bias 0.0022774843188623586\n",
      "conv2.weight 0.0008597216010093689\n",
      "conv2.bias 0.0009741192916408181\n",
      "fc1.weight 0.0002775260806083679\n",
      "fc1.bias 0.0008030654241641363\n",
      "fc2.weight 0.0007743710563296363\n",
      "fc2.bias 0.0009812024377641223\n",
      "fc3.weight 0.0011483090264456613\n",
      "fc3.bias 0.0006376135163009167\n",
      "\n",
      "Test set: Average loss: 1.8271 \n",
      "Accuracy: 3459/10000 (34.59%)\n",
      "\n",
      "Round  16, Average loss 1.827 Test accuracy 34.590\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0017075424724155002\n",
      "conv1.bias 0.0023547966654102006\n",
      "conv2.weight 0.0008652607599894205\n",
      "conv2.bias 0.0009918638970702887\n",
      "fc1.weight 0.00027882053454717003\n",
      "fc1.bias 0.0007835781822601954\n",
      "fc2.weight 0.0007762498798824491\n",
      "fc2.bias 0.0009378048458269664\n",
      "fc3.weight 0.0011549335150491623\n",
      "fc3.bias 0.0006118065677583218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.8275 \n",
      "Accuracy: 3460/10000 (34.60%)\n",
      "\n",
      "Round  17, Average loss 1.827 Test accuracy 34.600\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0017409222655826145\n",
      "conv1.bias 0.002388616092503071\n",
      "conv2.weight 0.0008671651283899943\n",
      "conv2.bias 0.001009092084132135\n",
      "fc1.weight 0.0002797712882359823\n",
      "fc1.bias 0.0007762903968493144\n",
      "fc2.weight 0.0007769887409512959\n",
      "fc2.bias 0.000906670643460183\n",
      "fc3.weight 0.0011568508687473478\n",
      "fc3.bias 0.0005797621328383684\n",
      "\n",
      "Test set: Average loss: 1.8273 \n",
      "Accuracy: 3462/10000 (34.62%)\n",
      "\n",
      "Round  18, Average loss 1.827 Test accuracy 34.620\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0017553271187676323\n",
      "conv1.bias 0.0024421041210492453\n",
      "conv2.weight 0.000867953896522522\n",
      "conv2.bias 0.0010486284736543894\n",
      "fc1.weight 0.00028104301293691\n",
      "fc1.bias 0.0007605125506718953\n",
      "fc2.weight 0.0007797532138370333\n",
      "fc2.bias 0.0008787594380832854\n",
      "fc3.weight 0.0011614589464096796\n",
      "fc3.bias 0.0005600970238447189\n",
      "\n",
      "Test set: Average loss: 1.8290 \n",
      "Accuracy: 3455/10000 (34.55%)\n",
      "\n",
      "Round  19, Average loss 1.829 Test accuracy 34.550\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0017645339171091716\n",
      "conv1.bias 0.00242055207490921\n",
      "conv2.weight 0.000866897702217102\n",
      "conv2.bias 0.001063601579517126\n",
      "fc1.weight 0.00028193233410517374\n",
      "fc1.bias 0.0007560169945160548\n",
      "fc2.weight 0.0007830734290773906\n",
      "fc2.bias 0.0008555297695455097\n",
      "fc3.weight 0.0011666648444675265\n",
      "fc3.bias 0.0005313648376613856\n",
      "\n",
      "Test set: Average loss: 1.8286 \n",
      "Accuracy: 3464/10000 (34.64%)\n",
      "\n",
      "Round  20, Average loss 1.829 Test accuracy 34.640\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0017769208219316271\n",
      "conv1.bias 0.0024834039310614267\n",
      "conv2.weight 0.0008676865696907043\n",
      "conv2.bias 0.001097942003980279\n",
      "fc1.weight 0.0002828840017318726\n",
      "fc1.bias 0.0007464119543631871\n",
      "fc2.weight 0.0007854175946069142\n",
      "fc2.bias 0.0008329116694983982\n",
      "fc3.weight 0.0011671604145140875\n",
      "fc3.bias 0.0005087926052510739\n",
      "\n",
      "Test set: Average loss: 1.8306 \n",
      "Accuracy: 3459/10000 (34.59%)\n",
      "\n",
      "Round  21, Average loss 1.831 Test accuracy 34.590\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0017852241463131376\n",
      "conv1.bias 0.002547520368049542\n",
      "conv2.weight 0.0008675292134284973\n",
      "conv2.bias 0.001107937772758305\n",
      "fc1.weight 0.0002838819026947021\n",
      "fc1.bias 0.0007442700366179149\n",
      "fc2.weight 0.0007884068148476737\n",
      "fc2.bias 0.000820580692518325\n",
      "fc3.weight 0.0011681042256809417\n",
      "fc3.bias 0.0004961371421813964\n",
      "\n",
      "Test set: Average loss: 1.8322 \n",
      "Accuracy: 3455/10000 (34.55%)\n",
      "\n",
      "Round  22, Average loss 1.832 Test accuracy 34.550\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0017966067790985108\n",
      "conv1.bias 0.0024924882066746554\n",
      "conv2.weight 0.0008654505014419556\n",
      "conv2.bias 0.0011373304296284914\n",
      "fc1.weight 0.0002844622532526652\n",
      "fc1.bias 0.0007369899501403173\n",
      "fc2.weight 0.0007910995256333125\n",
      "fc2.bias 0.0008004627057484218\n",
      "fc3.weight 0.0011703916958400182\n",
      "fc3.bias 0.0004769105464220047\n",
      "\n",
      "Test set: Average loss: 1.8327 \n",
      "Accuracy: 3451/10000 (34.51%)\n",
      "\n",
      "Round  23, Average loss 1.833 Test accuracy 34.510\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018047881126403808\n",
      "conv1.bias 0.0024818445866306624\n",
      "conv2.weight 0.0008641320466995239\n",
      "conv2.bias 0.0011430815793573856\n",
      "fc1.weight 0.00028523147106170656\n",
      "fc1.bias 0.0007259363308548927\n",
      "fc2.weight 0.0007948298302907792\n",
      "fc2.bias 0.0007821759652523767\n",
      "fc3.weight 0.0011751996619360787\n",
      "fc3.bias 0.000462489016354084\n",
      "\n",
      "Test set: Average loss: 1.8325 \n",
      "Accuracy: 3462/10000 (34.62%)\n",
      "\n",
      "Round  24, Average loss 1.833 Test accuracy 34.620\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001807640790939331\n",
      "conv1.bias 0.00245078606531024\n",
      "conv2.weight 0.0008603119850158691\n",
      "conv2.bias 0.0011715536238625646\n",
      "fc1.weight 0.0002861761252085368\n",
      "fc1.bias 0.0007162544876337052\n",
      "fc2.weight 0.0008005061792948889\n",
      "fc2.bias 0.0007661028454701105\n",
      "fc3.weight 0.0011770673450969514\n",
      "fc3.bias 0.00045139119029045103\n",
      "\n",
      "Test set: Average loss: 1.8337 \n",
      "Accuracy: 3451/10000 (34.51%)\n",
      "\n",
      "Round  25, Average loss 1.834 Test accuracy 34.510\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001808794339497884\n",
      "conv1.bias 0.0026064924895763397\n",
      "conv2.weight 0.0008580152193705241\n",
      "conv2.bias 0.001163314562290907\n",
      "fc1.weight 0.0002868541876475016\n",
      "fc1.bias 0.0007060017436742782\n",
      "fc2.weight 0.0008054385109553261\n",
      "fc2.bias 0.0007492946017356146\n",
      "fc3.weight 0.0011814303341365995\n",
      "fc3.bias 0.0004395171068608761\n",
      "\n",
      "Test set: Average loss: 1.8335 \n",
      "Accuracy: 3446/10000 (34.46%)\n",
      "\n",
      "Round  26, Average loss 1.834 Test accuracy 34.460\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018191228972540962\n",
      "conv1.bias 0.0026073179518183074\n",
      "conv2.weight 0.0008580741286277771\n",
      "conv2.bias 0.0011873834300786257\n",
      "fc1.weight 0.00028704243898391724\n",
      "fc1.bias 0.0006986291458209356\n",
      "fc2.weight 0.000809947271195669\n",
      "fc2.bias 0.0007372627123480751\n",
      "fc3.weight 0.001185095523084913\n",
      "fc3.bias 0.0004309824202209711\n",
      "\n",
      "Test set: Average loss: 1.8329 \n",
      "Accuracy: 3450/10000 (34.50%)\n",
      "\n",
      "Round  27, Average loss 1.833 Test accuracy 34.500\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018232840961880153\n",
      "conv1.bias 0.0026667152221004167\n",
      "conv2.weight 0.0008572499950726827\n",
      "conv2.bias 0.0012172464048489928\n",
      "fc1.weight 0.00028766369819641113\n",
      "fc1.bias 0.0006901202102502187\n",
      "fc2.weight 0.0008133371671040853\n",
      "fc2.bias 0.0007223572936796007\n",
      "fc3.weight 0.001188779090132032\n",
      "fc3.bias 0.0004183472599834204\n",
      "\n",
      "Test set: Average loss: 1.8338 \n",
      "Accuracy: 3447/10000 (34.47%)\n",
      "\n",
      "Round  28, Average loss 1.834 Test accuracy 34.470\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018353660901387532\n",
      "conv1.bias 0.002520304173231125\n",
      "conv2.weight 0.0008596794803937276\n",
      "conv2.bias 0.0012610482517629862\n",
      "fc1.weight 0.0002880483865737915\n",
      "fc1.bias 0.000687599057952563\n",
      "fc2.weight 0.000815416517711821\n",
      "fc2.bias 0.000717685247461001\n",
      "fc3.weight 0.0011911414918445405\n",
      "fc3.bias 0.00041337767615914347\n",
      "\n",
      "Test set: Average loss: 1.8345 \n",
      "Accuracy: 3444/10000 (34.44%)\n",
      "\n",
      "Round  29, Average loss 1.835 Test accuracy 34.440\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018452244334750706\n",
      "conv1.bias 0.0026018163189291954\n",
      "conv2.weight 0.0008591963847478231\n",
      "conv2.bias 0.001265334547497332\n",
      "fc1.weight 0.0002881466547648112\n",
      "fc1.bias 0.0006834624335169792\n",
      "fc2.weight 0.0008171836535135905\n",
      "fc2.bias 0.0007044633495665732\n",
      "fc3.weight 0.0011927982171376546\n",
      "fc3.bias 0.00040139653719961643\n",
      "\n",
      "Test set: Average loss: 1.8328 \n",
      "Accuracy: 3449/10000 (34.49%)\n",
      "\n",
      "Round  30, Average loss 1.833 Test accuracy 34.490\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018514082166883681\n",
      "conv1.bias 0.002612941898405552\n",
      "conv2.weight 0.0008584327499071756\n",
      "conv2.bias 0.001272330293431878\n",
      "fc1.weight 0.00028869569301605226\n",
      "fc1.bias 0.0006763134151697159\n",
      "fc2.weight 0.000820330021873353\n",
      "fc2.bias 0.0006921612435863132\n",
      "fc3.weight 0.0011973017737978982\n",
      "fc3.bias 0.0003931537736207247\n",
      "\n",
      "Test set: Average loss: 1.8338 \n",
      "Accuracy: 3441/10000 (34.41%)\n",
      "\n",
      "Round  31, Average loss 1.834 Test accuracy 34.410\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018656647205352783\n",
      "conv1.bias 0.0025840171923240027\n",
      "conv2.weight 0.0008603294690450032\n",
      "conv2.bias 0.0012803073041141033\n",
      "fc1.weight 0.0002890840172767639\n",
      "fc1.bias 0.0006673228616515795\n",
      "fc2.weight 0.0008223287642948211\n",
      "fc2.bias 0.0006818065331095741\n",
      "fc3.weight 0.0011994350524175735\n",
      "fc3.bias 0.0003896705573424697\n",
      "\n",
      "Test set: Average loss: 1.8331 \n",
      "Accuracy: 3442/10000 (34.42%)\n",
      "\n",
      "Round  32, Average loss 1.833 Test accuracy 34.420\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018759172492557102\n",
      "conv1.bias 0.00268381896118323\n",
      "conv2.weight 0.0008626474936803182\n",
      "conv2.bias 0.0012849163031205535\n",
      "fc1.weight 0.0002890399098396301\n",
      "fc1.bias 0.0006605332096417745\n",
      "fc2.weight 0.0008246638472118075\n",
      "fc2.bias 0.0006749679201415607\n",
      "fc3.weight 0.0012038869517190115\n",
      "fc3.bias 0.00038552633486688137\n",
      "\n",
      "Test set: Average loss: 1.8307 \n",
      "Accuracy: 3450/10000 (34.50%)\n",
      "\n",
      "Round  33, Average loss 1.831 Test accuracy 34.500\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018852600786421034\n",
      "conv1.bias 0.002685104807217916\n",
      "conv2.weight 0.00086506187915802\n",
      "conv2.bias 0.0013042603386566043\n",
      "fc1.weight 0.00028922601540883383\n",
      "fc1.bias 0.0006601298227906227\n",
      "fc2.weight 0.0008260208462911939\n",
      "fc2.bias 0.0006712404124083973\n",
      "fc3.weight 0.0012046491815930322\n",
      "fc3.bias 0.0003851422807201743\n",
      "\n",
      "Test set: Average loss: 1.8299 \n",
      "Accuracy: 3456/10000 (34.56%)\n",
      "\n",
      "Round  34, Average loss 1.830 Test accuracy 34.560\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018986513879564073\n",
      "conv1.bias 0.002736456381777922\n",
      "conv2.weight 0.000862990915775299\n",
      "conv2.bias 0.001326120924204588\n",
      "fc1.weight 0.0002893181840578715\n",
      "fc1.bias 0.000654999166727066\n",
      "fc2.weight 0.0008277858060503763\n",
      "fc2.bias 0.0006603644273820377\n",
      "fc3.weight 0.0012064750705446515\n",
      "fc3.bias 0.00037530139088630675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.8278 \n",
      "Accuracy: 3461/10000 (34.61%)\n",
      "\n",
      "Round  35, Average loss 1.828 Test accuracy 34.610\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019090323978000216\n",
      "conv1.bias 0.0027519483119249344\n",
      "conv2.weight 0.0008658796548843383\n",
      "conv2.bias 0.001343086245469749\n",
      "fc1.weight 0.0002889633377393087\n",
      "fc1.bias 0.00066351518034935\n",
      "fc2.weight 0.0008279765409136575\n",
      "fc2.bias 0.0006596932985952922\n",
      "fc3.weight 0.0012132840497153146\n",
      "fc3.bias 0.0003737880615517497\n",
      "\n",
      "Test set: Average loss: 1.8272 \n",
      "Accuracy: 3457/10000 (34.57%)\n",
      "\n",
      "Round  36, Average loss 1.827 Test accuracy 34.570\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019318784607781304\n",
      "conv1.bias 0.002712020960946878\n",
      "conv2.weight 0.0008684935172398885\n",
      "conv2.bias 0.001375715946778655\n",
      "fc1.weight 0.00028943357865015667\n",
      "fc1.bias 0.0006639072050650915\n",
      "fc2.weight 0.0008297557868654766\n",
      "fc2.bias 0.0006584252363869122\n",
      "fc3.weight 0.0012159259546370732\n",
      "fc3.bias 0.0003688642056658864\n",
      "\n",
      "Test set: Average loss: 1.8264 \n",
      "Accuracy: 3456/10000 (34.56%)\n",
      "\n",
      "Round  37, Average loss 1.826 Test accuracy 34.560\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019386564360724555\n",
      "conv1.bias 0.0026795025914907455\n",
      "conv2.weight 0.0008717825015385946\n",
      "conv2.bias 0.0013771114172413945\n",
      "fc1.weight 0.00028946457306543985\n",
      "fc1.bias 0.0006653430561224619\n",
      "fc2.weight 0.0008318038213820685\n",
      "fc2.bias 0.0006590231898285094\n",
      "fc3.weight 0.001217787606375558\n",
      "fc3.bias 0.0003656058572232723\n",
      "\n",
      "Test set: Average loss: 1.8252 \n",
      "Accuracy: 3462/10000 (34.62%)\n",
      "\n",
      "Round  38, Average loss 1.825 Test accuracy 34.620\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019520436392890083\n",
      "conv1.bias 0.0027256160974502563\n",
      "conv2.weight 0.000876049796740214\n",
      "conv2.bias 0.0013935628812760115\n",
      "fc1.weight 0.0002895740071932475\n",
      "fc1.bias 0.000667202038069566\n",
      "fc2.weight 0.0008325068723587763\n",
      "fc2.bias 0.000661731209783327\n",
      "fc3.weight 0.0012173314889272055\n",
      "fc3.bias 0.0003647033823654056\n",
      "\n",
      "Test set: Average loss: 1.8242 \n",
      "Accuracy: 3470/10000 (34.70%)\n",
      "\n",
      "Round  39, Average loss 1.824 Test accuracy 34.700\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001968783007727729\n",
      "conv1.bias 0.002748421703775724\n",
      "conv2.weight 0.0008802743752797445\n",
      "conv2.bias 0.001411122502759099\n",
      "fc1.weight 0.00028938605388005573\n",
      "fc1.bias 0.0006686937063932419\n",
      "fc2.weight 0.0008335470207153805\n",
      "fc2.bias 0.0006611098845799764\n",
      "fc3.weight 0.001221386023930141\n",
      "fc3.bias 0.00036437076050788164\n",
      "\n",
      "Test set: Average loss: 1.8237 \n",
      "Accuracy: 3479/10000 (34.79%)\n",
      "\n",
      "Round  40, Average loss 1.824 Test accuracy 34.790\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019934839672512477\n",
      "conv1.bias 0.002732904627919197\n",
      "conv2.weight 0.0008811879158020019\n",
      "conv2.bias 0.0014203060418367386\n",
      "fc1.weight 0.0002892886996269226\n",
      "fc1.bias 0.0006698737541834513\n",
      "fc2.weight 0.0008335339644598582\n",
      "fc2.bias 0.0006555910443975812\n",
      "fc3.weight 0.0012218820197241647\n",
      "fc3.bias 0.00036014255601912737\n",
      "\n",
      "Test set: Average loss: 1.8223 \n",
      "Accuracy: 3485/10000 (34.85%)\n",
      "\n",
      "Round  41, Average loss 1.822 Test accuracy 34.850\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020094128449757893\n",
      "conv1.bias 0.002750886914630731\n",
      "conv2.weight 0.0008806321024894715\n",
      "conv2.bias 0.0014019913505762815\n",
      "fc1.weight 0.00028942946592966715\n",
      "fc1.bias 0.0006731436277429263\n",
      "fc2.weight 0.0008345024926321847\n",
      "fc2.bias 0.0006548921089796792\n",
      "fc3.weight 0.0012243529160817464\n",
      "fc3.bias 0.0003578609088435769\n",
      "\n",
      "Test set: Average loss: 1.8223 \n",
      "Accuracy: 3483/10000 (34.83%)\n",
      "\n",
      "Round  42, Average loss 1.822 Test accuracy 34.830\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020212570826212565\n",
      "conv1.bias 0.0029000829284389815\n",
      "conv2.weight 0.0008827404181162516\n",
      "conv2.bias 0.0014011291787028313\n",
      "fc1.weight 0.0002899433175722758\n",
      "fc1.bias 0.0006673169632752736\n",
      "fc2.weight 0.0008365972647591243\n",
      "fc2.bias 0.0006489791419534456\n",
      "fc3.weight 0.0012268290633247012\n",
      "fc3.bias 0.00034885965287685395\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "K = 2\n",
    "T = 3\n",
    "sigma = 1\n",
    "Noise_Alloc = [0,2,4]\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "\n",
    "N_array = [2]\n",
    "B_array = [0.5]\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 50\n",
    "\n",
    "lr_array = [0.0001] # 0.001 is the bset\n",
    "\n",
    "sigma_array = [1]\n",
    "# lr_array = [0.1, 0.01, 0.005, 0.001,0.0005]\n",
    "\n",
    "\n",
    "loss_test_arr_K4_G1_v3 = np.zeros((len(sigma_array),len(lr_array),N_trials,N_epochs))\n",
    "acc_test_arr_K4_G1_v3  = np.zeros((len(sigma_array),len(lr_array),N_trials,N_epochs))\n",
    "\n",
    "N = 2\n",
    "for sigma_idx in range(len(sigma_array)):\n",
    "    \n",
    "    sigma = sigma_array[sigma_idx]\n",
    "    \n",
    "    print('\\n\\n')\n",
    "    print('sigma =',sigma)\n",
    "    print('\\n\\n')\n",
    "           \n",
    "        \n",
    "    # print(\"alpha_array: \",alpha_array,'\\n')\n",
    "    \n",
    "    \n",
    "    for lr_idx in range(len(lr_array)):\n",
    "        \n",
    "        args.lr = lr_array[lr_idx]\n",
    "        \n",
    "        print('Learning Rate =',args.lr)\n",
    "        print('\\n\\n')\n",
    "#         while(len(z_array)<N):\n",
    "#             z_tmp = np.random.uniform(-1,1,1)\n",
    "#             MIS_tmp = MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_tmp], 1,sigma)\n",
    "#             if MIS_tmp < B and MIS_tmp > 0.1:\n",
    "#                 z_array.append(z_tmp[0])\n",
    "#         \n",
    "#         z_array = np.sort(z_array)\n",
    "#         print(N_idx,'!!!')\n",
    "#         if N_idx==0:\n",
    "# #             z_array = np.array([-0.94,-0.534,0.534, 0.94])\n",
    "# #         elif N_idx==1:\n",
    "# #             z_array = np.array([-0.94, -0.73, 0.73, 0.94])\n",
    "#         elif N_idx==1:\n",
    "#             z_array = np.array([-0.94, -0.125, 0.125, 0.94])\n",
    "#         else:\n",
    "# #             z_array = np.array([-0.94, -0.73, -0.534, -0.125, 0.125, 0.534, 0.73, 0.94])\n",
    "# #             z_array = np.array([-0.9, -0.81, -0.22, -0.20, 0.20, 0.22, 0.81, 0.9])\n",
    "        \n",
    "        z_array = np.array([-0.81, 0.81])\n",
    "        \n",
    "        print('z_array:',z_array)\n",
    "        if sigma != 0:\n",
    "            for j in range(len(z_array)):\n",
    "                print(MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_array[j]], 1,sigma))\n",
    "\n",
    "        \n",
    "        _Noise_label = np.ones((25000*T,10)) * 0.1\n",
    "\n",
    "        X_tilde,a,b = BACC_Enc_Data_v3(encoding_input_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde,a,b = BACC_Enc_Data_v3(encoding_label_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "\n",
    "        m = N # m is the number of received result @ master\n",
    "    #     print('number of results:',m)\n",
    "\n",
    "        for trial_idx in range(N_trials):\n",
    "            print('(T, sigma)=',T,sigma,') ',trial_idx,'-th Trial!!')\n",
    "\n",
    "            net_glob = CNNCifar(args=args)\n",
    "            net_glob.cuda()\n",
    "            net_glob.train()\n",
    "\n",
    "            # copy weights\n",
    "            w_glob = net_glob.state_dict()\n",
    "\n",
    "            for iter in range(N_epochs): #args.epochs\n",
    "                w_locals, loss_locals = [], []\n",
    "                idxs_users = np.random.choice(range(N), m, replace=False)\n",
    "                idxs_users = np.sort(idxs_users)\n",
    "                print('selected users:',idxs_users)\n",
    "                \n",
    "                coded_net = BACC_Enc_Model_withNoise_v4(net_glob.cuda(), N, K, T, sigma, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "                dec_z_array = []\n",
    "                for idx in idxs_users: #for idx in range(N):\n",
    "            #         print(idx)\n",
    "                    local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[idx,:,:], label=y_tilde[idx,:,:])\n",
    "                    w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "#                     w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                    w_locals.append(copy.deepcopy(w))\n",
    "                    loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                    dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "                # update global weights\n",
    "                #w_glob = FedAvg(w_locals)\n",
    "                w_glob = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "\n",
    "                # copy weight to net_glob\n",
    "                net_glob.load_state_dict(w_glob)\n",
    "\n",
    "                # print loss\n",
    "            #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "            #     loss_train_arr.append(loss_train)\n",
    "\n",
    "                acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "                acc_test_arr_K4_G1_v3[sigma_idx][lr_idx][trial_idx][iter] = acc_test\n",
    "                loss_test_arr_K4_G1_v3[sigma_idx][lr_idx][trial_idx][iter] = loss_test\n",
    "                if iter % 1 ==0:\n",
    "                    print('Round {:3d}, Average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_test, acc_test))\n",
    "                #print(loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filehandler = open(\"./plot/CIFAR10_LeNet_K2_N2_T3_G1\",\"wb\")\n",
    "pickle.dump(acc_test_arr_K4_G1_v2,filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lr_array = [0.1, 0.01, 0.005, 0.001,0.0005]\n",
    "\n",
    "plt.plot(acc_test_arr_K4_G1_v2[0,0,0,0:50],label='w/o Grouping, K=2, N=2, sigma=0.1' )\n",
    "plt.plot(acc_test_arr_K4_G1_v2[1,0,0,0:50],label='w/o Grouping, K=2, N=2, sigma=0.3' )\n",
    "plt.plot(acc_test_arr_K4_G1_v2[2,0,0,0:50],label='w/o Grouping, K=2, N=2, sigma=0.5' )\n",
    "# plt.plot(acc_test_arr_K4_G1[0,3,0,0:30],label='w/o Grouping, K=4, N=4, G=1, lr=0.001' )\n",
    "# plt.plot(acc_test_arr_K4_G1[0,4,0,0:30],label='w/o Grouping, K=4, N=4, G=1, lr=0.0005' )\n",
    "\n",
    "\n",
    "# plt.plot(acc_test_arr_uncoded,'r',label='uncoded')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "# plt.title(title_name)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Test accuracy(%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2. K=2, T=0 (without noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.58778525  0.58778525]\n",
      "[-0.81, 0.81]\n",
      "[-0.81]\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "# K=2\n",
    "# T=3\n",
    "# j_array = np.array(range(K+T))\n",
    "# alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(alpha_array)\n",
    "\n",
    "print(alpha_array[Signal_Alloc])\n",
    "\n",
    "print(z_array)\n",
    "\n",
    "print(dec_z_array)\n",
    "\n",
    "print(idxs_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate = 0.001\n",
      "\n",
      "\n",
      "\n",
      "z_array: [-0.81, 0.81]\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 0 25000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 0 25000 \n",
      "\n",
      "(T, sigma)= 0 1 )  0 -th Trial!!\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 5634018.6192 \n",
      "Accuracy: 861/10000 (8.61%)\n",
      "\n",
      "Round   0, Average loss 5634018.619 Test accuracy 8.610\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 57287201.1812 \n",
      "Accuracy: 1474/10000 (14.74%)\n",
      "\n",
      "Round   1, Average loss 57287201.181 Test accuracy 14.740\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 314855690.2992 \n",
      "Accuracy: 1928/10000 (19.28%)\n",
      "\n",
      "Round   2, Average loss 314855690.299 Test accuracy 19.280\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 1074226853.8176 \n",
      "Accuracy: 2011/10000 (20.11%)\n",
      "\n",
      "Round   3, Average loss 1074226853.818 Test accuracy 20.110\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 2806019532.7168 \n",
      "Accuracy: 2053/10000 (20.53%)\n",
      "\n",
      "Round   4, Average loss 2806019532.717 Test accuracy 20.530\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 6193538510.8480 \n",
      "Accuracy: 1914/10000 (19.14%)\n",
      "\n",
      "Round   5, Average loss 6193538510.848 Test accuracy 19.140\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 12129739192.1536 \n",
      "Accuracy: 1949/10000 (19.49%)\n",
      "\n",
      "Round   6, Average loss 12129739192.154 Test accuracy 19.490\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 21831574532.4032 \n",
      "Accuracy: 1977/10000 (19.77%)\n",
      "\n",
      "Round   7, Average loss 21831574532.403 Test accuracy 19.770\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 36814444650.4960 \n",
      "Accuracy: 1842/10000 (18.42%)\n",
      "\n",
      "Round   8, Average loss 36814444650.496 Test accuracy 18.420\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 58734183510.5536 \n",
      "Accuracy: 2018/10000 (20.18%)\n",
      "\n",
      "Round   9, Average loss 58734183510.554 Test accuracy 20.180\n",
      "Learning Rate = 0.0003\n",
      "\n",
      "\n",
      "\n",
      "z_array: [-0.81, 0.81]\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 0 25000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 0 25000 \n",
      "\n",
      "(T, sigma)= 0 1 )  0 -th Trial!!\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 1098.7915 \n",
      "Accuracy: 2173/10000 (21.73%)\n",
      "\n",
      "Round   0, Average loss 1098.792 Test accuracy 21.730\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 29951.1897 \n",
      "Accuracy: 1964/10000 (19.64%)\n",
      "\n",
      "Round   1, Average loss 29951.190 Test accuracy 19.640\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 201100.2640 \n",
      "Accuracy: 1931/10000 (19.31%)\n",
      "\n",
      "Round   2, Average loss 201100.264 Test accuracy 19.310\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 702837.2578 \n",
      "Accuracy: 1559/10000 (15.59%)\n",
      "\n",
      "Round   3, Average loss 702837.258 Test accuracy 15.590\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 1837671.2020 \n",
      "Accuracy: 2053/10000 (20.53%)\n",
      "\n",
      "Round   4, Average loss 1837671.202 Test accuracy 20.530\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 4034149.4798 \n",
      "Accuracy: 2194/10000 (21.94%)\n",
      "\n",
      "Round   5, Average loss 4034149.480 Test accuracy 21.940\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 7867041.9956 \n",
      "Accuracy: 2053/10000 (20.53%)\n",
      "\n",
      "Round   6, Average loss 7867041.996 Test accuracy 20.530\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 14160261.8787 \n",
      "Accuracy: 1965/10000 (19.65%)\n",
      "\n",
      "Round   7, Average loss 14160261.879 Test accuracy 19.650\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 23888037.4280 \n",
      "Accuracy: 2016/10000 (20.16%)\n",
      "\n",
      "Round   8, Average loss 23888037.428 Test accuracy 20.160\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 38257028.8201 \n",
      "Accuracy: 1915/10000 (19.15%)\n",
      "\n",
      "Round   9, Average loss 38257028.820 Test accuracy 19.150\n",
      "Learning Rate = 0.0001\n",
      "\n",
      "\n",
      "\n",
      "z_array: [-0.81, 0.81]\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 0 25000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 0 25000 \n",
      "\n",
      "(T, sigma)= 0 1 )  0 -th Trial!!\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 2.4614 \n",
      "Accuracy: 3675/10000 (36.75%)\n",
      "\n",
      "Round   0, Average loss 2.461 Test accuracy 36.750\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 14.4971 \n",
      "Accuracy: 3758/10000 (37.58%)\n",
      "\n",
      "Round   1, Average loss 14.497 Test accuracy 37.580\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 126.3403 \n",
      "Accuracy: 3399/10000 (33.99%)\n",
      "\n",
      "Round   2, Average loss 126.340 Test accuracy 33.990\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 612.5828 \n",
      "Accuracy: 3048/10000 (30.48%)\n",
      "\n",
      "Round   3, Average loss 612.583 Test accuracy 30.480\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 2030.1067 \n",
      "Accuracy: 2844/10000 (28.44%)\n",
      "\n",
      "Round   4, Average loss 2030.107 Test accuracy 28.440\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 5315.4736 \n",
      "Accuracy: 2716/10000 (27.16%)\n",
      "\n",
      "Round   5, Average loss 5315.474 Test accuracy 27.160\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 11968.0919 \n",
      "Accuracy: 2850/10000 (28.50%)\n",
      "\n",
      "Round   6, Average loss 11968.092 Test accuracy 28.500\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 23884.8934 \n",
      "Accuracy: 2434/10000 (24.34%)\n",
      "\n",
      "Round   7, Average loss 23884.893 Test accuracy 24.340\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 43964.1225 \n",
      "Accuracy: 2273/10000 (22.73%)\n",
      "\n",
      "Round   8, Average loss 43964.123 Test accuracy 22.730\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 75984.9236 \n",
      "Accuracy: 2484/10000 (24.84%)\n",
      "\n",
      "Round   9, Average loss 75984.924 Test accuracy 24.840\n",
      "Learning Rate = 3e-05\n",
      "\n",
      "\n",
      "\n",
      "z_array: [-0.81, 0.81]\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 0 25000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 0 25000 \n",
      "\n",
      "(T, sigma)= 0 1 )  0 -th Trial!!\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 1.8613 \n",
      "Accuracy: 3399/10000 (33.99%)\n",
      "\n",
      "Round   0, Average loss 1.861 Test accuracy 33.990\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 1.7457 \n",
      "Accuracy: 3916/10000 (39.16%)\n",
      "\n",
      "Round   1, Average loss 1.746 Test accuracy 39.160\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 1.7886 \n",
      "Accuracy: 4133/10000 (41.33%)\n",
      "\n",
      "Round   2, Average loss 1.789 Test accuracy 41.330\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 1.9903 \n",
      "Accuracy: 4185/10000 (41.85%)\n",
      "\n",
      "Round   3, Average loss 1.990 Test accuracy 41.850\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 2.4602 \n",
      "Accuracy: 4198/10000 (41.98%)\n",
      "\n",
      "Round   4, Average loss 2.460 Test accuracy 41.980\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 3.4327 \n",
      "Accuracy: 4168/10000 (41.68%)\n",
      "\n",
      "Round   5, Average loss 3.433 Test accuracy 41.680\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 5.2844 \n",
      "Accuracy: 4118/10000 (41.18%)\n",
      "\n",
      "Round   6, Average loss 5.284 Test accuracy 41.180\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 8.6784 \n",
      "Accuracy: 4085/10000 (40.85%)\n",
      "\n",
      "Round   7, Average loss 8.678 Test accuracy 40.850\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 14.4921 \n",
      "Accuracy: 4011/10000 (40.11%)\n",
      "\n",
      "Round   8, Average loss 14.492 Test accuracy 40.110\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 24.2211 \n",
      "Accuracy: 4057/10000 (40.57%)\n",
      "\n",
      "Round   9, Average loss 24.221 Test accuracy 40.570\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "K = 2\n",
    "T = 0\n",
    "sigma = 1\n",
    "Noise_Alloc = []\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K+T))\n",
    "# alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "alpha_array = np.array([-5.87785252e-01, 5.87785252e-01])\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "\n",
    "N_array = [2]\n",
    "\n",
    "lr_array = [0.001, 0.0003, 0.0001, 0.00003] # 0.001 is the bset\n",
    "\n",
    "\n",
    "B_array = [0.5]\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 10\n",
    "\n",
    "\n",
    "\n",
    "loss_test_arr_K2_G1_T0_v1 = np.zeros((len(N_array),len(lr_array),N_trials,N_epochs))\n",
    "acc_test_arr_K2_G1_T0_v1  = np.zeros((len(N_array),len(lr_array),N_trials,N_epochs))\n",
    "\n",
    "for N_idx in range(len(N_array)):\n",
    "    \n",
    "    N = N_array[N_idx]\n",
    "    \n",
    "    \n",
    "           \n",
    "        \n",
    "    # print(\"alpha_array: \",alpha_array,'\\n')\n",
    "    \n",
    "    \n",
    "    for lr_idx in range(len(lr_array)):\n",
    "        \n",
    "        args.lr = lr_array[lr_idx]\n",
    "        \n",
    "        print('Learning Rate =',args.lr)\n",
    "        print('\\n\\n')\n",
    "        \n",
    "        \n",
    "        z_array = []\n",
    "#         while(len(z_array)<N):\n",
    "#             z_tmp = np.random.uniform(-1,1,1)\n",
    "#             MIS_tmp = MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_tmp], 1,sigma)\n",
    "#             if MIS_tmp < B and MIS_tmp > 0.1:\n",
    "#                 z_array.append(z_tmp[0])\n",
    "#         \n",
    "#         z_array = np.sort(z_array)\n",
    "#         print(N_idx,'!!!')\n",
    "#         if N_idx==0:\n",
    "# #             z_array = np.array([-0.94,-0.534,0.534, 0.94])\n",
    "# #         elif N_idx==1:\n",
    "# #             z_array = np.array([-0.94, -0.73, 0.73, 0.94])\n",
    "#         elif N_idx==1:\n",
    "#             z_array = np.array([-0.94, -0.125, 0.125, 0.94])\n",
    "#         else:\n",
    "# #             z_array = np.array([-0.94, -0.73, -0.534, -0.125, 0.125, 0.534, 0.73, 0.94])\n",
    "# #             z_array = np.array([-0.9, -0.81, -0.22, -0.20, 0.20, 0.22, 0.81, 0.9])\n",
    "        \n",
    "        i_array = np.array(range(N))\n",
    "#         z_array = np.cos(i_array*2*math.pi/N/2) # np.cos(i_array*2*math.pi/N/2)\n",
    "        z_array = [-0.81, 0.81]\n",
    "#         z_array = alpha_array - 0.05\n",
    "        \n",
    "        print('z_array:',z_array)\n",
    "#         for j in range(len(z_array)):\n",
    "#             print(MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_array[j]], 1,sigma))\n",
    "        \n",
    "        \n",
    "        _Noise_label = np.ones((15000*T,10)) * 0.1\n",
    "\n",
    "        X_tilde,a,b = BACC_Enc_Data_v3(encoding_input_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde,a,b = BACC_Enc_Data_v3(encoding_label_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "\n",
    "        m = N # m is the number of received result @ master\n",
    "    #     print('number of results:',m)\n",
    "\n",
    "        for trial_idx in range(N_trials):\n",
    "            print('(T, sigma)=',T,sigma,') ',trial_idx,'-th Trial!!')\n",
    "\n",
    "            net_glob = CNNCifar(args=args)\n",
    "            net_glob.cuda()\n",
    "            net_glob.train()\n",
    "\n",
    "            # copy weights\n",
    "            w_glob = net_glob.state_dict()\n",
    "\n",
    "            for iter in range(N_epochs): #args.epochs\n",
    "                w_locals, loss_locals = [], []\n",
    "                idxs_users = np.random.choice(range(N), m, replace=False)\n",
    "                idxs_users = np.sort(idxs_users)\n",
    "                print('selected users:',idxs_users)\n",
    "                \n",
    "#                 coded_net = BACC_Enc_Model_withNoise_v4(net_glob.cuda(), N, K, T, 1, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "                dec_z_array = []\n",
    "                for idx in idxs_users: #for idx in range(N):\n",
    "                    print('local update, idx=',idx)\n",
    "                    local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[idx,:,:], label=y_tilde[idx,:,:])\n",
    "#                     w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "                    w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                    w_locals.append(copy.deepcopy(w))\n",
    "                    loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                    dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "                # update global weights\n",
    "                #w_glob = FedAvg(w_locals)\n",
    "                print('dec z_array=',dec_z_array)\n",
    "                w_glob = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "\n",
    "                # copy weight to net_glob\n",
    "                net_glob.load_state_dict(w_glob)\n",
    "\n",
    "                # print loss\n",
    "            #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "            #     loss_train_arr.append(loss_train)\n",
    "\n",
    "                acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "                acc_test_arr_K2_G1_T0_v1[N_idx][lr_idx][trial_idx][iter] = acc_test\n",
    "                loss_test_arr_K2_G1_T0_v1[N_idx][lr_idx][trial_idx][iter] = loss_test\n",
    "                if iter % 1 ==0:\n",
    "                    print('Round {:3d}, Average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_test, acc_test))\n",
    "                #print(loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lr_array = [0.1, 0.01, 0.005, 0.001,0.0005]\n",
    "\n",
    "plt.plot(acc_test_arr_K4_G1[0,0,0,0:30],label='w/o Grouping, K=4, N=4, G=1, lr=0.1' )\n",
    "plt.plot(acc_test_arr_K4_G1[0,1,0,0:30],label='w/o Grouping, K=4, N=4, G=1, lr=0.01' )\n",
    "plt.plot(acc_test_arr_K4_G1[0,2,0,0:30],label='w/o Grouping, K=4, N=4, G=1, lr=0.005' )\n",
    "plt.plot(acc_test_arr_K4_G1[0,3,0,0:30],label='w/o Grouping, K=4, N=4, G=1, lr=0.001' )\n",
    "plt.plot(acc_test_arr_K4_G1[0,4,0,0:30],label='w/o Grouping, K=4, N=4, G=1, lr=0.0005' )\n",
    "\n",
    "\n",
    "# plt.plot(acc_test_arr_uncoded,'r',label='uncoded')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "# plt.title(title_name)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Test accuracy(%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(acc_test_arr_K4_G1_v1[0,0,0,0:30],label='w/o Grouping, K=4, N=4, G=1, sigma=0' )\n",
    "plt.plot(acc_test_arr_K4_G1_v1[1,0,0,0:30],label='w/o Grouping, K=4, N=4, G=1, sigma=0.1' )\n",
    "plt.plot(acc_test_arr_K4_G1_v1[2,0,0,0:30],label='w/o Grouping, K=4, N=4, G=1, sigma=0.3' )\n",
    "plt.plot(acc_test_arr_K4_G1_v1[3,0,0,0:30],label='w/o Grouping, K=4, N=4, G=1, sigma=0.5' )\n",
    "plt.plot(acc_test_arr_K4_G1_v1[4,0,0,0:30],label='w/o Grouping, K=4, N=4, G=1, sigma=1' )\n",
    "\n",
    "\n",
    "# plt.plot(acc_test_arr_uncoded,'r',label='uncoded')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "# plt.title(title_name)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Test accuracy(%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. K=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "class my_argument:    \n",
    "    epochs    = 16    #\"rounds of training\"\n",
    "    num_users = 4  # \"number of users: K\"\n",
    "    frac      = 0.5 #\"the fraction of clients: C\"\n",
    "    local_ep  = 5 #\"the number of local epochs: E\"\n",
    "    local_bs  = 50  #\"local batch size: B\"\n",
    "    bs        = 50 #\"test batch size\"\n",
    "    lr        = 0.0001 #\"learning rate\"\n",
    "    momentum  = 0.5 # \"SGD momentum (default: 0.5)\"\n",
    "    weight_decay = 5e-4\n",
    "    split     = 'user' # \"train-test split type, user or sample\"\n",
    "    opt='ADAM'\n",
    "    loss='Custom' # 'Custom' or 'Default'\n",
    "\n",
    "    # model arguments\n",
    "    model = 'cnn'\n",
    "    kernel_num=9 #, help='number of each kind of kernel')\n",
    "    kernel_sizes='3,4,5' #  help='comma-separated kernel size to use for convolution')\n",
    "    norm='batch_norm' #, help=\"batch_norm, layer_norm, or None\")\n",
    "    num_filters=32 #, help=\"number of filters for conv nets\")\n",
    "    max_pool='True' #help=\"Whether use max pooling rather than strided convolutions\")\n",
    "\n",
    "    # other arguments\n",
    "    dataset='cifar' #, help=\"name of dataset\")\n",
    "    iid=1\n",
    "    num_classes=10#, help=\"number of classes\")\n",
    "    num_channels=1#, help=\"number of channels of imges\")\n",
    "    gpu=1#, help=\"GPU ID, -1 for CPU\")\n",
    "    stopping_rounds=10#, help='rounds of early stopping')\n",
    "    verbose='False'#, help='verbose print')\n",
    "    seed=1#, help='random seed (default: 1)')\n",
    "    \n",
    "args = my_argument()\n",
    "\n",
    "args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)\n",
    "args.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(args.device)\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Normalize the test set same as training set without augmentation\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# trainset = torchvision.datasets.CIFAR10(\n",
    "#     root=\"./data/cifar\", train=True, download=True, transform=transform_train)\n",
    "# trainloader = torch.utils.data.DataLoader(\n",
    "#     trainset, batch_size=args.bs, shuffle=True, num_workers=2)\n",
    "\n",
    "# testset = torchvision.datasets.CIFAR10(\n",
    "#     root=\"./data/cifar\", train=False, download=True, transform=transform_test)\n",
    "# testloader = torch.utils.data.DataLoader(\n",
    "#     testset, batch_size=args.bs, shuffle=False, num_workers=2)\n",
    "\n",
    "dataset_train = datasets.CIFAR10('../data/cifar', train=True, download=True, transform=transform_train)\n",
    "dataset_test = datasets.CIFAR10('../data/cifar', train=False, download=True, transform=transform_test)\n",
    "if args.iid:\n",
    "    dict_users = cifar_iid(dataset_train, args.num_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of X: (50000, 3072)\n",
      "size of Y: (50000, 10)\n",
      "(50000, 10)\n"
     ]
    }
   ],
   "source": [
    "encoding_input_array_np = np.empty((len(dataset_train),32*32*3))\n",
    "encoding_label_array_np = np.empty((len(dataset_train),args.num_classes))\n",
    "print(\"size of X:\" ,encoding_input_array_np.shape)\n",
    "print(\"size of Y:\" ,encoding_label_array_np.shape)\n",
    "\n",
    "Size_submatrices = int(50000/args.num_users)\n",
    "\n",
    "\n",
    "for i in range(args.num_users):\n",
    "    \n",
    "    stt_pos = i*Size_submatrices\n",
    "    end_pos = (i+1)*Size_submatrices\n",
    "#     print(i,stt_pos,end_pos)\n",
    "    Temp_train = DataLoader(DatasetSplit(dataset_train, dict_users[i]), batch_size=Size_submatrices, shuffle=True)\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(Temp_train):\n",
    "        \n",
    "        images_np = images.detach().cpu().numpy()\n",
    "        \n",
    "#         print(np.size(images_np))\n",
    "        encoding_input_array_np[stt_pos:end_pos,:] = np.reshape(images_np, (Size_submatrices,32*32*3))\n",
    "#         print(encoding_input_array_np[stt_pos:end_pos,:].shape)\n",
    "\n",
    "        onehot_labels = torch.nn.functional.one_hot(labels,num_classes=args.num_classes)\n",
    "        labels_np = onehot_labels.detach().cpu().numpy()\n",
    "#         print(labels_np.shape)\n",
    "        encoding_label_array_np[stt_pos:end_pos,:] = labels_np\n",
    "    \n",
    "print(np.shape(encoding_label_array_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate = 0.0003\n",
      "\n",
      "\n",
      "\n",
      "z_array: [-0.94  -0.125  0.125  0.94 ]\n",
      "2.21641645756389\n",
      "2.3227453960929183\n",
      "2.3227453960929143\n",
      "2.2164164575638967\n",
      "@BACC_Enc: N,K,T, m_i= 4 4 5 12500 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 4 5 12500 \n",
      "\n",
      "(T, sigma)= 5 0.1 )  0 -th Trial!!\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.004408774905734592\n",
      "conv1.bias 0.0021447010027865567\n",
      "conv2.weight 0.0022142225503921508\n",
      "conv2.bias 0.0022520122583955526\n",
      "fc1.weight 0.0008317233721415202\n",
      "fc1.bias 0.001049309844772021\n",
      "fc2.weight 0.0027757147001841714\n",
      "fc2.bias 0.002541865089109966\n",
      "fc3.weight 0.003999469393775577\n",
      "fc3.bias 0.00372687429189682\n",
      "\n",
      "Test set: Average loss: 1.8705 \n",
      "Accuracy: 3354/10000 (33.54%)\n",
      "\n",
      "Round   0, Average loss 1.871 Test accuracy 33.540\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0016817679670121934\n",
      "conv1.bias 0.0021965603033701577\n",
      "conv2.weight 0.001045664350191752\n",
      "conv2.bias 0.000929871283005923\n",
      "fc1.weight 0.00014873554309209188\n",
      "fc1.bias 0.0006081864237785339\n",
      "fc2.weight 0.00029480980029181826\n",
      "fc2.bias 0.0009684354244243531\n",
      "fc3.weight 0.0009136009783971877\n",
      "fc3.bias 0.0016935722902417182\n",
      "\n",
      "Test set: Average loss: 1.7573 \n",
      "Accuracy: 3661/10000 (36.61%)\n",
      "\n",
      "Round   1, Average loss 1.757 Test accuracy 36.610\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0013733373747931586\n",
      "conv1.bias 0.0023555861165126166\n",
      "conv2.weight 0.001210329532623291\n",
      "conv2.bias 0.001318157184869051\n",
      "fc1.weight 0.000249091903368632\n",
      "fc1.bias 0.0013817587246497472\n",
      "fc2.weight 0.0004949566390779283\n",
      "fc2.bias 0.002484101624715896\n",
      "fc3.weight 0.0008855042713029044\n",
      "fc3.bias 0.002107338607311249\n",
      "\n",
      "Test set: Average loss: 1.7492 \n",
      "Accuracy: 3704/10000 (37.04%)\n",
      "\n",
      "Round   2, Average loss 1.749 Test accuracy 37.040\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0011315520604451498\n",
      "conv1.bias 0.002544678747653961\n",
      "conv2.weight 0.0011958414316177368\n",
      "conv2.bias 0.0016099287895485759\n",
      "fc1.weight 0.00028485220670700074\n",
      "fc1.bias 0.0018755681812763214\n",
      "fc2.weight 0.0006643619802263048\n",
      "fc2.bias 0.0032468352999006\n",
      "fc3.weight 0.0010439591038794744\n",
      "fc3.bias 0.002248436212539673\n",
      "\n",
      "Test set: Average loss: 1.7483 \n",
      "Accuracy: 3697/10000 (36.97%)\n",
      "\n",
      "Round   3, Average loss 1.748 Test accuracy 36.970\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0010366666979259914\n",
      "conv1.bias 0.0026698761309186616\n",
      "conv2.weight 0.00119363933801651\n",
      "conv2.bias 0.0017045099521055818\n",
      "fc1.weight 0.0003020707567532857\n",
      "fc1.bias 0.0019197339812914531\n",
      "fc2.weight 0.0007550990770733546\n",
      "fc2.bias 0.0031178841988245645\n",
      "fc3.weight 0.0011752710455939883\n",
      "fc3.bias 0.0019876282662153245\n",
      "\n",
      "Test set: Average loss: 1.7443 \n",
      "Accuracy: 3707/10000 (37.07%)\n",
      "\n",
      "Round   4, Average loss 1.744 Test accuracy 37.070\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0010107033782535129\n",
      "conv1.bias 0.0028688032180070877\n",
      "conv2.weight 0.0012026442090670268\n",
      "conv2.bias 0.001756768673658371\n",
      "fc1.weight 0.0003129654328028361\n",
      "fc1.bias 0.0017787297566731772\n",
      "fc2.weight 0.0007982846290346176\n",
      "fc2.bias 0.002753706382853644\n",
      "fc3.weight 0.0012543402966998872\n",
      "fc3.bias 0.0016989849507808685\n",
      "\n",
      "Test set: Average loss: 1.7356 \n",
      "Accuracy: 3743/10000 (37.43%)\n",
      "\n",
      "Round   5, Average loss 1.736 Test accuracy 37.430\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0010119211673736573\n",
      "conv1.bias 0.0031031897912422815\n",
      "conv2.weight 0.0012187419335047405\n",
      "conv2.bias 0.0018099151784554124\n",
      "fc1.weight 0.00032167341311772663\n",
      "fc1.bias 0.0016180795927842459\n",
      "fc2.weight 0.0008250508043501112\n",
      "fc2.bias 0.002442401966878346\n",
      "fc3.weight 0.0013058950503667195\n",
      "fc3.bias 0.001481483317911625\n",
      "\n",
      "Test set: Average loss: 1.7293 \n",
      "Accuracy: 3749/10000 (37.49%)\n",
      "\n",
      "Round   6, Average loss 1.729 Test accuracy 37.490\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0010248195462756687\n",
      "conv1.bias 0.003317980095744133\n",
      "conv2.weight 0.0012349232037862142\n",
      "conv2.bias 0.0018408821197226644\n",
      "fc1.weight 0.0003276030619939168\n",
      "fc1.bias 0.0015011824667453765\n",
      "fc2.weight 0.0008441551337166438\n",
      "fc2.bias 0.002201849151225317\n",
      "fc3.weight 0.0013449450333913168\n",
      "fc3.bias 0.0013276947662234305\n",
      "\n",
      "Test set: Average loss: 1.7255 \n",
      "Accuracy: 3757/10000 (37.57%)\n",
      "\n",
      "Round   7, Average loss 1.726 Test accuracy 37.570\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0010375830862257216\n",
      "conv1.bias 0.003507183864712715\n",
      "conv2.weight 0.0012484694520632426\n",
      "conv2.bias 0.0018611450213938951\n",
      "fc1.weight 0.00033233338594436645\n",
      "fc1.bias 0.0014277016123135885\n",
      "fc2.weight 0.0008575460267445398\n",
      "fc2.bias 0.0020303126601945785\n",
      "fc3.weight 0.0013740745328721547\n",
      "fc3.bias 0.001220694836229086\n",
      "\n",
      "Test set: Average loss: 1.7208 \n",
      "Accuracy: 3769/10000 (37.69%)\n",
      "\n",
      "Round   8, Average loss 1.721 Test accuracy 37.690\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0010493672556347318\n",
      "conv1.bias 0.0036473736787835755\n",
      "conv2.weight 0.001257726550102234\n",
      "conv2.bias 0.0018780296668410301\n",
      "fc1.weight 0.00033616173267364503\n",
      "fc1.bias 0.001381707936525345\n",
      "fc2.weight 0.0008686300307985337\n",
      "fc2.bias 0.001901085532846905\n",
      "fc3.weight 0.0013945429098038446\n",
      "fc3.bias 0.001143098995089531\n",
      "\n",
      "Test set: Average loss: 1.7187 \n",
      "Accuracy: 3779/10000 (37.79%)\n",
      "\n",
      "Round   9, Average loss 1.719 Test accuracy 37.790\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0010642092757754856\n",
      "conv1.bias 0.003771283974250158\n",
      "conv2.weight 0.0012665379047393799\n",
      "conv2.bias 0.0018811150221154094\n",
      "fc1.weight 0.0003397828737894694\n",
      "fc1.bias 0.0013532772660255431\n",
      "fc2.weight 0.0008785156976609003\n",
      "fc2.bias 0.0018018074333667755\n",
      "fc3.weight 0.001410131085486639\n",
      "fc3.bias 0.0010767222382128238\n",
      "\n",
      "Test set: Average loss: 1.7151 \n",
      "Accuracy: 3795/10000 (37.95%)\n",
      "\n",
      "Round  10, Average loss 1.715 Test accuracy 37.950\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00107668227619595\n",
      "conv1.bias 0.0038681688408056893\n",
      "conv2.weight 0.0012747615575790406\n",
      "conv2.bias 0.001886480487883091\n",
      "fc1.weight 0.00034332831700642903\n",
      "fc1.bias 0.001338606576124827\n",
      "fc2.weight 0.0008878652065519302\n",
      "fc2.bias 0.0017338160957608903\n",
      "fc3.weight 0.0014222697133109683\n",
      "fc3.bias 0.0010330308228731155\n",
      "\n",
      "Test set: Average loss: 1.7157 \n",
      "Accuracy: 3794/10000 (37.94%)\n",
      "\n",
      "Round  11, Average loss 1.716 Test accuracy 37.940\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0010874321063359578\n",
      "conv1.bias 0.003937559202313423\n",
      "conv2.weight 0.0012822014093399048\n",
      "conv2.bias 0.0018941037124022841\n",
      "fc1.weight 0.0003459943930308024\n",
      "fc1.bias 0.001331214358409246\n",
      "fc2.weight 0.0008965514008961027\n",
      "fc2.bias 0.0016844395015920912\n",
      "fc3.weight 0.0014311430000123523\n",
      "fc3.bias 0.000998767465353012\n",
      "\n",
      "Test set: Average loss: 1.7123 \n",
      "Accuracy: 3813/10000 (38.13%)\n",
      "\n",
      "Round  12, Average loss 1.712 Test accuracy 38.130\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0010961768362257216\n",
      "conv1.bias 0.003999592425922553\n",
      "conv2.weight 0.001287944515546163\n",
      "conv2.bias 0.0019020638428628445\n",
      "fc1.weight 0.0003479173183441162\n",
      "fc1.bias 0.0013260133564472198\n",
      "fc2.weight 0.0009023820597027976\n",
      "fc2.bias 0.0016415112075351534\n",
      "fc3.weight 0.0014364445493334815\n",
      "fc3.bias 0.0009695158340036869\n",
      "\n",
      "Test set: Average loss: 1.7099 \n",
      "Accuracy: 3824/10000 (38.24%)\n",
      "\n",
      "Round  13, Average loss 1.710 Test accuracy 38.240\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0011060023307800293\n",
      "conv1.bias 0.004048899747431278\n",
      "conv2.weight 0.0012941190600395203\n",
      "conv2.bias 0.0019100618083029985\n",
      "fc1.weight 0.0003494797150293986\n",
      "fc1.bias 0.001321999728679657\n",
      "fc2.weight 0.0009080018315996443\n",
      "fc2.bias 0.0016179425375802176\n",
      "fc3.weight 0.0014406788916814894\n",
      "fc3.bias 0.0009504841640591621\n",
      "\n",
      "Test set: Average loss: 1.7069 \n",
      "Accuracy: 3834/10000 (38.34%)\n",
      "\n",
      "Round  14, Average loss 1.707 Test accuracy 38.340\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.001114260090721978\n",
      "conv1.bias 0.00409125629812479\n",
      "conv2.weight 0.0013004420200983682\n",
      "conv2.bias 0.001923989038914442\n",
      "fc1.weight 0.00035093339284261065\n",
      "fc1.bias 0.001318051666021347\n",
      "fc2.weight 0.0009132183733440581\n",
      "fc2.bias 0.0015956821540991466\n",
      "fc3.weight 0.0014441876184372675\n",
      "fc3.bias 0.0009319528006017208\n",
      "\n",
      "Test set: Average loss: 1.7075 \n",
      "Accuracy: 3845/10000 (38.45%)\n",
      "\n",
      "Round  15, Average loss 1.708 Test accuracy 38.450\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0011206294430626763\n",
      "conv1.bias 0.004102378152310848\n",
      "conv2.weight 0.0013058788577715555\n",
      "conv2.bias 0.0019410387612879276\n",
      "fc1.weight 0.000351895809173584\n",
      "fc1.bias 0.0013138690342505773\n",
      "fc2.weight 0.0009159612277197459\n",
      "fc2.bias 0.0015771901678471338\n",
      "fc3.weight 0.0014463665939512707\n",
      "fc3.bias 0.0009154783561825752\n",
      "\n",
      "Test set: Average loss: 1.7068 \n",
      "Accuracy: 3843/10000 (38.43%)\n",
      "\n",
      "Round  16, Average loss 1.707 Test accuracy 38.430\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0011260573069254557\n",
      "conv1.bias 0.004135072852174441\n",
      "conv2.weight 0.0013115323583285014\n",
      "conv2.bias 0.001959820743650198\n",
      "fc1.weight 0.000352916955947876\n",
      "fc1.bias 0.0013088966409365336\n",
      "fc2.weight 0.0009200612703959148\n",
      "fc2.bias 0.001561762321562994\n",
      "fc3.weight 0.001448342346009754\n",
      "fc3.bias 0.0008993668481707573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.7057 \n",
      "Accuracy: 3847/10000 (38.47%)\n",
      "\n",
      "Round  17, Average loss 1.706 Test accuracy 38.470\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0011297432581583659\n",
      "conv1.bias 0.004149909441669782\n",
      "conv2.weight 0.0013155607382456462\n",
      "conv2.bias 0.0019702562130987644\n",
      "fc1.weight 0.0003535968065261841\n",
      "fc1.bias 0.0013037143896023433\n",
      "fc2.weight 0.0009229827494848343\n",
      "fc2.bias 0.0015515151123205821\n",
      "fc3.weight 0.0014504151684897287\n",
      "fc3.bias 0.0008864600211381912\n",
      "\n",
      "Test set: Average loss: 1.7075 \n",
      "Accuracy: 3842/10000 (38.42%)\n",
      "\n",
      "Round  18, Average loss 1.708 Test accuracy 38.420\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0011328507794274225\n",
      "conv1.bias 0.004153447225689888\n",
      "conv2.weight 0.001319304903348287\n",
      "conv2.bias 0.001985051902011037\n",
      "fc1.weight 0.00035400442282358804\n",
      "fc1.bias 0.0012982214490572612\n",
      "fc2.weight 0.0009255249348897783\n",
      "fc2.bias 0.001542101658525921\n",
      "fc3.weight 0.0014532635609308879\n",
      "fc3.bias 0.0008730735629796982\n",
      "\n",
      "Test set: Average loss: 1.7076 \n",
      "Accuracy: 3844/10000 (38.44%)\n",
      "\n",
      "Round  19, Average loss 1.708 Test accuracy 38.440\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0011354198720720078\n",
      "conv1.bias 0.004145962186157703\n",
      "conv2.weight 0.0013208835323651631\n",
      "conv2.bias 0.0020001130178570747\n",
      "fc1.weight 0.0003540338675181071\n",
      "fc1.bias 0.001292644813656807\n",
      "fc2.weight 0.0009274494080316453\n",
      "fc2.bias 0.0015388917000520798\n",
      "fc3.weight 0.0014558433067230951\n",
      "fc3.bias 0.000863895658403635\n",
      "\n",
      "Test set: Average loss: 1.7054 \n",
      "Accuracy: 3847/10000 (38.47%)\n",
      "\n",
      "Round  20, Average loss 1.705 Test accuracy 38.470\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0011380943987104627\n",
      "conv1.bias 0.004150594274202983\n",
      "conv2.weight 0.0013216077287991842\n",
      "conv2.bias 0.0020111207850277424\n",
      "fc1.weight 0.00035384730497996014\n",
      "fc1.bias 0.0012874020884434381\n",
      "fc2.weight 0.0009278916177295504\n",
      "fc2.bias 0.001531516867024558\n",
      "fc3.weight 0.0014552627290998186\n",
      "fc3.bias 0.0008551307022571564\n",
      "\n",
      "Test set: Average loss: 1.7084 \n",
      "Accuracy: 3839/10000 (38.39%)\n",
      "\n",
      "Round  21, Average loss 1.708 Test accuracy 38.390\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.001141705248090956\n",
      "conv1.bias 0.004141266147295634\n",
      "conv2.weight 0.0013223246733347575\n",
      "conv2.bias 0.0020271609537303448\n",
      "fc1.weight 0.00035366590817769366\n",
      "fc1.bias 0.0012833443780740103\n",
      "fc2.weight 0.0009285575813717312\n",
      "fc2.bias 0.0015278797419298264\n",
      "fc3.weight 0.0014572901385171073\n",
      "fc3.bias 0.0008499428629875183\n",
      "\n",
      "Test set: Average loss: 1.7062 \n",
      "Accuracy: 3846/10000 (38.46%)\n",
      "\n",
      "Round  22, Average loss 1.706 Test accuracy 38.460\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0011426885922749838\n",
      "conv1.bias 0.004138496083517869\n",
      "conv2.weight 0.0013213644425074259\n",
      "conv2.bias 0.002038016449660063\n",
      "fc1.weight 0.00035310534636179606\n",
      "fc1.bias 0.001279685894648234\n",
      "fc2.weight 0.0009280938950795976\n",
      "fc2.bias 0.0015256211516403017\n",
      "fc3.weight 0.001457289287022182\n",
      "fc3.bias 0.0008469979278743267\n",
      "\n",
      "Test set: Average loss: 1.7071 \n",
      "Accuracy: 3839/10000 (38.39%)\n",
      "\n",
      "Round  23, Average loss 1.707 Test accuracy 38.390\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0011448491944207086\n",
      "conv1.bias 0.004137562587857246\n",
      "conv2.weight 0.0013215101758639017\n",
      "conv2.bias 0.0020489180460572243\n",
      "fc1.weight 0.00035304641723632815\n",
      "fc1.bias 0.0012771092355251313\n",
      "fc2.weight 0.0009283436669243706\n",
      "fc2.bias 0.001525426194781349\n",
      "fc3.weight 0.0014577114865893409\n",
      "fc3.bias 0.0008431278169155121\n",
      "\n",
      "Test set: Average loss: 1.7059 \n",
      "Accuracy: 3847/10000 (38.47%)\n",
      "\n",
      "Round  24, Average loss 1.706 Test accuracy 38.470\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0011461877822875977\n",
      "conv1.bias 0.004135455936193466\n",
      "conv2.weight 0.001321945587793986\n",
      "conv2.bias 0.0020570552442222834\n",
      "fc1.weight 0.00035277636845906573\n",
      "fc1.bias 0.0012765886882940929\n",
      "fc2.weight 0.0009286500158764067\n",
      "fc2.bias 0.0015286200103305635\n",
      "fc3.weight 0.0014578189168657576\n",
      "fc3.bias 0.0008421426638960839\n",
      "\n",
      "Test set: Average loss: 1.7044 \n",
      "Accuracy: 3849/10000 (38.49%)\n",
      "\n",
      "Round  25, Average loss 1.704 Test accuracy 38.490\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0011469539006551106\n",
      "conv1.bias 0.004118188905219237\n",
      "conv2.weight 0.0013222735126813252\n",
      "conv2.bias 0.002067363588139415\n",
      "fc1.weight 0.0003528134822845459\n",
      "fc1.bias 0.0012764657537142436\n",
      "fc2.weight 0.0009293246836889358\n",
      "fc2.bias 0.0015279494580768404\n",
      "fc3.weight 0.0014600031432651338\n",
      "fc3.bias 0.0008386957459151744\n",
      "\n",
      "Test set: Average loss: 1.7049 \n",
      "Accuracy: 3850/10000 (38.50%)\n",
      "\n",
      "Round  26, Average loss 1.705 Test accuracy 38.500\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0011481707625918918\n",
      "conv1.bias 0.004119750112295151\n",
      "conv2.weight 0.0013225056727727254\n",
      "conv2.bias 0.002081046812236309\n",
      "fc1.weight 0.00035268660386403403\n",
      "fc1.bias 0.0012765770157178243\n",
      "fc2.weight 0.0009297050180889311\n",
      "fc2.bias 0.0015267559460231236\n",
      "fc3.weight 0.0014611970810663132\n",
      "fc3.bias 0.0008367684669792653\n",
      "\n",
      "Test set: Average loss: 1.7028 \n",
      "Accuracy: 3864/10000 (38.64%)\n",
      "\n",
      "Round  27, Average loss 1.703 Test accuracy 38.640\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0011495941215091282\n",
      "conv1.bias 0.004113472377260526\n",
      "conv2.weight 0.0013233109315236408\n",
      "conv2.bias 0.002087143948301673\n",
      "fc1.weight 0.0003524128595987956\n",
      "fc1.bias 0.0012770781914393107\n",
      "fc2.weight 0.0009291491811237638\n",
      "fc2.bias 0.001531179639555159\n",
      "fc3.weight 0.001461272012619745\n",
      "fc3.bias 0.0008379091508686542\n",
      "\n",
      "Test set: Average loss: 1.7036 \n",
      "Accuracy: 3855/10000 (38.55%)\n",
      "\n",
      "Round  28, Average loss 1.704 Test accuracy 38.550\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0011512056986490886\n",
      "conv1.bias 0.0040967560683687525\n",
      "conv2.weight 0.0013239428400993348\n",
      "conv2.bias 0.0020959083922207355\n",
      "fc1.weight 0.00035219724973042807\n",
      "fc1.bias 0.0012759743879238764\n",
      "fc2.weight 0.0009287083905840677\n",
      "fc2.bias 0.0015328852903275262\n",
      "fc3.weight 0.001461439331372579\n",
      "fc3.bias 0.000838504359126091\n",
      "\n",
      "Test set: Average loss: 1.7018 \n",
      "Accuracy: 3865/10000 (38.65%)\n",
      "\n",
      "Round  29, Average loss 1.702 Test accuracy 38.650\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0011508671442667644\n",
      "conv1.bias 0.004104725395639737\n",
      "conv2.weight 0.001323032279809316\n",
      "conv2.bias 0.002099844394251704\n",
      "fc1.weight 0.00035219581921895344\n",
      "fc1.bias 0.0012736608584721883\n",
      "fc2.weight 0.0009281971151866611\n",
      "fc2.bias 0.0015349038654849643\n",
      "fc3.weight 0.0014628827571868896\n",
      "fc3.bias 0.0008360577747225761\n",
      "\n",
      "Test set: Average loss: 1.7033 \n",
      "Accuracy: 3852/10000 (38.52%)\n",
      "\n",
      "Round  30, Average loss 1.703 Test accuracy 38.520\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0011522199047936334\n",
      "conv1.bias 0.0040953438729047775\n",
      "conv2.weight 0.0013225229581197103\n",
      "conv2.bias 0.002103327075019479\n",
      "fc1.weight 0.00035199562708536787\n",
      "fc1.bias 0.0012737215807040532\n",
      "fc2.weight 0.0009280482927958171\n",
      "fc2.bias 0.0015365536369028546\n",
      "fc3.weight 0.0014637197766985213\n",
      "fc3.bias 0.000836576335132122\n",
      "\n",
      "Test set: Average loss: 1.7042 \n",
      "Accuracy: 3851/10000 (38.51%)\n",
      "\n",
      "Round  31, Average loss 1.704 Test accuracy 38.510\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0011515769693586561\n",
      "conv1.bias 0.004094247085352738\n",
      "conv2.weight 0.0013225717345873515\n",
      "conv2.bias 0.0021090921945869923\n",
      "fc1.weight 0.00035204740365346273\n",
      "fc1.bias 0.0012738662461439767\n",
      "fc2.weight 0.0009281452686067612\n",
      "fc2.bias 0.0015369974786327\n",
      "fc3.weight 0.0014643958636692592\n",
      "fc3.bias 0.0008329281583428383\n",
      "\n",
      "Test set: Average loss: 1.7023 \n",
      "Accuracy: 3863/10000 (38.63%)\n",
      "\n",
      "Round  32, Average loss 1.702 Test accuracy 38.630\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.001151935789320204\n",
      "conv1.bias 0.004096775936583678\n",
      "conv2.weight 0.0013215744495391846\n",
      "conv2.bias 0.0021173167042434216\n",
      "fc1.weight 0.0003520881732304891\n",
      "fc1.bias 0.0012744167198737463\n",
      "fc2.weight 0.0009279105398390028\n",
      "fc2.bias 0.0015419725151289078\n",
      "fc3.weight 0.0014647929441361201\n",
      "fc3.bias 0.0008321467787027359\n",
      "\n",
      "Test set: Average loss: 1.7009 \n",
      "Accuracy: 3872/10000 (38.72%)\n",
      "\n",
      "Round  33, Average loss 1.701 Test accuracy 38.720\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.001151553922229343\n",
      "conv1.bias 0.004076581448316574\n",
      "conv2.weight 0.0013202733794848123\n",
      "conv2.bias 0.0021225621458142996\n",
      "fc1.weight 0.0003518044948577881\n",
      "fc1.bias 0.001272957275311152\n",
      "fc2.weight 0.00092749037439861\n",
      "fc2.bias 0.0015412680804729462\n",
      "fc3.weight 0.0014645697105498542\n",
      "fc3.bias 0.0008294910192489624\n",
      "\n",
      "Test set: Average loss: 1.7019 \n",
      "Accuracy: 3863/10000 (38.63%)\n",
      "\n",
      "Round  34, Average loss 1.702 Test accuracy 38.630\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.001152080562379625\n",
      "conv1.bias 0.004083119022349517\n",
      "conv2.weight 0.0013196893533070882\n",
      "conv2.bias 0.0021264981478452682\n",
      "fc1.weight 0.00035179940859476726\n",
      "fc1.bias 0.0012720284362634023\n",
      "fc2.weight 0.0009268599843221997\n",
      "fc2.bias 0.0015431374666236696\n",
      "fc3.weight 0.0014640308561779203\n",
      "fc3.bias 0.0008274493739008904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.7017 \n",
      "Accuracy: 3865/10000 (38.65%)\n",
      "\n",
      "Round  35, Average loss 1.702 Test accuracy 38.650\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0011526423030429416\n",
      "conv1.bias 0.004082895504931609\n",
      "conv2.weight 0.0013172326485315958\n",
      "conv2.bias 0.002127000829204917\n",
      "fc1.weight 0.0003516292969385783\n",
      "fc1.bias 0.001267355183760325\n",
      "fc2.weight 0.0009272112732841855\n",
      "fc2.bias 0.0015429412680012839\n",
      "fc3.weight 0.0014641466594877697\n",
      "fc3.bias 0.000826747715473175\n",
      "\n",
      "Test set: Average loss: 1.7019 \n",
      "Accuracy: 3860/10000 (38.60%)\n",
      "\n",
      "Round  36, Average loss 1.702 Test accuracy 38.600\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0011528632375929092\n",
      "conv1.bias 0.004077227475742499\n",
      "conv2.weight 0.0013163634141286215\n",
      "conv2.bias 0.0021307305432856083\n",
      "fc1.weight 0.00035136282444000246\n",
      "fc1.bias 0.0012667407592137656\n",
      "fc2.weight 0.0009270713442847842\n",
      "fc2.bias 0.0015449536343415577\n",
      "fc3.weight 0.001463214982123602\n",
      "fc3.bias 0.0008275261148810387\n",
      "\n",
      "Test set: Average loss: 1.7021 \n",
      "Accuracy: 3859/10000 (38.59%)\n",
      "\n",
      "Round  37, Average loss 1.702 Test accuracy 38.590\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0011538611518012152\n",
      "conv1.bias 0.004070035181939602\n",
      "conv2.weight 0.0013170152902603149\n",
      "conv2.bias 0.0021333510521799326\n",
      "fc1.weight 0.0003514070908228556\n",
      "fc1.bias 0.0012690431127945583\n",
      "fc2.weight 0.000927201717618912\n",
      "fc2.bias 0.0015518058623586381\n",
      "fc3.weight 0.0014648858989988055\n",
      "fc3.bias 0.0008296151645481586\n",
      "\n",
      "Test set: Average loss: 1.7030 \n",
      "Accuracy: 3852/10000 (38.52%)\n",
      "\n",
      "Round  38, Average loss 1.703 Test accuracy 38.520\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0011541948053571914\n",
      "conv1.bias 0.004074385700126489\n",
      "conv2.weight 0.0013159066438674927\n",
      "conv2.bias 0.0021452633664011955\n",
      "fc1.weight 0.0003513986666997274\n",
      "fc1.bias 0.0012708989282449086\n",
      "fc2.weight 0.0009274870630294557\n",
      "fc2.bias 0.0015550387047585986\n",
      "fc3.weight 0.001465698650905064\n",
      "fc3.bias 0.0008269541896879673\n",
      "\n",
      "Test set: Average loss: 1.7016 \n",
      "Accuracy: 3856/10000 (38.56%)\n",
      "\n",
      "Round  39, Average loss 1.702 Test accuracy 38.560\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0011554821332295736\n",
      "conv1.bias 0.004074735256532828\n",
      "conv2.weight 0.0013150612513224283\n",
      "conv2.bias 0.0021493390668183565\n",
      "fc1.weight 0.0003513766129811605\n",
      "fc1.bias 0.0012672313799460728\n",
      "fc2.weight 0.000927634560872638\n",
      "fc2.bias 0.0015571170619555882\n",
      "fc3.weight 0.0014668874797366914\n",
      "fc3.bias 0.0008268171921372413\n",
      "\n",
      "Test set: Average loss: 1.7026 \n",
      "Accuracy: 3858/10000 (38.58%)\n",
      "\n",
      "Round  40, Average loss 1.703 Test accuracy 38.580\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0011565041542053222\n",
      "conv1.bias 0.004070712874333064\n",
      "conv2.weight 0.0013136600454648336\n",
      "conv2.bias 0.002159250434488058\n",
      "fc1.weight 0.000351186990737915\n",
      "fc1.bias 0.0012654618670543034\n",
      "fc2.weight 0.0009277264277140299\n",
      "fc2.bias 0.0015581040864899045\n",
      "fc3.weight 0.0014677771500178746\n",
      "fc3.bias 0.000826420821249485\n",
      "\n",
      "Test set: Average loss: 1.7030 \n",
      "Accuracy: 3858/10000 (38.58%)\n",
      "\n",
      "Round  41, Average loss 1.703 Test accuracy 38.580\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0011567065450880263\n",
      "conv1.bias 0.004067549171547095\n",
      "conv2.weight 0.001312694549560547\n",
      "conv2.bias 0.0021708975546061993\n",
      "fc1.weight 0.0003511358102162679\n",
      "fc1.bias 0.0012651499360799789\n",
      "fc2.weight 0.000927821227482387\n",
      "fc2.bias 0.0015631472425801413\n",
      "fc3.weight 0.0014693969771975564\n",
      "fc3.bias 0.0008265464566648006\n",
      "\n",
      "Test set: Average loss: 1.7015 \n",
      "Accuracy: 3864/10000 (38.64%)\n",
      "\n",
      "Round  42, Average loss 1.701 Test accuracy 38.640\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0011574316024780273\n",
      "conv1.bias 0.004055364367862542\n",
      "conv2.weight 0.0013118714094161987\n",
      "conv2.bias 0.00217812554910779\n",
      "fc1.weight 0.0003510056734085083\n",
      "fc1.bias 0.001264129082361857\n",
      "fc2.weight 0.0009273810992165217\n",
      "fc2.bias 0.0015672468358562106\n",
      "fc3.weight 0.0014694619746435256\n",
      "fc3.bias 0.0008271586149930954\n",
      "\n",
      "Test set: Average loss: 1.7009 \n",
      "Accuracy: 3868/10000 (38.68%)\n",
      "\n",
      "Round  43, Average loss 1.701 Test accuracy 38.680\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0011589272816975912\n",
      "conv1.bias 0.0040472885593771935\n",
      "conv2.weight 0.0013117643197377523\n",
      "conv2.bias 0.002190222032368183\n",
      "fc1.weight 0.00035100634892781574\n",
      "fc1.bias 0.0012627472480138144\n",
      "fc2.weight 0.000927077777801998\n",
      "fc2.bias 0.0015668578091121855\n",
      "fc3.weight 0.0014698877221062069\n",
      "fc3.bias 0.0008256347849965095\n",
      "\n",
      "Test set: Average loss: 1.7010 \n",
      "Accuracy: 3861/10000 (38.61%)\n",
      "\n",
      "Round  44, Average loss 1.701 Test accuracy 38.610\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.001161192258199056\n",
      "conv1.bias 0.004062904665867488\n",
      "conv2.weight 0.0013104414939880372\n",
      "conv2.bias 0.0021942639723420143\n",
      "fc1.weight 0.0003512377738952637\n",
      "fc1.bias 0.0012607254087924958\n",
      "fc2.weight 0.0009279206631675599\n",
      "fc2.bias 0.0015676071246465046\n",
      "fc3.weight 0.0014714530536106656\n",
      "fc3.bias 0.0008229442872107029\n",
      "\n",
      "Test set: Average loss: 1.7019 \n",
      "Accuracy: 3857/10000 (38.57%)\n",
      "\n",
      "Round  45, Average loss 1.702 Test accuracy 38.570\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.001161606576707628\n",
      "conv1.bias 0.004048474133014679\n",
      "conv2.weight 0.0013094495733579\n",
      "conv2.bias 0.002204255200922489\n",
      "fc1.weight 0.00035114375750223796\n",
      "fc1.bias 0.0012599257131417593\n",
      "fc2.weight 0.0009276375884101504\n",
      "fc2.bias 0.0015702650305770692\n",
      "fc3.weight 0.0014719568547748383\n",
      "fc3.bias 0.0008236495777964592\n",
      "\n",
      "Test set: Average loss: 1.7010 \n",
      "Accuracy: 3861/10000 (38.61%)\n",
      "\n",
      "Round  46, Average loss 1.701 Test accuracy 38.610\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.001161879301071167\n",
      "conv1.bias 0.004045750324924787\n",
      "conv2.weight 0.0013089126348495483\n",
      "conv2.bias 0.002205610042437911\n",
      "fc1.weight 0.0003511807918548584\n",
      "fc1.bias 0.0012586695452531179\n",
      "fc2.weight 0.0009277733545454722\n",
      "fc2.bias 0.0015736763321218036\n",
      "fc3.weight 0.0014725416898727418\n",
      "fc3.bias 0.0008228926919400692\n",
      "\n",
      "Test set: Average loss: 1.7004 \n",
      "Accuracy: 3861/10000 (38.61%)\n",
      "\n",
      "Round  47, Average loss 1.700 Test accuracy 38.610\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0011599447992112902\n",
      "conv1.bias 0.0040360356991489725\n",
      "conv2.weight 0.0013069361448287964\n",
      "conv2.bias 0.0022101998329162598\n",
      "fc1.weight 0.0003511454661687215\n",
      "fc1.bias 0.0012558539708455404\n",
      "fc2.weight 0.0009278844273279584\n",
      "fc2.bias 0.0015732193631785257\n",
      "fc3.weight 0.001473150366828555\n",
      "fc3.bias 0.000822770781815052\n",
      "\n",
      "Test set: Average loss: 1.7004 \n",
      "Accuracy: 3859/10000 (38.59%)\n",
      "\n",
      "Round  48, Average loss 1.700 Test accuracy 38.590\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0011613278918796116\n",
      "conv1.bias 0.004028348873058955\n",
      "conv2.weight 0.0013073172171910604\n",
      "conv2.bias 0.0022088508121669292\n",
      "fc1.weight 0.00035145147641499835\n",
      "fc1.bias 0.0012516789138317108\n",
      "fc2.weight 0.0009285374293251643\n",
      "fc2.bias 0.0015705176407382602\n",
      "fc3.weight 0.0014741911774589901\n",
      "fc3.bias 0.0008213471621274949\n",
      "\n",
      "Test set: Average loss: 1.7008 \n",
      "Accuracy: 3861/10000 (38.61%)\n",
      "\n",
      "Round  49, Average loss 1.701 Test accuracy 38.610\n",
      "Learning Rate = 0.0001\n",
      "\n",
      "\n",
      "\n",
      "z_array: [-0.94  -0.125  0.125  0.94 ]\n",
      "2.21641645756389\n",
      "2.3227453960929183\n",
      "2.3227453960929143\n",
      "2.2164164575638967\n",
      "@BACC_Enc: N,K,T, m_i= 4 4 5 12500 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 4 5 12500 \n",
      "\n",
      "(T, sigma)= 5 0.1 )  0 -th Trial!!\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0042375233438279895\n",
      "conv1.bias 0.002538634774585565\n",
      "conv2.weight 0.0022188925743103025\n",
      "conv2.bias 0.0024748770520091057\n",
      "fc1.weight 0.0008296687602996826\n",
      "fc1.bias 0.0008710669974486033\n",
      "fc2.weight 0.0027785452585371716\n",
      "fc2.bias 0.002689564689284279\n",
      "fc3.weight 0.0041409191631135486\n",
      "fc3.bias 0.004995550960302353\n",
      "\n",
      "Test set: Average loss: 2.0076 \n",
      "Accuracy: 2770/10000 (27.70%)\n",
      "\n",
      "Round   0, Average loss 2.008 Test accuracy 27.700\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0011375351746877034\n",
      "conv1.bias 0.0016906363889575005\n",
      "conv2.weight 0.0006144323945045471\n",
      "conv2.bias 0.0011576643446460366\n",
      "fc1.weight 0.00010497680306434631\n",
      "fc1.bias 0.0002455501196285089\n",
      "fc2.weight 0.00022086136870914035\n",
      "fc2.bias 0.00043065016645760766\n",
      "fc3.weight 0.0007385362471852984\n",
      "fc3.bias 0.0007207822985947132\n",
      "\n",
      "Test set: Average loss: 1.9333 \n",
      "Accuracy: 2881/10000 (28.81%)\n",
      "\n",
      "Round   1, Average loss 1.933 Test accuracy 28.810\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0007915884918636745\n",
      "conv1.bias 0.001503677728275458\n",
      "conv2.weight 0.0006272300084431967\n",
      "conv2.bias 0.0007954948232509196\n",
      "fc1.weight 0.00015091806650161744\n",
      "fc1.bias 0.00024752641717592876\n",
      "fc2.weight 0.000237442103643266\n",
      "fc2.bias 0.00029554772412493114\n",
      "fc3.weight 0.00039923687775929767\n",
      "fc3.bias 0.00013444565702229737\n",
      "\n",
      "Test set: Average loss: 1.9115 \n",
      "Accuracy: 2988/10000 (29.88%)\n",
      "\n",
      "Round   2, Average loss 1.911 Test accuracy 29.880\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00074016273021698\n",
      "conv1.bias 0.0011998328069845836\n",
      "conv2.weight 0.0006294478972752889\n",
      "conv2.bias 0.0006107382941991091\n",
      "fc1.weight 0.00016882131497065225\n",
      "fc1.bias 0.00025362179925044376\n",
      "fc2.weight 0.00030918140260000076\n",
      "fc2.bias 0.0003375174876834665\n",
      "fc3.weight 0.00034097830454508465\n",
      "fc3.bias 8.76220059581101e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.8990 \n",
      "Accuracy: 3068/10000 (30.68%)\n",
      "\n",
      "Round   3, Average loss 1.899 Test accuracy 30.680\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0007166657182905409\n",
      "conv1.bias 0.0010365045939882596\n",
      "conv2.weight 0.0006200151642163595\n",
      "conv2.bias 0.0005718945758417249\n",
      "fc1.weight 0.00017848682403564454\n",
      "fc1.bias 0.00028984158610304196\n",
      "fc2.weight 0.00036740731152277143\n",
      "fc2.bias 0.0004028716967219398\n",
      "fc3.weight 0.00034995050657363164\n",
      "fc3.bias 0.00013132793828845023\n",
      "\n",
      "Test set: Average loss: 1.8957 \n",
      "Accuracy: 3054/10000 (30.54%)\n",
      "\n",
      "Round   4, Average loss 1.896 Test accuracy 30.540\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0006638177235921224\n",
      "conv1.bias 0.001014965819194913\n",
      "conv2.weight 0.0006196259458859762\n",
      "conv2.bias 0.0005901010008528829\n",
      "fc1.weight 0.00018434953689575194\n",
      "fc1.bias 0.0003316703562935193\n",
      "fc2.weight 0.00040758677891322546\n",
      "fc2.bias 0.00045321260889371234\n",
      "fc3.weight 0.0003725159026327587\n",
      "fc3.bias 0.0001826892839744687\n",
      "\n",
      "Test set: Average loss: 1.8951 \n",
      "Accuracy: 3037/10000 (30.37%)\n",
      "\n",
      "Round   5, Average loss 1.895 Test accuracy 30.370\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0006027654806772868\n",
      "conv1.bias 0.0010278965346515179\n",
      "conv2.weight 0.0006185075143973033\n",
      "conv2.bias 0.0006016693077981472\n",
      "fc1.weight 0.0001891689697901408\n",
      "fc1.bias 0.0003554526095589002\n",
      "fc2.weight 0.000437758035129971\n",
      "fc2.bias 0.0004783905599088896\n",
      "fc3.weight 0.0003952963011605399\n",
      "fc3.bias 0.0002203830750659108\n",
      "\n",
      "Test set: Average loss: 1.8935 \n",
      "Accuracy: 3020/10000 (30.20%)\n",
      "\n",
      "Round   6, Average loss 1.894 Test accuracy 30.200\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0005591789881388347\n",
      "conv1.bias 0.0010343291020641725\n",
      "conv2.weight 0.0006119861205418905\n",
      "conv2.bias 0.0006028404459357262\n",
      "fc1.weight 0.00019254098335901897\n",
      "fc1.bias 0.0003650244325399399\n",
      "fc2.weight 0.0004588217016250368\n",
      "fc2.bias 0.0004875172106992631\n",
      "fc3.weight 0.00041387443031583516\n",
      "fc3.bias 0.00024588217493146656\n",
      "\n",
      "Test set: Average loss: 1.8925 \n",
      "Accuracy: 3012/10000 (30.12%)\n",
      "\n",
      "Round   7, Average loss 1.893 Test accuracy 30.120\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0005318364169862535\n",
      "conv1.bias 0.0010263241517047088\n",
      "conv2.weight 0.0006041333576043447\n",
      "conv2.bias 0.0006032119854353368\n",
      "fc1.weight 0.00019474194447199504\n",
      "fc1.bias 0.0003684358671307564\n",
      "fc2.weight 0.00047225658855740984\n",
      "fc2.bias 0.0004923383572271892\n",
      "fc3.weight 0.0004277360581216358\n",
      "fc3.bias 0.00026413844898343086\n",
      "\n",
      "Test set: Average loss: 1.8916 \n",
      "Accuracy: 3015/10000 (30.15%)\n",
      "\n",
      "Round   8, Average loss 1.892 Test accuracy 30.150\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0005148533980051677\n",
      "conv1.bias 0.0010162523637215297\n",
      "conv2.weight 0.0005980515976746877\n",
      "conv2.bias 0.000602985208388418\n",
      "fc1.weight 0.00019618717829386392\n",
      "fc1.bias 0.0003688184544444084\n",
      "fc2.weight 0.000481072777793521\n",
      "fc2.bias 0.0004947146932993617\n",
      "fc3.weight 0.0004372850770042056\n",
      "fc3.bias 0.00027653032448142766\n",
      "\n",
      "Test set: Average loss: 1.8907 \n",
      "Accuracy: 3018/10000 (30.18%)\n",
      "\n",
      "Round   9, Average loss 1.891 Test accuracy 30.180\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0005043325490421719\n",
      "conv1.bias 0.0010062698274850845\n",
      "conv2.weight 0.0005942793687184651\n",
      "conv2.bias 0.0006046445341780782\n",
      "fc1.weight 0.00019728465874989826\n",
      "fc1.bias 0.0003676535872121652\n",
      "fc2.weight 0.0004873772935261802\n",
      "fc2.bias 0.0004950629635935737\n",
      "fc3.weight 0.00044367920075144085\n",
      "fc3.bias 0.00028568068519234656\n",
      "\n",
      "Test set: Average loss: 1.8901 \n",
      "Accuracy: 3015/10000 (30.15%)\n",
      "\n",
      "Round  10, Average loss 1.890 Test accuracy 30.150\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004971979061762492\n",
      "conv1.bias 0.0009973014239221811\n",
      "conv2.weight 0.0005921958883603413\n",
      "conv2.bias 0.0006050176452845335\n",
      "fc1.weight 0.0001980515718460083\n",
      "fc1.bias 0.0003658525956173738\n",
      "fc2.weight 0.0004916548255890135\n",
      "fc2.bias 0.0004952600935385341\n",
      "fc3.weight 0.00044795906259900047\n",
      "fc3.bias 0.0002921502338722348\n",
      "\n",
      "Test set: Average loss: 1.8890 \n",
      "Accuracy: 3015/10000 (30.15%)\n",
      "\n",
      "Round  11, Average loss 1.889 Test accuracy 30.150\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004921666781107585\n",
      "conv1.bias 0.0009882788484295209\n",
      "conv2.weight 0.0005906118949254354\n",
      "conv2.bias 0.0006050416850484908\n",
      "fc1.weight 0.0001985491911570231\n",
      "fc1.bias 0.00036417674273252486\n",
      "fc2.weight 0.0004945874214172363\n",
      "fc2.bias 0.0004943086366568293\n",
      "fc3.weight 0.00045091857512791954\n",
      "fc3.bias 0.0002969370922073722\n",
      "\n",
      "Test set: Average loss: 1.8881 \n",
      "Accuracy: 3016/10000 (30.16%)\n",
      "\n",
      "Round  12, Average loss 1.888 Test accuracy 30.160\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004882790644963582\n",
      "conv1.bias 0.0009804867052783568\n",
      "conv2.weight 0.0005900185306866964\n",
      "conv2.bias 0.0006049017538316548\n",
      "fc1.weight 0.0001989408532778422\n",
      "fc1.bias 0.0003623015557726224\n",
      "fc2.weight 0.000497029840000092\n",
      "fc2.bias 0.0004931101575493813\n",
      "fc3.weight 0.00045292036873953684\n",
      "fc3.bias 0.0003006085054948926\n",
      "\n",
      "Test set: Average loss: 1.8876 \n",
      "Accuracy: 3016/10000 (30.16%)\n",
      "\n",
      "Round  13, Average loss 1.888 Test accuracy 30.160\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00048497683472103543\n",
      "conv1.bias 0.0009731272390733162\n",
      "conv2.weight 0.000590146283308665\n",
      "conv2.bias 0.0006035741535015404\n",
      "fc1.weight 0.00019925681749979654\n",
      "fc1.bias 0.00036120132232705753\n",
      "fc2.weight 0.0004988856258846464\n",
      "fc2.bias 0.0004911974427245912\n",
      "fc3.weight 0.00045417100191116335\n",
      "fc3.bias 0.0003034015418961644\n",
      "\n",
      "Test set: Average loss: 1.8870 \n",
      "Accuracy: 3017/10000 (30.17%)\n",
      "\n",
      "Round  14, Average loss 1.887 Test accuracy 30.170\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004821291897031996\n",
      "conv1.bias 0.0009689259653290113\n",
      "conv2.weight 0.0005908979972203572\n",
      "conv2.bias 0.0006011347286403179\n",
      "fc1.weight 0.00019944381713867188\n",
      "fc1.bias 0.0003600056593616804\n",
      "fc2.weight 0.000500163199409606\n",
      "fc2.bias 0.0004895833720053945\n",
      "fc3.weight 0.0004549494811466762\n",
      "fc3.bias 0.00030536220874637364\n",
      "\n",
      "Test set: Average loss: 1.8874 \n",
      "Accuracy: 3016/10000 (30.16%)\n",
      "\n",
      "Round  15, Average loss 1.887 Test accuracy 30.160\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004798305696911282\n",
      "conv1.bias 0.0009633720231552919\n",
      "conv2.weight 0.0005917675793170929\n",
      "conv2.bias 0.0005993485683575273\n",
      "fc1.weight 0.00019951337575912476\n",
      "fc1.bias 0.0003597014894088109\n",
      "fc2.weight 0.0005011313491397434\n",
      "fc2.bias 0.0004888548116598811\n",
      "fc3.weight 0.0004553376209168207\n",
      "fc3.bias 0.0003072896506637335\n",
      "\n",
      "Test set: Average loss: 1.8871 \n",
      "Accuracy: 3019/10000 (30.19%)\n",
      "\n",
      "Round  16, Average loss 1.887 Test accuracy 30.190\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.000477702882554796\n",
      "conv1.bias 0.0009584062887976567\n",
      "conv2.weight 0.0005934786796569824\n",
      "conv2.bias 0.0005987288895994425\n",
      "fc1.weight 0.0001996156374613444\n",
      "fc1.bias 0.00035994288822015127\n",
      "fc2.weight 0.0005020491660587371\n",
      "fc2.bias 0.0004889453273443948\n",
      "fc3.weight 0.00045555737756547475\n",
      "fc3.bias 0.00030876335222274065\n",
      "\n",
      "Test set: Average loss: 1.8867 \n",
      "Accuracy: 3025/10000 (30.25%)\n",
      "\n",
      "Round  17, Average loss 1.887 Test accuracy 30.250\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00047570109367370603\n",
      "conv1.bias 0.0009539471939206123\n",
      "conv2.weight 0.0005952975153923034\n",
      "conv2.bias 0.0005978296976536512\n",
      "fc1.weight 0.00019957304000854493\n",
      "fc1.bias 0.0003600943833589554\n",
      "fc2.weight 0.0005024148358239068\n",
      "fc2.bias 0.0004899989636171432\n",
      "fc3.weight 0.00045563610536711557\n",
      "fc3.bias 0.00031048357486724855\n",
      "\n",
      "Test set: Average loss: 1.8867 \n",
      "Accuracy: 3024/10000 (30.24%)\n",
      "\n",
      "Round  18, Average loss 1.887 Test accuracy 30.240\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00047383556763331094\n",
      "conv1.bias 0.0009481987605492274\n",
      "conv2.weight 0.0005975455542405446\n",
      "conv2.bias 0.00059683428844437\n",
      "fc1.weight 0.00019954917828241984\n",
      "fc1.bias 0.0003605332225561142\n",
      "fc2.weight 0.0005026570388248988\n",
      "fc2.bias 0.000490490435844376\n",
      "fc3.weight 0.0004555842706135341\n",
      "fc3.bias 0.0003124012378975749\n",
      "\n",
      "Test set: Average loss: 1.8859 \n",
      "Accuracy: 3033/10000 (30.33%)\n",
      "\n",
      "Round  19, Average loss 1.886 Test accuracy 30.330\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004720013340314229\n",
      "conv1.bias 0.0009432943382610878\n",
      "conv2.weight 0.0005995755394299825\n",
      "conv2.bias 0.0005964061710983515\n",
      "fc1.weight 0.0001994461218516032\n",
      "fc1.bias 0.00036055957898497584\n",
      "fc2.weight 0.0005025108655293783\n",
      "fc2.bias 0.0004917748627208528\n",
      "fc3.weight 0.0004552140831947327\n",
      "fc3.bias 0.00031417291611433027\n",
      "\n",
      "Test set: Average loss: 1.8856 \n",
      "Accuracy: 3034/10000 (30.34%)\n",
      "\n",
      "Round  20, Average loss 1.886 Test accuracy 30.340\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004705876774258084\n",
      "conv1.bias 0.0009383182041347027\n",
      "conv2.weight 0.0006017428636550903\n",
      "conv2.bias 0.00059550441801548\n",
      "fc1.weight 0.00019937880833943684\n",
      "fc1.bias 0.0003604409284889698\n",
      "fc2.weight 0.0005025191912575374\n",
      "fc2.bias 0.000492686050988379\n",
      "fc3.weight 0.00045475502099309647\n",
      "fc3.bias 0.000315948692150414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.8852 \n",
      "Accuracy: 3033/10000 (30.33%)\n",
      "\n",
      "Round  21, Average loss 1.885 Test accuracy 30.330\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004693902532259623\n",
      "conv1.bias 0.0009330962784588337\n",
      "conv2.weight 0.0006042206287384033\n",
      "conv2.bias 0.0005942056886851788\n",
      "fc1.weight 0.00019933895270029704\n",
      "fc1.bias 0.000360421525935332\n",
      "fc2.weight 0.0005026133287520636\n",
      "fc2.bias 0.0004941824646223159\n",
      "fc3.weight 0.0004544671092714582\n",
      "fc3.bias 0.00031767126638442277\n",
      "\n",
      "Test set: Average loss: 1.8849 \n",
      "Accuracy: 3038/10000 (30.38%)\n",
      "\n",
      "Round  22, Average loss 1.885 Test accuracy 30.380\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00046814673476749\n",
      "conv1.bias 0.0009320816025137901\n",
      "conv2.weight 0.0006066239873568217\n",
      "conv2.bias 0.000592504336964339\n",
      "fc1.weight 0.00019925816853841145\n",
      "fc1.bias 0.0003604581579566002\n",
      "fc2.weight 0.0005024419417456975\n",
      "fc2.bias 0.0004955436590881575\n",
      "fc3.weight 0.00045412775306474595\n",
      "fc3.bias 0.00031893253326416017\n",
      "\n",
      "Test set: Average loss: 1.8845 \n",
      "Accuracy: 3040/10000 (30.40%)\n",
      "\n",
      "Round  23, Average loss 1.885 Test accuracy 30.400\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004670851098166572\n",
      "conv1.bias 0.0009300829066584507\n",
      "conv2.weight 0.0006090094149112701\n",
      "conv2.bias 0.0005902471020817757\n",
      "fc1.weight 0.0001991692582766215\n",
      "fc1.bias 0.0003605249027411143\n",
      "fc2.weight 0.0005024210801200261\n",
      "fc2.bias 0.0004970596748448553\n",
      "fc3.weight 0.0004538885539486295\n",
      "fc3.bias 0.0003206579713150859\n",
      "\n",
      "Test set: Average loss: 1.8843 \n",
      "Accuracy: 3040/10000 (30.40%)\n",
      "\n",
      "Round  24, Average loss 1.884 Test accuracy 30.400\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004661458730697632\n",
      "conv1.bias 0.0009277125354856253\n",
      "conv2.weight 0.0006115554769833883\n",
      "conv2.bias 0.0005874863127246499\n",
      "fc1.weight 0.00019910113016764324\n",
      "fc1.bias 0.0003603830312689145\n",
      "fc2.weight 0.0005022881996063959\n",
      "fc2.bias 0.0004985832298795382\n",
      "fc3.weight 0.0004534663189025152\n",
      "fc3.bias 0.0003224525135010481\n",
      "\n",
      "Test set: Average loss: 1.8842 \n",
      "Accuracy: 3043/10000 (30.43%)\n",
      "\n",
      "Round  25, Average loss 1.884 Test accuracy 30.430\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004651404089397854\n",
      "conv1.bias 0.0009264016213516394\n",
      "conv2.weight 0.0006140418847401937\n",
      "conv2.bias 0.0005840249359607697\n",
      "fc1.weight 0.0001989187002182007\n",
      "fc1.bias 0.0003597630187869072\n",
      "fc2.weight 0.0005020889024885874\n",
      "fc2.bias 0.0005006201210476103\n",
      "fc3.weight 0.0004533288024720692\n",
      "fc3.bias 0.0003242942038923502\n",
      "\n",
      "Test set: Average loss: 1.8842 \n",
      "Accuracy: 3040/10000 (30.40%)\n",
      "\n",
      "Round  26, Average loss 1.884 Test accuracy 30.400\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004641123281584846\n",
      "conv1.bias 0.0009230865786472956\n",
      "conv2.weight 0.0006166409949461619\n",
      "conv2.bias 0.0005819558864459395\n",
      "fc1.weight 0.00019878387451171875\n",
      "fc1.bias 0.0003591957812507947\n",
      "fc2.weight 0.0005020583905870952\n",
      "fc2.bias 0.0005021505944785618\n",
      "fc3.weight 0.00045333156983057656\n",
      "fc3.bias 0.00032629519701004027\n",
      "\n",
      "Test set: Average loss: 1.8838 \n",
      "Accuracy: 3041/10000 (30.41%)\n",
      "\n",
      "Round  27, Average loss 1.884 Test accuracy 30.410\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00046311504311031767\n",
      "conv1.bias 0.0009212691026429335\n",
      "conv2.weight 0.0006192460159460704\n",
      "conv2.bias 0.0005806303233839571\n",
      "fc1.weight 0.00019870338837305705\n",
      "fc1.bias 0.0003594322130084038\n",
      "fc2.weight 0.0005022490308398292\n",
      "fc2.bias 0.0005038860475733166\n",
      "fc3.weight 0.00045338109845206853\n",
      "fc3.bias 0.0003278544405475259\n",
      "\n",
      "Test set: Average loss: 1.8841 \n",
      "Accuracy: 3042/10000 (30.42%)\n",
      "\n",
      "Round  28, Average loss 1.884 Test accuracy 30.420\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00046185804737938773\n",
      "conv1.bias 0.0009212139993906021\n",
      "conv2.weight 0.0006215819716453553\n",
      "conv2.bias 0.0005790700088255107\n",
      "fc1.weight 0.00019854255517323812\n",
      "fc1.bias 0.0003597074188292027\n",
      "fc2.weight 0.0005021339371090843\n",
      "fc2.bias 0.0005057738827807563\n",
      "fc3.weight 0.00045350066253117154\n",
      "fc3.bias 0.0003296796465292573\n",
      "\n",
      "Test set: Average loss: 1.8841 \n",
      "Accuracy: 3037/10000 (30.37%)\n",
      "\n",
      "Round  29, Average loss 1.884 Test accuracy 30.370\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00046075340774324205\n",
      "conv1.bias 0.0009212850903471311\n",
      "conv2.weight 0.0006238550941149393\n",
      "conv2.bias 0.0005781071376986802\n",
      "fc1.weight 0.00019843260447184244\n",
      "fc1.bias 0.00035994419207175575\n",
      "fc2.weight 0.0005021933525327652\n",
      "fc2.bias 0.0005071132133404413\n",
      "fc3.weight 0.00045362703856967747\n",
      "fc3.bias 0.00033156122080981734\n",
      "\n",
      "Test set: Average loss: 1.8840 \n",
      "Accuracy: 3038/10000 (30.38%)\n",
      "\n",
      "Round  30, Average loss 1.884 Test accuracy 30.380\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00045968638526068794\n",
      "conv1.bias 0.0009195798387130102\n",
      "conv2.weight 0.0006259538729985556\n",
      "conv2.bias 0.000576939492020756\n",
      "fc1.weight 0.00019831538200378417\n",
      "fc1.bias 0.00036051940793792406\n",
      "fc2.weight 0.0005021384311100793\n",
      "fc2.bias 0.000508674198672885\n",
      "fc3.weight 0.00045372350584892997\n",
      "fc3.bias 0.0003334786975756288\n",
      "\n",
      "Test set: Average loss: 1.8839 \n",
      "Accuracy: 3041/10000 (30.41%)\n",
      "\n",
      "Round  31, Average loss 1.884 Test accuracy 30.410\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004586683710416158\n",
      "conv1.bias 0.000919027409205834\n",
      "conv2.weight 0.0006280291577180227\n",
      "conv2.bias 0.0005759592168033123\n",
      "fc1.weight 0.00019822996854782104\n",
      "fc1.bias 0.000361267663538456\n",
      "fc2.weight 0.0005019944811624194\n",
      "fc2.bias 0.0005100804957605543\n",
      "fc3.weight 0.0004537079305875869\n",
      "fc3.bias 0.00033515426330268385\n",
      "\n",
      "Test set: Average loss: 1.8843 \n",
      "Accuracy: 3041/10000 (30.41%)\n",
      "\n",
      "Round  32, Average loss 1.884 Test accuracy 30.410\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00045772496196958755\n",
      "conv1.bias 0.000914944801479578\n",
      "conv2.weight 0.000629880428314209\n",
      "conv2.bias 0.0005759114865213633\n",
      "fc1.weight 0.00019812329610188802\n",
      "fc1.bias 0.00036242309336860973\n",
      "fc2.weight 0.0005018039355202327\n",
      "fc2.bias 0.0005118175897569884\n",
      "fc3.weight 0.00045368742375146776\n",
      "fc3.bias 0.00033631082624197006\n",
      "\n",
      "Test set: Average loss: 1.8846 \n",
      "Accuracy: 3038/10000 (30.38%)\n",
      "\n",
      "Round  33, Average loss 1.885 Test accuracy 30.380\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00045696457227071126\n",
      "conv1.bias 0.0009150894669195017\n",
      "conv2.weight 0.0006315099199612935\n",
      "conv2.bias 0.0005753188161179423\n",
      "fc1.weight 0.00019797245661417644\n",
      "fc1.bias 0.00036349333822727203\n",
      "fc2.weight 0.0005017342548521739\n",
      "fc2.bias 0.0005126829658235822\n",
      "fc3.weight 0.00045343598439579917\n",
      "fc3.bias 0.00033745088148862123\n",
      "\n",
      "Test set: Average loss: 1.8850 \n",
      "Accuracy: 3038/10000 (30.38%)\n",
      "\n",
      "Round  34, Average loss 1.885 Test accuracy 30.380\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004562250441975064\n",
      "conv1.bias 0.0009114937856793404\n",
      "conv2.weight 0.0006326938668886821\n",
      "conv2.bias 0.0005758091574534774\n",
      "fc1.weight 0.00019782036542892456\n",
      "fc1.bias 0.00036470405757427216\n",
      "fc2.weight 0.0005015854797666035\n",
      "fc2.bias 0.0005135159229948407\n",
      "fc3.weight 0.0004532188886687869\n",
      "fc3.bias 0.0003389855148270726\n",
      "\n",
      "Test set: Average loss: 1.8851 \n",
      "Accuracy: 3034/10000 (30.34%)\n",
      "\n",
      "Round  35, Average loss 1.885 Test accuracy 30.340\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00045521510971917046\n",
      "conv1.bias 0.0009123776108026505\n",
      "conv2.weight 0.0006335745751857758\n",
      "conv2.bias 0.0005750376731157303\n",
      "fc1.weight 0.0001976884603500366\n",
      "fc1.bias 0.0003654844438036283\n",
      "fc2.weight 0.0005014276693737696\n",
      "fc2.bias 0.0005139912079487528\n",
      "fc3.weight 0.0004530684578986395\n",
      "fc3.bias 0.0003401327645406127\n",
      "\n",
      "Test set: Average loss: 1.8851 \n",
      "Accuracy: 3036/10000 (30.36%)\n",
      "\n",
      "Round  36, Average loss 1.885 Test accuracy 30.360\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004543096820513407\n",
      "conv1.bias 0.0009086420759558678\n",
      "conv2.weight 0.0006344689925511678\n",
      "conv2.bias 0.0005754637531936169\n",
      "fc1.weight 0.00019760270913441976\n",
      "fc1.bias 0.0003666988263527552\n",
      "fc2.weight 0.0005013986239357601\n",
      "fc2.bias 0.0005141929945065861\n",
      "fc3.weight 0.0004530411036241622\n",
      "fc3.bias 0.00034105600789189337\n",
      "\n",
      "Test set: Average loss: 1.8851 \n",
      "Accuracy: 3038/10000 (30.38%)\n",
      "\n",
      "Round  37, Average loss 1.885 Test accuracy 30.380\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004534626007080078\n",
      "conv1.bias 0.0009100716561079025\n",
      "conv2.weight 0.0006350674728552501\n",
      "conv2.bias 0.0005757695180363953\n",
      "fc1.weight 0.00019753770033518474\n",
      "fc1.bias 0.0003682427729169528\n",
      "fc2.weight 0.0005012910044382489\n",
      "fc2.bias 0.0005145629513121786\n",
      "fc3.weight 0.00045300147363117765\n",
      "fc3.bias 0.0003416858147829771\n",
      "\n",
      "Test set: Average loss: 1.8854 \n",
      "Accuracy: 3040/10000 (30.40%)\n",
      "\n",
      "Round  38, Average loss 1.885 Test accuracy 30.400\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00045283195045259266\n",
      "conv1.bias 0.0009085147175937891\n",
      "conv2.weight 0.000635434091091156\n",
      "conv2.bias 0.000575876678340137\n",
      "fc1.weight 0.00019752593835194906\n",
      "fc1.bias 0.0003691729158163071\n",
      "fc2.weight 0.0005011210365900917\n",
      "fc2.bias 0.0005148423037358693\n",
      "fc3.weight 0.0004528926596755073\n",
      "fc3.bias 0.00034237473737448455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.8856 \n",
      "Accuracy: 3037/10000 (30.37%)\n",
      "\n",
      "Round  39, Average loss 1.886 Test accuracy 30.370\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00045220461156633164\n",
      "conv1.bias 0.0009071256499737501\n",
      "conv2.weight 0.0006356530388196309\n",
      "conv2.bias 0.0005758445477113128\n",
      "fc1.weight 0.00019747430086135864\n",
      "fc1.bias 0.0003702091984450817\n",
      "fc2.weight 0.0005009447298352681\n",
      "fc2.bias 0.0005147172847674007\n",
      "fc3.weight 0.0004530283312002818\n",
      "fc3.bias 0.00034312300849705937\n",
      "\n",
      "Test set: Average loss: 1.8858 \n",
      "Accuracy: 3035/10000 (30.35%)\n",
      "\n",
      "Round  40, Average loss 1.886 Test accuracy 30.350\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004516772760285272\n",
      "conv1.bias 0.0009058659585813681\n",
      "conv2.weight 0.000636013150215149\n",
      "conv2.bias 0.0005766746471635997\n",
      "fc1.weight 0.00019751209020614624\n",
      "fc1.bias 0.00037142746150493623\n",
      "fc2.weight 0.0005007069262247237\n",
      "fc2.bias 0.0005142957947793461\n",
      "fc3.weight 0.00045297209705625263\n",
      "fc3.bias 0.0003433974925428629\n",
      "\n",
      "Test set: Average loss: 1.8859 \n",
      "Accuracy: 3038/10000 (30.38%)\n",
      "\n",
      "Round  41, Average loss 1.886 Test accuracy 30.380\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004512419634395176\n",
      "conv1.bias 0.0009061147769292196\n",
      "conv2.weight 0.0006363321344057719\n",
      "conv2.bias 0.0005762958899140358\n",
      "fc1.weight 0.00019749383131663004\n",
      "fc1.bias 0.00037241817141572635\n",
      "fc2.weight 0.0005005166171089051\n",
      "fc2.bias 0.0005142347709763618\n",
      "fc3.weight 0.0004529201558658055\n",
      "fc3.bias 0.00034391249064356086\n",
      "\n",
      "Test set: Average loss: 1.8859 \n",
      "Accuracy: 3036/10000 (30.36%)\n",
      "\n",
      "Round  42, Average loss 1.886 Test accuracy 30.360\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004509074489275614\n",
      "conv1.bias 0.0009055384434759617\n",
      "conv2.weight 0.0006365917126337687\n",
      "conv2.bias 0.0005765961832366884\n",
      "fc1.weight 0.00019756484031677247\n",
      "fc1.bias 0.000373205728828907\n",
      "fc2.weight 0.0005004278251102992\n",
      "fc2.bias 0.0005141295758741242\n",
      "fc3.weight 0.00045295535098938713\n",
      "fc3.bias 0.000344494916498661\n",
      "\n",
      "Test set: Average loss: 1.8859 \n",
      "Accuracy: 3037/10000 (30.37%)\n",
      "\n",
      "Round  43, Average loss 1.886 Test accuracy 30.370\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00045054203934139674\n",
      "conv1.bias 0.0009065840082863966\n",
      "conv2.weight 0.0006367178757985432\n",
      "conv2.bias 0.0005767106777057052\n",
      "fc1.weight 0.00019756992657979328\n",
      "fc1.bias 0.00037453987946112953\n",
      "fc2.weight 0.00050034485166035\n",
      "fc2.bias 0.0005141637243685268\n",
      "fc3.weight 0.0004529640078544617\n",
      "fc3.bias 0.00034471629187464713\n",
      "\n",
      "Test set: Average loss: 1.8864 \n",
      "Accuracy: 3036/10000 (30.36%)\n",
      "\n",
      "Round  44, Average loss 1.886 Test accuracy 30.360\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004501331514782376\n",
      "conv1.bias 0.0009049115081628164\n",
      "conv2.weight 0.0006367534399032593\n",
      "conv2.bias 0.0005769439158029854\n",
      "fc1.weight 0.00019759168227513632\n",
      "fc1.bias 0.00037590029338995613\n",
      "fc2.weight 0.0005002473081861223\n",
      "fc2.bias 0.0005139623812976337\n",
      "fc3.weight 0.0004528516814822242\n",
      "fc3.bias 0.0003450917545706034\n",
      "\n",
      "Test set: Average loss: 1.8867 \n",
      "Accuracy: 3034/10000 (30.34%)\n",
      "\n",
      "Round  45, Average loss 1.887 Test accuracy 30.340\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00044986367225646973\n",
      "conv1.bias 0.0009044358351578315\n",
      "conv2.weight 0.0006368395686149597\n",
      "conv2.bias 0.0005769690033048391\n",
      "fc1.weight 0.00019759313265482585\n",
      "fc1.bias 0.00037736228356758755\n",
      "fc2.weight 0.000499969484314086\n",
      "fc2.bias 0.000513794698885509\n",
      "fc3.weight 0.00045267339973222644\n",
      "fc3.bias 0.0003454799763858318\n",
      "\n",
      "Test set: Average loss: 1.8868 \n",
      "Accuracy: 3034/10000 (30.34%)\n",
      "\n",
      "Round  46, Average loss 1.887 Test accuracy 30.340\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00044973045587539675\n",
      "conv1.bias 0.0009057872618238131\n",
      "conv2.weight 0.0006369106968243917\n",
      "conv2.bias 0.0005761671345680952\n",
      "fc1.weight 0.00019762182235717773\n",
      "fc1.bias 0.00037824691583712895\n",
      "fc2.weight 0.0005001400671308003\n",
      "fc2.bias 0.00051313057719242\n",
      "fc3.weight 0.0004526652750514802\n",
      "fc3.bias 0.00034566845279186963\n",
      "\n",
      "Test set: Average loss: 1.8868 \n",
      "Accuracy: 3035/10000 (30.35%)\n",
      "\n",
      "Round  47, Average loss 1.887 Test accuracy 30.350\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004495253165562948\n",
      "conv1.bias 0.0009042400245865186\n",
      "conv2.weight 0.0006368077794710795\n",
      "conv2.bias 0.0005766992107965052\n",
      "fc1.weight 0.00019751216967900593\n",
      "fc1.bias 0.00037966805199782054\n",
      "fc2.weight 0.0004998390636746845\n",
      "fc2.bias 0.0005125481457937331\n",
      "fc3.weight 0.000452535351117452\n",
      "fc3.bias 0.0003458892228081822\n",
      "\n",
      "Test set: Average loss: 1.8869 \n",
      "Accuracy: 3036/10000 (30.36%)\n",
      "\n",
      "Round  48, Average loss 1.887 Test accuracy 30.360\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0004493464363945855\n",
      "conv1.bias 0.0009034833249946436\n",
      "conv2.weight 0.0006367352604866028\n",
      "conv2.bias 0.0005766720860265195\n",
      "fc1.weight 0.00019748721520105998\n",
      "fc1.bias 0.0003806856150428454\n",
      "fc2.weight 0.0004996898155363779\n",
      "fc2.bias 0.0005120443446295602\n",
      "fc3.weight 0.0004525857312338693\n",
      "fc3.bias 0.00034648445434868334\n",
      "\n",
      "Test set: Average loss: 1.8872 \n",
      "Accuracy: 3035/10000 (30.35%)\n",
      "\n",
      "Round  49, Average loss 1.887 Test accuracy 30.350\n",
      "Learning Rate = 1e-05\n",
      "\n",
      "\n",
      "\n",
      "z_array: [-0.94  -0.125  0.125  0.94 ]\n",
      "2.21641645756389\n",
      "2.3227453960929183\n",
      "2.3227453960929143\n",
      "2.2164164575638967\n",
      "@BACC_Enc: N,K,T, m_i= 4 4 5 12500 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 4 5 12500 \n",
      "\n",
      "(T, sigma)= 5 0.1 )  0 -th Trial!!\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00448243882921007\n",
      "conv1.bias 0.0071154919763406115\n",
      "conv2.weight 0.002247312068939209\n",
      "conv2.bias 0.0025562348309904337\n",
      "fc1.weight 0.0008360095024108887\n",
      "fc1.bias 0.0007005666693051656\n",
      "fc2.weight 0.0027734548326522586\n",
      "fc2.bias 0.0030677680458341327\n",
      "fc3.weight 0.00407467456091018\n",
      "fc3.bias 0.0031637750566005708\n",
      "\n",
      "Test set: Average loss: 2.2399 \n",
      "Accuracy: 1575/10000 (15.75%)\n",
      "\n",
      "Round   0, Average loss 2.240 Test accuracy 15.750\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0009118753671646118\n",
      "conv1.bias 0.0017129221620659034\n",
      "conv2.weight 0.0003882308304309845\n",
      "conv2.bias 0.0004953804891556501\n",
      "fc1.weight 7.094001770019531e-05\n",
      "fc1.bias 0.00012054643593728542\n",
      "fc2.weight 0.0002974935467281039\n",
      "fc2.bias 0.0004223242313379333\n",
      "fc3.weight 0.0006187434707369123\n",
      "fc3.bias 0.0005804839078336954\n",
      "\n",
      "Test set: Average loss: 2.2075 \n",
      "Accuracy: 1737/10000 (17.37%)\n",
      "\n",
      "Round   1, Average loss 2.208 Test accuracy 17.370\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00035208006699879963\n",
      "conv1.bias 0.0005556178900102774\n",
      "conv2.weight 0.00024091285963853202\n",
      "conv2.bias 0.00015004270244389772\n",
      "fc1.weight 5.138707160949707e-05\n",
      "fc1.bias 8.149445056915283e-05\n",
      "fc2.weight 7.242615970354232e-05\n",
      "fc2.bias 0.0001038158911147288\n",
      "fc3.weight 0.00014897693125974563\n",
      "fc3.bias 0.0001598857343196869\n",
      "\n",
      "Test set: Average loss: 2.1956 \n",
      "Accuracy: 1548/10000 (15.48%)\n",
      "\n",
      "Round   2, Average loss 2.196 Test accuracy 15.480\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00021086636516782971\n",
      "conv1.bias 0.0002841055781270067\n",
      "conv2.weight 0.00022347758213678995\n",
      "conv2.bias 6.516010034829378e-05\n",
      "fc1.weight 6.308055420716604e-05\n",
      "fc1.bias 9.496094814191262e-05\n",
      "fc2.weight 6.581072414678241e-05\n",
      "fc2.bias 9.472396535178025e-05\n",
      "fc3.weight 7.601999455974216e-05\n",
      "fc3.bias 7.743611931800842e-05\n",
      "\n",
      "Test set: Average loss: 2.1883 \n",
      "Accuracy: 1561/10000 (15.61%)\n",
      "\n",
      "Round   3, Average loss 2.188 Test accuracy 15.610\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00015742977460225422\n",
      "conv1.bias 0.00020112873365481695\n",
      "conv2.weight 0.00021625113983949026\n",
      "conv2.bias 4.509967038757168e-05\n",
      "fc1.weight 6.977403163909912e-05\n",
      "fc1.bias 0.00010638552096982797\n",
      "fc2.weight 8.778126821631476e-05\n",
      "fc2.bias 0.00010903627567348026\n",
      "fc3.weight 6.516026776461374e-05\n",
      "fc3.bias 6.089498638175428e-05\n",
      "\n",
      "Test set: Average loss: 2.1857 \n",
      "Accuracy: 1518/10000 (15.18%)\n",
      "\n",
      "Round   4, Average loss 2.186 Test accuracy 15.180\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00012712462080849543\n",
      "conv1.bias 0.00017297324181223908\n",
      "conv2.weight 0.000211943785349528\n",
      "conv2.bias 4.008459291071631e-05\n",
      "fc1.weight 7.464876274267832e-05\n",
      "fc1.bias 0.00011513388405243556\n",
      "fc2.weight 0.00010714436334276956\n",
      "fc2.bias 0.00011245073706266426\n",
      "fc3.weight 6.842795493347304e-05\n",
      "fc3.bias 5.8434269158169626e-05\n",
      "\n",
      "Test set: Average loss: 2.1870 \n",
      "Accuracy: 1438/10000 (14.38%)\n",
      "\n",
      "Round   5, Average loss 2.187 Test accuracy 14.380\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.00010392892691824172\n",
      "conv1.bias 0.00016239869485919675\n",
      "conv2.weight 0.00020912759006023408\n",
      "conv2.bias 3.612432919908315e-05\n",
      "fc1.weight 7.872911791006725e-05\n",
      "fc1.bias 0.00012488213057319322\n",
      "fc2.weight 0.00012148688473398724\n",
      "fc2.bias 0.0001201593272742771\n",
      "fc3.weight 7.596556097269058e-05\n",
      "fc3.bias 5.924347788095474e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.1862 \n",
      "Accuracy: 1450/10000 (14.50%)\n",
      "\n",
      "Round   6, Average loss 2.186 Test accuracy 14.500\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 8.37604370382097e-05\n",
      "conv1.bias 0.00014556870640565953\n",
      "conv2.weight 0.00020015568782885869\n",
      "conv2.bias 3.0725823307875544e-05\n",
      "fc1.weight 8.13421110312144e-05\n",
      "fc1.bias 0.00013441710422436396\n",
      "fc2.weight 0.00013337615463468764\n",
      "fc2.bias 0.0001325411500320548\n",
      "fc3.weight 8.650473540737516e-05\n",
      "fc3.bias 6.0153903905302283e-05\n",
      "\n",
      "Test set: Average loss: 2.1860 \n",
      "Accuracy: 1721/10000 (17.21%)\n",
      "\n",
      "Round   7, Average loss 2.186 Test accuracy 17.210\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 7.274679839611053e-05\n",
      "conv1.bias 0.00012754156099011502\n",
      "conv2.weight 0.00019004767139752706\n",
      "conv2.bias 2.9042539608781226e-05\n",
      "fc1.weight 8.242616057395935e-05\n",
      "fc1.bias 0.00013979047847290834\n",
      "fc2.weight 0.00014356722434361776\n",
      "fc2.bias 0.00014518221308078085\n",
      "fc3.weight 9.761220287709009e-05\n",
      "fc3.bias 5.999448476359248e-05\n",
      "\n",
      "Test set: Average loss: 2.1849 \n",
      "Accuracy: 1753/10000 (17.53%)\n",
      "\n",
      "Round   8, Average loss 2.185 Test accuracy 17.530\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 6.762969411081738e-05\n",
      "conv1.bias 0.00011793512385338545\n",
      "conv2.weight 0.00018345175931851069\n",
      "conv2.bias 2.9655731850652955e-05\n",
      "fc1.weight 8.308745920658111e-05\n",
      "fc1.bias 0.00013996378208200138\n",
      "fc2.weight 0.0001528031769252959\n",
      "fc2.bias 0.0001548517694962876\n",
      "fc3.weight 0.00010875577018374489\n",
      "fc3.bias 5.941179697401822e-05\n",
      "\n",
      "Test set: Average loss: 2.1836 \n",
      "Accuracy: 1761/10000 (17.61%)\n",
      "\n",
      "Round   9, Average loss 2.184 Test accuracy 17.610\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 6.50189361638493e-05\n",
      "conv1.bias 0.00011256663128733635\n",
      "conv2.weight 0.00017878336211045584\n",
      "conv2.bias 3.079786620219238e-05\n",
      "fc1.weight 8.365970849990845e-05\n",
      "fc1.bias 0.0001372022864719232\n",
      "fc2.weight 0.00016049128912744068\n",
      "fc2.bias 0.0001610388003644489\n",
      "fc3.weight 0.0001180278846905345\n",
      "fc3.bias 5.882305558770895e-05\n",
      "\n",
      "Test set: Average loss: 2.1822 \n",
      "Accuracy: 1758/10000 (17.58%)\n",
      "\n",
      "Round  10, Average loss 2.182 Test accuracy 17.580\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 6.376633213626014e-05\n",
      "conv1.bias 0.00010909394283468525\n",
      "conv2.weight 0.00017569765448570253\n",
      "conv2.bias 3.197106343577616e-05\n",
      "fc1.weight 8.461649219195048e-05\n",
      "fc1.bias 0.00013441587798297406\n",
      "fc2.weight 0.00016712235316397651\n",
      "fc2.bias 0.00016519487170236452\n",
      "fc3.weight 0.00012537298635357903\n",
      "fc3.bias 5.8367452584207055e-05\n",
      "\n",
      "Test set: Average loss: 2.1807 \n",
      "Accuracy: 1767/10000 (17.67%)\n",
      "\n",
      "Round  11, Average loss 2.181 Test accuracy 17.670\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 6.330194158686532e-05\n",
      "conv1.bias 0.00010611552473468085\n",
      "conv2.weight 0.00017368036011854808\n",
      "conv2.bias 3.271602690801956e-05\n",
      "fc1.weight 8.567595481872559e-05\n",
      "fc1.bias 0.00013216690470774968\n",
      "fc2.weight 0.00017238139869674805\n",
      "fc2.bias 0.0001677260734140873\n",
      "fc3.weight 0.00013073100043194634\n",
      "fc3.bias 5.812857998535037e-05\n",
      "\n",
      "Test set: Average loss: 2.1794 \n",
      "Accuracy: 1767/10000 (17.67%)\n",
      "\n",
      "Round  12, Average loss 2.179 Test accuracy 17.670\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 6.326805386278365e-05\n",
      "conv1.bias 0.000103728350950405\n",
      "conv2.weight 0.0001723298927148183\n",
      "conv2.bias 3.310573083581403e-05\n",
      "fc1.weight 8.659768104553222e-05\n",
      "fc1.bias 0.00013035810552537442\n",
      "fc2.weight 0.00017614362258759755\n",
      "fc2.bias 0.00016918501240156946\n",
      "fc3.weight 0.0001343030837320146\n",
      "fc3.bias 5.80551684834063e-05\n",
      "\n",
      "Test set: Average loss: 2.1784 \n",
      "Accuracy: 1765/10000 (17.65%)\n",
      "\n",
      "Round  13, Average loss 2.178 Test accuracy 17.650\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 6.3438903954294e-05\n",
      "conv1.bias 0.00010167750103088717\n",
      "conv2.weight 0.00017139008889595667\n",
      "conv2.bias 3.325409852550365e-05\n",
      "fc1.weight 8.733378847440084e-05\n",
      "fc1.bias 0.00012988767897089323\n",
      "fc2.weight 0.00017891529770124525\n",
      "fc2.bias 0.00016996726792837893\n",
      "fc3.weight 0.0001367933161201931\n",
      "fc3.bias 5.8129377430304884e-05\n",
      "\n",
      "Test set: Average loss: 2.1777 \n",
      "Accuracy: 1759/10000 (17.59%)\n",
      "\n",
      "Round  14, Average loss 2.178 Test accuracy 17.590\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 6.366281873650022e-05\n",
      "conv1.bias 9.988525804753105e-05\n",
      "conv2.weight 0.00017064372698465983\n",
      "conv2.bias 3.3270542189711705e-05\n",
      "fc1.weight 8.789499600728353e-05\n",
      "fc1.bias 0.00012984803567330042\n",
      "fc2.weight 0.00018089265813903203\n",
      "fc2.bias 0.000170470547995397\n",
      "fc3.weight 0.00013861252615849178\n",
      "fc3.bias 5.827475106343627e-05\n",
      "\n",
      "Test set: Average loss: 2.1771 \n",
      "Accuracy: 1751/10000 (17.51%)\n",
      "\n",
      "Round  15, Average loss 2.177 Test accuracy 17.510\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 6.391325344642003e-05\n",
      "conv1.bias 9.843476194267471e-05\n",
      "conv2.weight 0.0001701728378732999\n",
      "conv2.bias 3.327302692923695e-05\n",
      "fc1.weight 8.835616707801819e-05\n",
      "fc1.bias 0.00013004603485266368\n",
      "fc2.weight 0.00018234022316478547\n",
      "fc2.bias 0.000170801899262837\n",
      "fc3.weight 0.00013987128401086443\n",
      "fc3.bias 5.843299441039562e-05\n",
      "\n",
      "Test set: Average loss: 2.1766 \n",
      "Accuracy: 1754/10000 (17.54%)\n",
      "\n",
      "Round  16, Average loss 2.177 Test accuracy 17.540\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 6.412790053420597e-05\n",
      "conv1.bias 9.724795624303321e-05\n",
      "conv2.weight 0.00016980273028214773\n",
      "conv2.bias 3.3261672797380015e-05\n",
      "fc1.weight 8.870101968447367e-05\n",
      "fc1.bias 0.00013028752679626146\n",
      "fc2.weight 0.00018336038504328047\n",
      "fc2.bias 0.00017107590766889707\n",
      "fc3.weight 0.00014083477712812878\n",
      "fc3.bias 5.858905497007072e-05\n",
      "\n",
      "Test set: Average loss: 2.1763 \n",
      "Accuracy: 1755/10000 (17.55%)\n",
      "\n",
      "Round  17, Average loss 2.176 Test accuracy 17.550\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 6.431016657087538e-05\n",
      "conv1.bias 9.635091798069577e-05\n",
      "conv2.weight 0.00016953798631827037\n",
      "conv2.bias 3.322384873172268e-05\n",
      "fc1.weight 8.897034327189127e-05\n",
      "fc1.bias 0.00013045497859517734\n",
      "fc2.weight 0.0001841221418645647\n",
      "fc2.bias 0.00017131870568153404\n",
      "fc3.weight 0.0001415593106122244\n",
      "fc3.bias 5.871863104403019e-05\n",
      "\n",
      "Test set: Average loss: 2.1760 \n",
      "Accuracy: 1757/10000 (17.57%)\n",
      "\n",
      "Round  18, Average loss 2.176 Test accuracy 17.570\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 6.444028268257777e-05\n",
      "conv1.bias 9.562736765171091e-05\n",
      "conv2.weight 0.00016934265693028768\n",
      "conv2.bias 3.319622192066163e-05\n",
      "fc1.weight 8.917383352915447e-05\n",
      "fc1.bias 0.0001306142968436082\n",
      "fc2.weight 0.00018468215352012996\n",
      "fc2.bias 0.0001714844700126421\n",
      "fc3.weight 0.0001420786426890464\n",
      "fc3.bias 5.8825872838497165e-05\n",
      "\n",
      "Test set: Average loss: 2.1758 \n",
      "Accuracy: 1757/10000 (17.57%)\n",
      "\n",
      "Round  19, Average loss 2.176 Test accuracy 17.570\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 6.455346941947937e-05\n",
      "conv1.bias 9.515893179923296e-05\n",
      "conv2.weight 0.00016923405230045318\n",
      "conv2.bias 3.317645314382389e-05\n",
      "fc1.weight 8.93271267414093e-05\n",
      "fc1.bias 0.00013064085505902766\n",
      "fc2.weight 0.00018510574859286112\n",
      "fc2.bias 0.0001715964504650661\n",
      "fc3.weight 0.00014246213471605663\n",
      "fc3.bias 5.890181637369096e-05\n",
      "\n",
      "Test set: Average loss: 2.1756 \n",
      "Accuracy: 1760/10000 (17.60%)\n",
      "\n",
      "Round  20, Average loss 2.176 Test accuracy 17.600\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 6.463992926809523e-05\n",
      "conv1.bias 9.474519174546003e-05\n",
      "conv2.weight 0.00016914145400126774\n",
      "conv2.bias 3.315426511107944e-05\n",
      "fc1.weight 8.944123983383179e-05\n",
      "fc1.bias 0.00013071832557519277\n",
      "fc2.weight 0.00018544109567763314\n",
      "fc2.bias 0.00017172665823073614\n",
      "fc3.weight 0.00014275103097870237\n",
      "fc3.bias 5.8970763348042965e-05\n",
      "\n",
      "Test set: Average loss: 2.1755 \n",
      "Accuracy: 1759/10000 (17.59%)\n",
      "\n",
      "Round  21, Average loss 2.175 Test accuracy 17.590\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 6.469796515173382e-05\n",
      "conv1.bias 9.439943823963404e-05\n",
      "conv2.weight 0.00016906362026929855\n",
      "conv2.bias 3.313503839308396e-05\n",
      "fc1.weight 8.952951431274414e-05\n",
      "fc1.bias 0.00013079503551125527\n",
      "fc2.weight 0.0001856996426506648\n",
      "fc2.bias 0.00017181216251282465\n",
      "fc3.weight 0.00014299507297220685\n",
      "fc3.bias 5.9009710093960165e-05\n",
      "\n",
      "Test set: Average loss: 2.1754 \n",
      "Accuracy: 1757/10000 (17.57%)\n",
      "\n",
      "Round  22, Average loss 2.175 Test accuracy 17.570\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 6.474556608332528e-05\n",
      "conv1.bias 9.410200679364304e-05\n",
      "conv2.weight 0.00016900164385636647\n",
      "conv2.bias 3.311646651127376e-05\n",
      "fc1.weight 8.959283431371053e-05\n",
      "fc1.bias 0.0001308072047928969\n",
      "fc2.weight 0.00018587308743643383\n",
      "fc2.bias 0.00017178870205368315\n",
      "fc3.weight 0.0001431743126539957\n",
      "fc3.bias 5.90466836001724e-05\n",
      "\n",
      "Test set: Average loss: 2.1753 \n",
      "Accuracy: 1756/10000 (17.56%)\n",
      "\n",
      "Round  23, Average loss 2.175 Test accuracy 17.560\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 6.478146546416812e-05\n",
      "conv1.bias 9.396309421087305e-05\n",
      "conv2.weight 0.00016897156834602356\n",
      "conv2.bias 3.3115669793915004e-05\n",
      "fc1.weight 8.964428305625916e-05\n",
      "fc1.bias 0.00013083723994592825\n",
      "fc2.weight 0.00018601599666807388\n",
      "fc2.bias 0.00017179689547490505\n",
      "fc3.weight 0.0001433169912724268\n",
      "fc3.bias 5.9075630269944666e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.1753 \n",
      "Accuracy: 1756/10000 (17.56%)\n",
      "\n",
      "Round  24, Average loss 2.175 Test accuracy 17.560\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 6.480139990647633e-05\n",
      "conv1.bias 9.374887061615785e-05\n",
      "conv2.weight 0.00016895057012637456\n",
      "conv2.bias 3.3105723559856415e-05\n",
      "fc1.weight 8.968120813369752e-05\n",
      "fc1.bias 0.00013089569595952827\n",
      "fc2.weight 0.00018611372936339605\n",
      "fc2.bias 0.0001717913518881514\n",
      "fc3.weight 0.00014343237770455224\n",
      "fc3.bias 5.90900017414242e-05\n",
      "\n",
      "Test set: Average loss: 2.1752 \n",
      "Accuracy: 1754/10000 (17.54%)\n",
      "\n",
      "Round  25, Average loss 2.175 Test accuracy 17.540\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 6.481821752256817e-05\n",
      "conv1.bias 9.365565104720493e-05\n",
      "conv2.weight 0.00016892507672309877\n",
      "conv2.bias 3.30942748405505e-05\n",
      "fc1.weight 8.97061824798584e-05\n",
      "fc1.bias 0.00013085834992428622\n",
      "fc2.weight 0.00018619216150707668\n",
      "fc2.bias 0.00017177331305685497\n",
      "fc3.weight 0.0001434977122005962\n",
      "fc3.bias 5.912710330449045e-05\n",
      "\n",
      "Test set: Average loss: 2.1752 \n",
      "Accuracy: 1753/10000 (17.53%)\n",
      "\n",
      "Round  26, Average loss 2.175 Test accuracy 17.530\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 6.483183966742621e-05\n",
      "conv1.bias 9.361021996786197e-05\n",
      "conv2.weight 0.00016890669862429302\n",
      "conv2.bias 3.3089650969486684e-05\n",
      "fc1.weight 8.972615003585816e-05\n",
      "fc1.bias 0.00013087953751285872\n",
      "fc2.weight 0.0001862690798820011\n",
      "fc2.bias 0.00017181627585419586\n",
      "fc3.weight 0.0001435651488247372\n",
      "fc3.bias 5.9141317615285514e-05\n",
      "\n",
      "Test set: Average loss: 2.1752 \n",
      "Accuracy: 1755/10000 (17.55%)\n",
      "\n",
      "Round  27, Average loss 2.175 Test accuracy 17.550\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 6.484516378906038e-05\n",
      "conv1.bias 9.354429009060065e-05\n",
      "conv2.weight 0.00016889782001574833\n",
      "conv2.bias 3.309478415758349e-05\n",
      "fc1.weight 8.973983923594157e-05\n",
      "fc1.bias 0.00013089127217729885\n",
      "fc2.weight 0.00018632650848418946\n",
      "fc2.bias 0.00017185337353675139\n",
      "fc3.weight 0.00014361261079708735\n",
      "fc3.bias 5.91292220633477e-05\n",
      "\n",
      "Test set: Average loss: 2.1752 \n",
      "Accuracy: 1755/10000 (17.55%)\n",
      "\n",
      "Round  28, Average loss 2.175 Test accuracy 17.550\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 6.485572705666225e-05\n",
      "conv1.bias 9.353217319585383e-05\n",
      "conv2.weight 0.00016888509194056193\n",
      "conv2.bias 3.309789826744236e-05\n",
      "fc1.weight 8.975255489349365e-05\n",
      "fc1.bias 0.00013088608781496684\n",
      "fc2.weight 0.00018637341166299486\n",
      "fc2.bias 0.0001718328179170688\n",
      "fc3.weight 0.00014365403247731072\n",
      "fc3.bias 5.914338398724794e-05\n",
      "\n",
      "Test set: Average loss: 2.1751 \n",
      "Accuracy: 1754/10000 (17.54%)\n",
      "\n",
      "Round  29, Average loss 2.175 Test accuracy 17.540\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 6.486607922448052e-05\n",
      "conv1.bias 9.350808492551248e-05\n",
      "conv2.weight 0.00016888474424680075\n",
      "conv2.bias 3.3107149647548795e-05\n",
      "fc1.weight 8.976836005846659e-05\n",
      "fc1.bias 0.0001309161695341269\n",
      "fc2.weight 0.00018642305854767088\n",
      "fc2.bias 0.00017184297376800152\n",
      "fc3.weight 0.00014367981680801938\n",
      "fc3.bias 5.91504096519202e-05\n",
      "\n",
      "Test set: Average loss: 2.1751 \n",
      "Accuracy: 1756/10000 (17.56%)\n",
      "\n",
      "Round  30, Average loss 2.175 Test accuracy 17.560\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 6.487187825971179e-05\n",
      "conv1.bias 9.34976851567626e-05\n",
      "conv2.weight 0.00016886508713165918\n",
      "conv2.bias 3.3101743611041456e-05\n",
      "fc1.weight 8.977594971656799e-05\n",
      "fc1.bias 0.00013092424099644026\n",
      "fc2.weight 0.0001864503063852825\n",
      "fc2.bias 0.00017183701995582808\n",
      "fc3.weight 0.0001437061776717504\n",
      "fc3.bias 5.917724338360131e-05\n",
      "\n",
      "Test set: Average loss: 2.1751 \n",
      "Accuracy: 1754/10000 (17.54%)\n",
      "\n",
      "Round  31, Average loss 2.175 Test accuracy 17.540\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 6.487928330898285e-05\n",
      "conv1.bias 9.342424649124344e-05\n",
      "conv2.weight 0.00016886193305253982\n",
      "conv2.bias 3.310321699245833e-05\n",
      "fc1.weight 8.97825558980306e-05\n",
      "fc1.bias 0.00013095224276185035\n",
      "fc2.weight 0.00018646404856727236\n",
      "fc2.bias 0.00017181547757770334\n",
      "fc3.weight 0.0001437151538474219\n",
      "fc3.bias 5.918234237469733e-05\n",
      "\n",
      "Test set: Average loss: 2.1751 \n",
      "Accuracy: 1754/10000 (17.54%)\n",
      "\n",
      "Round  32, Average loss 2.175 Test accuracy 17.540\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 6.48778925339381e-05\n",
      "conv1.bias 9.337502221266429e-05\n",
      "conv2.weight 0.0001688583195209503\n",
      "conv2.bias 3.3098269341280684e-05\n",
      "fc1.weight 8.978668848673503e-05\n",
      "fc1.bias 0.00013096149389942488\n",
      "fc2.weight 0.00018647567383826724\n",
      "fc2.bias 0.00017181296078931717\n",
      "fc3.weight 0.0001437288842030934\n",
      "fc3.bias 5.917601520195603e-05\n",
      "\n",
      "Test set: Average loss: 2.1751 \n",
      "Accuracy: 1754/10000 (17.54%)\n",
      "\n",
      "Round  33, Average loss 2.175 Test accuracy 17.540\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 6.487967239485846e-05\n",
      "conv1.bias 9.337866989274819e-05\n",
      "conv2.weight 0.00016885116696357727\n",
      "conv2.bias 3.309455496491864e-05\n",
      "fc1.weight 8.979188402493795e-05\n",
      "fc1.bias 0.00013095309647421043\n",
      "fc2.weight 0.00018649394550020732\n",
      "fc2.bias 0.0001718082820020971\n",
      "fc3.weight 0.00014373367386204855\n",
      "fc3.bias 5.9181667165830734e-05\n",
      "\n",
      "Test set: Average loss: 2.1751 \n",
      "Accuracy: 1755/10000 (17.55%)\n",
      "\n",
      "Round  34, Average loss 2.175 Test accuracy 17.550\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 6.488122459914949e-05\n",
      "conv1.bias 9.336558287031949e-05\n",
      "conv2.weight 0.00016885358840227127\n",
      "conv2.bias 3.309236853965558e-05\n",
      "fc1.weight 8.979355295499166e-05\n",
      "fc1.bias 0.00013095340691506862\n",
      "fc2.weight 0.0001865045064025455\n",
      "fc2.bias 0.00017181833806846823\n",
      "fc3.weight 0.0001437464196767126\n",
      "fc3.bias 5.9170013992115854e-05\n",
      "\n",
      "Test set: Average loss: 2.1751 \n",
      "Accuracy: 1755/10000 (17.55%)\n",
      "\n",
      "Round  35, Average loss 2.175 Test accuracy 17.550\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 6.488009045521418e-05\n",
      "conv1.bias 9.340371858949463e-05\n",
      "conv2.weight 0.00016884921739498775\n",
      "conv2.bias 3.3091586374212056e-05\n",
      "fc1.weight 8.979307611783346e-05\n",
      "fc1.bias 0.00013095056638121605\n",
      "fc2.weight 0.0001865107152197096\n",
      "fc2.bias 0.00017183149854342142\n",
      "fc3.weight 0.00014374199367704846\n",
      "fc3.bias 5.9161969693377615e-05\n",
      "\n",
      "Test set: Average loss: 2.1751 \n",
      "Accuracy: 1755/10000 (17.55%)\n",
      "\n",
      "Round  36, Average loss 2.175 Test accuracy 17.550\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 6.48795399400923e-05\n",
      "conv1.bias 9.341480714889865e-05\n",
      "conv2.weight 0.00016885079443454743\n",
      "conv2.bias 3.309408566565253e-05\n",
      "fc1.weight 8.979513247807821e-05\n",
      "fc1.bias 0.00013093830396731693\n",
      "fc2.weight 0.00018651879259518216\n",
      "fc2.bias 0.00017186733228819712\n",
      "fc3.weight 0.0001437443530275708\n",
      "fc3.bias 5.916929803788662e-05\n",
      "\n",
      "Test set: Average loss: 2.1751 \n",
      "Accuracy: 1755/10000 (17.55%)\n",
      "\n",
      "Round  37, Average loss 2.175 Test accuracy 17.550\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 6.488193654351764e-05\n",
      "conv1.bias 9.33984701987356e-05\n",
      "conv2.weight 0.00016885629544655483\n",
      "conv2.bias 3.3090000215452164e-05\n",
      "fc1.weight 8.979512254397074e-05\n",
      "fc1.bias 0.0001309483932952086\n",
      "fc2.weight 0.0001865140384151822\n",
      "fc2.bias 0.00017183520165937289\n",
      "fc3.weight 0.0001437461447148096\n",
      "fc3.bias 5.917975213378668e-05\n",
      "\n",
      "Test set: Average loss: 2.1751 \n",
      "Accuracy: 1754/10000 (17.54%)\n",
      "\n",
      "Round  38, Average loss 2.175 Test accuracy 17.540\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 6.488261123498281e-05\n",
      "conv1.bias 9.340258354010682e-05\n",
      "conv2.weight 0.00016884325693051022\n",
      "conv2.bias 3.309708336018957e-05\n",
      "fc1.weight 8.979492386182149e-05\n",
      "fc1.bias 0.00013094935566186904\n",
      "fc2.weight 0.00018651154306199817\n",
      "fc2.bias 0.00017184549055638767\n",
      "fc3.weight 0.00014375212291876474\n",
      "fc3.bias 5.9168291045352817e-05\n",
      "\n",
      "Test set: Average loss: 2.1751 \n",
      "Accuracy: 1754/10000 (17.54%)\n",
      "\n",
      "Round  39, Average loss 2.175 Test accuracy 17.540\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 6.488069891929626e-05\n",
      "conv1.bias 9.338959353044629e-05\n",
      "conv2.weight 0.00016883991658687593\n",
      "conv2.bias 3.3094751415774226e-05\n",
      "fc1.weight 8.979554971059164e-05\n",
      "fc1.bias 0.00013097270081440608\n",
      "fc2.weight 0.0001865235822541373\n",
      "fc2.bias 0.00017184149917392504\n",
      "fc3.weight 0.00014375762215682438\n",
      "fc3.bias 5.917064845561981e-05\n",
      "\n",
      "Test set: Average loss: 2.1751 \n",
      "Accuracy: 1751/10000 (17.51%)\n",
      "\n",
      "Round  40, Average loss 2.175 Test accuracy 17.510\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 6.487997041808234e-05\n",
      "conv1.bias 9.338513094310959e-05\n",
      "conv2.weight 0.00016884468495845795\n",
      "conv2.bias 3.309144085505977e-05\n",
      "fc1.weight 8.979636430740356e-05\n",
      "fc1.bias 0.00013097142800688744\n",
      "fc2.weight 0.0001865204009744856\n",
      "fc2.bias 0.00017182718563292707\n",
      "fc3.weight 0.0001437526551030931\n",
      "fc3.bias 5.9195200446993115e-05\n",
      "\n",
      "Test set: Average loss: 2.1751 \n",
      "Accuracy: 1756/10000 (17.56%)\n",
      "\n",
      "Round  41, Average loss 2.175 Test accuracy 17.560\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 6.487585190269683e-05\n",
      "conv1.bias 9.335920913144946e-05\n",
      "conv2.weight 0.0001688464234272639\n",
      "conv2.bias 3.310024840175174e-05\n",
      "fc1.weight 8.979638417561849e-05\n",
      "fc1.bias 0.00013099140487611295\n",
      "fc2.weight 0.00018652405530687362\n",
      "fc2.bias 0.00017182732976618267\n",
      "fc3.weight 0.00014375230918327968\n",
      "fc3.bias 5.919313407503069e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.1751 \n",
      "Accuracy: 1754/10000 (17.54%)\n",
      "\n",
      "Round  42, Average loss 2.175 Test accuracy 17.540\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 6.487645208835602e-05\n",
      "conv1.bias 9.336433140560985e-05\n",
      "conv2.weight 0.0001688415432969729\n",
      "conv2.bias 3.3102034649346024e-05\n",
      "fc1.weight 8.979696035385131e-05\n",
      "fc1.bias 0.00013096630573272705\n",
      "fc2.weight 0.000186523109201401\n",
      "fc2.bias 0.00017181166359001682\n",
      "fc3.weight 0.00014376010568369004\n",
      "fc3.bias 5.917678936384618e-05\n",
      "\n",
      "Test set: Average loss: 2.1751 \n",
      "Accuracy: 1754/10000 (17.54%)\n",
      "\n",
      "Round  43, Average loss 2.175 Test accuracy 17.540\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 6.487151400910483e-05\n",
      "conv1.bias 9.334313411576052e-05\n",
      "conv2.weight 0.0001688415308793386\n",
      "conv2.bias 3.309065505163744e-05\n",
      "fc1.weight 8.979596694310506e-05\n",
      "fc1.bias 0.00013095860679944357\n",
      "fc2.weight 0.00018652178465373932\n",
      "fc2.bias 0.00017183555644892512\n",
      "fc3.weight 0.0001437586865254811\n",
      "fc3.bias 5.916402442380786e-05\n",
      "\n",
      "Test set: Average loss: 2.1751 \n",
      "Accuracy: 1754/10000 (17.54%)\n",
      "\n",
      "Round  44, Average loss 2.175 Test accuracy 17.540\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 6.487016048696305e-05\n",
      "conv1.bias 9.333498504323264e-05\n",
      "conv2.weight 0.00016885124146938325\n",
      "conv2.bias 3.3086947951233014e-05\n",
      "fc1.weight 8.979503313700357e-05\n",
      "fc1.bias 0.00013095826531449954\n",
      "fc2.weight 0.00018651156671463498\n",
      "fc2.bias 0.00017181006703703177\n",
      "fc3.weight 0.0001437653654388019\n",
      "fc3.bias 5.915219080634415e-05\n",
      "\n",
      "Test set: Average loss: 2.1751 \n",
      "Accuracy: 1754/10000 (17.54%)\n",
      "\n",
      "Round  45, Average loss 2.175 Test accuracy 17.540\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 6.48776607380973e-05\n",
      "conv1.bias 9.331616456620395e-05\n",
      "conv2.weight 0.0001688394322991371\n",
      "conv2.bias 3.3094751415774226e-05\n",
      "fc1.weight 8.979428807894389e-05\n",
      "fc1.bias 0.00013096785793701807\n",
      "fc2.weight 0.0001865123590779683\n",
      "fc2.bias 0.00017183134332299232\n",
      "fc3.weight 0.00014375760441734677\n",
      "fc3.bias 5.9159210650250316e-05\n",
      "\n",
      "Test set: Average loss: 2.1751 \n",
      "Accuracy: 1754/10000 (17.54%)\n",
      "\n",
      "Round  46, Average loss 2.175 Test accuracy 17.540\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 6.487567805581622e-05\n",
      "conv1.bias 9.33246046770364e-05\n",
      "conv2.weight 0.00016883992900451026\n",
      "conv2.bias 3.3090298529714346e-05\n",
      "fc1.weight 8.979452649752299e-05\n",
      "fc1.bias 0.00013097093130151431\n",
      "fc2.weight 0.0001865132578781673\n",
      "fc2.bias 0.00017180463432201317\n",
      "fc3.weight 0.0001437684077592123\n",
      "fc3.bias 5.9177580988034605e-05\n",
      "\n",
      "Test set: Average loss: 2.1751 \n",
      "Accuracy: 1754/10000 (17.54%)\n",
      "\n",
      "Round  47, Average loss 2.175 Test accuracy 17.540\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 6.487614578670925e-05\n",
      "conv1.bias 9.333981627908845e-05\n",
      "conv2.weight 0.00016884572803974152\n",
      "conv2.bias 3.3089800126617774e-05\n",
      "fc1.weight 8.979412913322449e-05\n",
      "fc1.bias 0.00013096447413166363\n",
      "fc2.weight 0.00018650908318776932\n",
      "fc2.bias 0.00017184675449416752\n",
      "fc3.weight 0.00014375504106283187\n",
      "fc3.bias 5.91725402045995e-05\n",
      "\n",
      "Test set: Average loss: 2.1751 \n",
      "Accuracy: 1754/10000 (17.54%)\n",
      "\n",
      "Round  48, Average loss 2.175 Test accuracy 17.540\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 6.487929986582862e-05\n",
      "conv1.bias 9.340692001084487e-05\n",
      "conv2.weight 0.0001688393329580625\n",
      "conv2.bias 3.308961458969861e-05\n",
      "fc1.weight 8.979092041651408e-05\n",
      "fc1.bias 0.00013095528508226076\n",
      "fc2.weight 0.0001865054170290629\n",
      "fc2.bias 0.00017182831652462482\n",
      "fc3.weight 0.00014375380816913786\n",
      "fc3.bias 5.915762740187347e-05\n",
      "\n",
      "Test set: Average loss: 2.1751 \n",
      "Accuracy: 1756/10000 (17.56%)\n",
      "\n",
      "Round  49, Average loss 2.175 Test accuracy 17.560\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "K = 4\n",
    "T = 5\n",
    "sigma = 0.1\n",
    "Noise_Alloc = [0,2,4,6,8]\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "\n",
    "N_array = [4]\n",
    "lr_array = [0.0003, 0.0001,0.00001]\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 50\n",
    "\n",
    "\n",
    "\n",
    "loss_test_arr_K4_G1 = np.zeros((len(N_array),len(lr_array),N_trials,N_epochs))\n",
    "acc_test_arr_K4_G1  = np.zeros((len(N_array),len(lr_array),N_trials,N_epochs))\n",
    "\n",
    "for N_idx in range(len(N_array)):\n",
    "    \n",
    "    N = N_array[N_idx]\n",
    "    \n",
    "    \n",
    "           \n",
    "        \n",
    "    # print(\"alpha_array: \",alpha_array,'\\n')\n",
    "    \n",
    "    \n",
    "    for lr_idx in range(len(lr_array)):\n",
    "        \n",
    "        args.lr = lr_array[lr_idx]\n",
    "        \n",
    "        print('Learning Rate =',args.lr)\n",
    "        print('\\n\\n')\n",
    "        \n",
    "        z_array = []\n",
    "#         while(len(z_array)<N):\n",
    "#             z_tmp = np.random.uniform(-1,1,1)\n",
    "#             MIS_tmp = MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_tmp], 1,sigma)\n",
    "#             if MIS_tmp < B and MIS_tmp > 0.1:\n",
    "#                 z_array.append(z_tmp[0])\n",
    "#         \n",
    "#         z_array = np.sort(z_array)\n",
    "#         print(N_idx,'!!!')\n",
    "#         if N_idx==0:\n",
    "# #             z_array = np.array([-0.94,-0.534,0.534, 0.94])\n",
    "# #         elif N_idx==1:\n",
    "# #             z_array = np.array([-0.94, -0.73, 0.73, 0.94])\n",
    "#         elif N_idx==1:\n",
    "#             z_array = np.array([-0.94, -0.125, 0.125, 0.94])\n",
    "#         else:\n",
    "# #             z_array = np.array([-0.94, -0.73, -0.534, -0.125, 0.125, 0.534, 0.73, 0.94])\n",
    "# #             z_array = np.array([-0.9, -0.81, -0.22, -0.20, 0.20, 0.22, 0.81, 0.9])\n",
    "        \n",
    "        z_array = np.array([-0.94, -0.125, 0.125, 0.94])\n",
    "        \n",
    "        print('z_array:',z_array)\n",
    "        for j in range(len(z_array)):\n",
    "            print(MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_array[j]], 1,sigma))\n",
    "        \n",
    "        \n",
    "        _Noise_label = np.ones((15000*T,10)) * 0.1\n",
    "\n",
    "        X_tilde,a,b = BACC_Enc_Data_v3(encoding_input_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde,a,b = BACC_Enc_Data_v3(encoding_label_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "\n",
    "        m = N # m is the number of received result @ master\n",
    "    #     print('number of results:',m)\n",
    "\n",
    "        for trial_idx in range(N_trials):\n",
    "            print('(T, sigma)=',T,sigma,') ',trial_idx,'-th Trial!!')\n",
    "\n",
    "            net_glob = CNNCifar(args=args)\n",
    "            net_glob.cuda()\n",
    "            net_glob.train()\n",
    "\n",
    "            # copy weights\n",
    "            w_glob = net_glob.state_dict()\n",
    "\n",
    "            for iter in range(N_epochs): #args.epochs\n",
    "                w_locals, loss_locals = [], []\n",
    "                idxs_users = np.random.choice(range(N), m, replace=False)\n",
    "                idxs_users = np.sort(idxs_users)\n",
    "                print('selected users:',idxs_users)\n",
    "                \n",
    "                coded_net = BACC_Enc_Model_withNoise_v4(net_glob.cuda(), N, K, T, sigma, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "                dec_z_array = []\n",
    "                for idx in idxs_users: #for idx in range(N):\n",
    "            #         print(idx)\n",
    "                    local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[idx,:,:], label=y_tilde[idx,:,:])\n",
    "                    w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "#                     w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                    w_locals.append(copy.deepcopy(w))\n",
    "                    loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                    dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "                # update global weights\n",
    "                #w_glob = FedAvg(w_locals)\n",
    "                w_glob = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "\n",
    "                # copy weight to net_glob\n",
    "                net_glob.load_state_dict(w_glob)\n",
    "\n",
    "                # print loss\n",
    "            #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "            #     loss_train_arr.append(loss_train)\n",
    "\n",
    "                acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "                acc_test_arr_K4_G1[N_idx][lr_idx][trial_idx][iter] = acc_test\n",
    "                loss_test_arr_K4_G1[N_idx][lr_idx][trial_idx][iter] = loss_test\n",
    "                if iter % 1 ==0:\n",
    "                    print('Round {:3d}, Average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_test, acc_test))\n",
    "                #print(loss_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3. K=4, N=4, T=3, G=2, with grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2 3\n",
      "##########################################\n",
      "Learning Rate = 0.0003\n",
      "\n",
      "\n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 12500 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 12500 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 12500 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 12500 \n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.004624962276882596\n",
      "conv1.bias 0.00348116767903169\n",
      "conv2.weight 0.0022326672077178956\n",
      "conv2.bias 0.00137176807038486\n",
      "fc1.weight 0.0008272260030110677\n",
      "fc1.bias 0.000765727957089742\n",
      "fc2.weight 0.002760102635338193\n",
      "fc2.bias 0.002314757200933638\n",
      "fc3.weight 0.0039475418272472565\n",
      "fc3.bias 0.006210942566394806\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.004624962276882596\n",
      "conv1.bias 0.00348116767903169\n",
      "conv2.weight 0.0022326672077178956\n",
      "conv2.bias 0.00137176807038486\n",
      "fc1.weight 0.0008272260030110677\n",
      "fc1.bias 0.000765727957089742\n",
      "fc2.weight 0.002760102635338193\n",
      "fc2.bias 0.002314757200933638\n",
      "fc3.weight 0.0039475418272472565\n",
      "fc3.bias 0.006210942566394806\n",
      "\n",
      "Test set: Average loss: 1.8556 \n",
      "Accuracy: 3461/10000 (34.61%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00246012634701199\n",
      "conv1.bias 0.0054500512778759\n",
      "conv2.weight 0.0010336283842722574\n",
      "conv2.bias 0.0008778090123087168\n",
      "fc1.weight 0.00017506919304529825\n",
      "fc1.bias 0.000610062728325526\n",
      "fc2.weight 0.0003501035627864656\n",
      "fc2.bias 0.000823196821979114\n",
      "fc3.weight 0.001008930660429455\n",
      "fc3.bias 0.001519412361085415\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00246012634701199\n",
      "conv1.bias 0.0054500512778759\n",
      "conv2.weight 0.0010336283842722574\n",
      "conv2.bias 0.0008778090123087168\n",
      "fc1.weight 0.00017506919304529825\n",
      "fc1.bias 0.000610062728325526\n",
      "fc2.weight 0.0003501035627864656\n",
      "fc2.bias 0.000823196821979114\n",
      "fc3.weight 0.001008930660429455\n",
      "fc3.bias 0.001519412361085415\n",
      "\n",
      "Test set: Average loss: 1.7328 \n",
      "Accuracy: 3695/10000 (36.95%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002392725149790446\n",
      "conv1.bias 0.005024485290050507\n",
      "conv2.weight 0.001095038652420044\n",
      "conv2.bias 0.0013217530213296413\n",
      "fc1.weight 0.00029874428113301594\n",
      "fc1.bias 0.0014378617207209269\n",
      "fc2.weight 0.0005847453124939449\n",
      "fc2.bias 0.001960471272468567\n",
      "fc3.weight 0.0010265754801886422\n",
      "fc3.bias 0.001595151610672474\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002392725149790446\n",
      "conv1.bias 0.005024485290050507\n",
      "conv2.weight 0.001095038652420044\n",
      "conv2.bias 0.0013217530213296413\n",
      "fc1.weight 0.00029874428113301594\n",
      "fc1.bias 0.0014378617207209269\n",
      "fc2.weight 0.0005847453124939449\n",
      "fc2.bias 0.001960471272468567\n",
      "fc3.weight 0.0010265754801886422\n",
      "fc3.bias 0.001595151610672474\n",
      "\n",
      "Test set: Average loss: 1.7069 \n",
      "Accuracy: 3853/10000 (38.53%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0024922302034166123\n",
      "conv1.bias 0.005739570905764897\n",
      "conv2.weight 0.0011293909947077433\n",
      "conv2.bias 0.0016076856991276145\n",
      "fc1.weight 0.00033496026198069254\n",
      "fc1.bias 0.0021989400188128154\n",
      "fc2.weight 0.0007969392670525444\n",
      "fc2.bias 0.0032003262922877355\n",
      "fc3.weight 0.0012697006974901471\n",
      "fc3.bias 0.0023154014721512794\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0024922302034166123\n",
      "conv1.bias 0.005739570905764897\n",
      "conv2.weight 0.0011293909947077433\n",
      "conv2.bias 0.0016076856991276145\n",
      "fc1.weight 0.00033496026198069254\n",
      "fc1.bias 0.0021989400188128154\n",
      "fc2.weight 0.0007969392670525444\n",
      "fc2.bias 0.0032003262922877355\n",
      "fc3.weight 0.0012697006974901471\n",
      "fc3.bias 0.0023154014721512794\n",
      "\n",
      "Test set: Average loss: 1.6905 \n",
      "Accuracy: 3937/10000 (39.37%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002495523558722602\n",
      "conv1.bias 0.0071192265798648196\n",
      "conv2.weight 0.0011416566371917724\n",
      "conv2.bias 0.0017544012516736984\n",
      "fc1.weight 0.00035587521394093833\n",
      "fc1.bias 0.0025258123874664306\n",
      "fc2.weight 0.000916728897700234\n",
      "fc2.bias 0.0035522973963192533\n",
      "fc3.weight 0.0014927994637262253\n",
      "fc3.bias 0.002400286868214607\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002495523558722602\n",
      "conv1.bias 0.0071192265798648196\n",
      "conv2.weight 0.0011416566371917724\n",
      "conv2.bias 0.0017544012516736984\n",
      "fc1.weight 0.00035587521394093833\n",
      "fc1.bias 0.0025258123874664306\n",
      "fc2.weight 0.000916728897700234\n",
      "fc2.bias 0.0035522973963192533\n",
      "fc3.weight 0.0014927994637262253\n",
      "fc3.bias 0.002400286868214607\n",
      "\n",
      "Test set: Average loss: 1.6759 \n",
      "Accuracy: 4003/10000 (40.03%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0024208264880710177\n",
      "conv1.bias 0.008085675537586212\n",
      "conv2.weight 0.001164661447207133\n",
      "conv2.bias 0.002027885289862752\n",
      "fc1.weight 0.00037087257703145344\n",
      "fc1.bias 0.0024903963009516397\n",
      "fc2.weight 0.0009934694994063605\n",
      "fc2.bias 0.003405030639398666\n",
      "fc3.weight 0.0016765053783144269\n",
      "fc3.bias 0.002211949601769447\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0024208264880710177\n",
      "conv1.bias 0.008085675537586212\n",
      "conv2.weight 0.001164661447207133\n",
      "conv2.bias 0.002027885289862752\n",
      "fc1.weight 0.00037087257703145344\n",
      "fc1.bias 0.0024903963009516397\n",
      "fc2.weight 0.0009934694994063605\n",
      "fc2.bias 0.003405030639398666\n",
      "fc3.weight 0.0016765053783144269\n",
      "fc3.bias 0.002211949601769447\n",
      "\n",
      "Test set: Average loss: 1.6622 \n",
      "Accuracy: 4052/10000 (40.52%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002336387899186876\n",
      "conv1.bias 0.008140106375018755\n",
      "conv2.weight 0.0011806634068489074\n",
      "conv2.bias 0.0024158575106412172\n",
      "fc1.weight 0.00038332271575927733\n",
      "fc1.bias 0.002374635140101115\n",
      "fc2.weight 0.0010538815505920895\n",
      "fc2.bias 0.0031859218364670163\n",
      "fc3.weight 0.0018287414596194313\n",
      "fc3.bias 0.0020160334184765815\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002336387899186876\n",
      "conv1.bias 0.008140106375018755\n",
      "conv2.weight 0.0011806634068489074\n",
      "conv2.bias 0.0024158575106412172\n",
      "fc1.weight 0.00038332271575927733\n",
      "fc1.bias 0.002374635140101115\n",
      "fc2.weight 0.0010538815505920895\n",
      "fc2.bias 0.0031859218364670163\n",
      "fc3.weight 0.0018287414596194313\n",
      "fc3.bias 0.0020160334184765815\n",
      "\n",
      "Test set: Average loss: 1.6475 \n",
      "Accuracy: 4105/10000 (41.05%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0022729958428276908\n",
      "conv1.bias 0.007310583566625913\n",
      "conv2.weight 0.001177778442700704\n",
      "conv2.bias 0.002763178199529648\n",
      "fc1.weight 0.0003956960042317708\n",
      "fc1.bias 0.0022751833001772564\n",
      "fc2.weight 0.001102222242052593\n",
      "fc2.bias 0.0030091585857527597\n",
      "fc3.weight 0.0019505925121761504\n",
      "fc3.bias 0.0018720746040344239\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0022729958428276908\n",
      "conv1.bias 0.007310583566625913\n",
      "conv2.weight 0.001177778442700704\n",
      "conv2.bias 0.002763178199529648\n",
      "fc1.weight 0.0003956960042317708\n",
      "fc1.bias 0.0022751833001772564\n",
      "fc2.weight 0.001102222242052593\n",
      "fc2.bias 0.0030091585857527597\n",
      "fc3.weight 0.0019505925121761504\n",
      "fc3.bias 0.0018720746040344239\n",
      "\n",
      "Test set: Average loss: 1.6386 \n",
      "Accuracy: 4154/10000 (41.54%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0022124290466308595\n",
      "conv1.bias 0.006345650802055995\n",
      "conv2.weight 0.0011670515934626262\n",
      "conv2.bias 0.0029464566614478827\n",
      "fc1.weight 0.0004062707424163818\n",
      "fc1.bias 0.002166565259297689\n",
      "fc2.weight 0.001140020385621086\n",
      "fc2.bias 0.0028480037692047303\n",
      "fc3.weight 0.0020456911552520027\n",
      "fc3.bias 0.0017519153654575348\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0022124290466308595\n",
      "conv1.bias 0.006345650802055995\n",
      "conv2.weight 0.0011670515934626262\n",
      "conv2.bias 0.0029464566614478827\n",
      "fc1.weight 0.0004062707424163818\n",
      "fc1.bias 0.002166565259297689\n",
      "fc2.weight 0.001140020385621086\n",
      "fc2.bias 0.0028480037692047303\n",
      "fc3.weight 0.0020456911552520027\n",
      "fc3.bias 0.0017519153654575348\n",
      "\n",
      "Test set: Average loss: 1.6336 \n",
      "Accuracy: 4166/10000 (41.66%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002152995268503825\n",
      "conv1.bias 0.00571868692835172\n",
      "conv2.weight 0.0011586739619572958\n",
      "conv2.bias 0.0029759490862488747\n",
      "fc1.weight 0.0004134379227956136\n",
      "fc1.bias 0.0020481699456771215\n",
      "fc2.weight 0.0011684837795439221\n",
      "fc2.bias 0.0026718214863822574\n",
      "fc3.weight 0.0021141634100959416\n",
      "fc3.bias 0.0016290150582790376\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002152995268503825\n",
      "conv1.bias 0.00571868692835172\n",
      "conv2.weight 0.0011586739619572958\n",
      "conv2.bias 0.0029759490862488747\n",
      "fc1.weight 0.0004134379227956136\n",
      "fc1.bias 0.0020481699456771215\n",
      "fc2.weight 0.0011684837795439221\n",
      "fc2.bias 0.0026718214863822574\n",
      "fc3.weight 0.0021141634100959416\n",
      "fc3.bias 0.0016290150582790376\n",
      "\n",
      "Test set: Average loss: 1.6299 \n",
      "Accuracy: 4182/10000 (41.82%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00210537142223782\n",
      "conv1.bias 0.00531902660926183\n",
      "conv2.weight 0.001164308786392212\n",
      "conv2.bias 0.00296257552690804\n",
      "fc1.weight 0.0004186913967132568\n",
      "fc1.bias 0.001959960038463275\n",
      "fc2.weight 0.0011899230972168937\n",
      "fc2.bias 0.002518037954966227\n",
      "fc3.weight 0.0021689094248272126\n",
      "fc3.bias 0.0015142531134188174\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00210537142223782\n",
      "conv1.bias 0.00531902660926183\n",
      "conv2.weight 0.001164308786392212\n",
      "conv2.bias 0.00296257552690804\n",
      "fc1.weight 0.0004186913967132568\n",
      "fc1.bias 0.001959960038463275\n",
      "fc2.weight 0.0011899230972168937\n",
      "fc2.bias 0.002518037954966227\n",
      "fc3.weight 0.0021689094248272126\n",
      "fc3.bias 0.0015142531134188174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.6272 \n",
      "Accuracy: 4183/10000 (41.83%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002068025536007351\n",
      "conv1.bias 0.0050210921714703245\n",
      "conv2.weight 0.0011708125472068786\n",
      "conv2.bias 0.0029851372819393873\n",
      "fc1.weight 0.00042226672172546387\n",
      "fc1.bias 0.0019063770771026612\n",
      "fc2.weight 0.001205138365427653\n",
      "fc2.bias 0.002389322966337204\n",
      "fc3.weight 0.002207654146921067\n",
      "fc3.bias 0.0014099580235779285\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002068025536007351\n",
      "conv1.bias 0.0050210921714703245\n",
      "conv2.weight 0.0011708125472068786\n",
      "conv2.bias 0.0029851372819393873\n",
      "fc1.weight 0.00042226672172546387\n",
      "fc1.bias 0.0019063770771026612\n",
      "fc2.weight 0.001205138365427653\n",
      "fc2.bias 0.002389322966337204\n",
      "fc3.weight 0.002207654146921067\n",
      "fc3.bias 0.0014099580235779285\n",
      "\n",
      "Test set: Average loss: 1.6266 \n",
      "Accuracy: 4182/10000 (41.82%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020387825700971815\n",
      "conv1.bias 0.004741770525773366\n",
      "conv2.weight 0.0011775829394658406\n",
      "conv2.bias 0.003014493966475129\n",
      "fc1.weight 0.0004245690902074178\n",
      "fc1.bias 0.0018776106337706248\n",
      "fc2.weight 0.001214992432367234\n",
      "fc2.bias 0.0022883590842996326\n",
      "fc3.weight 0.0022332144635064262\n",
      "fc3.bias 0.0013212461955845356\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020387825700971815\n",
      "conv1.bias 0.004741770525773366\n",
      "conv2.weight 0.0011775829394658406\n",
      "conv2.bias 0.003014493966475129\n",
      "fc1.weight 0.0004245690902074178\n",
      "fc1.bias 0.0018776106337706248\n",
      "fc2.weight 0.001214992432367234\n",
      "fc2.bias 0.0022883590842996326\n",
      "fc3.weight 0.0022332144635064262\n",
      "fc3.bias 0.0013212461955845356\n",
      "\n",
      "Test set: Average loss: 1.6251 \n",
      "Accuracy: 4180/10000 (41.80%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002013258271747165\n",
      "conv1.bias 0.004500748589634895\n",
      "conv2.weight 0.0011853534976641337\n",
      "conv2.bias 0.0030377190560102463\n",
      "fc1.weight 0.0004259528319040934\n",
      "fc1.bias 0.001862615222732226\n",
      "fc2.weight 0.001222241965551225\n",
      "fc2.bias 0.002208160147780464\n",
      "fc3.weight 0.0022520444222858976\n",
      "fc3.bias 0.0012505510821938515\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002013258271747165\n",
      "conv1.bias 0.004500748589634895\n",
      "conv2.weight 0.0011853534976641337\n",
      "conv2.bias 0.0030377190560102463\n",
      "fc1.weight 0.0004259528319040934\n",
      "fc1.bias 0.001862615222732226\n",
      "fc2.weight 0.001222241965551225\n",
      "fc2.bias 0.002208160147780464\n",
      "fc3.weight 0.0022520444222858976\n",
      "fc3.bias 0.0012505510821938515\n",
      "\n",
      "Test set: Average loss: 1.6232 \n",
      "Accuracy: 4193/10000 (41.93%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001991746425628662\n",
      "conv1.bias 0.0043462105095386505\n",
      "conv2.weight 0.0011938888827959697\n",
      "conv2.bias 0.0030469538178294897\n",
      "fc1.weight 0.00042679929733276367\n",
      "fc1.bias 0.0018523829678694407\n",
      "fc2.weight 0.0012277924825274756\n",
      "fc2.bias 0.002143552615529015\n",
      "fc3.weight 0.0022660533587137857\n",
      "fc3.bias 0.001194173190742731\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001991746425628662\n",
      "conv1.bias 0.0043462105095386505\n",
      "conv2.weight 0.0011938888827959697\n",
      "conv2.bias 0.0030469538178294897\n",
      "fc1.weight 0.00042679929733276367\n",
      "fc1.bias 0.0018523829678694407\n",
      "fc2.weight 0.0012277924825274756\n",
      "fc2.bias 0.002143552615529015\n",
      "fc3.weight 0.0022660533587137857\n",
      "fc3.bias 0.001194173190742731\n",
      "\n",
      "Test set: Average loss: 1.6219 \n",
      "Accuracy: 4202/10000 (42.02%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001973502900865343\n",
      "conv1.bias 0.0042429231107234955\n",
      "conv2.weight 0.0012031681338946026\n",
      "conv2.bias 0.003052063984796405\n",
      "fc1.weight 0.00042682739098866783\n",
      "fc1.bias 0.0018484726548194884\n",
      "fc2.weight 0.0012300484710269504\n",
      "fc2.bias 0.002094975717010952\n",
      "fc3.weight 0.0022752716427757625\n",
      "fc3.bias 0.0011504864320158958\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001973502900865343\n",
      "conv1.bias 0.0042429231107234955\n",
      "conv2.weight 0.0012031681338946026\n",
      "conv2.bias 0.003052063984796405\n",
      "fc1.weight 0.00042682739098866783\n",
      "fc1.bias 0.0018484726548194884\n",
      "fc2.weight 0.0012300484710269504\n",
      "fc2.bias 0.002094975717010952\n",
      "fc3.weight 0.0022752716427757625\n",
      "fc3.bias 0.0011504864320158958\n",
      "\n",
      "Test set: Average loss: 1.6218 \n",
      "Accuracy: 4205/10000 (42.05%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019626090261671276\n",
      "conv1.bias 0.00417608084777991\n",
      "conv2.weight 0.0012127857406934103\n",
      "conv2.bias 0.003057295922189951\n",
      "fc1.weight 0.00042675697803497314\n",
      "fc1.bias 0.001842387393116951\n",
      "fc2.weight 0.0012306246492597792\n",
      "fc2.bias 0.0020590032495203473\n",
      "fc3.weight 0.002281574975876581\n",
      "fc3.bias 0.0011114551685750484\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019626090261671276\n",
      "conv1.bias 0.00417608084777991\n",
      "conv2.weight 0.0012127857406934103\n",
      "conv2.bias 0.003057295922189951\n",
      "fc1.weight 0.00042675697803497314\n",
      "fc1.bias 0.001842387393116951\n",
      "fc2.weight 0.0012306246492597792\n",
      "fc2.bias 0.0020590032495203473\n",
      "fc3.weight 0.002281574975876581\n",
      "fc3.bias 0.0011114551685750484\n",
      "\n",
      "Test set: Average loss: 1.6193 \n",
      "Accuracy: 4212/10000 (42.12%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001958597633573744\n",
      "conv1.bias 0.004127096695204576\n",
      "conv2.weight 0.0012211327751477559\n",
      "conv2.bias 0.0030447510071098804\n",
      "fc1.weight 0.000426404595375061\n",
      "fc1.bias 0.0018392837295929591\n",
      "fc2.weight 0.0012304612568446568\n",
      "fc2.bias 0.0020322629383632113\n",
      "fc3.weight 0.002288052581605457\n",
      "fc3.bias 0.0010850402526557446\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001958597633573744\n",
      "conv1.bias 0.004127096695204576\n",
      "conv2.weight 0.0012211327751477559\n",
      "conv2.bias 0.0030447510071098804\n",
      "fc1.weight 0.000426404595375061\n",
      "fc1.bias 0.0018392837295929591\n",
      "fc2.weight 0.0012304612568446568\n",
      "fc2.bias 0.0020322629383632113\n",
      "fc3.weight 0.002288052581605457\n",
      "fc3.bias 0.0010850402526557446\n",
      "\n",
      "Test set: Average loss: 1.6193 \n",
      "Accuracy: 4227/10000 (42.27%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019607239299350315\n",
      "conv1.bias 0.0040830519671241445\n",
      "conv2.weight 0.0012296640872955322\n",
      "conv2.bias 0.0030403509736061096\n",
      "fc1.weight 0.0004261476198832194\n",
      "fc1.bias 0.0018413275480270385\n",
      "fc2.weight 0.0012311596719045488\n",
      "fc2.bias 0.002013565706355231\n",
      "fc3.weight 0.002293551252001808\n",
      "fc3.bias 0.0010644407942891122\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019607239299350315\n",
      "conv1.bias 0.0040830519671241445\n",
      "conv2.weight 0.0012296640872955322\n",
      "conv2.bias 0.0030403509736061096\n",
      "fc1.weight 0.0004261476198832194\n",
      "fc1.bias 0.0018413275480270385\n",
      "fc2.weight 0.0012311596719045488\n",
      "fc2.bias 0.002013565706355231\n",
      "fc3.weight 0.002293551252001808\n",
      "fc3.bias 0.0010644407942891122\n",
      "\n",
      "Test set: Average loss: 1.6179 \n",
      "Accuracy: 4233/10000 (42.33%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00196446074379815\n",
      "conv1.bias 0.0040558744221925735\n",
      "conv2.weight 0.001236901879310608\n",
      "conv2.bias 0.0030341334640979767\n",
      "fc1.weight 0.0004255573352177938\n",
      "fc1.bias 0.0018444336950778962\n",
      "fc2.weight 0.0012315629020569817\n",
      "fc2.bias 0.0019961709067935034\n",
      "fc3.weight 0.0022962362993331183\n",
      "fc3.bias 0.001047056633979082\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00196446074379815\n",
      "conv1.bias 0.0040558744221925735\n",
      "conv2.weight 0.001236901879310608\n",
      "conv2.bias 0.0030341334640979767\n",
      "fc1.weight 0.0004255573352177938\n",
      "fc1.bias 0.0018444336950778962\n",
      "fc2.weight 0.0012315629020569817\n",
      "fc2.bias 0.0019961709067935034\n",
      "fc3.weight 0.0022962362993331183\n",
      "fc3.bias 0.001047056633979082\n",
      "\n",
      "Test set: Average loss: 1.6156 \n",
      "Accuracy: 4243/10000 (42.43%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001970822016398112\n",
      "conv1.bias 0.004017635869483153\n",
      "conv2.weight 0.0012432903051376343\n",
      "conv2.bias 0.0030317206401377916\n",
      "fc1.weight 0.00042547353108723956\n",
      "fc1.bias 0.0018484783669312796\n",
      "fc2.weight 0.0012330658852107942\n",
      "fc2.bias 0.001984052714847383\n",
      "fc3.weight 0.0022998259181068056\n",
      "fc3.bias 0.0010348758660256863\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001970822016398112\n",
      "conv1.bias 0.004017635869483153\n",
      "conv2.weight 0.0012432903051376343\n",
      "conv2.bias 0.0030317206401377916\n",
      "fc1.weight 0.00042547353108723956\n",
      "fc1.bias 0.0018484783669312796\n",
      "fc2.weight 0.0012330658852107942\n",
      "fc2.bias 0.001984052714847383\n",
      "fc3.weight 0.0022998259181068056\n",
      "fc3.bias 0.0010348758660256863\n",
      "\n",
      "Test set: Average loss: 1.6142 \n",
      "Accuracy: 4248/10000 (42.48%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019780169592963324\n",
      "conv1.bias 0.003995116179188092\n",
      "conv2.weight 0.0012484222650527954\n",
      "conv2.bias 0.003028538078069687\n",
      "fc1.weight 0.00042544269561767576\n",
      "fc1.bias 0.0018511260549227396\n",
      "fc2.weight 0.001234141607133169\n",
      "fc2.bias 0.001976607633488519\n",
      "fc3.weight 0.0023040889274506342\n",
      "fc3.bias 0.001027398556470871\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019780169592963324\n",
      "conv1.bias 0.003995116179188092\n",
      "conv2.weight 0.0012484222650527954\n",
      "conv2.bias 0.003028538078069687\n",
      "fc1.weight 0.00042544269561767576\n",
      "fc1.bias 0.0018511260549227396\n",
      "fc2.weight 0.001234141607133169\n",
      "fc2.bias 0.001976607633488519\n",
      "fc3.weight 0.0023040889274506342\n",
      "fc3.bias 0.001027398556470871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.6143 \n",
      "Accuracy: 4253/10000 (42.53%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001985349390241835\n",
      "conv1.bias 0.0039871906240781145\n",
      "conv2.weight 0.001252311368783315\n",
      "conv2.bias 0.0030225543305277824\n",
      "fc1.weight 0.00042547714710235596\n",
      "fc1.bias 0.0018574825177590053\n",
      "fc2.weight 0.0012355556563725548\n",
      "fc2.bias 0.0019663751480125244\n",
      "fc3.weight 0.002306251298813593\n",
      "fc3.bias 0.001021323073655367\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001985349390241835\n",
      "conv1.bias 0.0039871906240781145\n",
      "conv2.weight 0.001252311368783315\n",
      "conv2.bias 0.0030225543305277824\n",
      "fc1.weight 0.00042547714710235596\n",
      "fc1.bias 0.0018574825177590053\n",
      "fc2.weight 0.0012355556563725548\n",
      "fc2.bias 0.0019663751480125244\n",
      "fc3.weight 0.002306251298813593\n",
      "fc3.bias 0.001021323073655367\n",
      "\n",
      "Test set: Average loss: 1.6136 \n",
      "Accuracy: 4252/10000 (42.52%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019939475589328343\n",
      "conv1.bias 0.003974707797169685\n",
      "conv2.weight 0.001254809300104777\n",
      "conv2.bias 0.003010490909218788\n",
      "fc1.weight 0.0004253844420115153\n",
      "fc1.bias 0.001861762503782908\n",
      "fc2.weight 0.0012368572136712453\n",
      "fc2.bias 0.0019572398492268155\n",
      "fc3.weight 0.002308838140396845\n",
      "fc3.bias 0.0010150675661861897\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019939475589328343\n",
      "conv1.bias 0.003974707797169685\n",
      "conv2.weight 0.001254809300104777\n",
      "conv2.bias 0.003010490909218788\n",
      "fc1.weight 0.0004253844420115153\n",
      "fc1.bias 0.001861762503782908\n",
      "fc2.weight 0.0012368572136712453\n",
      "fc2.bias 0.0019572398492268155\n",
      "fc3.weight 0.002308838140396845\n",
      "fc3.bias 0.0010150675661861897\n",
      "\n",
      "Test set: Average loss: 1.6141 \n",
      "Accuracy: 4250/10000 (42.50%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002001820802688599\n",
      "conv1.bias 0.003950572883089383\n",
      "conv2.weight 0.0012573814392089844\n",
      "conv2.bias 0.0030106191989034414\n",
      "fc1.weight 0.00042528355121612547\n",
      "fc1.bias 0.0018681677679220836\n",
      "fc2.weight 0.0012378441909002879\n",
      "fc2.bias 0.0019523732009388152\n",
      "fc3.weight 0.0023124003694171\n",
      "fc3.bias 0.0010106626898050309\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002001820802688599\n",
      "conv1.bias 0.003950572883089383\n",
      "conv2.weight 0.0012573814392089844\n",
      "conv2.bias 0.0030106191989034414\n",
      "fc1.weight 0.00042528355121612547\n",
      "fc1.bias 0.0018681677679220836\n",
      "fc2.weight 0.0012378441909002879\n",
      "fc2.bias 0.0019523732009388152\n",
      "fc3.weight 0.0023124003694171\n",
      "fc3.bias 0.0010106626898050309\n",
      "\n",
      "Test set: Average loss: 1.6139 \n",
      "Accuracy: 4247/10000 (42.47%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020130956172943115\n",
      "conv1.bias 0.003937937008837859\n",
      "conv2.weight 0.001260523796081543\n",
      "conv2.bias 0.0030098501592874527\n",
      "fc1.weight 0.0004252347548802694\n",
      "fc1.bias 0.0018742814660072326\n",
      "fc2.weight 0.001239037135290721\n",
      "fc2.bias 0.0019492657766455696\n",
      "fc3.weight 0.002314568701244536\n",
      "fc3.bias 0.0010070092976093292\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020130956172943115\n",
      "conv1.bias 0.003937937008837859\n",
      "conv2.weight 0.001260523796081543\n",
      "conv2.bias 0.0030098501592874527\n",
      "fc1.weight 0.0004252347548802694\n",
      "fc1.bias 0.0018742814660072326\n",
      "fc2.weight 0.001239037135290721\n",
      "fc2.bias 0.0019492657766455696\n",
      "fc3.weight 0.002314568701244536\n",
      "fc3.bias 0.0010070092976093292\n",
      "\n",
      "Test set: Average loss: 1.6132 \n",
      "Accuracy: 4247/10000 (42.47%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002022819783952501\n",
      "conv1.bias 0.003926188995440801\n",
      "conv2.weight 0.0012623726328214009\n",
      "conv2.bias 0.003012122353538871\n",
      "fc1.weight 0.0004251300493876139\n",
      "fc1.bias 0.0018829995145400364\n",
      "fc2.weight 0.0012400627136230468\n",
      "fc2.bias 0.001946209619442622\n",
      "fc3.weight 0.002315987859453474\n",
      "fc3.bias 0.0010059501975774765\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002022819783952501\n",
      "conv1.bias 0.003926188995440801\n",
      "conv2.weight 0.0012623726328214009\n",
      "conv2.bias 0.003012122353538871\n",
      "fc1.weight 0.0004251300493876139\n",
      "fc1.bias 0.0018829995145400364\n",
      "fc2.weight 0.0012400627136230468\n",
      "fc2.bias 0.001946209619442622\n",
      "fc3.weight 0.002315987859453474\n",
      "fc3.bias 0.0010059501975774765\n",
      "\n",
      "Test set: Average loss: 1.6127 \n",
      "Accuracy: 4256/10000 (42.56%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020329472753736707\n",
      "conv1.bias 0.0039345985278487206\n",
      "conv2.weight 0.0012652417023976644\n",
      "conv2.bias 0.0030127328354865313\n",
      "fc1.weight 0.0004254602591196696\n",
      "fc1.bias 0.001891558865706126\n",
      "fc2.weight 0.0012416754450116838\n",
      "fc2.bias 0.0019413183132807414\n",
      "fc3.weight 0.0023187618880044845\n",
      "fc3.bias 0.0010060392320156097\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020329472753736707\n",
      "conv1.bias 0.0039345985278487206\n",
      "conv2.weight 0.0012652417023976644\n",
      "conv2.bias 0.0030127328354865313\n",
      "fc1.weight 0.0004254602591196696\n",
      "fc1.bias 0.001891558865706126\n",
      "fc2.weight 0.0012416754450116838\n",
      "fc2.bias 0.0019413183132807414\n",
      "fc3.weight 0.0023187618880044845\n",
      "fc3.bias 0.0010060392320156097\n",
      "\n",
      "Test set: Average loss: 1.6123 \n",
      "Accuracy: 4259/10000 (42.59%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020409748289320203\n",
      "conv1.bias 0.003930268498758475\n",
      "conv2.weight 0.0012673847874005635\n",
      "conv2.bias 0.0030130124650895596\n",
      "fc1.weight 0.0004252451260884603\n",
      "fc1.bias 0.001901916911204656\n",
      "fc2.weight 0.0012418608816843184\n",
      "fc2.bias 0.001942181693656104\n",
      "fc3.weight 0.0023212364741734097\n",
      "fc3.bias 0.0010057535022497178\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020409748289320203\n",
      "conv1.bias 0.003930268498758475\n",
      "conv2.weight 0.0012673847874005635\n",
      "conv2.bias 0.0030130124650895596\n",
      "fc1.weight 0.0004252451260884603\n",
      "fc1.bias 0.001901916911204656\n",
      "fc2.weight 0.0012418608816843184\n",
      "fc2.bias 0.001942181693656104\n",
      "fc3.weight 0.0023212364741734097\n",
      "fc3.bias 0.0010057535022497178\n",
      "\n",
      "Test set: Average loss: 1.6134 \n",
      "Accuracy: 4264/10000 (42.64%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020504726303948295\n",
      "conv1.bias 0.003934277842442195\n",
      "conv2.weight 0.0012692787249883016\n",
      "conv2.bias 0.003012116299942136\n",
      "fc1.weight 0.00042532761891682944\n",
      "fc1.bias 0.0019136587778727213\n",
      "fc2.weight 0.001242725622086298\n",
      "fc2.bias 0.0019420042988799867\n",
      "fc3.weight 0.002323507411139352\n",
      "fc3.bias 0.0010071792639791965\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020504726303948295\n",
      "conv1.bias 0.003934277842442195\n",
      "conv2.weight 0.0012692787249883016\n",
      "conv2.bias 0.003012116299942136\n",
      "fc1.weight 0.00042532761891682944\n",
      "fc1.bias 0.0019136587778727213\n",
      "fc2.weight 0.001242725622086298\n",
      "fc2.bias 0.0019420042988799867\n",
      "fc3.weight 0.002323507411139352\n",
      "fc3.bias 0.0010071792639791965\n",
      "\n",
      "Test set: Average loss: 1.6135 \n",
      "Accuracy: 4258/10000 (42.58%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002059986326429579\n",
      "conv1.bias 0.003941443438331286\n",
      "conv2.weight 0.0012711783250172932\n",
      "conv2.bias 0.0030218232423067093\n",
      "fc1.weight 0.0004255587259928385\n",
      "fc1.bias 0.0019234391550223032\n",
      "fc2.weight 0.0012439427867768304\n",
      "fc2.bias 0.0019391771583330063\n",
      "fc3.weight 0.002328888291404361\n",
      "fc3.bias 0.0010046346113085746\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002059986326429579\n",
      "conv1.bias 0.003941443438331286\n",
      "conv2.weight 0.0012711783250172932\n",
      "conv2.bias 0.0030218232423067093\n",
      "fc1.weight 0.0004255587259928385\n",
      "fc1.bias 0.0019234391550223032\n",
      "fc2.weight 0.0012439427867768304\n",
      "fc2.bias 0.0019391771583330063\n",
      "fc3.weight 0.002328888291404361\n",
      "fc3.bias 0.0010046346113085746\n",
      "\n",
      "Test set: Average loss: 1.6125 \n",
      "Accuracy: 4261/10000 (42.61%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020697634749942354\n",
      "conv1.bias 0.003947945932547252\n",
      "conv2.weight 0.001272795597712199\n",
      "conv2.bias 0.0030236593447625637\n",
      "fc1.weight 0.00042605920632680255\n",
      "fc1.bias 0.0019319212685028713\n",
      "fc2.weight 0.0012451077264452738\n",
      "fc2.bias 0.0019355742704300653\n",
      "fc3.weight 0.002332287033398946\n",
      "fc3.bias 0.0010046119801700114\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020697634749942354\n",
      "conv1.bias 0.003947945932547252\n",
      "conv2.weight 0.001272795597712199\n",
      "conv2.bias 0.0030236593447625637\n",
      "fc1.weight 0.00042605920632680255\n",
      "fc1.bias 0.0019319212685028713\n",
      "fc2.weight 0.0012451077264452738\n",
      "fc2.bias 0.0019355742704300653\n",
      "fc3.weight 0.002332287033398946\n",
      "fc3.bias 0.0010046119801700114\n",
      "\n",
      "Test set: Average loss: 1.6122 \n",
      "Accuracy: 4256/10000 (42.56%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020783889293670655\n",
      "conv1.bias 0.003948291453222434\n",
      "conv2.weight 0.001274799903233846\n",
      "conv2.bias 0.003021061886101961\n",
      "fc1.weight 0.0004263722896575928\n",
      "fc1.bias 0.0019419106344381969\n",
      "fc2.weight 0.0012457045297774058\n",
      "fc2.bias 0.0019322168968972705\n",
      "fc3.weight 0.002334537392570859\n",
      "fc3.bias 0.0010015773586928844\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020783889293670655\n",
      "conv1.bias 0.003948291453222434\n",
      "conv2.weight 0.001274799903233846\n",
      "conv2.bias 0.003021061886101961\n",
      "fc1.weight 0.0004263722896575928\n",
      "fc1.bias 0.0019419106344381969\n",
      "fc2.weight 0.0012457045297774058\n",
      "fc2.bias 0.0019322168968972705\n",
      "fc3.weight 0.002334537392570859\n",
      "fc3.bias 0.0010015773586928844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.6113 \n",
      "Accuracy: 4257/10000 (42.57%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020865628454420303\n",
      "conv1.bias 0.003981519490480423\n",
      "conv2.weight 0.0012780530254046121\n",
      "conv2.bias 0.003024316392838955\n",
      "fc1.weight 0.00042677577336629234\n",
      "fc1.bias 0.0019537687301635744\n",
      "fc2.weight 0.0012462912097809806\n",
      "fc2.bias 0.001934131518715904\n",
      "fc3.weight 0.0023360996019272575\n",
      "fc3.bias 0.0010004233568906783\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020865628454420303\n",
      "conv1.bias 0.003981519490480423\n",
      "conv2.weight 0.0012780530254046121\n",
      "conv2.bias 0.003024316392838955\n",
      "fc1.weight 0.00042677577336629234\n",
      "fc1.bias 0.0019537687301635744\n",
      "fc2.weight 0.0012462912097809806\n",
      "fc2.bias 0.001934131518715904\n",
      "fc3.weight 0.0023360996019272575\n",
      "fc3.bias 0.0010004233568906783\n",
      "\n",
      "Test set: Average loss: 1.6102 \n",
      "Accuracy: 4259/10000 (42.59%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020952455202738443\n",
      "conv1.bias 0.0039873675753672915\n",
      "conv2.weight 0.0012810722986857097\n",
      "conv2.bias 0.0030325278639793396\n",
      "fc1.weight 0.0004272246360778809\n",
      "fc1.bias 0.001961570729811986\n",
      "fc2.weight 0.0012465529971652561\n",
      "fc2.bias 0.0019317927459875743\n",
      "fc3.weight 0.0023367410614376976\n",
      "fc3.bias 0.0010000692680478096\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020952455202738443\n",
      "conv1.bias 0.0039873675753672915\n",
      "conv2.weight 0.0012810722986857097\n",
      "conv2.bias 0.0030325278639793396\n",
      "fc1.weight 0.0004272246360778809\n",
      "fc1.bias 0.001961570729811986\n",
      "fc2.weight 0.0012465529971652561\n",
      "fc2.bias 0.0019317927459875743\n",
      "fc3.weight 0.0023367410614376976\n",
      "fc3.bias 0.0010000692680478096\n",
      "\n",
      "Test set: Average loss: 1.6104 \n",
      "Accuracy: 4264/10000 (42.64%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021042267481486\n",
      "conv1.bias 0.00400758037964503\n",
      "conv2.weight 0.0012843408187230428\n",
      "conv2.bias 0.0030360964592546225\n",
      "fc1.weight 0.00042754697799682616\n",
      "fc1.bias 0.001969543844461441\n",
      "fc2.weight 0.001246632280803862\n",
      "fc2.bias 0.0019311103082838513\n",
      "fc3.weight 0.0023368697790872484\n",
      "fc3.bias 0.0010017748922109605\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021042267481486\n",
      "conv1.bias 0.00400758037964503\n",
      "conv2.weight 0.0012843408187230428\n",
      "conv2.bias 0.0030360964592546225\n",
      "fc1.weight 0.00042754697799682616\n",
      "fc1.bias 0.001969543844461441\n",
      "fc2.weight 0.001246632280803862\n",
      "fc2.bias 0.0019311103082838513\n",
      "fc3.weight 0.0023368697790872484\n",
      "fc3.bias 0.0010017748922109605\n",
      "\n",
      "Test set: Average loss: 1.6085 \n",
      "Accuracy: 4282/10000 (42.82%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021124351024627685\n",
      "conv1.bias 0.004019608721137047\n",
      "conv2.weight 0.00128776748975118\n",
      "conv2.bias 0.0030395053327083588\n",
      "fc1.weight 0.00042795010407765705\n",
      "fc1.bias 0.0019761322687069575\n",
      "fc2.weight 0.001246732757205055\n",
      "fc2.bias 0.0019302758432569959\n",
      "fc3.weight 0.0023378744011833554\n",
      "fc3.bias 0.0010009379126131534\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021124351024627685\n",
      "conv1.bias 0.004019608721137047\n",
      "conv2.weight 0.00128776748975118\n",
      "conv2.bias 0.0030395053327083588\n",
      "fc1.weight 0.00042795010407765705\n",
      "fc1.bias 0.0019761322687069575\n",
      "fc2.weight 0.001246732757205055\n",
      "fc2.bias 0.0019302758432569959\n",
      "fc3.weight 0.0023378744011833554\n",
      "fc3.bias 0.0010009379126131534\n",
      "\n",
      "Test set: Average loss: 1.6096 \n",
      "Accuracy: 4281/10000 (42.81%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002118168142106798\n",
      "conv1.bias 0.004036044701933861\n",
      "conv2.weight 0.001291276216506958\n",
      "conv2.bias 0.0030286344699561596\n",
      "fc1.weight 0.0004278964598973592\n",
      "fc1.bias 0.0019800012310345967\n",
      "fc2.weight 0.0012456105815039742\n",
      "fc2.bias 0.0019274428486824036\n",
      "fc3.weight 0.0023361311072394963\n",
      "fc3.bias 0.00100048603489995\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002118168142106798\n",
      "conv1.bias 0.004036044701933861\n",
      "conv2.weight 0.001291276216506958\n",
      "conv2.bias 0.0030286344699561596\n",
      "fc1.weight 0.0004278964598973592\n",
      "fc1.bias 0.0019800012310345967\n",
      "fc2.weight 0.0012456105815039742\n",
      "fc2.bias 0.0019274428486824036\n",
      "fc3.weight 0.0023361311072394963\n",
      "fc3.bias 0.00100048603489995\n",
      "\n",
      "Test set: Average loss: 1.6091 \n",
      "Accuracy: 4291/10000 (42.91%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021249756548139785\n",
      "conv1.bias 0.004058504787584146\n",
      "conv2.weight 0.0012958750128746032\n",
      "conv2.bias 0.0030303369276225567\n",
      "fc1.weight 0.0004282589356104533\n",
      "fc1.bias 0.0019882604479789733\n",
      "fc2.weight 0.0012452863511585054\n",
      "fc2.bias 0.0019259804061480931\n",
      "fc3.weight 0.002336108258792332\n",
      "fc3.bias 0.0010028084740042686\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021249756548139785\n",
      "conv1.bias 0.004058504787584146\n",
      "conv2.weight 0.0012958750128746032\n",
      "conv2.bias 0.0030303369276225567\n",
      "fc1.weight 0.0004282589356104533\n",
      "fc1.bias 0.0019882604479789733\n",
      "fc2.weight 0.0012452863511585054\n",
      "fc2.bias 0.0019259804061480931\n",
      "fc3.weight 0.002336108258792332\n",
      "fc3.bias 0.0010028084740042686\n",
      "\n",
      "Test set: Average loss: 1.6073 \n",
      "Accuracy: 4302/10000 (43.02%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002130145894156562\n",
      "conv1.bias 0.004089649145801862\n",
      "conv2.weight 0.0013009519378344217\n",
      "conv2.bias 0.003035479225218296\n",
      "fc1.weight 0.00042816062768300374\n",
      "fc1.bias 0.001997332771619161\n",
      "fc2.weight 0.0012448032697041829\n",
      "fc2.bias 0.0019254741214570544\n",
      "fc3.weight 0.00233503140154339\n",
      "fc3.bias 0.001002437248826027\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002130145894156562\n",
      "conv1.bias 0.004089649145801862\n",
      "conv2.weight 0.0013009519378344217\n",
      "conv2.bias 0.003035479225218296\n",
      "fc1.weight 0.00042816062768300374\n",
      "fc1.bias 0.001997332771619161\n",
      "fc2.weight 0.0012448032697041829\n",
      "fc2.bias 0.0019254741214570544\n",
      "fc3.weight 0.00233503140154339\n",
      "fc3.bias 0.001002437248826027\n",
      "\n",
      "Test set: Average loss: 1.6077 \n",
      "Accuracy: 4307/10000 (43.07%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021378917164272732\n",
      "conv1.bias 0.004110625945031643\n",
      "conv2.weight 0.0013046873609224956\n",
      "conv2.bias 0.0030408776365220547\n",
      "fc1.weight 0.0004282972812652588\n",
      "fc1.bias 0.0020032225797573726\n",
      "fc2.weight 0.001244512436881898\n",
      "fc2.bias 0.0019260368176868983\n",
      "fc3.weight 0.002335407052721296\n",
      "fc3.bias 0.0010033798404037952\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021378917164272732\n",
      "conv1.bias 0.004110625945031643\n",
      "conv2.weight 0.0013046873609224956\n",
      "conv2.bias 0.0030408776365220547\n",
      "fc1.weight 0.0004282972812652588\n",
      "fc1.bias 0.0020032225797573726\n",
      "fc2.weight 0.001244512436881898\n",
      "fc2.bias 0.0019260368176868983\n",
      "fc3.weight 0.002335407052721296\n",
      "fc3.bias 0.0010033798404037952\n",
      "\n",
      "Test set: Average loss: 1.6078 \n",
      "Accuracy: 4304/10000 (43.04%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002144818041059706\n",
      "conv1.bias 0.004130149570604165\n",
      "conv2.weight 0.0013079804182052613\n",
      "conv2.bias 0.0030365148559212685\n",
      "fc1.weight 0.0004283783435821533\n",
      "fc1.bias 0.0020116942624251047\n",
      "fc2.weight 0.0012435341638232034\n",
      "fc2.bias 0.0019253300768988474\n",
      "fc3.weight 0.0023346852688562302\n",
      "fc3.bias 0.0010051467455923558\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002144818041059706\n",
      "conv1.bias 0.004130149570604165\n",
      "conv2.weight 0.0013079804182052613\n",
      "conv2.bias 0.0030365148559212685\n",
      "fc1.weight 0.0004283783435821533\n",
      "fc1.bias 0.0020116942624251047\n",
      "fc2.weight 0.0012435341638232034\n",
      "fc2.bias 0.0019253300768988474\n",
      "fc3.weight 0.0023346852688562302\n",
      "fc3.bias 0.0010051467455923558\n",
      "\n",
      "Test set: Average loss: 1.6078 \n",
      "Accuracy: 4311/10000 (43.11%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002151845826043023\n",
      "conv1.bias 0.004154904745519161\n",
      "conv2.weight 0.0013109570741653442\n",
      "conv2.bias 0.0030366014689207077\n",
      "fc1.weight 0.00042826493581136067\n",
      "fc1.bias 0.0020200132081906\n",
      "fc2.weight 0.0012429088827163454\n",
      "fc2.bias 0.001924384030557814\n",
      "fc3.weight 0.002332694757552374\n",
      "fc3.bias 0.0010076352395117284\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002151845826043023\n",
      "conv1.bias 0.004154904745519161\n",
      "conv2.weight 0.0013109570741653442\n",
      "conv2.bias 0.0030366014689207077\n",
      "fc1.weight 0.00042826493581136067\n",
      "fc1.bias 0.0020200132081906\n",
      "fc2.weight 0.0012429088827163454\n",
      "fc2.bias 0.001924384030557814\n",
      "fc3.weight 0.002332694757552374\n",
      "fc3.bias 0.0010076352395117284\n",
      "\n",
      "Test set: Average loss: 1.6060 \n",
      "Accuracy: 4320/10000 (43.20%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021579419242011176\n",
      "conv1.bias 0.004207818458477656\n",
      "conv2.weight 0.001313981314500173\n",
      "conv2.bias 0.0030335714109241962\n",
      "fc1.weight 0.0004283066987991333\n",
      "fc1.bias 0.002026927967866262\n",
      "fc2.weight 0.0012424934478033156\n",
      "fc2.bias 0.0019254079532055627\n",
      "fc3.weight 0.002332601376942226\n",
      "fc3.bias 0.0010058771818876266\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021579419242011176\n",
      "conv1.bias 0.004207818458477656\n",
      "conv2.weight 0.001313981314500173\n",
      "conv2.bias 0.0030335714109241962\n",
      "fc1.weight 0.0004283066987991333\n",
      "fc1.bias 0.002026927967866262\n",
      "fc2.weight 0.0012424934478033156\n",
      "fc2.bias 0.0019254079532055627\n",
      "fc3.weight 0.002332601376942226\n",
      "fc3.bias 0.0010058771818876266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.6066 \n",
      "Accuracy: 4321/10000 (43.21%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021628603670332167\n",
      "conv1.bias 0.004232822917401791\n",
      "conv2.weight 0.0013162465890248617\n",
      "conv2.bias 0.0030311881564557552\n",
      "fc1.weight 0.0004281757672627767\n",
      "fc1.bias 0.0020319387316703797\n",
      "fc2.weight 0.0012420713901519775\n",
      "fc2.bias 0.001923813528957821\n",
      "fc3.weight 0.002332566465650286\n",
      "fc3.bias 0.0010060954838991166\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021628603670332167\n",
      "conv1.bias 0.004232822917401791\n",
      "conv2.weight 0.0013162465890248617\n",
      "conv2.bias 0.0030311881564557552\n",
      "fc1.weight 0.0004281757672627767\n",
      "fc1.bias 0.0020319387316703797\n",
      "fc2.weight 0.0012420713901519775\n",
      "fc2.bias 0.001923813528957821\n",
      "fc3.weight 0.002332566465650286\n",
      "fc3.bias 0.0010060954838991166\n",
      "\n",
      "Test set: Average loss: 1.6061 \n",
      "Accuracy: 4320/10000 (43.20%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002170461548699273\n",
      "conv1.bias 0.004270423203706741\n",
      "conv2.weight 0.001318687895933787\n",
      "conv2.bias 0.0030280309729278088\n",
      "fc1.weight 0.00042805484930674237\n",
      "fc1.bias 0.002036656066775322\n",
      "fc2.weight 0.0012404813652946835\n",
      "fc2.bias 0.0019208668243317377\n",
      "fc3.weight 0.0023317866382144745\n",
      "fc3.bias 0.0010058686137199401\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002170461548699273\n",
      "conv1.bias 0.004270423203706741\n",
      "conv2.weight 0.001318687895933787\n",
      "conv2.bias 0.0030280309729278088\n",
      "fc1.weight 0.00042805484930674237\n",
      "fc1.bias 0.002036656066775322\n",
      "fc2.weight 0.0012404813652946835\n",
      "fc2.bias 0.0019208668243317377\n",
      "fc3.weight 0.0023317866382144745\n",
      "fc3.bias 0.0010058686137199401\n",
      "\n",
      "Test set: Average loss: 1.6062 \n",
      "Accuracy: 4322/10000 (43.22%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021778493457370336\n",
      "conv1.bias 0.004307473699251811\n",
      "conv2.weight 0.0013217211763064066\n",
      "conv2.bias 0.003030146937817335\n",
      "fc1.weight 0.0004282370408376058\n",
      "fc1.bias 0.0020438638826211293\n",
      "fc2.weight 0.0012399111475263323\n",
      "fc2.bias 0.001920373844248908\n",
      "fc3.weight 0.002331317322594779\n",
      "fc3.bias 0.0010045874863862992\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021778493457370336\n",
      "conv1.bias 0.004307473699251811\n",
      "conv2.weight 0.0013217211763064066\n",
      "conv2.bias 0.003030146937817335\n",
      "fc1.weight 0.0004282370408376058\n",
      "fc1.bias 0.0020438638826211293\n",
      "fc2.weight 0.0012399111475263323\n",
      "fc2.bias 0.001920373844248908\n",
      "fc3.weight 0.002331317322594779\n",
      "fc3.bias 0.0010045874863862992\n",
      "\n",
      "Test set: Average loss: 1.6073 \n",
      "Accuracy: 4317/10000 (43.17%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021833528412712943\n",
      "conv1.bias 0.004343426786363125\n",
      "conv2.weight 0.0013229194283485412\n",
      "conv2.bias 0.00303122540935874\n",
      "fc1.weight 0.0004282232920328776\n",
      "fc1.bias 0.0020499863972266515\n",
      "fc2.weight 0.0012397686640421548\n",
      "fc2.bias 0.0019205334995474135\n",
      "fc3.weight 0.0023314605156580606\n",
      "fc3.bias 0.0010035190731287002\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021833528412712943\n",
      "conv1.bias 0.004343426786363125\n",
      "conv2.weight 0.0013229194283485412\n",
      "conv2.bias 0.00303122540935874\n",
      "fc1.weight 0.0004282232920328776\n",
      "fc1.bias 0.0020499863972266515\n",
      "fc2.weight 0.0012397686640421548\n",
      "fc2.bias 0.0019205334995474135\n",
      "fc3.weight 0.0023314605156580606\n",
      "fc3.bias 0.0010035190731287002\n",
      "\n",
      "Test set: Average loss: 1.6067 \n",
      "Accuracy: 4324/10000 (43.24%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021897440486484104\n",
      "conv1.bias 0.004392610241969426\n",
      "conv2.weight 0.0013252889116605123\n",
      "conv2.bias 0.003022497519850731\n",
      "fc1.weight 0.0004283783435821533\n",
      "fc1.bias 0.002054453765352567\n",
      "fc2.weight 0.0012396047985742962\n",
      "fc2.bias 0.0019203004028115953\n",
      "fc3.weight 0.002332637139729091\n",
      "fc3.bias 0.0010039974004030228\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021897440486484104\n",
      "conv1.bias 0.004392610241969426\n",
      "conv2.weight 0.0013252889116605123\n",
      "conv2.bias 0.003022497519850731\n",
      "fc1.weight 0.0004283783435821533\n",
      "fc1.bias 0.002054453765352567\n",
      "fc2.weight 0.0012396047985742962\n",
      "fc2.bias 0.0019203004028115953\n",
      "fc3.weight 0.002332637139729091\n",
      "fc3.bias 0.0010039974004030228\n",
      "\n",
      "Test set: Average loss: 1.6064 \n",
      "Accuracy: 4327/10000 (43.27%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002195522387822469\n",
      "conv1.bias 0.004445217239360015\n",
      "conv2.weight 0.0013263249397277833\n",
      "conv2.bias 0.003018391551449895\n",
      "fc1.weight 0.00042861465613047283\n",
      "fc1.bias 0.0020607341080904006\n",
      "fc2.weight 0.0012394346888103182\n",
      "fc2.bias 0.0019205471589451744\n",
      "fc3.weight 0.0023326723348526725\n",
      "fc3.bias 0.001004541665315628\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002195522387822469\n",
      "conv1.bias 0.004445217239360015\n",
      "conv2.weight 0.0013263249397277833\n",
      "conv2.bias 0.003018391551449895\n",
      "fc1.weight 0.00042861465613047283\n",
      "fc1.bias 0.0020607341080904006\n",
      "fc2.weight 0.0012394346888103182\n",
      "fc2.bias 0.0019205471589451744\n",
      "fc3.weight 0.0023326723348526725\n",
      "fc3.bias 0.001004541665315628\n",
      "\n",
      "Test set: Average loss: 1.6075 \n",
      "Accuracy: 4321/10000 (43.21%)\n",
      "\n",
      "##########################################\n",
      "Learning Rate = 0.0001\n",
      "\n",
      "\n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 12500 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 12500 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 12500 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 12500 \n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.004522545602586535\n",
      "conv1.bias 0.004613685111204783\n",
      "conv2.weight 0.002255882422129313\n",
      "conv2.bias 0.0018020905554294586\n",
      "fc1.weight 0.0008355108102162679\n",
      "fc1.bias 0.00086956520875295\n",
      "fc2.weight 0.0027452353447202653\n",
      "fc2.bias 0.002933568720306669\n",
      "fc3.weight 0.00393795683270409\n",
      "fc3.bias 0.002452295646071434\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.004522545602586535\n",
      "conv1.bias 0.004613685111204783\n",
      "conv2.weight 0.002255882422129313\n",
      "conv2.bias 0.0018020905554294586\n",
      "fc1.weight 0.0008355108102162679\n",
      "fc1.bias 0.00086956520875295\n",
      "fc2.weight 0.0027452353447202653\n",
      "fc2.bias 0.002933568720306669\n",
      "fc3.weight 0.00393795683270409\n",
      "fc3.bias 0.002452295646071434\n",
      "\n",
      "Test set: Average loss: 1.9493 \n",
      "Accuracy: 2900/10000 (29.00%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0016842118899027506\n",
      "conv1.bias 0.002615834896763166\n",
      "conv2.weight 0.0008658709128697713\n",
      "conv2.bias 0.0009661234216764569\n",
      "fc1.weight 0.00014378132422765096\n",
      "fc1.bias 0.00032226039717594783\n",
      "fc2.weight 0.00037917397798053803\n",
      "fc2.bias 0.0007135102170563879\n",
      "fc3.weight 0.0009966071162905011\n",
      "fc3.bias 0.0006158805917948484\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0016842118899027506\n",
      "conv1.bias 0.002615834896763166\n",
      "conv2.weight 0.0008658709128697713\n",
      "conv2.bias 0.0009661234216764569\n",
      "fc1.weight 0.00014378132422765096\n",
      "fc1.bias 0.00032226039717594783\n",
      "fc2.weight 0.00037917397798053803\n",
      "fc2.bias 0.0007135102170563879\n",
      "fc3.weight 0.0009966071162905011\n",
      "fc3.bias 0.0006158805917948484\n",
      "\n",
      "Test set: Average loss: 1.8888 \n",
      "Accuracy: 3062/10000 (30.62%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0011814271079169378\n",
      "conv1.bias 0.002138763045271238\n",
      "conv2.weight 0.0007694068551063537\n",
      "conv2.bias 0.0006259523797780275\n",
      "fc1.weight 0.00016659486293792725\n",
      "fc1.bias 0.0003382576008637746\n",
      "fc2.weight 0.0003118194048366849\n",
      "fc2.bias 0.0004761504630247752\n",
      "fc3.weight 0.0005685280831087203\n",
      "fc3.bias 0.0002560969442129135\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0011814271079169378\n",
      "conv1.bias 0.002138763045271238\n",
      "conv2.weight 0.0007694068551063537\n",
      "conv2.bias 0.0006259523797780275\n",
      "fc1.weight 0.00016659486293792725\n",
      "fc1.bias 0.0003382576008637746\n",
      "fc2.weight 0.0003118194048366849\n",
      "fc2.bias 0.0004761504630247752\n",
      "fc3.weight 0.0005685280831087203\n",
      "fc3.bias 0.0002560969442129135\n",
      "\n",
      "Test set: Average loss: 1.8788 \n",
      "Accuracy: 3076/10000 (30.76%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0010960345135794746\n",
      "conv1.bias 0.0017916498084863026\n",
      "conv2.weight 0.0006732215980688731\n",
      "conv2.bias 0.0005045227590017021\n",
      "fc1.weight 0.0001849700609842936\n",
      "fc1.bias 0.0003945199151833852\n",
      "fc2.weight 0.0003759344418843587\n",
      "fc2.bias 0.0005858090395728747\n",
      "fc3.weight 0.0004752947815826961\n",
      "fc3.bias 0.00020918499212712048\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0010960345135794746\n",
      "conv1.bias 0.0017916498084863026\n",
      "conv2.weight 0.0006732215980688731\n",
      "conv2.bias 0.0005045227590017021\n",
      "fc1.weight 0.0001849700609842936\n",
      "fc1.bias 0.0003945199151833852\n",
      "fc2.weight 0.0003759344418843587\n",
      "fc2.bias 0.0005858090395728747\n",
      "fc3.weight 0.0004752947815826961\n",
      "fc3.bias 0.00020918499212712048\n",
      "\n",
      "Test set: Average loss: 1.8780 \n",
      "Accuracy: 3093/10000 (30.93%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0010019915633731418\n",
      "conv1.bias 0.0015912467303375404\n",
      "conv2.weight 0.0006062085429827372\n",
      "conv2.bias 0.0005206484347581863\n",
      "fc1.weight 0.00019820082187652587\n",
      "fc1.bias 0.0004529726381103198\n",
      "fc2.weight 0.00044906290750654915\n",
      "fc2.bias 0.0007180345821238699\n",
      "fc3.weight 0.00047662066561835153\n",
      "fc3.bias 0.000245605013333261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected users: [0 1]\n",
      "conv1.weight 0.0010019915633731418\n",
      "conv1.bias 0.0015912467303375404\n",
      "conv2.weight 0.0006062085429827372\n",
      "conv2.bias 0.0005206484347581863\n",
      "fc1.weight 0.00019820082187652587\n",
      "fc1.bias 0.0004529726381103198\n",
      "fc2.weight 0.00044906290750654915\n",
      "fc2.bias 0.0007180345821238699\n",
      "fc3.weight 0.00047662066561835153\n",
      "fc3.bias 0.000245605013333261\n",
      "\n",
      "Test set: Average loss: 1.8787 \n",
      "Accuracy: 3099/10000 (30.99%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0008946618768903944\n",
      "conv1.bias 0.0014858466262618701\n",
      "conv2.weight 0.0005687315265337626\n",
      "conv2.bias 0.0005766261601820588\n",
      "fc1.weight 0.00020881307125091554\n",
      "fc1.bias 0.0005083927263816198\n",
      "fc2.weight 0.0005082405275768704\n",
      "fc2.bias 0.000800465898854392\n",
      "fc3.weight 0.0005038786502111525\n",
      "fc3.bias 0.0002924830187112093\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0008946618768903944\n",
      "conv1.bias 0.0014858466262618701\n",
      "conv2.weight 0.0005687315265337626\n",
      "conv2.bias 0.0005766261601820588\n",
      "fc1.weight 0.00020881307125091554\n",
      "fc1.bias 0.0005083927263816198\n",
      "fc2.weight 0.0005082405275768704\n",
      "fc2.bias 0.000800465898854392\n",
      "fc3.weight 0.0005038786502111525\n",
      "fc3.bias 0.0002924830187112093\n",
      "\n",
      "Test set: Average loss: 1.8811 \n",
      "Accuracy: 3099/10000 (30.99%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0007892205317815145\n",
      "conv1.bias 0.0014563960333665211\n",
      "conv2.weight 0.0005543362100919088\n",
      "conv2.bias 0.0006583657232113183\n",
      "fc1.weight 0.0002167235811551412\n",
      "fc1.bias 0.0005625762666265169\n",
      "fc2.weight 0.0005489490808002533\n",
      "fc2.bias 0.0008459583457027163\n",
      "fc3.weight 0.0005336475869019826\n",
      "fc3.bias 0.000334259239025414\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0007892205317815145\n",
      "conv1.bias 0.0014563960333665211\n",
      "conv2.weight 0.0005543362100919088\n",
      "conv2.bias 0.0006583657232113183\n",
      "fc1.weight 0.0002167235811551412\n",
      "fc1.bias 0.0005625762666265169\n",
      "fc2.weight 0.0005489490808002533\n",
      "fc2.bias 0.0008459583457027163\n",
      "fc3.weight 0.0005336475869019826\n",
      "fc3.bias 0.000334259239025414\n",
      "\n",
      "Test set: Average loss: 1.8843 \n",
      "Accuracy: 3083/10000 (30.83%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0007150659296247694\n",
      "conv1.bias 0.0014628552210827668\n",
      "conv2.weight 0.0005594121416409811\n",
      "conv2.bias 0.0007278143893927336\n",
      "fc1.weight 0.00022121566534042357\n",
      "fc1.bias 0.0005981378878156344\n",
      "fc2.weight 0.0005724812310839456\n",
      "fc2.bias 0.0008677329335893903\n",
      "fc3.weight 0.0005560221771399181\n",
      "fc3.bias 0.000372495292685926\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0007150659296247694\n",
      "conv1.bias 0.0014628552210827668\n",
      "conv2.weight 0.0005594121416409811\n",
      "conv2.bias 0.0007278143893927336\n",
      "fc1.weight 0.00022121566534042357\n",
      "fc1.bias 0.0005981378878156344\n",
      "fc2.weight 0.0005724812310839456\n",
      "fc2.bias 0.0008677329335893903\n",
      "fc3.weight 0.0005560221771399181\n",
      "fc3.bias 0.000372495292685926\n",
      "\n",
      "Test set: Average loss: 1.8859 \n",
      "Accuracy: 3083/10000 (30.83%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000676181846194797\n",
      "conv1.bias 0.0014446050239106019\n",
      "conv2.weight 0.0005748359858989715\n",
      "conv2.bias 0.0007681146962568164\n",
      "fc1.weight 0.00022370155652364095\n",
      "fc1.bias 0.0006126526122291882\n",
      "fc2.weight 0.0005857164897615948\n",
      "fc2.bias 0.0008748257089228858\n",
      "fc3.weight 0.0005707626896245139\n",
      "fc3.bias 0.00040490441024303435\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000676181846194797\n",
      "conv1.bias 0.0014446050239106019\n",
      "conv2.weight 0.0005748359858989715\n",
      "conv2.bias 0.0007681146962568164\n",
      "fc1.weight 0.00022370155652364095\n",
      "fc1.bias 0.0006126526122291882\n",
      "fc2.weight 0.0005857164897615948\n",
      "fc2.bias 0.0008748257089228858\n",
      "fc3.weight 0.0005707626896245139\n",
      "fc3.bias 0.00040490441024303435\n",
      "\n",
      "Test set: Average loss: 1.8858 \n",
      "Accuracy: 3084/10000 (30.84%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006575677129957411\n",
      "conv1.bias 0.0014072200283408165\n",
      "conv2.weight 0.0005907253424326579\n",
      "conv2.bias 0.0007886142702773213\n",
      "fc1.weight 0.00022525777419408162\n",
      "fc1.bias 0.0006178850928942362\n",
      "fc2.weight 0.0005937910269177149\n",
      "fc2.bias 0.0008768257463262195\n",
      "fc3.weight 0.0005799056518645514\n",
      "fc3.bias 0.0004281758330762386\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006575677129957411\n",
      "conv1.bias 0.0014072200283408165\n",
      "conv2.weight 0.0005907253424326579\n",
      "conv2.bias 0.0007886142702773213\n",
      "fc1.weight 0.00022525777419408162\n",
      "fc1.bias 0.0006178850928942362\n",
      "fc2.weight 0.0005937910269177149\n",
      "fc2.bias 0.0008768257463262195\n",
      "fc3.weight 0.0005799056518645514\n",
      "fc3.bias 0.0004281758330762386\n",
      "\n",
      "Test set: Average loss: 1.8848 \n",
      "Accuracy: 3090/10000 (30.90%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006478105651007758\n",
      "conv1.bias 0.0013654371723532677\n",
      "conv2.weight 0.0006035566329956055\n",
      "conv2.bias 0.0007989555597305298\n",
      "fc1.weight 0.000226529061794281\n",
      "fc1.bias 0.0006190317372481029\n",
      "fc2.weight 0.0005993596145084926\n",
      "fc2.bias 0.000876395209204583\n",
      "fc3.weight 0.0005855991017250788\n",
      "fc3.bias 0.0004455269780009985\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006478105651007758\n",
      "conv1.bias 0.0013654371723532677\n",
      "conv2.weight 0.0006035566329956055\n",
      "conv2.bias 0.0007989555597305298\n",
      "fc1.weight 0.000226529061794281\n",
      "fc1.bias 0.0006190317372481029\n",
      "fc2.weight 0.0005993596145084926\n",
      "fc2.bias 0.000876395209204583\n",
      "fc3.weight 0.0005855991017250788\n",
      "fc3.bias 0.0004455269780009985\n",
      "\n",
      "Test set: Average loss: 1.8841 \n",
      "Accuracy: 3082/10000 (30.82%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006418485111660427\n",
      "conv1.bias 0.0013266629539430141\n",
      "conv2.weight 0.0006131438414255778\n",
      "conv2.bias 0.000802674563601613\n",
      "fc1.weight 0.00022793368498484292\n",
      "fc1.bias 0.0006171859800815582\n",
      "fc2.weight 0.0006033390287369017\n",
      "fc2.bias 0.0008762375939460028\n",
      "fc3.weight 0.000589370337270555\n",
      "fc3.bias 0.00045820591039955614\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006418485111660427\n",
      "conv1.bias 0.0013266629539430141\n",
      "conv2.weight 0.0006131438414255778\n",
      "conv2.bias 0.000802674563601613\n",
      "fc1.weight 0.00022793368498484292\n",
      "fc1.bias 0.0006171859800815582\n",
      "fc2.weight 0.0006033390287369017\n",
      "fc2.bias 0.0008762375939460028\n",
      "fc3.weight 0.000589370337270555\n",
      "fc3.bias 0.00045820591039955614\n",
      "\n",
      "Test set: Average loss: 1.8836 \n",
      "Accuracy: 3079/10000 (30.79%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006367681423823039\n",
      "conv1.bias 0.0012943991459906101\n",
      "conv2.weight 0.0006201529502868653\n",
      "conv2.bias 0.0008036481449380517\n",
      "fc1.weight 0.00022933671871821086\n",
      "fc1.bias 0.0006147667765617371\n",
      "fc2.weight 0.0006068073094837249\n",
      "fc2.bias 0.0008752730985482534\n",
      "fc3.weight 0.0005919632457551502\n",
      "fc3.bias 0.0004676868673413992\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006367681423823039\n",
      "conv1.bias 0.0012943991459906101\n",
      "conv2.weight 0.0006201529502868653\n",
      "conv2.bias 0.0008036481449380517\n",
      "fc1.weight 0.00022933671871821086\n",
      "fc1.bias 0.0006147667765617371\n",
      "fc2.weight 0.0006068073094837249\n",
      "fc2.bias 0.0008752730985482534\n",
      "fc3.weight 0.0005919632457551502\n",
      "fc3.bias 0.0004676868673413992\n",
      "\n",
      "Test set: Average loss: 1.8825 \n",
      "Accuracy: 3079/10000 (30.79%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006321395768059625\n",
      "conv1.bias 0.001274012727662921\n",
      "conv2.weight 0.0006256780028343201\n",
      "conv2.bias 0.000801302376203239\n",
      "fc1.weight 0.00023077185948689778\n",
      "fc1.bias 0.0006108520552515984\n",
      "fc2.weight 0.0006095513937965272\n",
      "fc2.bias 0.0008734563986460367\n",
      "fc3.weight 0.0005941011721179598\n",
      "fc3.bias 0.00047586960718035696\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006321395768059625\n",
      "conv1.bias 0.001274012727662921\n",
      "conv2.weight 0.0006256780028343201\n",
      "conv2.bias 0.000801302376203239\n",
      "fc1.weight 0.00023077185948689778\n",
      "fc1.bias 0.0006108520552515984\n",
      "fc2.weight 0.0006095513937965272\n",
      "fc2.bias 0.0008734563986460367\n",
      "fc3.weight 0.0005941011721179598\n",
      "fc3.bias 0.00047586960718035696\n",
      "\n",
      "Test set: Average loss: 1.8815 \n",
      "Accuracy: 3077/10000 (30.77%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006282615661621093\n",
      "conv1.bias 0.0012530157497773569\n",
      "conv2.weight 0.0006295189261436463\n",
      "conv2.bias 0.0007969149155542254\n",
      "fc1.weight 0.00023204479614893597\n",
      "fc1.bias 0.0006066535289088885\n",
      "fc2.weight 0.0006119801884605771\n",
      "fc2.bias 0.0008703150032531648\n",
      "fc3.weight 0.0005961479175658453\n",
      "fc3.bias 0.0004818988963961601\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006282615661621093\n",
      "conv1.bias 0.0012530157497773569\n",
      "conv2.weight 0.0006295189261436463\n",
      "conv2.bias 0.0007969149155542254\n",
      "fc1.weight 0.00023204479614893597\n",
      "fc1.bias 0.0006066535289088885\n",
      "fc2.weight 0.0006119801884605771\n",
      "fc2.bias 0.0008703150032531648\n",
      "fc3.weight 0.0005961479175658453\n",
      "fc3.bias 0.0004818988963961601\n",
      "\n",
      "Test set: Average loss: 1.8811 \n",
      "Accuracy: 3073/10000 (30.73%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006252874268425835\n",
      "conv1.bias 0.0012373197823762894\n",
      "conv2.weight 0.0006320318082968394\n",
      "conv2.bias 0.0007912039291113615\n",
      "fc1.weight 0.00023296227057774863\n",
      "fc1.bias 0.000603011002143224\n",
      "fc2.weight 0.0006138760892171708\n",
      "fc2.bias 0.0008676293350401379\n",
      "fc3.weight 0.0005979090929031372\n",
      "fc3.bias 0.00048734401352703573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected users: [0 1]\n",
      "conv1.weight 0.0006252874268425835\n",
      "conv1.bias 0.0012373197823762894\n",
      "conv2.weight 0.0006320318082968394\n",
      "conv2.bias 0.0007912039291113615\n",
      "fc1.weight 0.00023296227057774863\n",
      "fc1.bias 0.000603011002143224\n",
      "fc2.weight 0.0006138760892171708\n",
      "fc2.bias 0.0008676293350401379\n",
      "fc3.weight 0.0005979090929031372\n",
      "fc3.bias 0.00048734401352703573\n",
      "\n",
      "Test set: Average loss: 1.8804 \n",
      "Accuracy: 3077/10000 (30.77%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006227070755428738\n",
      "conv1.bias 0.0012227777236451705\n",
      "conv2.weight 0.0006343868374824524\n",
      "conv2.bias 0.0007857909658923745\n",
      "fc1.weight 0.00023368322849273682\n",
      "fc1.bias 0.0006005254263679187\n",
      "fc2.weight 0.0006156520237998356\n",
      "fc2.bias 0.0008656177669763565\n",
      "fc3.weight 0.0005995645409538632\n",
      "fc3.bias 0.0004927472211420536\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006227070755428738\n",
      "conv1.bias 0.0012227777236451705\n",
      "conv2.weight 0.0006343868374824524\n",
      "conv2.bias 0.0007857909658923745\n",
      "fc1.weight 0.00023368322849273682\n",
      "fc1.bias 0.0006005254263679187\n",
      "fc2.weight 0.0006156520237998356\n",
      "fc2.bias 0.0008656177669763565\n",
      "fc3.weight 0.0005995645409538632\n",
      "fc3.bias 0.0004927472211420536\n",
      "\n",
      "Test set: Average loss: 1.8794 \n",
      "Accuracy: 3087/10000 (30.87%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006202664640214708\n",
      "conv1.bias 0.0012091649696230888\n",
      "conv2.weight 0.0006363970537980397\n",
      "conv2.bias 0.000780872069299221\n",
      "fc1.weight 0.0002342295249303182\n",
      "fc1.bias 0.0005983305474122365\n",
      "fc2.weight 0.0006171697661990211\n",
      "fc2.bias 0.0008641828206323442\n",
      "fc3.weight 0.000601094819250561\n",
      "fc3.bias 0.0004968889988958836\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006202664640214708\n",
      "conv1.bias 0.0012091649696230888\n",
      "conv2.weight 0.0006363970537980397\n",
      "conv2.bias 0.000780872069299221\n",
      "fc1.weight 0.0002342295249303182\n",
      "fc1.bias 0.0005983305474122365\n",
      "fc2.weight 0.0006171697661990211\n",
      "fc2.bias 0.0008641828206323442\n",
      "fc3.weight 0.000601094819250561\n",
      "fc3.bias 0.0004968889988958836\n",
      "\n",
      "Test set: Average loss: 1.8791 \n",
      "Accuracy: 3085/10000 (30.85%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006185179286532932\n",
      "conv1.bias 0.0011970132278899352\n",
      "conv2.weight 0.0006383945047855377\n",
      "conv2.bias 0.0007749045616947114\n",
      "fc1.weight 0.0002347427010536194\n",
      "fc1.bias 0.0005953462173541387\n",
      "fc2.weight 0.0006184278026459709\n",
      "fc2.bias 0.0008627046786603474\n",
      "fc3.weight 0.0006024245704923357\n",
      "fc3.bias 0.000501627055928111\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006185179286532932\n",
      "conv1.bias 0.0011970132278899352\n",
      "conv2.weight 0.0006383945047855377\n",
      "conv2.bias 0.0007749045616947114\n",
      "fc1.weight 0.0002347427010536194\n",
      "fc1.bias 0.0005953462173541387\n",
      "fc2.weight 0.0006184278026459709\n",
      "fc2.bias 0.0008627046786603474\n",
      "fc3.weight 0.0006024245704923357\n",
      "fc3.bias 0.000501627055928111\n",
      "\n",
      "Test set: Average loss: 1.8782 \n",
      "Accuracy: 3092/10000 (30.92%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000616557863023546\n",
      "conv1.bias 0.0011856257915496826\n",
      "conv2.weight 0.0006400903065999348\n",
      "conv2.bias 0.0007694679079577327\n",
      "fc1.weight 0.00023506919542948406\n",
      "fc1.bias 0.000591989482442538\n",
      "fc2.weight 0.0006192618892306373\n",
      "fc2.bias 0.0008611103431099938\n",
      "fc3.weight 0.0006034805661156064\n",
      "fc3.bias 0.0005058368667960166\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000616557863023546\n",
      "conv1.bias 0.0011856257915496826\n",
      "conv2.weight 0.0006400903065999348\n",
      "conv2.bias 0.0007694679079577327\n",
      "fc1.weight 0.00023506919542948406\n",
      "fc1.bias 0.000591989482442538\n",
      "fc2.weight 0.0006192618892306373\n",
      "fc2.bias 0.0008611103431099938\n",
      "fc3.weight 0.0006034805661156064\n",
      "fc3.bias 0.0005058368667960166\n",
      "\n",
      "Test set: Average loss: 1.8778 \n",
      "Accuracy: 3081/10000 (30.81%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006151680151621501\n",
      "conv1.bias 0.0011746454983949661\n",
      "conv2.weight 0.0006414288282394409\n",
      "conv2.bias 0.0007643629796802998\n",
      "fc1.weight 0.00023541098833084107\n",
      "fc1.bias 0.0005884439994891485\n",
      "fc2.weight 0.0006201878899619693\n",
      "fc2.bias 0.0008602390686670939\n",
      "fc3.weight 0.000604599430447533\n",
      "fc3.bias 0.0005100059788674116\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006151680151621501\n",
      "conv1.bias 0.0011746454983949661\n",
      "conv2.weight 0.0006414288282394409\n",
      "conv2.bias 0.0007643629796802998\n",
      "fc1.weight 0.00023541098833084107\n",
      "fc1.bias 0.0005884439994891485\n",
      "fc2.weight 0.0006201878899619693\n",
      "fc2.bias 0.0008602390686670939\n",
      "fc3.weight 0.000604599430447533\n",
      "fc3.bias 0.0005100059788674116\n",
      "\n",
      "Test set: Average loss: 1.8769 \n",
      "Accuracy: 3088/10000 (30.88%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006137409475114611\n",
      "conv1.bias 0.0011661401949822903\n",
      "conv2.weight 0.0006423752009868622\n",
      "conv2.bias 0.0007601106772199273\n",
      "fc1.weight 0.00023564702272415162\n",
      "fc1.bias 0.0005843217174212137\n",
      "fc2.weight 0.000620798459128728\n",
      "fc2.bias 0.0008598965193544115\n",
      "fc3.weight 0.0006054383658227467\n",
      "fc3.bias 0.0005131728947162628\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006137409475114611\n",
      "conv1.bias 0.0011661401949822903\n",
      "conv2.weight 0.0006423752009868622\n",
      "conv2.bias 0.0007601106772199273\n",
      "fc1.weight 0.00023564702272415162\n",
      "fc1.bias 0.0005843217174212137\n",
      "fc2.weight 0.000620798459128728\n",
      "fc2.bias 0.0008598965193544115\n",
      "fc3.weight 0.0006054383658227467\n",
      "fc3.bias 0.0005131728947162628\n",
      "\n",
      "Test set: Average loss: 1.8764 \n",
      "Accuracy: 3090/10000 (30.90%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006123404370413886\n",
      "conv1.bias 0.0011607485357671976\n",
      "conv2.weight 0.0006431833902994791\n",
      "conv2.bias 0.0007557976059615612\n",
      "fc1.weight 0.00023581318060557048\n",
      "fc1.bias 0.0005799507101376852\n",
      "fc2.weight 0.0006209655413551937\n",
      "fc2.bias 0.000859509798742476\n",
      "fc3.weight 0.0006059483403251285\n",
      "fc3.bias 0.0005154058337211609\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006123404370413886\n",
      "conv1.bias 0.0011607485357671976\n",
      "conv2.weight 0.0006431833902994791\n",
      "conv2.bias 0.0007557976059615612\n",
      "fc1.weight 0.00023581318060557048\n",
      "fc1.bias 0.0005799507101376852\n",
      "fc2.weight 0.0006209655413551937\n",
      "fc2.bias 0.000859509798742476\n",
      "fc3.weight 0.0006059483403251285\n",
      "fc3.bias 0.0005154058337211609\n",
      "\n",
      "Test set: Average loss: 1.8764 \n",
      "Accuracy: 3094/10000 (30.94%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000611157152387831\n",
      "conv1.bias 0.001153197915603717\n",
      "conv2.weight 0.0006437641382217407\n",
      "conv2.bias 0.0007518368074670434\n",
      "fc1.weight 0.00023608662684758503\n",
      "fc1.bias 0.0005757789437969526\n",
      "fc2.weight 0.0006212251526968819\n",
      "fc2.bias 0.0008577075565145129\n",
      "fc3.weight 0.0006065688672519865\n",
      "fc3.bias 0.0005178174469619989\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000611157152387831\n",
      "conv1.bias 0.001153197915603717\n",
      "conv2.weight 0.0006437641382217407\n",
      "conv2.bias 0.0007518368074670434\n",
      "fc1.weight 0.00023608662684758503\n",
      "fc1.bias 0.0005757789437969526\n",
      "fc2.weight 0.0006212251526968819\n",
      "fc2.bias 0.0008577075565145129\n",
      "fc3.weight 0.0006065688672519865\n",
      "fc3.bias 0.0005178174469619989\n",
      "\n",
      "Test set: Average loss: 1.8760 \n",
      "Accuracy: 3094/10000 (30.94%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006099488337834676\n",
      "conv1.bias 0.001148028454432885\n",
      "conv2.weight 0.0006441223621368408\n",
      "conv2.bias 0.0007484471425414085\n",
      "fc1.weight 0.0002362735867500305\n",
      "fc1.bias 0.0005712476248542468\n",
      "fc2.weight 0.0006214465413774763\n",
      "fc2.bias 0.0008558085454361779\n",
      "fc3.weight 0.0006069120906648181\n",
      "fc3.bias 0.0005199420265853405\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006099488337834676\n",
      "conv1.bias 0.001148028454432885\n",
      "conv2.weight 0.0006441223621368408\n",
      "conv2.bias 0.0007484471425414085\n",
      "fc1.weight 0.0002362735867500305\n",
      "fc1.bias 0.0005712476248542468\n",
      "fc2.weight 0.0006214465413774763\n",
      "fc2.bias 0.0008558085454361779\n",
      "fc3.weight 0.0006069120906648181\n",
      "fc3.bias 0.0005199420265853405\n",
      "\n",
      "Test set: Average loss: 1.8758 \n",
      "Accuracy: 3102/10000 (31.02%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006089615821838379\n",
      "conv1.bias 0.001143111614510417\n",
      "conv2.weight 0.0006446782747904459\n",
      "conv2.bias 0.0007455727318301797\n",
      "fc1.weight 0.0002364081343015035\n",
      "fc1.bias 0.0005669891834259033\n",
      "fc2.weight 0.000621500327473595\n",
      "fc2.bias 0.0008529799857309886\n",
      "fc3.weight 0.0006072973921185448\n",
      "fc3.bias 0.0005215233657509088\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006089615821838379\n",
      "conv1.bias 0.001143111614510417\n",
      "conv2.weight 0.0006446782747904459\n",
      "conv2.bias 0.0007455727318301797\n",
      "fc1.weight 0.0002364081343015035\n",
      "fc1.bias 0.0005669891834259033\n",
      "fc2.weight 0.000621500327473595\n",
      "fc2.bias 0.0008529799857309886\n",
      "fc3.weight 0.0006072973921185448\n",
      "fc3.bias 0.0005215233657509088\n",
      "\n",
      "Test set: Average loss: 1.8757 \n",
      "Accuracy: 3099/10000 (30.99%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006080614195929634\n",
      "conv1.bias 0.0011378748652835686\n",
      "conv2.weight 0.0006448291738828023\n",
      "conv2.bias 0.0007430773111991584\n",
      "fc1.weight 0.0002365602453549703\n",
      "fc1.bias 0.0005627918367584546\n",
      "fc2.weight 0.0006214027839993673\n",
      "fc2.bias 0.0008497917581172217\n",
      "fc3.weight 0.000607715972832271\n",
      "fc3.bias 0.0005234021693468094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected users: [0 1]\n",
      "conv1.weight 0.0006080614195929634\n",
      "conv1.bias 0.0011378748652835686\n",
      "conv2.weight 0.0006448291738828023\n",
      "conv2.bias 0.0007430773111991584\n",
      "fc1.weight 0.0002365602453549703\n",
      "fc1.bias 0.0005627918367584546\n",
      "fc2.weight 0.0006214027839993673\n",
      "fc2.bias 0.0008497917581172217\n",
      "fc3.weight 0.000607715972832271\n",
      "fc3.bias 0.0005234021693468094\n",
      "\n",
      "Test set: Average loss: 1.8754 \n",
      "Accuracy: 3108/10000 (31.08%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006072478824191624\n",
      "conv1.bias 0.0011330663692206144\n",
      "conv2.weight 0.0006446332236131032\n",
      "conv2.bias 0.0007396798464469612\n",
      "fc1.weight 0.00023666687806447347\n",
      "fc1.bias 0.0005587667226791381\n",
      "fc2.weight 0.0006212096838724045\n",
      "fc2.bias 0.0008465720429306938\n",
      "fc3.weight 0.0006080017912955512\n",
      "fc3.bias 0.0005238738842308522\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006072478824191624\n",
      "conv1.bias 0.0011330663692206144\n",
      "conv2.weight 0.0006446332236131032\n",
      "conv2.bias 0.0007396798464469612\n",
      "fc1.weight 0.00023666687806447347\n",
      "fc1.bias 0.0005587667226791381\n",
      "fc2.weight 0.0006212096838724045\n",
      "fc2.bias 0.0008465720429306938\n",
      "fc3.weight 0.0006080017912955512\n",
      "fc3.bias 0.0005238738842308522\n",
      "\n",
      "Test set: Average loss: 1.8754 \n",
      "Accuracy: 3104/10000 (31.04%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006066899167166816\n",
      "conv1.bias 0.001130943497021993\n",
      "conv2.weight 0.0006448163588841756\n",
      "conv2.bias 0.000736297108232975\n",
      "fc1.weight 0.00023686355352401734\n",
      "fc1.bias 0.0005537291367848714\n",
      "fc2.weight 0.0006211602025561863\n",
      "fc2.bias 0.0008427027967714128\n",
      "fc3.weight 0.0006084019229525611\n",
      "fc3.bias 0.0005253859795629978\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006066899167166816\n",
      "conv1.bias 0.001130943497021993\n",
      "conv2.weight 0.0006448163588841756\n",
      "conv2.bias 0.000736297108232975\n",
      "fc1.weight 0.00023686355352401734\n",
      "fc1.bias 0.0005537291367848714\n",
      "fc2.weight 0.0006211602025561863\n",
      "fc2.bias 0.0008427027967714128\n",
      "fc3.weight 0.0006084019229525611\n",
      "fc3.bias 0.0005253859795629978\n",
      "\n",
      "Test set: Average loss: 1.8753 \n",
      "Accuracy: 3104/10000 (31.04%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006062042713165283\n",
      "conv1.bias 0.001129408987859885\n",
      "conv2.weight 0.0006447974344094595\n",
      "conv2.bias 0.0007345033809542656\n",
      "fc1.weight 0.00023700857162475586\n",
      "fc1.bias 0.000548672986527284\n",
      "fc2.weight 0.0006212031084393698\n",
      "fc2.bias 0.0008389573721658616\n",
      "fc3.weight 0.0006085974119958424\n",
      "fc3.bias 0.0005260375328361988\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006062042713165283\n",
      "conv1.bias 0.001129408987859885\n",
      "conv2.weight 0.0006447974344094595\n",
      "conv2.bias 0.0007345033809542656\n",
      "fc1.weight 0.00023700857162475586\n",
      "fc1.bias 0.000548672986527284\n",
      "fc2.weight 0.0006212031084393698\n",
      "fc2.bias 0.0008389573721658616\n",
      "fc3.weight 0.0006085974119958424\n",
      "fc3.bias 0.0005260375328361988\n",
      "\n",
      "Test set: Average loss: 1.8754 \n",
      "Accuracy: 3104/10000 (31.04%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006057797537909614\n",
      "conv1.bias 0.0011255679031213124\n",
      "conv2.weight 0.0006446349620819092\n",
      "conv2.bias 0.0007315052207559347\n",
      "fc1.weight 0.00023721245924631753\n",
      "fc1.bias 0.000543460746606191\n",
      "fc2.weight 0.0006213868894274272\n",
      "fc2.bias 0.0008351561569032215\n",
      "fc3.weight 0.00060882568359375\n",
      "fc3.bias 0.0005263843107968569\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006057797537909614\n",
      "conv1.bias 0.0011255679031213124\n",
      "conv2.weight 0.0006446349620819092\n",
      "conv2.bias 0.0007315052207559347\n",
      "fc1.weight 0.00023721245924631753\n",
      "fc1.bias 0.000543460746606191\n",
      "fc2.weight 0.0006213868894274272\n",
      "fc2.bias 0.0008351561569032215\n",
      "fc3.weight 0.00060882568359375\n",
      "fc3.bias 0.0005263843107968569\n",
      "\n",
      "Test set: Average loss: 1.8757 \n",
      "Accuracy: 3100/10000 (31.00%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006052161587609185\n",
      "conv1.bias 0.0011232714168727398\n",
      "conv2.weight 0.0006445455054442088\n",
      "conv2.bias 0.0007294715032912791\n",
      "fc1.weight 0.00023745214939117432\n",
      "fc1.bias 0.0005374990403652191\n",
      "fc2.weight 0.0006214406282182723\n",
      "fc2.bias 0.0008309793968995413\n",
      "fc3.weight 0.0006090700626373291\n",
      "fc3.bias 0.0005277733318507672\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006052161587609185\n",
      "conv1.bias 0.0011232714168727398\n",
      "conv2.weight 0.0006445455054442088\n",
      "conv2.bias 0.0007294715032912791\n",
      "fc1.weight 0.00023745214939117432\n",
      "fc1.bias 0.0005374990403652191\n",
      "fc2.weight 0.0006214406282182723\n",
      "fc2.bias 0.0008309793968995413\n",
      "fc3.weight 0.0006090700626373291\n",
      "fc3.bias 0.0005277733318507672\n",
      "\n",
      "Test set: Average loss: 1.8755 \n",
      "Accuracy: 3104/10000 (31.04%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000604716142018636\n",
      "conv1.bias 0.0011193312238901854\n",
      "conv2.weight 0.0006444276372591654\n",
      "conv2.bias 0.0007267359178513288\n",
      "fc1.weight 0.00023771458864212035\n",
      "fc1.bias 0.0005311354373892149\n",
      "fc2.weight 0.0006215717111315046\n",
      "fc2.bias 0.0008270748491798129\n",
      "fc3.weight 0.0006093645379656838\n",
      "fc3.bias 0.0005284078419208526\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000604716142018636\n",
      "conv1.bias 0.0011193312238901854\n",
      "conv2.weight 0.0006444276372591654\n",
      "conv2.bias 0.0007267359178513288\n",
      "fc1.weight 0.00023771458864212035\n",
      "fc1.bias 0.0005311354373892149\n",
      "fc2.weight 0.0006215717111315046\n",
      "fc2.bias 0.0008270748491798129\n",
      "fc3.weight 0.0006093645379656838\n",
      "fc3.bias 0.0005284078419208526\n",
      "\n",
      "Test set: Average loss: 1.8752 \n",
      "Accuracy: 3107/10000 (31.07%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006041805611716376\n",
      "conv1.bias 0.00111646701892217\n",
      "conv2.weight 0.0006441731254259746\n",
      "conv2.bias 0.0007254814845509827\n",
      "fc1.weight 0.0002379763126373291\n",
      "fc1.bias 0.0005243805547555288\n",
      "fc2.weight 0.0006214677814453367\n",
      "fc2.bias 0.0008225127877224059\n",
      "fc3.weight 0.0006093665247871762\n",
      "fc3.bias 0.0005285851657390594\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006041805611716376\n",
      "conv1.bias 0.00111646701892217\n",
      "conv2.weight 0.0006441731254259746\n",
      "conv2.bias 0.0007254814845509827\n",
      "fc1.weight 0.0002379763126373291\n",
      "fc1.bias 0.0005243805547555288\n",
      "fc2.weight 0.0006214677814453367\n",
      "fc2.bias 0.0008225127877224059\n",
      "fc3.weight 0.0006093665247871762\n",
      "fc3.bias 0.0005285851657390594\n",
      "\n",
      "Test set: Average loss: 1.8748 \n",
      "Accuracy: 3110/10000 (31.10%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006033947070439656\n",
      "conv1.bias 0.0011105918480704229\n",
      "conv2.weight 0.0006437541544437409\n",
      "conv2.bias 0.0007239737897180021\n",
      "fc1.weight 0.0002381664514541626\n",
      "fc1.bias 0.000516760473450025\n",
      "fc2.weight 0.0006212663082849412\n",
      "fc2.bias 0.0008182651585056668\n",
      "fc3.weight 0.0006093660990397136\n",
      "fc3.bias 0.0005284848622977734\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006033947070439656\n",
      "conv1.bias 0.0011105918480704229\n",
      "conv2.weight 0.0006437541544437409\n",
      "conv2.bias 0.0007239737897180021\n",
      "fc1.weight 0.0002381664514541626\n",
      "fc1.bias 0.000516760473450025\n",
      "fc2.weight 0.0006212663082849412\n",
      "fc2.bias 0.0008182651585056668\n",
      "fc3.weight 0.0006093660990397136\n",
      "fc3.bias 0.0005284848622977734\n",
      "\n",
      "Test set: Average loss: 1.8749 \n",
      "Accuracy: 3107/10000 (31.07%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006026448143853082\n",
      "conv1.bias 0.0011101618874818087\n",
      "conv2.weight 0.0006434362630049387\n",
      "conv2.bias 0.0007220666739158332\n",
      "fc1.weight 0.00023839370409647624\n",
      "fc1.bias 0.0005074165451029937\n",
      "fc2.weight 0.0006214193408451383\n",
      "fc2.bias 0.000813399131099383\n",
      "fc3.weight 0.0006093530427841913\n",
      "fc3.bias 0.0005285145714879036\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006026448143853082\n",
      "conv1.bias 0.0011101618874818087\n",
      "conv2.weight 0.0006434362630049387\n",
      "conv2.bias 0.0007220666739158332\n",
      "fc1.weight 0.00023839370409647624\n",
      "fc1.bias 0.0005074165451029937\n",
      "fc2.weight 0.0006214193408451383\n",
      "fc2.bias 0.000813399131099383\n",
      "fc3.weight 0.0006093530427841913\n",
      "fc3.bias 0.0005285145714879036\n",
      "\n",
      "Test set: Average loss: 1.8750 \n",
      "Accuracy: 3109/10000 (31.09%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006016172965367635\n",
      "conv1.bias 0.0011089610246320565\n",
      "conv2.weight 0.000643156369527181\n",
      "conv2.bias 0.000719717238098383\n",
      "fc1.weight 0.00023854881525039673\n",
      "fc1.bias 0.0004988775278131167\n",
      "fc2.weight 0.0006210909003303164\n",
      "fc2.bias 0.000808712804601306\n",
      "fc3.weight 0.0006092639906065805\n",
      "fc3.bias 0.0005282780155539512\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006016172965367635\n",
      "conv1.bias 0.0011089610246320565\n",
      "conv2.weight 0.000643156369527181\n",
      "conv2.bias 0.000719717238098383\n",
      "fc1.weight 0.00023854881525039673\n",
      "fc1.bias 0.0004988775278131167\n",
      "fc2.weight 0.0006210909003303164\n",
      "fc2.bias 0.000808712804601306\n",
      "fc3.weight 0.0006092639906065805\n",
      "fc3.bias 0.0005282780155539512\n",
      "\n",
      "Test set: Average loss: 1.8752 \n",
      "Accuracy: 3111/10000 (31.11%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006007762749989827\n",
      "conv1.bias 0.0011071295011788607\n",
      "conv2.weight 0.0006428345044453939\n",
      "conv2.bias 0.0007177084335125983\n",
      "fc1.weight 0.00023867477973302204\n",
      "fc1.bias 0.0004912075276176135\n",
      "fc2.weight 0.0006211754821595692\n",
      "fc2.bias 0.0008043022382827033\n",
      "fc3.weight 0.000609235181694939\n",
      "fc3.bias 0.0005283777136355639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected users: [0 1]\n",
      "conv1.weight 0.0006007762749989827\n",
      "conv1.bias 0.0011071295011788607\n",
      "conv2.weight 0.0006428345044453939\n",
      "conv2.bias 0.0007177084335125983\n",
      "fc1.weight 0.00023867477973302204\n",
      "fc1.bias 0.0004912075276176135\n",
      "fc2.weight 0.0006211754821595692\n",
      "fc2.bias 0.0008043022382827033\n",
      "fc3.weight 0.000609235181694939\n",
      "fc3.bias 0.0005283777136355639\n",
      "\n",
      "Test set: Average loss: 1.8757 \n",
      "Accuracy: 3104/10000 (31.04%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006001252598232693\n",
      "conv1.bias 0.0011050514876842499\n",
      "conv2.weight 0.0006425145268440246\n",
      "conv2.bias 0.0007145157433114946\n",
      "fc1.weight 0.00023883897066116332\n",
      "fc1.bias 0.00048418333753943445\n",
      "fc2.weight 0.0006214070414739942\n",
      "fc2.bias 0.0007998923815432049\n",
      "fc3.weight 0.0006093177057447887\n",
      "fc3.bias 0.0005283576436340808\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0006001252598232693\n",
      "conv1.bias 0.0011050514876842499\n",
      "conv2.weight 0.0006425145268440246\n",
      "conv2.bias 0.0007145157433114946\n",
      "fc1.weight 0.00023883897066116332\n",
      "fc1.bias 0.00048418333753943445\n",
      "fc2.weight 0.0006214070414739942\n",
      "fc2.bias 0.0007998923815432049\n",
      "fc3.weight 0.0006093177057447887\n",
      "fc3.bias 0.0005283576436340808\n",
      "\n",
      "Test set: Average loss: 1.8753 \n",
      "Accuracy: 3105/10000 (31.05%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005992737081315782\n",
      "conv1.bias 0.0011033962170283\n",
      "conv2.weight 0.0006424011786778768\n",
      "conv2.bias 0.0007129819132387638\n",
      "fc1.weight 0.00023891894022623697\n",
      "fc1.bias 0.00047884161273638407\n",
      "fc2.weight 0.0006215504710636442\n",
      "fc2.bias 0.0007958067137570609\n",
      "fc3.weight 0.000609297411782401\n",
      "fc3.bias 0.0005283558741211892\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005992737081315782\n",
      "conv1.bias 0.0011033962170283\n",
      "conv2.weight 0.0006424011786778768\n",
      "conv2.bias 0.0007129819132387638\n",
      "fc1.weight 0.00023891894022623697\n",
      "fc1.bias 0.00047884161273638407\n",
      "fc2.weight 0.0006215504710636442\n",
      "fc2.bias 0.0007958067137570609\n",
      "fc3.weight 0.000609297411782401\n",
      "fc3.bias 0.0005283558741211892\n",
      "\n",
      "Test set: Average loss: 1.8753 \n",
      "Accuracy: 3108/10000 (31.08%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005987016359965007\n",
      "conv1.bias 0.001101846806704998\n",
      "conv2.weight 0.0006423176825046539\n",
      "conv2.bias 0.0007116235792636871\n",
      "fc1.weight 0.0002390105128288269\n",
      "fc1.bias 0.00047467947006225587\n",
      "fc2.weight 0.0006217692579541887\n",
      "fc2.bias 0.0007927417755126953\n",
      "fc3.weight 0.0006094102348600115\n",
      "fc3.bias 0.0005278600845485925\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005987016359965007\n",
      "conv1.bias 0.001101846806704998\n",
      "conv2.weight 0.0006423176825046539\n",
      "conv2.bias 0.0007116235792636871\n",
      "fc1.weight 0.0002390105128288269\n",
      "fc1.bias 0.00047467947006225587\n",
      "fc2.weight 0.0006217692579541887\n",
      "fc2.bias 0.0007927417755126953\n",
      "fc3.weight 0.0006094102348600115\n",
      "fc3.bias 0.0005278600845485925\n",
      "\n",
      "Test set: Average loss: 1.8755 \n",
      "Accuracy: 3103/10000 (31.03%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005982026126649645\n",
      "conv1.bias 0.0011001776438206434\n",
      "conv2.weight 0.0006423000494639079\n",
      "conv2.bias 0.0007095549954101443\n",
      "fc1.weight 0.00023905118306477864\n",
      "fc1.bias 0.0004715993069112301\n",
      "fc2.weight 0.0006219119306594607\n",
      "fc2.bias 0.0007900170804489227\n",
      "fc3.weight 0.0006095412231626964\n",
      "fc3.bias 0.0005277088377624751\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005982026126649645\n",
      "conv1.bias 0.0011001776438206434\n",
      "conv2.weight 0.0006423000494639079\n",
      "conv2.bias 0.0007095549954101443\n",
      "fc1.weight 0.00023905118306477864\n",
      "fc1.bias 0.0004715993069112301\n",
      "fc2.weight 0.0006219119306594607\n",
      "fc2.bias 0.0007900170804489227\n",
      "fc3.weight 0.0006095412231626964\n",
      "fc3.bias 0.0005277088377624751\n",
      "\n",
      "Test set: Average loss: 1.8751 \n",
      "Accuracy: 3106/10000 (31.06%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005976692173216078\n",
      "conv1.bias 0.0011013321733723085\n",
      "conv2.weight 0.0006426441669464112\n",
      "conv2.bias 0.0007078334456309676\n",
      "fc1.weight 0.00023909751574198405\n",
      "fc1.bias 0.0004695267726977666\n",
      "fc2.weight 0.0006222272676134866\n",
      "fc2.bias 0.000787511822723207\n",
      "fc3.weight 0.0006096988916397094\n",
      "fc3.bias 0.0005275418050587177\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005976692173216078\n",
      "conv1.bias 0.0011013321733723085\n",
      "conv2.weight 0.0006426441669464112\n",
      "conv2.bias 0.0007078334456309676\n",
      "fc1.weight 0.00023909751574198405\n",
      "fc1.bias 0.0004695267726977666\n",
      "fc2.weight 0.0006222272676134866\n",
      "fc2.bias 0.000787511822723207\n",
      "fc3.weight 0.0006096988916397094\n",
      "fc3.bias 0.0005275418050587177\n",
      "\n",
      "Test set: Average loss: 1.8754 \n",
      "Accuracy: 3107/10000 (31.07%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005972012546327379\n",
      "conv1.bias 0.0011020901147276163\n",
      "conv2.weight 0.000643017590045929\n",
      "conv2.bias 0.0007055557216517627\n",
      "fc1.weight 0.0002391446034113566\n",
      "fc1.bias 0.00046767999107638993\n",
      "fc2.weight 0.0006226359378723871\n",
      "fc2.bias 0.0007845855184963771\n",
      "fc3.weight 0.0006097218820026942\n",
      "fc3.bias 0.0005279585719108581\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005972012546327379\n",
      "conv1.bias 0.0011020901147276163\n",
      "conv2.weight 0.000643017590045929\n",
      "conv2.bias 0.0007055557216517627\n",
      "fc1.weight 0.0002391446034113566\n",
      "fc1.bias 0.00046767999107638993\n",
      "fc2.weight 0.0006226359378723871\n",
      "fc2.bias 0.0007845855184963771\n",
      "fc3.weight 0.0006097218820026942\n",
      "fc3.bias 0.0005279585719108581\n",
      "\n",
      "Test set: Average loss: 1.8759 \n",
      "Accuracy: 3103/10000 (31.03%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005966586536831326\n",
      "conv1.bias 0.001102305327852567\n",
      "conv2.weight 0.0006431885063648224\n",
      "conv2.bias 0.0007037477334961295\n",
      "fc1.weight 0.000239194393157959\n",
      "fc1.bias 0.00046644403288761775\n",
      "fc2.weight 0.00062290420607915\n",
      "fc2.bias 0.0007822599616788683\n",
      "fc3.weight 0.0006097401891435896\n",
      "fc3.bias 0.0005279184319078922\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005966586536831326\n",
      "conv1.bias 0.001102305327852567\n",
      "conv2.weight 0.0006431885063648224\n",
      "conv2.bias 0.0007037477334961295\n",
      "fc1.weight 0.000239194393157959\n",
      "fc1.bias 0.00046644403288761775\n",
      "fc2.weight 0.00062290420607915\n",
      "fc2.bias 0.0007822599616788683\n",
      "fc3.weight 0.0006097401891435896\n",
      "fc3.bias 0.0005279184319078922\n",
      "\n",
      "Test set: Average loss: 1.8753 \n",
      "Accuracy: 3107/10000 (31.07%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005958670377731323\n",
      "conv1.bias 0.0011013108305633068\n",
      "conv2.weight 0.0006436797976493836\n",
      "conv2.bias 0.0007018456235527992\n",
      "fc1.weight 0.00023916792869567872\n",
      "fc1.bias 0.0004661422533293565\n",
      "fc2.weight 0.0006230223746526809\n",
      "fc2.bias 0.0007806993311359769\n",
      "fc3.weight 0.0006096091298829941\n",
      "fc3.bias 0.0005277984775602818\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005958670377731323\n",
      "conv1.bias 0.0011013108305633068\n",
      "conv2.weight 0.0006436797976493836\n",
      "conv2.bias 0.0007018456235527992\n",
      "fc1.weight 0.00023916792869567872\n",
      "fc1.bias 0.0004661422533293565\n",
      "fc2.weight 0.0006230223746526809\n",
      "fc2.bias 0.0007806993311359769\n",
      "fc3.weight 0.0006096091298829941\n",
      "fc3.bias 0.0005277984775602818\n",
      "\n",
      "Test set: Average loss: 1.8752 \n",
      "Accuracy: 3108/10000 (31.08%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005954178836610582\n",
      "conv1.bias 0.0011014474245409172\n",
      "conv2.weight 0.000644293079773585\n",
      "conv2.bias 0.0007008318789303303\n",
      "fc1.weight 0.0002391405502955119\n",
      "fc1.bias 0.0004656878610452016\n",
      "fc2.weight 0.0006232654291485983\n",
      "fc2.bias 0.0007793609762475604\n",
      "fc3.weight 0.0006095896164576213\n",
      "fc3.bias 0.0005276037845760584\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005954178836610582\n",
      "conv1.bias 0.0011014474245409172\n",
      "conv2.weight 0.000644293079773585\n",
      "conv2.bias 0.0007008318789303303\n",
      "fc1.weight 0.0002391405502955119\n",
      "fc1.bias 0.0004656878610452016\n",
      "fc2.weight 0.0006232654291485983\n",
      "fc2.bias 0.0007793609762475604\n",
      "fc3.weight 0.0006095896164576213\n",
      "fc3.bias 0.0005276037845760584\n",
      "\n",
      "Test set: Average loss: 1.8757 \n",
      "Accuracy: 3109/10000 (31.09%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005947737561331855\n",
      "conv1.bias 0.0011023873618493478\n",
      "conv2.weight 0.0006447562575340271\n",
      "conv2.bias 0.0006989777903072536\n",
      "fc1.weight 0.00023912636439005533\n",
      "fc1.bias 0.00046490486711263656\n",
      "fc2.weight 0.0006233121667589461\n",
      "fc2.bias 0.000778320821977797\n",
      "fc3.weight 0.0006095572596504574\n",
      "fc3.bias 0.0005274272989481688\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005947737561331855\n",
      "conv1.bias 0.0011023873618493478\n",
      "conv2.weight 0.0006447562575340271\n",
      "conv2.bias 0.0006989777903072536\n",
      "fc1.weight 0.00023912636439005533\n",
      "fc1.bias 0.00046490486711263656\n",
      "fc2.weight 0.0006233121667589461\n",
      "fc2.bias 0.000778320821977797\n",
      "fc3.weight 0.0006095572596504574\n",
      "fc3.bias 0.0005274272989481688\n",
      "\n",
      "Test set: Average loss: 1.8754 \n",
      "Accuracy: 3107/10000 (31.07%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005940681033664279\n",
      "conv1.bias 0.0011041201651096344\n",
      "conv2.weight 0.0006452515224615733\n",
      "conv2.bias 0.0006967938970774412\n",
      "fc1.weight 0.00023906517028808593\n",
      "fc1.bias 0.00046481369063258173\n",
      "fc2.weight 0.0006235794415549626\n",
      "fc2.bias 0.0007774287923460914\n",
      "fc3.weight 0.0006095977056594122\n",
      "fc3.bias 0.0005267974454909563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected users: [0 1]\n",
      "conv1.weight 0.0005940681033664279\n",
      "conv1.bias 0.0011041201651096344\n",
      "conv2.weight 0.0006452515224615733\n",
      "conv2.bias 0.0006967938970774412\n",
      "fc1.weight 0.00023906517028808593\n",
      "fc1.bias 0.00046481369063258173\n",
      "fc2.weight 0.0006235794415549626\n",
      "fc2.bias 0.0007774287923460914\n",
      "fc3.weight 0.0006095977056594122\n",
      "fc3.bias 0.0005267974454909563\n",
      "\n",
      "Test set: Average loss: 1.8751 \n",
      "Accuracy: 3109/10000 (31.09%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005931397941377428\n",
      "conv1.bias 0.0011035130204011996\n",
      "conv2.weight 0.0006459497412045796\n",
      "conv2.bias 0.0006955794524401426\n",
      "fc1.weight 0.00023896968364715577\n",
      "fc1.bias 0.0004653523986538251\n",
      "fc2.weight 0.0006236214486379472\n",
      "fc2.bias 0.0007774063519069127\n",
      "fc3.weight 0.0006095400168782189\n",
      "fc3.bias 0.0005278576165437699\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005931397941377428\n",
      "conv1.bias 0.0011035130204011996\n",
      "conv2.weight 0.0006459497412045796\n",
      "conv2.bias 0.0006955794524401426\n",
      "fc1.weight 0.00023896968364715577\n",
      "fc1.bias 0.0004653523986538251\n",
      "fc2.weight 0.0006236214486379472\n",
      "fc2.bias 0.0007774063519069127\n",
      "fc3.weight 0.0006095400168782189\n",
      "fc3.bias 0.0005278576165437699\n",
      "\n",
      "Test set: Average loss: 1.8750 \n",
      "Accuracy: 3108/10000 (31.08%)\n",
      "\n",
      "##########################################\n",
      "Learning Rate = 1e-05\n",
      "\n",
      "\n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 12500 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 12500 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 12500 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 12500 \n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.004458412594265408\n",
      "conv1.bias 0.006399042283495267\n",
      "conv2.weight 0.002278123696645101\n",
      "conv2.bias 0.002139180898666382\n",
      "fc1.weight 0.0008291574319203694\n",
      "fc1.bias 0.0008370652794837952\n",
      "fc2.weight 0.0027548029309227354\n",
      "fc2.bias 0.0024089132036481586\n",
      "fc3.weight 0.004074791499546596\n",
      "fc3.bias 0.0023772725835442543\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.004458412594265408\n",
      "conv1.bias 0.006399042283495267\n",
      "conv2.weight 0.002278123696645101\n",
      "conv2.bias 0.002139180898666382\n",
      "fc1.weight 0.0008291574319203694\n",
      "fc1.bias 0.0008370652794837952\n",
      "fc2.weight 0.0027548029309227354\n",
      "fc2.bias 0.0024089132036481586\n",
      "fc3.weight 0.004074791499546596\n",
      "fc3.bias 0.0023772725835442543\n",
      "\n",
      "Test set: Average loss: 2.2115 \n",
      "Accuracy: 1911/10000 (19.11%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0012287418047587076\n",
      "conv1.bias 0.001023553777486086\n",
      "conv2.weight 0.00047712773084640503\n",
      "conv2.bias 0.0004392392002046108\n",
      "fc1.weight 9.452635049819946e-05\n",
      "fc1.bias 0.00018523363396525384\n",
      "fc2.weight 0.0004045336019425165\n",
      "fc2.bias 0.0004332494434146654\n",
      "fc3.weight 0.0008359640836715698\n",
      "fc3.bias 0.0005249479785561561\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0012287418047587076\n",
      "conv1.bias 0.001023553777486086\n",
      "conv2.weight 0.00047712773084640503\n",
      "conv2.bias 0.0004392392002046108\n",
      "fc1.weight 9.452635049819946e-05\n",
      "fc1.bias 0.00018523363396525384\n",
      "fc2.weight 0.0004045336019425165\n",
      "fc2.bias 0.0004332494434146654\n",
      "fc3.weight 0.0008359640836715698\n",
      "fc3.bias 0.0005249479785561561\n",
      "\n",
      "Test set: Average loss: 2.1608 \n",
      "Accuracy: 1863/10000 (18.63%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005472065342797173\n",
      "conv1.bias 0.00023039855295792222\n",
      "conv2.weight 0.0002540476620197296\n",
      "conv2.bias 0.0001619291288079694\n",
      "fc1.weight 6.80001974105835e-05\n",
      "fc1.bias 8.712124545127154e-05\n",
      "fc2.weight 0.00011849198786039201\n",
      "fc2.bias 0.00011559162244555496\n",
      "fc3.weight 0.0002574197238399869\n",
      "fc3.bias 0.0001341925235465169\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005472065342797173\n",
      "conv1.bias 0.00023039855295792222\n",
      "conv2.weight 0.0002540476620197296\n",
      "conv2.bias 0.0001619291288079694\n",
      "fc1.weight 6.80001974105835e-05\n",
      "fc1.bias 8.712124545127154e-05\n",
      "fc2.weight 0.00011849198786039201\n",
      "fc2.bias 0.00011559162244555496\n",
      "fc3.weight 0.0002574197238399869\n",
      "fc3.bias 0.0001341925235465169\n",
      "\n",
      "Test set: Average loss: 2.1422 \n",
      "Accuracy: 1837/10000 (18.37%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00032806171311272516\n",
      "conv1.bias 0.00019119751717274389\n",
      "conv2.weight 0.00019439452638228734\n",
      "conv2.bias 0.00013814936392009258\n",
      "fc1.weight 8.21277250846227e-05\n",
      "fc1.bias 8.75512215619286e-05\n",
      "fc2.weight 9.218255678812662e-05\n",
      "fc2.bias 6.68208369807828e-05\n",
      "fc3.weight 0.00012492606682436806\n",
      "fc3.bias 3.7092852289788424e-05\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00032806171311272516\n",
      "conv1.bias 0.00019119751717274389\n",
      "conv2.weight 0.00019439452638228734\n",
      "conv2.bias 0.00013814936392009258\n",
      "fc1.weight 8.21277250846227e-05\n",
      "fc1.bias 8.75512215619286e-05\n",
      "fc2.weight 9.218255678812662e-05\n",
      "fc2.bias 6.68208369807828e-05\n",
      "fc3.weight 0.00012492606682436806\n",
      "fc3.bias 3.7092852289788424e-05\n",
      "\n",
      "Test set: Average loss: 2.1214 \n",
      "Accuracy: 1829/10000 (18.29%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00024841816888915167\n",
      "conv1.bias 0.00025392135527605814\n",
      "conv2.weight 0.0001802759120861689\n",
      "conv2.bias 0.00015320740931201726\n",
      "fc1.weight 9.924529989560445e-05\n",
      "fc1.bias 0.00010391039152940114\n",
      "fc2.weight 0.00011585644549793667\n",
      "fc2.bias 7.591617204958485e-05\n",
      "fc3.weight 9.477839228652773e-05\n",
      "fc3.bias 1.3217404193710535e-05\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00024841816888915167\n",
      "conv1.bias 0.00025392135527605814\n",
      "conv2.weight 0.0001802759120861689\n",
      "conv2.bias 0.00015320740931201726\n",
      "fc1.weight 9.924529989560445e-05\n",
      "fc1.bias 0.00010391039152940114\n",
      "fc2.weight 0.00011585644549793667\n",
      "fc2.bias 7.591617204958485e-05\n",
      "fc3.weight 9.477839228652773e-05\n",
      "fc3.bias 1.3217404193710535e-05\n",
      "\n",
      "Test set: Average loss: 2.1095 \n",
      "Accuracy: 1847/10000 (18.47%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00020890093512005277\n",
      "conv1.bias 0.0002927129001667102\n",
      "conv2.weight 0.00017560556530952454\n",
      "conv2.bias 0.0001690719072939828\n",
      "fc1.weight 0.00011001136898994446\n",
      "fc1.bias 0.00011660341794292131\n",
      "fc2.weight 0.00014290717386064076\n",
      "fc2.bias 9.235272937942119e-05\n",
      "fc3.weight 9.303093843516849e-05\n",
      "fc3.bias 8.235336281359196e-06\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00020890093512005277\n",
      "conv1.bias 0.0002927129001667102\n",
      "conv2.weight 0.00017560556530952454\n",
      "conv2.bias 0.0001690719072939828\n",
      "fc1.weight 0.00011001136898994446\n",
      "fc1.bias 0.00011660341794292131\n",
      "fc2.weight 0.00014290717386064076\n",
      "fc2.bias 9.235272937942119e-05\n",
      "fc3.weight 9.303093843516849e-05\n",
      "fc3.bias 8.235336281359196e-06\n",
      "\n",
      "Test set: Average loss: 2.1058 \n",
      "Accuracy: 1838/10000 (18.38%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0001860675877994961\n",
      "conv1.bias 0.00030285380004594725\n",
      "conv2.weight 0.00016958939532438913\n",
      "conv2.bias 0.00017709372332319617\n",
      "fc1.weight 0.00011389999588330586\n",
      "fc1.bias 0.00012349361398567756\n",
      "fc2.weight 0.00015939264779999142\n",
      "fc2.bias 0.00010682174581147375\n",
      "fc3.weight 9.590126574039459e-05\n",
      "fc3.bias 7.870881381677463e-06\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0001860675877994961\n",
      "conv1.bias 0.00030285380004594725\n",
      "conv2.weight 0.00016958939532438913\n",
      "conv2.bias 0.00017709372332319617\n",
      "fc1.weight 0.00011389999588330586\n",
      "fc1.bias 0.00012349361398567756\n",
      "fc2.weight 0.00015939264779999142\n",
      "fc2.bias 0.00010682174581147375\n",
      "fc3.weight 9.590126574039459e-05\n",
      "fc3.bias 7.870881381677463e-06\n",
      "\n",
      "Test set: Average loss: 2.1039 \n",
      "Accuracy: 1830/10000 (18.30%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00017467121283213296\n",
      "conv1.bias 0.0003056931503427525\n",
      "conv2.weight 0.00016536186138788858\n",
      "conv2.bias 0.00018204293155577034\n",
      "fc1.weight 0.00011558094620704651\n",
      "fc1.bias 0.00012806371475259463\n",
      "fc2.weight 0.00016891894832489983\n",
      "fc2.bias 0.00011705629350174041\n",
      "fc3.weight 9.91483706803549e-05\n",
      "fc3.bias 8.354744932148605e-06\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00017467121283213296\n",
      "conv1.bias 0.0003056931503427525\n",
      "conv2.weight 0.00016536186138788858\n",
      "conv2.bias 0.00018204293155577034\n",
      "fc1.weight 0.00011558094620704651\n",
      "fc1.bias 0.00012806371475259463\n",
      "fc2.weight 0.00016891894832489983\n",
      "fc2.bias 0.00011705629350174041\n",
      "fc3.weight 9.91483706803549e-05\n",
      "fc3.bias 8.354744932148605e-06\n",
      "\n",
      "Test set: Average loss: 2.1031 \n",
      "Accuracy: 1837/10000 (18.37%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000168939299053616\n",
      "conv1.bias 0.00030637522771333653\n",
      "conv2.weight 0.00016288209706544877\n",
      "conv2.bias 0.0001850831031333655\n",
      "fc1.weight 0.00011629631121953328\n",
      "fc1.bias 0.00013109884845713776\n",
      "fc2.weight 0.00017454054147478135\n",
      "fc2.bias 0.0001232093976189693\n",
      "fc3.weight 0.00010168155034383138\n",
      "fc3.bias 8.860558591550215e-06\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000168939299053616\n",
      "conv1.bias 0.00030637522771333653\n",
      "conv2.weight 0.00016288209706544877\n",
      "conv2.bias 0.0001850831031333655\n",
      "fc1.weight 0.00011629631121953328\n",
      "fc1.bias 0.00013109884845713776\n",
      "fc2.weight 0.00017454054147478135\n",
      "fc2.bias 0.0001232093976189693\n",
      "fc3.weight 0.00010168155034383138\n",
      "fc3.bias 8.860558591550215e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.1028 \n",
      "Accuracy: 1834/10000 (18.34%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0001660061213705275\n",
      "conv1.bias 0.0003069932960594694\n",
      "conv2.weight 0.00016129213074843088\n",
      "conv2.bias 0.00018667717813514173\n",
      "fc1.weight 0.00011655691266059876\n",
      "fc1.bias 0.00013296666244665782\n",
      "fc2.weight 0.00017790540106712825\n",
      "fc2.bias 0.0001264628843360004\n",
      "fc3.weight 0.00010336854805548986\n",
      "fc3.bias 9.206641698256135e-06\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0001660061213705275\n",
      "conv1.bias 0.0003069932960594694\n",
      "conv2.weight 0.00016129213074843088\n",
      "conv2.bias 0.00018667717813514173\n",
      "fc1.weight 0.00011655691266059876\n",
      "fc1.bias 0.00013296666244665782\n",
      "fc2.weight 0.00017790540106712825\n",
      "fc2.bias 0.0001264628843360004\n",
      "fc3.weight 0.00010336854805548986\n",
      "fc3.bias 9.206641698256135e-06\n",
      "\n",
      "Test set: Average loss: 2.1026 \n",
      "Accuracy: 1827/10000 (18.27%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016460327638520136\n",
      "conv1.bias 0.00030780376012747485\n",
      "conv2.weight 0.00016014815618594489\n",
      "conv2.bias 0.00018735404592007399\n",
      "fc1.weight 0.00011662856737772623\n",
      "fc1.bias 0.0001341862604022026\n",
      "fc2.weight 0.00017988310446814885\n",
      "fc2.bias 0.0001281155495061761\n",
      "fc3.weight 0.00010442578544219334\n",
      "fc3.bias 9.420336573384702e-06\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016460327638520136\n",
      "conv1.bias 0.00030780376012747485\n",
      "conv2.weight 0.00016014815618594489\n",
      "conv2.bias 0.00018735404592007399\n",
      "fc1.weight 0.00011662856737772623\n",
      "fc1.bias 0.0001341862604022026\n",
      "fc2.weight 0.00017988310446814885\n",
      "fc2.bias 0.0001281155495061761\n",
      "fc3.weight 0.00010442578544219334\n",
      "fc3.bias 9.420336573384702e-06\n",
      "\n",
      "Test set: Average loss: 2.1026 \n",
      "Accuracy: 1826/10000 (18.26%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0001640097465780046\n",
      "conv1.bias 0.00030899377694974345\n",
      "conv2.weight 0.00015928223729133606\n",
      "conv2.bias 0.00018773105693981051\n",
      "fc1.weight 0.00011665886640548706\n",
      "fc1.bias 0.00013498344148198764\n",
      "fc2.weight 0.00018100733794863263\n",
      "fc2.bias 0.00012888598610602675\n",
      "fc3.weight 0.00010506014916158857\n",
      "fc3.bias 9.55071154749021e-06\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0001640097465780046\n",
      "conv1.bias 0.00030899377694974345\n",
      "conv2.weight 0.00015928223729133606\n",
      "conv2.bias 0.00018773105693981051\n",
      "fc1.weight 0.00011665886640548706\n",
      "fc1.bias 0.00013498344148198764\n",
      "fc2.weight 0.00018100733794863263\n",
      "fc2.bias 0.00012888598610602675\n",
      "fc3.weight 0.00010506014916158857\n",
      "fc3.bias 9.55071154749021e-06\n",
      "\n",
      "Test set: Average loss: 2.1026 \n",
      "Accuracy: 1827/10000 (18.27%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0001637698213259379\n",
      "conv1.bias 0.0003097191608200471\n",
      "conv2.weight 0.00015859293440977733\n",
      "conv2.bias 0.00018792619812302291\n",
      "fc1.weight 0.00011667997638384502\n",
      "fc1.bias 0.0001356413122266531\n",
      "fc2.weight 0.00018160103095902336\n",
      "fc2.bias 0.00012921198226866268\n",
      "fc3.weight 0.00010539767820210684\n",
      "fc3.bias 9.627948020352051e-06\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0001637698213259379\n",
      "conv1.bias 0.0003097191608200471\n",
      "conv2.weight 0.00015859293440977733\n",
      "conv2.bias 0.00018792619812302291\n",
      "fc1.weight 0.00011667997638384502\n",
      "fc1.bias 0.0001356413122266531\n",
      "fc2.weight 0.00018160103095902336\n",
      "fc2.bias 0.00012921198226866268\n",
      "fc3.weight 0.00010539767820210684\n",
      "fc3.bias 9.627948020352051e-06\n",
      "\n",
      "Test set: Average loss: 2.1026 \n",
      "Accuracy: 1827/10000 (18.27%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0001636284589767456\n",
      "conv1.bias 0.0003104162751697004\n",
      "conv2.weight 0.000158042311668396\n",
      "conv2.bias 0.00018819281831383705\n",
      "fc1.weight 0.00011673390865325928\n",
      "fc1.bias 0.00013613519258797168\n",
      "fc2.weight 0.0001818613637061346\n",
      "fc2.bias 0.00012934052695830664\n",
      "fc3.weight 0.00010556080156848544\n",
      "fc3.bias 9.67282394412905e-06\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0001636284589767456\n",
      "conv1.bias 0.0003104162751697004\n",
      "conv2.weight 0.000158042311668396\n",
      "conv2.bias 0.00018819281831383705\n",
      "fc1.weight 0.00011673390865325928\n",
      "fc1.bias 0.00013613519258797168\n",
      "fc2.weight 0.0001818613637061346\n",
      "fc2.bias 0.00012934052695830664\n",
      "fc3.weight 0.00010556080156848544\n",
      "fc3.bias 9.67282394412905e-06\n",
      "\n",
      "Test set: Average loss: 2.1026 \n",
      "Accuracy: 1829/10000 (18.29%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0001635364360279507\n",
      "conv1.bias 0.00031085215353717405\n",
      "conv2.weight 0.00015764923145373662\n",
      "conv2.bias 0.00018851498316507787\n",
      "fc1.weight 0.00011682941516240438\n",
      "fc1.bias 0.00013664686121046544\n",
      "fc2.weight 0.00018195375090553648\n",
      "fc2.bias 0.00012931950567733673\n",
      "fc3.weight 0.0001056141530474027\n",
      "fc3.bias 9.701107046566904e-06\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0001635364360279507\n",
      "conv1.bias 0.00031085215353717405\n",
      "conv2.weight 0.00015764923145373662\n",
      "conv2.bias 0.00018851498316507787\n",
      "fc1.weight 0.00011682941516240438\n",
      "fc1.bias 0.00013664686121046544\n",
      "fc2.weight 0.00018195375090553648\n",
      "fc2.bias 0.00012931950567733673\n",
      "fc3.weight 0.0001056141530474027\n",
      "fc3.bias 9.701107046566904e-06\n",
      "\n",
      "Test set: Average loss: 2.1027 \n",
      "Accuracy: 1827/10000 (18.27%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016343305508295696\n",
      "conv1.bias 0.00031121447682380676\n",
      "conv2.weight 0.0001573662335673968\n",
      "conv2.bias 0.00018884929886553437\n",
      "fc1.weight 0.00011694578329722087\n",
      "fc1.bias 0.00013706339523196221\n",
      "fc2.weight 0.00018197805398986454\n",
      "fc2.bias 0.00012919942050107887\n",
      "fc3.weight 0.00010560025416669391\n",
      "fc3.bias 9.714902262203395e-06\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016343305508295696\n",
      "conv1.bias 0.00031121447682380676\n",
      "conv2.weight 0.0001573662335673968\n",
      "conv2.bias 0.00018884929886553437\n",
      "fc1.weight 0.00011694578329722087\n",
      "fc1.bias 0.00013706339523196221\n",
      "fc2.weight 0.00018197805398986454\n",
      "fc2.bias 0.00012919942050107887\n",
      "fc3.weight 0.00010560025416669391\n",
      "fc3.bias 9.714902262203395e-06\n",
      "\n",
      "Test set: Average loss: 2.1027 \n",
      "Accuracy: 1827/10000 (18.27%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016338292095396253\n",
      "conv1.bias 0.00031157499567295116\n",
      "conv2.weight 0.00015719542900721232\n",
      "conv2.bias 0.00018911695224232972\n",
      "fc1.weight 0.00011704287926355997\n",
      "fc1.bias 0.00013743269567688307\n",
      "fc2.weight 0.00018200089061071002\n",
      "fc2.bias 0.00012908775048951307\n",
      "fc3.weight 0.00010557211935520172\n",
      "fc3.bias 9.729353041620926e-06\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016338292095396253\n",
      "conv1.bias 0.00031157499567295116\n",
      "conv2.weight 0.00015719542900721232\n",
      "conv2.bias 0.00018911695224232972\n",
      "fc1.weight 0.00011704287926355997\n",
      "fc1.bias 0.00013743269567688307\n",
      "fc2.weight 0.00018200089061071002\n",
      "fc2.bias 0.00012908775048951307\n",
      "fc3.weight 0.00010557211935520172\n",
      "fc3.bias 9.729353041620926e-06\n",
      "\n",
      "Test set: Average loss: 2.1027 \n",
      "Accuracy: 1827/10000 (18.27%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016333777043554517\n",
      "conv1.bias 0.00031182175735011697\n",
      "conv2.weight 0.0001570813606182734\n",
      "conv2.bias 0.0001893266016850248\n",
      "fc1.weight 0.00011710487802823384\n",
      "fc1.bias 0.00013768714852631093\n",
      "fc2.weight 0.00018203453648658026\n",
      "fc2.bias 0.00012900205772547495\n",
      "fc3.weight 0.00010553972706908271\n",
      "fc3.bias 9.733442129800097e-06\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016333777043554517\n",
      "conv1.bias 0.00031182175735011697\n",
      "conv2.weight 0.0001570813606182734\n",
      "conv2.bias 0.0001893266016850248\n",
      "fc1.weight 0.00011710487802823384\n",
      "fc1.bias 0.00013768714852631093\n",
      "fc2.weight 0.00018203453648658026\n",
      "fc2.bias 0.00012900205772547495\n",
      "fc3.weight 0.00010553972706908271\n",
      "fc3.bias 9.733442129800097e-06\n",
      "\n",
      "Test set: Average loss: 2.1027 \n",
      "Accuracy: 1827/10000 (18.27%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016332364744610256\n",
      "conv1.bias 0.00031203479738906026\n",
      "conv2.weight 0.00015697763611872991\n",
      "conv2.bias 0.00018944969633594155\n",
      "fc1.weight 0.00011715187629063924\n",
      "fc1.bias 0.00013794166346391042\n",
      "fc2.weight 0.0001820745449217539\n",
      "fc2.bias 0.00012894682142706144\n",
      "fc3.weight 0.00010551460796878451\n",
      "fc3.bias 9.737368964124471e-06\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016332364744610256\n",
      "conv1.bias 0.00031203479738906026\n",
      "conv2.weight 0.00015697763611872991\n",
      "conv2.bias 0.00018944969633594155\n",
      "fc1.weight 0.00011715187629063924\n",
      "fc1.bias 0.00013794166346391042\n",
      "fc2.weight 0.0001820745449217539\n",
      "fc2.bias 0.00012894682142706144\n",
      "fc3.weight 0.00010551460796878451\n",
      "fc3.bias 9.737368964124471e-06\n",
      "\n",
      "Test set: Average loss: 2.1027 \n",
      "Accuracy: 1827/10000 (18.27%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0001633169584804111\n",
      "conv1.bias 0.0003122390868763129\n",
      "conv2.weight 0.00015692442655563355\n",
      "conv2.bias 0.0001895982277346775\n",
      "fc1.weight 0.00011717134714126587\n",
      "fc1.bias 0.00013809987964729468\n",
      "fc2.weight 0.00018211340620404197\n",
      "fc2.bias 0.0001288713732113441\n",
      "fc3.weight 0.00010548496530169533\n",
      "fc3.bias 9.740786481415853e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected users: [0 1]\n",
      "conv1.weight 0.0001633169584804111\n",
      "conv1.bias 0.0003122390868763129\n",
      "conv2.weight 0.00015692442655563355\n",
      "conv2.bias 0.0001895982277346775\n",
      "fc1.weight 0.00011717134714126587\n",
      "fc1.bias 0.00013809987964729468\n",
      "fc2.weight 0.00018211340620404197\n",
      "fc2.bias 0.0001288713732113441\n",
      "fc3.weight 0.00010548496530169533\n",
      "fc3.bias 9.740786481415853e-06\n",
      "\n",
      "Test set: Average loss: 2.1027 \n",
      "Accuracy: 1827/10000 (18.27%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0001633619103166792\n",
      "conv1.bias 0.00031216904365768033\n",
      "conv2.weight 0.00015687428414821626\n",
      "conv2.bias 0.00018966490461025387\n",
      "fc1.weight 0.00011719325184822083\n",
      "fc1.bias 0.00013826461508870125\n",
      "fc2.weight 0.00018213722440931532\n",
      "fc2.bias 0.00012884509661013172\n",
      "fc3.weight 0.00010546362471012842\n",
      "fc3.bias 9.748512820806354e-06\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0001633619103166792\n",
      "conv1.bias 0.00031216904365768033\n",
      "conv2.weight 0.00015687428414821626\n",
      "conv2.bias 0.00018966490461025387\n",
      "fc1.weight 0.00011719325184822083\n",
      "fc1.bias 0.00013826461508870125\n",
      "fc2.weight 0.00018213722440931532\n",
      "fc2.bias 0.00012884509661013172\n",
      "fc3.weight 0.00010546362471012842\n",
      "fc3.bias 9.748512820806354e-06\n",
      "\n",
      "Test set: Average loss: 2.1028 \n",
      "Accuracy: 1826/10000 (18.26%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016338982515864901\n",
      "conv1.bias 0.00031259052533035475\n",
      "conv2.weight 0.00015684922536214193\n",
      "conv2.bias 0.00018967805954162031\n",
      "fc1.weight 0.0001172058085600535\n",
      "fc1.bias 0.00013837066168586413\n",
      "fc2.weight 0.00018216446042060853\n",
      "fc2.bias 0.0001287833410536959\n",
      "fc3.weight 0.00010544808492774055\n",
      "fc3.bias 9.754024358699098e-06\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016338982515864901\n",
      "conv1.bias 0.00031259052533035475\n",
      "conv2.weight 0.00015684922536214193\n",
      "conv2.bias 0.00018967805954162031\n",
      "fc1.weight 0.0001172058085600535\n",
      "fc1.bias 0.00013837066168586413\n",
      "fc2.weight 0.00018216446042060853\n",
      "fc2.bias 0.0001287833410536959\n",
      "fc3.weight 0.00010544808492774055\n",
      "fc3.bias 9.754024358699098e-06\n",
      "\n",
      "Test set: Average loss: 2.1028 \n",
      "Accuracy: 1826/10000 (18.26%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016340305407842\n",
      "conv1.bias 0.0003123720719789465\n",
      "conv2.weight 0.00015683130671580633\n",
      "conv2.bias 0.00018966739298775792\n",
      "fc1.weight 0.0001172143816947937\n",
      "fc1.bias 0.00013845713498691719\n",
      "fc2.weight 0.00018218493177777245\n",
      "fc2.bias 0.00012876696529842558\n",
      "fc3.weight 0.00010542586623203187\n",
      "fc3.bias 9.755873907124624e-06\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016340305407842\n",
      "conv1.bias 0.0003123720719789465\n",
      "conv2.weight 0.00015683130671580633\n",
      "conv2.bias 0.00018966739298775792\n",
      "fc1.weight 0.0001172143816947937\n",
      "fc1.bias 0.00013845713498691719\n",
      "fc2.weight 0.00018218493177777245\n",
      "fc2.bias 0.00012876696529842558\n",
      "fc3.weight 0.00010542586623203187\n",
      "fc3.bias 9.755873907124624e-06\n",
      "\n",
      "Test set: Average loss: 2.1028 \n",
      "Accuracy: 1826/10000 (18.26%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016342133283615113\n",
      "conv1.bias 0.0003124737413600087\n",
      "conv2.weight 0.00015681552390257517\n",
      "conv2.bias 0.00018967701180372387\n",
      "fc1.weight 0.00011722349127133687\n",
      "fc1.bias 0.0001385352574288845\n",
      "fc2.weight 0.00018220418502414037\n",
      "fc2.bias 0.00012875598899665333\n",
      "fc3.weight 0.0001054243938553901\n",
      "fc3.bias 9.757917723618448e-06\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016342133283615113\n",
      "conv1.bias 0.0003124737413600087\n",
      "conv2.weight 0.00015681552390257517\n",
      "conv2.bias 0.00018967701180372387\n",
      "fc1.weight 0.00011722349127133687\n",
      "fc1.bias 0.0001385352574288845\n",
      "fc2.weight 0.00018220418502414037\n",
      "fc2.bias 0.00012875598899665333\n",
      "fc3.weight 0.0001054243938553901\n",
      "fc3.bias 9.757917723618448e-06\n",
      "\n",
      "Test set: Average loss: 2.1028 \n",
      "Accuracy: 1826/10000 (18.26%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0001634337173567878\n",
      "conv1.bias 0.0003127040884767969\n",
      "conv2.weight 0.00015680121878782909\n",
      "conv2.bias 0.00018967680807691067\n",
      "fc1.weight 0.00011722481250762939\n",
      "fc1.bias 0.0001385776015619437\n",
      "fc2.weight 0.00018221137542573233\n",
      "fc2.bias 0.00012872164093312763\n",
      "fc3.weight 0.00010540999826930818\n",
      "fc3.bias 9.759610838955268e-06\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0001634337173567878\n",
      "conv1.bias 0.0003127040884767969\n",
      "conv2.weight 0.00015680121878782909\n",
      "conv2.bias 0.00018967680807691067\n",
      "fc1.weight 0.00011722481250762939\n",
      "fc1.bias 0.0001385776015619437\n",
      "fc2.weight 0.00018221137542573233\n",
      "fc2.bias 0.00012872164093312763\n",
      "fc3.weight 0.00010540999826930818\n",
      "fc3.bias 9.759610838955268e-06\n",
      "\n",
      "Test set: Average loss: 2.1028 \n",
      "Accuracy: 1827/10000 (18.27%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016342769066492717\n",
      "conv1.bias 0.0003124701712901394\n",
      "conv2.weight 0.00015679885943730673\n",
      "conv2.bias 0.00018972148245666176\n",
      "fc1.weight 0.00011721684535344441\n",
      "fc1.bias 0.00013861991465091705\n",
      "fc2.weight 0.00018221222692065768\n",
      "fc2.bias 0.00012870213859492825\n",
      "fc3.weight 0.00010539634774128595\n",
      "fc3.bias 9.758534724824131e-06\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016342769066492717\n",
      "conv1.bias 0.0003124701712901394\n",
      "conv2.weight 0.00015679885943730673\n",
      "conv2.bias 0.00018972148245666176\n",
      "fc1.weight 0.00011721684535344441\n",
      "fc1.bias 0.00013861991465091705\n",
      "fc2.weight 0.00018221222692065768\n",
      "fc2.bias 0.00012870213859492825\n",
      "fc3.weight 0.00010539634774128595\n",
      "fc3.bias 9.758534724824131e-06\n",
      "\n",
      "Test set: Average loss: 2.1028 \n",
      "Accuracy: 1827/10000 (18.27%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016344600253634983\n",
      "conv1.bias 0.00031263425868625444\n",
      "conv2.weight 0.00015677774945894877\n",
      "conv2.bias 0.00018968047515954822\n",
      "fc1.weight 0.00011721436182657878\n",
      "fc1.bias 0.00013866367128988107\n",
      "fc2.weight 0.00018220898650941395\n",
      "fc2.bias 0.0001286937566917567\n",
      "fc3.weight 0.00010539608164912178\n",
      "fc3.bias 9.761041292222217e-06\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016344600253634983\n",
      "conv1.bias 0.00031263425868625444\n",
      "conv2.weight 0.00015677774945894877\n",
      "conv2.bias 0.00018968047515954822\n",
      "fc1.weight 0.00011721436182657878\n",
      "fc1.bias 0.00013866367128988107\n",
      "fc2.weight 0.00018220898650941395\n",
      "fc2.bias 0.0001286937566917567\n",
      "fc3.weight 0.00010539608164912178\n",
      "fc3.bias 9.761041292222217e-06\n",
      "\n",
      "Test set: Average loss: 2.1028 \n",
      "Accuracy: 1827/10000 (18.27%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016345067156685723\n",
      "conv1.bias 0.0003128031773182253\n",
      "conv2.weight 0.0001567785193522771\n",
      "conv2.bias 0.0001896962057799101\n",
      "fc1.weight 0.0001172143816947937\n",
      "fc1.bias 0.00013869414106011392\n",
      "fc2.weight 0.00018220539130861798\n",
      "fc2.bias 0.000128687425915684\n",
      "fc3.weight 0.00010538932290815172\n",
      "fc3.bias 9.76281298790127e-06\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016345067156685723\n",
      "conv1.bias 0.0003128031773182253\n",
      "conv2.weight 0.0001567785193522771\n",
      "conv2.bias 0.0001896962057799101\n",
      "fc1.weight 0.0001172143816947937\n",
      "fc1.bias 0.00013869414106011392\n",
      "fc2.weight 0.00018220539130861798\n",
      "fc2.bias 0.000128687425915684\n",
      "fc3.weight 0.00010538932290815172\n",
      "fc3.bias 9.76281298790127e-06\n",
      "\n",
      "Test set: Average loss: 2.1028 \n",
      "Accuracy: 1827/10000 (18.27%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0001634707384639316\n",
      "conv1.bias 0.0003129004423196117\n",
      "conv2.weight 0.0001567720000942548\n",
      "conv2.bias 0.00018969080701936036\n",
      "fc1.weight 0.00011720811327298482\n",
      "fc1.bias 0.00013875414927800496\n",
      "fc2.weight 0.0001821984847386678\n",
      "fc2.bias 0.00012869668370556263\n",
      "fc3.weight 0.00010538777070386069\n",
      "fc3.bias 9.763879643287509e-06\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0001634707384639316\n",
      "conv1.bias 0.0003129004423196117\n",
      "conv2.weight 0.0001567720000942548\n",
      "conv2.bias 0.00018969080701936036\n",
      "fc1.weight 0.00011720811327298482\n",
      "fc1.bias 0.00013875414927800496\n",
      "fc2.weight 0.0001821984847386678\n",
      "fc2.bias 0.00012869668370556263\n",
      "fc3.weight 0.00010538777070386069\n",
      "fc3.bias 9.763879643287509e-06\n",
      "\n",
      "Test set: Average loss: 2.1028 \n",
      "Accuracy: 1827/10000 (18.27%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016348246071073744\n",
      "conv1.bias 0.00031255513507251936\n",
      "conv2.weight 0.0001567645122607549\n",
      "conv2.bias 0.00018965144408866763\n",
      "fc1.weight 0.00011720767617225647\n",
      "fc1.bias 0.00013876485948761304\n",
      "fc2.weight 0.00018220023503379216\n",
      "fc2.bias 0.00012868248103629974\n",
      "fc3.weight 0.00010538417845964431\n",
      "fc3.bias 9.763463458511978e-06\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016348246071073744\n",
      "conv1.bias 0.00031255513507251936\n",
      "conv2.weight 0.0001567645122607549\n",
      "conv2.bias 0.00018965144408866763\n",
      "fc1.weight 0.00011720767617225647\n",
      "fc1.bias 0.00013876485948761304\n",
      "fc2.weight 0.00018220023503379216\n",
      "fc2.bias 0.00012868248103629974\n",
      "fc3.weight 0.00010538417845964431\n",
      "fc3.bias 9.763463458511978e-06\n",
      "\n",
      "Test set: Average loss: 2.1028 \n",
      "Accuracy: 1827/10000 (18.27%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016347683138317532\n",
      "conv1.bias 0.00031285814475268126\n",
      "conv2.weight 0.0001567595824599266\n",
      "conv2.bias 0.00018973441910929978\n",
      "fc1.weight 0.00011720552047093709\n",
      "fc1.bias 0.00013877299303809802\n",
      "fc2.weight 0.0001821958829486181\n",
      "fc2.bias 0.00012869488758345446\n",
      "fc3.weight 0.00010538026690483093\n",
      "fc3.bias 9.764605783857405e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected users: [0 1]\n",
      "conv1.weight 0.00016347683138317532\n",
      "conv1.bias 0.00031285814475268126\n",
      "conv2.weight 0.0001567595824599266\n",
      "conv2.bias 0.00018973441910929978\n",
      "fc1.weight 0.00011720552047093709\n",
      "fc1.bias 0.00013877299303809802\n",
      "fc2.weight 0.0001821958829486181\n",
      "fc2.bias 0.00012869488758345446\n",
      "fc3.weight 0.00010538026690483093\n",
      "fc3.bias 9.764605783857405e-06\n",
      "\n",
      "Test set: Average loss: 2.1028 \n",
      "Accuracy: 1827/10000 (18.27%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016347509291436937\n",
      "conv1.bias 0.00031280853242302936\n",
      "conv2.weight 0.00015675269067287446\n",
      "conv2.bias 0.00018973901751451194\n",
      "fc1.weight 0.00011720357338587443\n",
      "fc1.bias 0.0001387767493724823\n",
      "fc2.weight 0.00018220172514991155\n",
      "fc2.bias 0.00012869840221745626\n",
      "fc3.weight 0.0001053892874291965\n",
      "fc3.bias 9.763317939359694e-06\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016347509291436937\n",
      "conv1.bias 0.00031280853242302936\n",
      "conv2.weight 0.00015675269067287446\n",
      "conv2.bias 0.00018973901751451194\n",
      "fc1.weight 0.00011720357338587443\n",
      "fc1.bias 0.0001387767493724823\n",
      "fc2.weight 0.00018220172514991155\n",
      "fc2.bias 0.00012869840221745626\n",
      "fc3.weight 0.0001053892874291965\n",
      "fc3.bias 9.763317939359694e-06\n",
      "\n",
      "Test set: Average loss: 2.1028 \n",
      "Accuracy: 1827/10000 (18.27%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016349309020572238\n",
      "conv1.bias 0.00031270730930070084\n",
      "conv2.weight 0.00015674956142902374\n",
      "conv2.bias 0.00018976753926835954\n",
      "fc1.weight 0.00011720148722330729\n",
      "fc1.bias 0.0001387675292789936\n",
      "fc2.weight 0.00018220207993946378\n",
      "fc2.bias 0.00012870476625504948\n",
      "fc3.weight 0.00010539305706818899\n",
      "fc3.bias 9.765592403709889e-06\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016349309020572238\n",
      "conv1.bias 0.00031270730930070084\n",
      "conv2.weight 0.00015674956142902374\n",
      "conv2.bias 0.00018976753926835954\n",
      "fc1.weight 0.00011720148722330729\n",
      "fc1.bias 0.0001387675292789936\n",
      "fc2.weight 0.00018220207993946378\n",
      "fc2.bias 0.00012870476625504948\n",
      "fc3.weight 0.00010539305706818899\n",
      "fc3.bias 9.765592403709889e-06\n",
      "\n",
      "Test set: Average loss: 2.1028 \n",
      "Accuracy: 1827/10000 (18.27%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016348974572287666\n",
      "conv1.bias 0.0003128483852682014\n",
      "conv2.weight 0.0001567437748114268\n",
      "conv2.bias 0.0001898169721243903\n",
      "fc1.weight 0.00011720114946365356\n",
      "fc1.bias 0.00013878072301546732\n",
      "fc2.weight 0.00018220170149727474\n",
      "fc2.bias 0.00012870046443172863\n",
      "fc3.weight 0.00010539295063132332\n",
      "fc3.bias 9.76479786913842e-06\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016348974572287666\n",
      "conv1.bias 0.0003128483852682014\n",
      "conv2.weight 0.0001567437748114268\n",
      "conv2.bias 0.0001898169721243903\n",
      "fc1.weight 0.00011720114946365356\n",
      "fc1.bias 0.00013878072301546732\n",
      "fc2.weight 0.00018220170149727474\n",
      "fc2.bias 0.00012870046443172863\n",
      "fc3.weight 0.00010539295063132332\n",
      "fc3.bias 9.76479786913842e-06\n",
      "\n",
      "Test set: Average loss: 2.1028 \n",
      "Accuracy: 1827/10000 (18.27%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016349979572825962\n",
      "conv1.bias 0.0003128812531940639\n",
      "conv2.weight 0.00015673814962307612\n",
      "conv2.bias 0.00018977673607878387\n",
      "fc1.weight 0.0001172005037466685\n",
      "fc1.bias 0.00013877322586874167\n",
      "fc2.weight 0.00018219655704876733\n",
      "fc2.bias 0.00012871431231143928\n",
      "fc3.weight 0.00010539305706818899\n",
      "fc3.bias 9.765825234353543e-06\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016349979572825962\n",
      "conv1.bias 0.0003128812531940639\n",
      "conv2.weight 0.00015673814962307612\n",
      "conv2.bias 0.00018977673607878387\n",
      "fc1.weight 0.0001172005037466685\n",
      "fc1.bias 0.00013877322586874167\n",
      "fc2.weight 0.00018219655704876733\n",
      "fc2.bias 0.00012871431231143928\n",
      "fc3.weight 0.00010539305706818899\n",
      "fc3.bias 9.765825234353543e-06\n",
      "\n",
      "Test set: Average loss: 2.1028 \n",
      "Accuracy: 1827/10000 (18.27%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016349827249844868\n",
      "conv1.bias 0.0003129412846950193\n",
      "conv2.weight 0.0001567459354797999\n",
      "conv2.bias 0.0001897712063509971\n",
      "fc1.weight 0.00011720207333564758\n",
      "fc1.bias 0.00013878357907136282\n",
      "fc2.weight 0.00018219396708503603\n",
      "fc2.bias 0.00012873555533587933\n",
      "fc3.weight 0.00010539600182147253\n",
      "fc3.bias 9.765812137629837e-06\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016349827249844868\n",
      "conv1.bias 0.0003129412846950193\n",
      "conv2.weight 0.0001567459354797999\n",
      "conv2.bias 0.0001897712063509971\n",
      "fc1.weight 0.00011720207333564758\n",
      "fc1.bias 0.00013878357907136282\n",
      "fc2.weight 0.00018219396708503603\n",
      "fc2.bias 0.00012873555533587933\n",
      "fc3.weight 0.00010539600182147253\n",
      "fc3.bias 9.765812137629837e-06\n",
      "\n",
      "Test set: Average loss: 2.1028 \n",
      "Accuracy: 1827/10000 (18.27%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016349565651681687\n",
      "conv1.bias 0.00031272230747466284\n",
      "conv2.weight 0.00015675367166598637\n",
      "conv2.bias 0.0001897323818411678\n",
      "fc1.weight 0.000117202361424764\n",
      "fc1.bias 0.0001387958104411761\n",
      "fc2.weight 0.00018219840195443895\n",
      "fc2.bias 0.00012872079830794107\n",
      "fc3.weight 0.00010539059128080095\n",
      "fc3.bias 9.76593219093047e-06\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016349565651681687\n",
      "conv1.bias 0.00031272230747466284\n",
      "conv2.weight 0.00015675367166598637\n",
      "conv2.bias 0.0001897323818411678\n",
      "fc1.weight 0.000117202361424764\n",
      "fc1.bias 0.0001387958104411761\n",
      "fc2.weight 0.00018219840195443895\n",
      "fc2.bias 0.00012872079830794107\n",
      "fc3.weight 0.00010539059128080095\n",
      "fc3.bias 9.76593219093047e-06\n",
      "\n",
      "Test set: Average loss: 2.1028 \n",
      "Accuracy: 1827/10000 (18.27%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000163508587413364\n",
      "conv1.bias 0.0003127516635383169\n",
      "conv2.weight 0.0001567550003528595\n",
      "conv2.bias 0.00018974117119796574\n",
      "fc1.weight 0.0001172026793162028\n",
      "fc1.bias 0.00013880681556959946\n",
      "fc2.weight 0.00018219736123841905\n",
      "fc2.bias 0.00012870652911563715\n",
      "fc3.weight 0.0001053885512408756\n",
      "fc3.bias 9.766237053554504e-06\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.000163508587413364\n",
      "conv1.bias 0.0003127516635383169\n",
      "conv2.weight 0.0001567550003528595\n",
      "conv2.bias 0.00018974117119796574\n",
      "fc1.weight 0.0001172026793162028\n",
      "fc1.bias 0.00013880681556959946\n",
      "fc2.weight 0.00018219736123841905\n",
      "fc2.bias 0.00012870652911563715\n",
      "fc3.weight 0.0001053885512408756\n",
      "fc3.bias 9.766237053554504e-06\n",
      "\n",
      "Test set: Average loss: 2.1028 \n",
      "Accuracy: 1827/10000 (18.27%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016351360413763258\n",
      "conv1.bias 0.0003128280901970963\n",
      "conv2.weight 0.0001567483941713969\n",
      "conv2.bias 0.00018968853692058474\n",
      "fc1.weight 0.00011720012625058493\n",
      "fc1.bias 0.00013880014109114805\n",
      "fc2.weight 0.00018219357681652855\n",
      "fc2.bias 0.00012869334646633694\n",
      "fc3.weight 0.00010539008570568903\n",
      "fc3.bias 9.7662654297892e-06\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016351360413763258\n",
      "conv1.bias 0.0003128280901970963\n",
      "conv2.weight 0.0001567483941713969\n",
      "conv2.bias 0.00018968853692058474\n",
      "fc1.weight 0.00011720012625058493\n",
      "fc1.bias 0.00013880014109114805\n",
      "fc2.weight 0.00018219357681652855\n",
      "fc2.bias 0.00012869334646633694\n",
      "fc3.weight 0.00010539008570568903\n",
      "fc3.bias 9.7662654297892e-06\n",
      "\n",
      "Test set: Average loss: 2.1028 \n",
      "Accuracy: 1827/10000 (18.27%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016350706418355305\n",
      "conv1.bias 0.0003128054862221082\n",
      "conv2.weight 0.00015674526492754617\n",
      "conv2.bias 0.00018964103946927935\n",
      "fc1.weight 0.00011720374226570129\n",
      "fc1.bias 0.00013882297401626904\n",
      "fc2.weight 0.00018219503145369272\n",
      "fc2.bias 0.00012868759222328663\n",
      "fc3.weight 0.00010538812549341293\n",
      "fc3.bias 9.767813025973737e-06\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016350706418355305\n",
      "conv1.bias 0.0003128054862221082\n",
      "conv2.weight 0.00015674526492754617\n",
      "conv2.bias 0.00018964103946927935\n",
      "fc1.weight 0.00011720374226570129\n",
      "fc1.bias 0.00013882297401626904\n",
      "fc2.weight 0.00018219503145369272\n",
      "fc2.bias 0.00012868759222328663\n",
      "fc3.weight 0.00010538812549341293\n",
      "fc3.bias 9.767813025973737e-06\n",
      "\n",
      "Test set: Average loss: 2.1028 \n",
      "Accuracy: 1827/10000 (18.27%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016351630290349325\n",
      "conv1.bias 0.0003128597551646332\n",
      "conv2.weight 0.00015674911439418792\n",
      "conv2.bias 0.00018964242190122604\n",
      "fc1.weight 0.00011720158656438192\n",
      "fc1.bias 0.0001388315266619126\n",
      "fc2.weight 0.00018219581199070765\n",
      "fc2.bias 0.00012868627284963927\n",
      "fc3.weight 0.00010538405428330103\n",
      "fc3.bias 9.766653238330037e-06\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016351630290349325\n",
      "conv1.bias 0.0003128597551646332\n",
      "conv2.weight 0.00015674911439418792\n",
      "conv2.bias 0.00018964242190122604\n",
      "fc1.weight 0.00011720158656438192\n",
      "fc1.bias 0.0001388315266619126\n",
      "fc2.weight 0.00018219581199070765\n",
      "fc2.bias 0.00012868627284963927\n",
      "fc3.weight 0.00010538405428330103\n",
      "fc3.bias 9.766653238330037e-06\n",
      "\n",
      "Test set: Average loss: 2.1028 \n",
      "Accuracy: 1827/10000 (18.27%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016350713041093614\n",
      "conv1.bias 0.00031276214091728133\n",
      "conv2.weight 0.00015674407283465067\n",
      "conv2.bias 0.00018968936637975276\n",
      "fc1.weight 0.00011719847718874613\n",
      "fc1.bias 0.00013882100271681944\n",
      "fc2.weight 0.00018219283175846887\n",
      "fc2.bias 0.00012867652722412632\n",
      "fc3.weight 0.00010538425828729356\n",
      "fc3.bias 9.767893061507493e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected users: [0 1]\n",
      "conv1.weight 0.00016350713041093614\n",
      "conv1.bias 0.00031276214091728133\n",
      "conv2.weight 0.00015674407283465067\n",
      "conv2.bias 0.00018968936637975276\n",
      "fc1.weight 0.00011719847718874613\n",
      "fc1.bias 0.00013882100271681944\n",
      "fc2.weight 0.00018219283175846887\n",
      "fc2.bias 0.00012867652722412632\n",
      "fc3.weight 0.00010538425828729356\n",
      "fc3.bias 9.767893061507493e-06\n",
      "\n",
      "Test set: Average loss: 2.1028 \n",
      "Accuracy: 1827/10000 (18.27%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0001635232236650255\n",
      "conv1.bias 0.0003128237440250814\n",
      "conv2.weight 0.00015674586097399394\n",
      "conv2.bias 0.0001897143665701151\n",
      "fc1.weight 0.0001171966791152954\n",
      "fc1.bias 0.00013880888000130653\n",
      "fc2.weight 0.0001821912233791654\n",
      "fc2.bias 0.00012868495347599188\n",
      "fc3.weight 0.00010538167719330107\n",
      "fc3.bias 9.767166920937597e-06\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0001635232236650255\n",
      "conv1.bias 0.0003128237440250814\n",
      "conv2.weight 0.00015674586097399394\n",
      "conv2.bias 0.0001897143665701151\n",
      "fc1.weight 0.0001171966791152954\n",
      "fc1.bias 0.00013880888000130653\n",
      "fc2.weight 0.0001821912233791654\n",
      "fc2.bias 0.00012868495347599188\n",
      "fc3.weight 0.00010538167719330107\n",
      "fc3.bias 9.767166920937597e-06\n",
      "\n",
      "Test set: Average loss: 2.1028 \n",
      "Accuracy: 1827/10000 (18.27%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016351550817489624\n",
      "conv1.bias 0.0003127148568940659\n",
      "conv2.weight 0.00015675303836663564\n",
      "conv2.bias 0.0001897021575132385\n",
      "fc1.weight 0.00011719731489817302\n",
      "fc1.bias 0.00013879725399116674\n",
      "fc2.weight 0.00018219918249145386\n",
      "fc2.bias 0.0001287027040407771\n",
      "fc3.weight 0.00010538741591430846\n",
      "fc3.bias 9.767313167685643e-06\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016351550817489624\n",
      "conv1.bias 0.0003127148568940659\n",
      "conv2.weight 0.00015675303836663564\n",
      "conv2.bias 0.0001897021575132385\n",
      "fc1.weight 0.00011719731489817302\n",
      "fc1.bias 0.00013879725399116674\n",
      "fc2.weight 0.00018219918249145386\n",
      "fc2.bias 0.0001287027040407771\n",
      "fc3.weight 0.00010538741591430846\n",
      "fc3.bias 9.767313167685643e-06\n",
      "\n",
      "Test set: Average loss: 2.1028 \n",
      "Accuracy: 1827/10000 (18.27%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016351663404040866\n",
      "conv1.bias 0.0003127926611341536\n",
      "conv2.weight 0.00015675291419029236\n",
      "conv2.bias 0.00018976072897203267\n",
      "fc1.weight 0.00011719758311907451\n",
      "fc1.bias 0.000138800498098135\n",
      "fc2.weight 0.00018219555181170265\n",
      "fc2.bias 0.00012870925656031994\n",
      "fc3.weight 0.00010538542909281595\n",
      "fc3.bias 9.766129369381816e-06\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016351663404040866\n",
      "conv1.bias 0.0003127926611341536\n",
      "conv2.weight 0.00015675291419029236\n",
      "conv2.bias 0.00018976072897203267\n",
      "fc1.weight 0.00011719758311907451\n",
      "fc1.bias 0.000138800498098135\n",
      "fc2.weight 0.00018219555181170265\n",
      "fc2.bias 0.00012870925656031994\n",
      "fc3.weight 0.00010538542909281595\n",
      "fc3.bias 9.766129369381816e-06\n",
      "\n",
      "Test set: Average loss: 2.1028 \n",
      "Accuracy: 1827/10000 (18.27%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016352226336797078\n",
      "conv1.bias 0.00031280094602455694\n",
      "conv2.weight 0.00015675333638985953\n",
      "conv2.bias 0.00018978060688823462\n",
      "fc1.weight 0.00011719842751820882\n",
      "fc1.bias 0.00013882145285606384\n",
      "fc2.weight 0.00018219614312762306\n",
      "fc2.bias 0.0001287226720402638\n",
      "fc3.weight 0.00010538683051154727\n",
      "fc3.bias 9.76690062088892e-06\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016352226336797078\n",
      "conv1.bias 0.00031280094602455694\n",
      "conv2.weight 0.00015675333638985953\n",
      "conv2.bias 0.00018978060688823462\n",
      "fc1.weight 0.00011719842751820882\n",
      "fc1.bias 0.00013882145285606384\n",
      "fc2.weight 0.00018219614312762306\n",
      "fc2.bias 0.0001287226720402638\n",
      "fc3.weight 0.00010538683051154727\n",
      "fc3.bias 9.76690062088892e-06\n",
      "\n",
      "Test set: Average loss: 2.1028 \n",
      "Accuracy: 1827/10000 (18.27%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0001635191175672743\n",
      "conv1.bias 0.00031298639563222724\n",
      "conv2.weight 0.0001567470779021581\n",
      "conv2.bias 0.0001897024194477126\n",
      "fc1.weight 0.00011719766259193421\n",
      "fc1.bias 0.000138802919536829\n",
      "fc2.weight 0.00018219507875896635\n",
      "fc2.bias 0.00012869608499819325\n",
      "fc3.weight 0.00010538995265960694\n",
      "fc3.bias 9.766915172804148e-06\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0001635191175672743\n",
      "conv1.bias 0.00031298639563222724\n",
      "conv2.weight 0.0001567470779021581\n",
      "conv2.bias 0.0001897024194477126\n",
      "fc1.weight 0.00011719766259193421\n",
      "fc1.bias 0.000138802919536829\n",
      "fc2.weight 0.00018219507875896635\n",
      "fc2.bias 0.00012869608499819325\n",
      "fc3.weight 0.00010538995265960694\n",
      "fc3.bias 9.766915172804148e-06\n",
      "\n",
      "Test set: Average loss: 2.1028 \n",
      "Accuracy: 1827/10000 (18.27%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016351666715410022\n",
      "conv1.bias 0.0003129590768367052\n",
      "conv2.weight 0.0001567409187555313\n",
      "conv2.bias 0.00018975825514644384\n",
      "fc1.weight 0.00011719826857248941\n",
      "fc1.bias 0.00013882657513022422\n",
      "fc2.weight 0.000182193907953444\n",
      "fc2.bias 0.0001286914505596672\n",
      "fc3.weight 0.00010539041388602484\n",
      "fc3.bias 9.766787115950137e-06\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016351666715410022\n",
      "conv1.bias 0.0003129590768367052\n",
      "conv2.weight 0.0001567409187555313\n",
      "conv2.bias 0.00018975825514644384\n",
      "fc1.weight 0.00011719826857248941\n",
      "fc1.bias 0.00013882657513022422\n",
      "fc2.weight 0.000182193907953444\n",
      "fc2.bias 0.0001286914505596672\n",
      "fc3.weight 0.00010539041388602484\n",
      "fc3.bias 9.766787115950137e-06\n",
      "\n",
      "Test set: Average loss: 2.1028 \n",
      "Accuracy: 1827/10000 (18.27%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016352530982759265\n",
      "conv1.bias 0.00031294458312913775\n",
      "conv2.weight 0.00015673937896887462\n",
      "conv2.bias 0.00018977656145580113\n",
      "fc1.weight 0.00011719788114229838\n",
      "fc1.bias 0.0001388144058485826\n",
      "fc2.weight 0.0001821919566109067\n",
      "fc2.bias 0.00012869114011880898\n",
      "fc3.weight 0.00010539123877173378\n",
      "fc3.bias 9.765817958395928e-06\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016352530982759265\n",
      "conv1.bias 0.00031294458312913775\n",
      "conv2.weight 0.00015673937896887462\n",
      "conv2.bias 0.00018977656145580113\n",
      "fc1.weight 0.00011719788114229838\n",
      "fc1.bias 0.0001388144058485826\n",
      "fc2.weight 0.0001821919566109067\n",
      "fc2.bias 0.00012869114011880898\n",
      "fc3.weight 0.00010539123877173378\n",
      "fc3.bias 9.765817958395928e-06\n",
      "\n",
      "Test set: Average loss: 2.1028 \n",
      "Accuracy: 1827/10000 (18.27%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016353062457508512\n",
      "conv1.bias 0.0003128807875327766\n",
      "conv2.weight 0.0001567337413628896\n",
      "conv2.bias 0.00018979325250256807\n",
      "fc1.weight 0.00011719629168510437\n",
      "fc1.bias 0.00013880499949057897\n",
      "fc2.weight 0.00018219400256399125\n",
      "fc2.bias 0.00012869241514376232\n",
      "fc3.weight 0.00010539061789001737\n",
      "fc3.bias 9.765868162503466e-06\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016353062457508512\n",
      "conv1.bias 0.0003128807875327766\n",
      "conv2.weight 0.0001567337413628896\n",
      "conv2.bias 0.00018979325250256807\n",
      "fc1.weight 0.00011719629168510437\n",
      "fc1.bias 0.00013880499949057897\n",
      "fc2.weight 0.00018219400256399125\n",
      "fc2.bias 0.00012869241514376232\n",
      "fc3.weight 0.00010539061789001737\n",
      "fc3.bias 9.765868162503466e-06\n",
      "\n",
      "Test set: Average loss: 2.1028 \n",
      "Accuracy: 1827/10000 (18.27%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "G = 2\n",
    "N = 4 # N should be divisible by G\n",
    "K = 4 # N should be divisible by G\n",
    "\n",
    "size_per_group = int(50000/G)\n",
    "\n",
    "X_group = np.reshape(encoding_input_array_np, (G,size_per_group,32*32*3))\n",
    "y_group = np.reshape(encoding_label_array_np, (G,size_per_group,args.num_classes)) \n",
    "\n",
    "N_i = int(N/G) # = 2\n",
    "K_i = int(K/G) # = 2\n",
    "T = 3\n",
    "sigma = 0.1\n",
    "Noise_Alloc = [0,2,4]\n",
    "m = N_i # number of selected workers (if there is no straggler, m=N_i)\n",
    "\n",
    "print(N_i,K_i,T)\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K_i+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K_i+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K_i+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "z_array = np.array([-0.81, 0.81])\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "N_trials = 1\n",
    "\n",
    "N_epochs = 50\n",
    "\n",
    "lr_array = [0.0003, 0.0001, 0.00001]\n",
    "\n",
    "\n",
    "loss_test_arr_K4_G2 = np.zeros((len(lr_array),N_epochs))\n",
    "acc_test_arr_K4_G2  = np.zeros((len(lr_array),N_epochs))\n",
    "\n",
    "for lr_idx in range(len(lr_array)):\n",
    "    \n",
    "    args.lr = lr_array[lr_idx]\n",
    "    \n",
    "    print('##########################################')\n",
    "    print('Learning Rate =',args.lr)\n",
    "    print('\\n\\n')\n",
    "        \n",
    "#     print('##########################################')\n",
    "#     print('######',trial_idx,'-th Trial!! ###########')\n",
    "    \n",
    "    net_glob = CNNCifar(args=args)\n",
    "    net_glob.cuda()\n",
    "    net_glob.train()\n",
    "    \n",
    "    # copy weights\n",
    "    w_glob = net_glob.state_dict()\n",
    "    \n",
    "    X_tilde = np.empty((N,Size_submatrices,32*32*3))\n",
    "    y_tilde = np.empty((N,Size_submatrices,10))\n",
    "    \n",
    "    for G_idx in range(G):\n",
    "        \n",
    "        _Noise_label = np.ones((size_per_group*T,10)) * 0.1\n",
    "        \n",
    "        X_tilde_tmp,a,b = BACC_Enc_Data_v3(X_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde_tmp,a,b = BACC_Enc_Data_v3(y_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "        \n",
    "        stt_pos = G_idx * N_i\n",
    "        end_pos = (G_idx+1) * N_i\n",
    "        \n",
    "        X_tilde[stt_pos:end_pos,:,:] = X_tilde_tmp\n",
    "        y_tilde[stt_pos:end_pos,:,:] = y_tilde_tmp\n",
    "        \n",
    "   \n",
    "\n",
    "    for iter in range(N_epochs): #args.epochs\n",
    "        \n",
    "        w_group_array = []\n",
    "        for G_idx in range(G):\n",
    "            w_locals, loss_locals = [], []\n",
    "            idxs_users = np.random.choice(range(N_i), m, replace=False)\n",
    "            idxs_users = np.sort(idxs_users)\n",
    "            print('selected users:',idxs_users)\n",
    "\n",
    "            coded_net = BACC_Enc_Model_withNoise_v4(net_glob.cuda(), N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "            dec_z_array = []\n",
    "            for idx in idxs_users: #for idx in range(N):\n",
    "        #         print(idx)\n",
    "                local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[G_idx*N_i+idx,:,:], label=y_tilde[G_idx*N_i+idx,:,:])\n",
    "                w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "#                 w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                w_locals.append(copy.deepcopy(w))\n",
    "                loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "            # update global weights\n",
    "            #w_glob = FedAvg(w_locals)\n",
    "            w_group = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "            \n",
    "            w_group_array.append(copy.deepcopy(w_group))\n",
    "        \n",
    "        w_glob = copy.deepcopy(w_group_array[0])\n",
    "        for k in w_glob.keys():\n",
    "            for G_idx in range(1,G):\n",
    "                w_glob[k] += w_group_array[G_idx][k]\n",
    "            w_glob[k] = torch.div(w_glob[k], len(w_group_array))\n",
    "        \n",
    "        # copy weight to net_glob\n",
    "        net_glob.load_state_dict(w_glob)\n",
    "\n",
    "        # print loss\n",
    "    #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "    #     loss_train_arr.append(loss_train)\n",
    "\n",
    "        acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "        acc_test_arr_K4_G2[lr_idx][iter] = acc_test\n",
    "        loss_test_arr_K4_G2[lr_idx][iter] = loss_test\n",
    "        \n",
    "        if iter % 1 ==0:\n",
    "            print('Round {:3d}, Average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_test, acc_test))\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(acc_test_arr_K4_G1[0,0,0,0:30],label='w/o Grouping, K=4, N=4, G=1, sigma=0.1' )\n",
    "plt.plot(acc_test_arr_K4_G2[0,0:30],label='w/   Grouping, K=4, N=4, G=2, sigma=0.1' )\n",
    "\n",
    "\n",
    "# plt.plot(acc_test_arr_uncoded,'r',label='uncoded')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "# plt.title(title_name)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Test accuracy(%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4. K=4, N=4, T=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z_array: [ 1.          0.80901699  0.30901699 -0.30901699 -0.80901699]\n",
      "@BACC_Enc: N,K,T, m_i= 5 4 0 12500 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 5 4 0 12500 \n",
      "\n",
      "(T, sigma)= 0 1 )  0 -th Trial!!\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2.0013 \n",
      "Accuracy: 2079/10000 (20.79%)\n",
      "\n",
      "Round   0, Average loss 2.001 Test accuracy 20.790\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 22.4331 \n",
      "Accuracy: 2126/10000 (21.26%)\n",
      "\n",
      "Round   1, Average loss 22.433 Test accuracy 21.260\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 62.9370 \n",
      "Accuracy: 1621/10000 (16.21%)\n",
      "\n",
      "Round   2, Average loss 62.937 Test accuracy 16.210\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 142.2053 \n",
      "Accuracy: 1528/10000 (15.28%)\n",
      "\n",
      "Round   3, Average loss 142.205 Test accuracy 15.280\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 278.6917 \n",
      "Accuracy: 1094/10000 (10.94%)\n",
      "\n",
      "Round   4, Average loss 278.692 Test accuracy 10.940\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 464.2436 \n",
      "Accuracy: 994/10000 (9.94%)\n",
      "\n",
      "Round   5, Average loss 464.244 Test accuracy 9.940\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 731.8757 \n",
      "Accuracy: 1295/10000 (12.95%)\n",
      "\n",
      "Round   6, Average loss 731.876 Test accuracy 12.950\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 1015.5546 \n",
      "Accuracy: 1045/10000 (10.45%)\n",
      "\n",
      "Round   7, Average loss 1015.555 Test accuracy 10.450\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 927.3284 \n",
      "Accuracy: 1307/10000 (13.07%)\n",
      "\n",
      "Round   8, Average loss 927.328 Test accuracy 13.070\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 1318.9473 \n",
      "Accuracy: 1226/10000 (12.26%)\n",
      "\n",
      "Round   9, Average loss 1318.947 Test accuracy 12.260\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 1940.8309 \n",
      "Accuracy: 1241/10000 (12.41%)\n",
      "\n",
      "Round  10, Average loss 1940.831 Test accuracy 12.410\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 1930.8685 \n",
      "Accuracy: 1261/10000 (12.61%)\n",
      "\n",
      "Round  11, Average loss 1930.868 Test accuracy 12.610\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2573.0307 \n",
      "Accuracy: 1528/10000 (15.28%)\n",
      "\n",
      "Round  12, Average loss 2573.031 Test accuracy 15.280\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2119.8672 \n",
      "Accuracy: 1507/10000 (15.07%)\n",
      "\n",
      "Round  13, Average loss 2119.867 Test accuracy 15.070\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2354.4168 \n",
      "Accuracy: 1236/10000 (12.36%)\n",
      "\n",
      "Round  14, Average loss 2354.417 Test accuracy 12.360\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2906.5175 \n",
      "Accuracy: 1543/10000 (15.43%)\n",
      "\n",
      "Round  15, Average loss 2906.518 Test accuracy 15.430\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2534.7729 \n",
      "Accuracy: 1421/10000 (14.21%)\n",
      "\n",
      "Round  16, Average loss 2534.773 Test accuracy 14.210\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 3068.6160 \n",
      "Accuracy: 1431/10000 (14.31%)\n",
      "\n",
      "Round  17, Average loss 3068.616 Test accuracy 14.310\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 3550.0068 \n",
      "Accuracy: 1551/10000 (15.51%)\n",
      "\n",
      "Round  18, Average loss 3550.007 Test accuracy 15.510\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2569.2794 \n",
      "Accuracy: 1526/10000 (15.26%)\n",
      "\n",
      "Round  19, Average loss 2569.279 Test accuracy 15.260\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2762.7893 \n",
      "Accuracy: 1514/10000 (15.14%)\n",
      "\n",
      "Round  20, Average loss 2762.789 Test accuracy 15.140\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2563.3975 \n",
      "Accuracy: 1249/10000 (12.49%)\n",
      "\n",
      "Round  21, Average loss 2563.397 Test accuracy 12.490\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2752.4890 \n",
      "Accuracy: 1416/10000 (14.16%)\n",
      "\n",
      "Round  22, Average loss 2752.489 Test accuracy 14.160\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2627.6617 \n",
      "Accuracy: 1224/10000 (12.24%)\n",
      "\n",
      "Round  23, Average loss 2627.662 Test accuracy 12.240\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2355.0212 \n",
      "Accuracy: 1496/10000 (14.96%)\n",
      "\n",
      "Round  24, Average loss 2355.021 Test accuracy 14.960\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2735.5141 \n",
      "Accuracy: 1522/10000 (15.22%)\n",
      "\n",
      "Round  25, Average loss 2735.514 Test accuracy 15.220\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2336.3985 \n",
      "Accuracy: 1234/10000 (12.34%)\n",
      "\n",
      "Round  26, Average loss 2336.399 Test accuracy 12.340\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2626.6136 \n",
      "Accuracy: 1254/10000 (12.54%)\n",
      "\n",
      "Round  27, Average loss 2626.614 Test accuracy 12.540\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2403.0958 \n",
      "Accuracy: 1477/10000 (14.77%)\n",
      "\n",
      "Round  28, Average loss 2403.096 Test accuracy 14.770\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2083.6738 \n",
      "Accuracy: 1489/10000 (14.89%)\n",
      "\n",
      "Round  29, Average loss 2083.674 Test accuracy 14.890\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2242.6737 \n",
      "Accuracy: 1492/10000 (14.92%)\n",
      "\n",
      "Round  30, Average loss 2242.674 Test accuracy 14.920\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2407.4988 \n",
      "Accuracy: 1221/10000 (12.21%)\n",
      "\n",
      "Round  31, Average loss 2407.499 Test accuracy 12.210\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 1742.0270 \n",
      "Accuracy: 1464/10000 (14.64%)\n",
      "\n",
      "Round  32, Average loss 1742.027 Test accuracy 14.640\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2549.6610 \n",
      "Accuracy: 1502/10000 (15.02%)\n",
      "\n",
      "Round  33, Average loss 2549.661 Test accuracy 15.020\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2259.9596 \n",
      "Accuracy: 1492/10000 (14.92%)\n",
      "\n",
      "Round  34, Average loss 2259.960 Test accuracy 14.920\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2154.7875 \n",
      "Accuracy: 1488/10000 (14.88%)\n",
      "\n",
      "Round  35, Average loss 2154.787 Test accuracy 14.880\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2241.1927 \n",
      "Accuracy: 1497/10000 (14.97%)\n",
      "\n",
      "Round  36, Average loss 2241.193 Test accuracy 14.970\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2334.9361 \n",
      "Accuracy: 1502/10000 (15.02%)\n",
      "\n",
      "Round  37, Average loss 2334.936 Test accuracy 15.020\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2292.1609 \n",
      "Accuracy: 1501/10000 (15.01%)\n",
      "\n",
      "Round  38, Average loss 2292.161 Test accuracy 15.010\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 1993.5540 \n",
      "Accuracy: 1215/10000 (12.15%)\n",
      "\n",
      "Round  39, Average loss 1993.554 Test accuracy 12.150\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 1801.5985 \n",
      "Accuracy: 1477/10000 (14.77%)\n",
      "\n",
      "Round  40, Average loss 1801.598 Test accuracy 14.770\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 1975.0935 \n",
      "Accuracy: 1461/10000 (14.61%)\n",
      "\n",
      "Round  41, Average loss 1975.093 Test accuracy 14.610\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2435.0091 \n",
      "Accuracy: 1501/10000 (15.01%)\n",
      "\n",
      "Round  42, Average loss 2435.009 Test accuracy 15.010\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2283.9878 \n",
      "Accuracy: 1386/10000 (13.86%)\n",
      "\n",
      "Round  43, Average loss 2283.988 Test accuracy 13.860\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2302.2507 \n",
      "Accuracy: 1489/10000 (14.89%)\n",
      "\n",
      "Round  44, Average loss 2302.251 Test accuracy 14.890\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 1908.4606 \n",
      "Accuracy: 1213/10000 (12.13%)\n",
      "\n",
      "Round  45, Average loss 1908.461 Test accuracy 12.130\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2110.1607 \n",
      "Accuracy: 1386/10000 (13.86%)\n",
      "\n",
      "Round  46, Average loss 2110.161 Test accuracy 13.860\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2333.1768 \n",
      "Accuracy: 1497/10000 (14.97%)\n",
      "\n",
      "Round  47, Average loss 2333.177 Test accuracy 14.970\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2754.0662 \n",
      "Accuracy: 1525/10000 (15.25%)\n",
      "\n",
      "Round  48, Average loss 2754.066 Test accuracy 15.250\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2333.7001 \n",
      "Accuracy: 1508/10000 (15.08%)\n",
      "\n",
      "Round  49, Average loss 2333.700 Test accuracy 15.080\n",
      "z_array: [ 1.          0.90096887  0.6234898   0.22252093 -0.22252093 -0.6234898\n",
      " -0.90096887]\n",
      "@BACC_Enc: N,K,T, m_i= 7 4 0 12500 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 7 4 0 12500 \n",
      "\n",
      "(T, sigma)= 0 1 )  0 -th Trial!!\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 31.1908 \n",
      "Accuracy: 1485/10000 (14.85%)\n",
      "\n",
      "Round   0, Average loss 31.191 Test accuracy 14.850\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 205.9583 \n",
      "Accuracy: 1420/10000 (14.20%)\n",
      "\n",
      "Round   1, Average loss 205.958 Test accuracy 14.200\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 354.6270 \n",
      "Accuracy: 1415/10000 (14.15%)\n",
      "\n",
      "Round   2, Average loss 354.627 Test accuracy 14.150\n",
      "selected users: [0 1 2 3 4 5 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 510.0565 \n",
      "Accuracy: 1251/10000 (12.51%)\n",
      "\n",
      "Round   3, Average loss 510.057 Test accuracy 12.510\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 587.4965 \n",
      "Accuracy: 1110/10000 (11.10%)\n",
      "\n",
      "Round   4, Average loss 587.496 Test accuracy 11.100\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 659.0045 \n",
      "Accuracy: 1268/10000 (12.68%)\n",
      "\n",
      "Round   5, Average loss 659.005 Test accuracy 12.680\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 735.8657 \n",
      "Accuracy: 1467/10000 (14.67%)\n",
      "\n",
      "Round   6, Average loss 735.866 Test accuracy 14.670\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 775.0035 \n",
      "Accuracy: 1334/10000 (13.34%)\n",
      "\n",
      "Round   7, Average loss 775.003 Test accuracy 13.340\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 802.8011 \n",
      "Accuracy: 1260/10000 (12.60%)\n",
      "\n",
      "Round   8, Average loss 802.801 Test accuracy 12.600\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 786.0416 \n",
      "Accuracy: 1192/10000 (11.92%)\n",
      "\n",
      "Round   9, Average loss 786.042 Test accuracy 11.920\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 844.7958 \n",
      "Accuracy: 1455/10000 (14.55%)\n",
      "\n",
      "Round  10, Average loss 844.796 Test accuracy 14.550\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 856.2704 \n",
      "Accuracy: 1339/10000 (13.39%)\n",
      "\n",
      "Round  11, Average loss 856.270 Test accuracy 13.390\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 913.2858 \n",
      "Accuracy: 1210/10000 (12.10%)\n",
      "\n",
      "Round  12, Average loss 913.286 Test accuracy 12.100\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 927.2470 \n",
      "Accuracy: 1245/10000 (12.45%)\n",
      "\n",
      "Round  13, Average loss 927.247 Test accuracy 12.450\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 976.4723 \n",
      "Accuracy: 1252/10000 (12.52%)\n",
      "\n",
      "Round  14, Average loss 976.472 Test accuracy 12.520\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1024.9172 \n",
      "Accuracy: 1453/10000 (14.53%)\n",
      "\n",
      "Round  15, Average loss 1024.917 Test accuracy 14.530\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1024.2809 \n",
      "Accuracy: 1449/10000 (14.49%)\n",
      "\n",
      "Round  16, Average loss 1024.281 Test accuracy 14.490\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1179.9972 \n",
      "Accuracy: 1261/10000 (12.61%)\n",
      "\n",
      "Round  17, Average loss 1179.997 Test accuracy 12.610\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1082.2636 \n",
      "Accuracy: 1249/10000 (12.49%)\n",
      "\n",
      "Round  18, Average loss 1082.264 Test accuracy 12.490\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1191.1273 \n",
      "Accuracy: 1463/10000 (14.63%)\n",
      "\n",
      "Round  19, Average loss 1191.127 Test accuracy 14.630\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1195.9824 \n",
      "Accuracy: 1438/10000 (14.38%)\n",
      "\n",
      "Round  20, Average loss 1195.982 Test accuracy 14.380\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1269.9107 \n",
      "Accuracy: 1179/10000 (11.79%)\n",
      "\n",
      "Round  21, Average loss 1269.911 Test accuracy 11.790\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1312.8683 \n",
      "Accuracy: 1456/10000 (14.56%)\n",
      "\n",
      "Round  22, Average loss 1312.868 Test accuracy 14.560\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1326.0638 \n",
      "Accuracy: 1458/10000 (14.58%)\n",
      "\n",
      "Round  23, Average loss 1326.064 Test accuracy 14.580\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1214.6747 \n",
      "Accuracy: 1307/10000 (13.07%)\n",
      "\n",
      "Round  24, Average loss 1214.675 Test accuracy 13.070\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1247.9809 \n",
      "Accuracy: 1236/10000 (12.36%)\n",
      "\n",
      "Round  25, Average loss 1247.981 Test accuracy 12.360\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1375.2655 \n",
      "Accuracy: 1254/10000 (12.54%)\n",
      "\n",
      "Round  26, Average loss 1375.265 Test accuracy 12.540\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1299.3356 \n",
      "Accuracy: 1444/10000 (14.44%)\n",
      "\n",
      "Round  27, Average loss 1299.336 Test accuracy 14.440\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1344.0458 \n",
      "Accuracy: 1316/10000 (13.16%)\n",
      "\n",
      "Round  28, Average loss 1344.046 Test accuracy 13.160\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1394.5425 \n",
      "Accuracy: 1248/10000 (12.48%)\n",
      "\n",
      "Round  29, Average loss 1394.542 Test accuracy 12.480\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1317.0398 \n",
      "Accuracy: 1243/10000 (12.43%)\n",
      "\n",
      "Round  30, Average loss 1317.040 Test accuracy 12.430\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1409.0819 \n",
      "Accuracy: 1325/10000 (13.25%)\n",
      "\n",
      "Round  31, Average loss 1409.082 Test accuracy 13.250\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1360.2535 \n",
      "Accuracy: 1315/10000 (13.15%)\n",
      "\n",
      "Round  32, Average loss 1360.253 Test accuracy 13.150\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1423.9918 \n",
      "Accuracy: 1453/10000 (14.53%)\n",
      "\n",
      "Round  33, Average loss 1423.992 Test accuracy 14.530\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1304.5999 \n",
      "Accuracy: 1239/10000 (12.39%)\n",
      "\n",
      "Round  34, Average loss 1304.600 Test accuracy 12.390\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1404.1398 \n",
      "Accuracy: 1452/10000 (14.52%)\n",
      "\n",
      "Round  35, Average loss 1404.140 Test accuracy 14.520\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1466.3629 \n",
      "Accuracy: 1457/10000 (14.57%)\n",
      "\n",
      "Round  36, Average loss 1466.363 Test accuracy 14.570\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1463.4525 \n",
      "Accuracy: 1250/10000 (12.50%)\n",
      "\n",
      "Round  37, Average loss 1463.453 Test accuracy 12.500\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1359.1480 \n",
      "Accuracy: 1418/10000 (14.18%)\n",
      "\n",
      "Round  38, Average loss 1359.148 Test accuracy 14.180\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1482.9653 \n",
      "Accuracy: 1457/10000 (14.57%)\n",
      "\n",
      "Round  39, Average loss 1482.965 Test accuracy 14.570\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1568.1809 \n",
      "Accuracy: 1458/10000 (14.58%)\n",
      "\n",
      "Round  40, Average loss 1568.181 Test accuracy 14.580\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1474.7328 \n",
      "Accuracy: 1451/10000 (14.51%)\n",
      "\n",
      "Round  41, Average loss 1474.733 Test accuracy 14.510\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1541.0887 \n",
      "Accuracy: 1251/10000 (12.51%)\n",
      "\n",
      "Round  42, Average loss 1541.089 Test accuracy 12.510\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1511.9371 \n",
      "Accuracy: 1253/10000 (12.53%)\n",
      "\n",
      "Round  43, Average loss 1511.937 Test accuracy 12.530\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1394.1591 \n",
      "Accuracy: 1235/10000 (12.35%)\n",
      "\n",
      "Round  44, Average loss 1394.159 Test accuracy 12.350\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1534.0807 \n",
      "Accuracy: 1251/10000 (12.51%)\n",
      "\n",
      "Round  45, Average loss 1534.081 Test accuracy 12.510\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1572.7553 \n",
      "Accuracy: 1255/10000 (12.55%)\n",
      "\n",
      "Round  46, Average loss 1572.755 Test accuracy 12.550\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1510.3011 \n",
      "Accuracy: 1251/10000 (12.51%)\n",
      "\n",
      "Round  47, Average loss 1510.301 Test accuracy 12.510\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1586.9163 \n",
      "Accuracy: 1252/10000 (12.52%)\n",
      "\n",
      "Round  48, Average loss 1586.916 Test accuracy 12.520\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1594.6156 \n",
      "Accuracy: 1474/10000 (14.74%)\n",
      "\n",
      "Round  49, Average loss 1594.616 Test accuracy 14.740\n",
      "z_array: [ 1.          0.93969262  0.76604444  0.5         0.17364818 -0.17364818\n",
      " -0.5        -0.76604444 -0.93969262]\n",
      "@BACC_Enc: N,K,T, m_i= 9 4 0 12500 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 9 4 0 12500 \n",
      "\n",
      "(T, sigma)= 0 1 )  0 -th Trial!!\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 4.7435 \n",
      "Accuracy: 1702/10000 (17.02%)\n",
      "\n",
      "Round   0, Average loss 4.744 Test accuracy 17.020\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 10.1085 \n",
      "Accuracy: 1850/10000 (18.50%)\n",
      "\n",
      "Round   1, Average loss 10.108 Test accuracy 18.500\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 14.7920 \n",
      "Accuracy: 1436/10000 (14.36%)\n",
      "\n",
      "Round   2, Average loss 14.792 Test accuracy 14.360\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 16.5223 \n",
      "Accuracy: 1789/10000 (17.89%)\n",
      "\n",
      "Round   3, Average loss 16.522 Test accuracy 17.890\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 13.7797 \n",
      "Accuracy: 1183/10000 (11.83%)\n",
      "\n",
      "Round   4, Average loss 13.780 Test accuracy 11.830\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 17.8871 \n",
      "Accuracy: 1461/10000 (14.61%)\n",
      "\n",
      "Round   5, Average loss 17.887 Test accuracy 14.610\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 13.2764 \n",
      "Accuracy: 1775/10000 (17.75%)\n",
      "\n",
      "Round   6, Average loss 13.276 Test accuracy 17.750\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 15.9428 \n",
      "Accuracy: 1579/10000 (15.79%)\n",
      "\n",
      "Round   7, Average loss 15.943 Test accuracy 15.790\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 19.9954 \n",
      "Accuracy: 1795/10000 (17.95%)\n",
      "\n",
      "Round   8, Average loss 19.995 Test accuracy 17.950\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 24.7406 \n",
      "Accuracy: 1563/10000 (15.63%)\n",
      "\n",
      "Round   9, Average loss 24.741 Test accuracy 15.630\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 19.8102 \n",
      "Accuracy: 1564/10000 (15.64%)\n",
      "\n",
      "Round  10, Average loss 19.810 Test accuracy 15.640\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 7.8099 \n",
      "Accuracy: 1366/10000 (13.66%)\n",
      "\n",
      "Round  11, Average loss 7.810 Test accuracy 13.660\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 22.3433 \n",
      "Accuracy: 1245/10000 (12.45%)\n",
      "\n",
      "Round  12, Average loss 22.343 Test accuracy 12.450\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 25.3971 \n",
      "Accuracy: 1733/10000 (17.33%)\n",
      "\n",
      "Round  13, Average loss 25.397 Test accuracy 17.330\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 11.7445 \n",
      "Accuracy: 1432/10000 (14.32%)\n",
      "\n",
      "Round  14, Average loss 11.744 Test accuracy 14.320\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 18.2777 \n",
      "Accuracy: 1720/10000 (17.20%)\n",
      "\n",
      "Round  15, Average loss 18.278 Test accuracy 17.200\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 24.1944 \n",
      "Accuracy: 1299/10000 (12.99%)\n",
      "\n",
      "Round  16, Average loss 24.194 Test accuracy 12.990\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 23.5129 \n",
      "Accuracy: 1746/10000 (17.46%)\n",
      "\n",
      "Round  17, Average loss 23.513 Test accuracy 17.460\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 19.2197 \n",
      "Accuracy: 1381/10000 (13.81%)\n",
      "\n",
      "Round  18, Average loss 19.220 Test accuracy 13.810\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 26.0941 \n",
      "Accuracy: 1299/10000 (12.99%)\n",
      "\n",
      "Round  19, Average loss 26.094 Test accuracy 12.990\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 16.2975 \n",
      "Accuracy: 996/10000 (9.96%)\n",
      "\n",
      "Round  20, Average loss 16.297 Test accuracy 9.960\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 22.6457 \n",
      "Accuracy: 1786/10000 (17.86%)\n",
      "\n",
      "Round  21, Average loss 22.646 Test accuracy 17.860\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 24.6036 \n",
      "Accuracy: 719/10000 (7.19%)\n",
      "\n",
      "Round  22, Average loss 24.604 Test accuracy 7.190\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 16.9104 \n",
      "Accuracy: 1495/10000 (14.95%)\n",
      "\n",
      "Round  23, Average loss 16.910 Test accuracy 14.950\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 19.9233 \n",
      "Accuracy: 1040/10000 (10.40%)\n",
      "\n",
      "Round  24, Average loss 19.923 Test accuracy 10.400\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 13.2589 \n",
      "Accuracy: 1104/10000 (11.04%)\n",
      "\n",
      "Round  25, Average loss 13.259 Test accuracy 11.040\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 21.1113 \n",
      "Accuracy: 1237/10000 (12.37%)\n",
      "\n",
      "Round  26, Average loss 21.111 Test accuracy 12.370\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 22.5768 \n",
      "Accuracy: 1488/10000 (14.88%)\n",
      "\n",
      "Round  27, Average loss 22.577 Test accuracy 14.880\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 17.2608 \n",
      "Accuracy: 1041/10000 (10.41%)\n",
      "\n",
      "Round  28, Average loss 17.261 Test accuracy 10.410\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 16.1813 \n",
      "Accuracy: 1778/10000 (17.78%)\n",
      "\n",
      "Round  29, Average loss 16.181 Test accuracy 17.780\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 14.9773 \n",
      "Accuracy: 854/10000 (8.54%)\n",
      "\n",
      "Round  30, Average loss 14.977 Test accuracy 8.540\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 20.3241 \n",
      "Accuracy: 780/10000 (7.80%)\n",
      "\n",
      "Round  31, Average loss 20.324 Test accuracy 7.800\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 9.3785 \n",
      "Accuracy: 897/10000 (8.97%)\n",
      "\n",
      "Round  32, Average loss 9.378 Test accuracy 8.970\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 15.4156 \n",
      "Accuracy: 542/10000 (5.42%)\n",
      "\n",
      "Round  33, Average loss 15.416 Test accuracy 5.420\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 14.5373 \n",
      "Accuracy: 948/10000 (9.48%)\n",
      "\n",
      "Round  34, Average loss 14.537 Test accuracy 9.480\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 19.6934 \n",
      "Accuracy: 840/10000 (8.40%)\n",
      "\n",
      "Round  35, Average loss 19.693 Test accuracy 8.400\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 14.1269 \n",
      "Accuracy: 913/10000 (9.13%)\n",
      "\n",
      "Round  36, Average loss 14.127 Test accuracy 9.130\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 13.0321 \n",
      "Accuracy: 805/10000 (8.05%)\n",
      "\n",
      "Round  37, Average loss 13.032 Test accuracy 8.050\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 18.3588 \n",
      "Accuracy: 952/10000 (9.52%)\n",
      "\n",
      "Round  38, Average loss 18.359 Test accuracy 9.520\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 13.8616 \n",
      "Accuracy: 704/10000 (7.04%)\n",
      "\n",
      "Round  39, Average loss 13.862 Test accuracy 7.040\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 15.1838 \n",
      "Accuracy: 771/10000 (7.71%)\n",
      "\n",
      "Round  40, Average loss 15.184 Test accuracy 7.710\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 6.5382 \n",
      "Accuracy: 812/10000 (8.12%)\n",
      "\n",
      "Round  41, Average loss 6.538 Test accuracy 8.120\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 12.4725 \n",
      "Accuracy: 1296/10000 (12.96%)\n",
      "\n",
      "Round  42, Average loss 12.473 Test accuracy 12.960\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 27.6372 \n",
      "Accuracy: 860/10000 (8.60%)\n",
      "\n",
      "Round  43, Average loss 27.637 Test accuracy 8.600\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 27.9071 \n",
      "Accuracy: 503/10000 (5.03%)\n",
      "\n",
      "Round  44, Average loss 27.907 Test accuracy 5.030\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 10.4443 \n",
      "Accuracy: 833/10000 (8.33%)\n",
      "\n",
      "Round  45, Average loss 10.444 Test accuracy 8.330\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 11.0233 \n",
      "Accuracy: 942/10000 (9.42%)\n",
      "\n",
      "Round  46, Average loss 11.023 Test accuracy 9.420\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 13.8320 \n",
      "Accuracy: 755/10000 (7.55%)\n",
      "\n",
      "Round  47, Average loss 13.832 Test accuracy 7.550\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 18.4996 \n",
      "Accuracy: 950/10000 (9.50%)\n",
      "\n",
      "Round  48, Average loss 18.500 Test accuracy 9.500\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 25.1535 \n",
      "Accuracy: 842/10000 (8.42%)\n",
      "\n",
      "Round  49, Average loss 25.154 Test accuracy 8.420\n",
      "z_array: [ 1.          0.97094182  0.88545603  0.74851075  0.56806475  0.35460489\n",
      "  0.12053668 -0.12053668 -0.35460489 -0.56806475 -0.74851075 -0.88545603\n",
      " -0.97094182]\n",
      "@BACC_Enc: N,K,T, m_i= 13 4 0 12500 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 13 4 0 12500 \n",
      "\n",
      "(T, sigma)= 0 1 )  0 -th Trial!!\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3039 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round   0, Average loss 2.304 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3035 \n",
      "Accuracy: 1021/10000 (10.21%)\n",
      "\n",
      "Round   1, Average loss 2.303 Test accuracy 10.210\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3032 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round   2, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3027 \n",
      "Accuracy: 1339/10000 (13.39%)\n",
      "\n",
      "Round   3, Average loss 2.303 Test accuracy 13.390\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3031 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round   4, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3030 \n",
      "Accuracy: 1100/10000 (11.00%)\n",
      "\n",
      "Round   5, Average loss 2.303 Test accuracy 11.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3024 \n",
      "Accuracy: 1102/10000 (11.02%)\n",
      "\n",
      "Round   6, Average loss 2.302 Test accuracy 11.020\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3029 \n",
      "Accuracy: 1275/10000 (12.75%)\n",
      "\n",
      "Round   7, Average loss 2.303 Test accuracy 12.750\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.3027 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round   8, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3027 \n",
      "Accuracy: 1018/10000 (10.18%)\n",
      "\n",
      "Round   9, Average loss 2.303 Test accuracy 10.180\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3027 \n",
      "Accuracy: 1020/10000 (10.20%)\n",
      "\n",
      "Round  10, Average loss 2.303 Test accuracy 10.200\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3020 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  11, Average loss 2.302 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3021 \n",
      "Accuracy: 1005/10000 (10.05%)\n",
      "\n",
      "Round  12, Average loss 2.302 Test accuracy 10.050\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3025 \n",
      "Accuracy: 1125/10000 (11.25%)\n",
      "\n",
      "Round  13, Average loss 2.302 Test accuracy 11.250\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3029 \n",
      "Accuracy: 1025/10000 (10.25%)\n",
      "\n",
      "Round  14, Average loss 2.303 Test accuracy 10.250\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3033 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  15, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  16, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3029 \n",
      "Accuracy: 1054/10000 (10.54%)\n",
      "\n",
      "Round  17, Average loss 2.303 Test accuracy 10.540\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3032 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  18, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3034 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  19, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3033 \n",
      "Accuracy: 1001/10000 (10.01%)\n",
      "\n",
      "Round  20, Average loss 2.303 Test accuracy 10.010\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3010 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  21, Average loss 2.301 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3032 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  22, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  23, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  24, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3021 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  25, Average loss 2.302 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3031 \n",
      "Accuracy: 905/10000 (9.05%)\n",
      "\n",
      "Round  26, Average loss 2.303 Test accuracy 9.050\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3024 \n",
      "Accuracy: 1004/10000 (10.04%)\n",
      "\n",
      "Round  27, Average loss 2.302 Test accuracy 10.040\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3031 \n",
      "Accuracy: 1133/10000 (11.33%)\n",
      "\n",
      "Round  28, Average loss 2.303 Test accuracy 11.330\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3031 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  29, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3024 \n",
      "Accuracy: 1164/10000 (11.64%)\n",
      "\n",
      "Round  30, Average loss 2.302 Test accuracy 11.640\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3027 \n",
      "Accuracy: 1141/10000 (11.41%)\n",
      "\n",
      "Round  31, Average loss 2.303 Test accuracy 11.410\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3029 \n",
      "Accuracy: 1147/10000 (11.47%)\n",
      "\n",
      "Round  32, Average loss 2.303 Test accuracy 11.470\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3029 \n",
      "Accuracy: 1120/10000 (11.20%)\n",
      "\n",
      "Round  33, Average loss 2.303 Test accuracy 11.200\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  34, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3025 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  35, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3027 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  36, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3028 \n",
      "Accuracy: 1074/10000 (10.74%)\n",
      "\n",
      "Round  37, Average loss 2.303 Test accuracy 10.740\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3027 \n",
      "Accuracy: 1004/10000 (10.04%)\n",
      "\n",
      "Round  38, Average loss 2.303 Test accuracy 10.040\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  39, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3030 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  40, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3028 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  41, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3028 \n",
      "Accuracy: 1198/10000 (11.98%)\n",
      "\n",
      "Round  42, Average loss 2.303 Test accuracy 11.980\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3030 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  43, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3033 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  44, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3029 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  45, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3031 \n",
      "Accuracy: 1001/10000 (10.01%)\n",
      "\n",
      "Round  46, Average loss 2.303 Test accuracy 10.010\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3028 \n",
      "Accuracy: 1054/10000 (10.54%)\n",
      "\n",
      "Round  47, Average loss 2.303 Test accuracy 10.540\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3001 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  48, Average loss 2.300 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3028 \n",
      "Accuracy: 1001/10000 (10.01%)\n",
      "\n",
      "Round  49, Average loss 2.303 Test accuracy 10.010\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "K = 4\n",
    "T = 0\n",
    "sigma = 1\n",
    "Noise_Alloc = []\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "\n",
    "N_array = [5,7,9,13]\n",
    "B_array = [0.5]\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 50\n",
    "\n",
    "\n",
    "\n",
    "loss_test_arr_K4_G1_T0 = np.zeros((len(N_array),len(B_array),N_trials,N_epochs))\n",
    "acc_test_arr_K4_G1_T0  = np.zeros((len(N_array),len(B_array),N_trials,N_epochs))\n",
    "\n",
    "for N_idx in range(len(N_array)):\n",
    "    \n",
    "    N = N_array[N_idx]\n",
    "    \n",
    "    \n",
    "           \n",
    "        \n",
    "    # print(\"alpha_array: \",alpha_array,'\\n')\n",
    "    \n",
    "    \n",
    "    for B_idx in range(len(B_array)):\n",
    "        \n",
    "        B = B_array[B_idx]\n",
    "        z_array = []\n",
    "#         while(len(z_array)<N):\n",
    "#             z_tmp = np.random.uniform(-1,1,1)\n",
    "#             MIS_tmp = MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_tmp], 1,sigma)\n",
    "#             if MIS_tmp < B and MIS_tmp > 0.1:\n",
    "#                 z_array.append(z_tmp[0])\n",
    "#         \n",
    "#         z_array = np.sort(z_array)\n",
    "#         print(N_idx,'!!!')\n",
    "#         if N_idx==0:\n",
    "# #             z_array = np.array([-0.94,-0.534,0.534, 0.94])\n",
    "# #         elif N_idx==1:\n",
    "# #             z_array = np.array([-0.94, -0.73, 0.73, 0.94])\n",
    "#         elif N_idx==1:\n",
    "#             z_array = np.array([-0.94, -0.125, 0.125, 0.94])\n",
    "#         else:\n",
    "# #             z_array = np.array([-0.94, -0.73, -0.534, -0.125, 0.125, 0.534, 0.73, 0.94])\n",
    "# #             z_array = np.array([-0.9, -0.81, -0.22, -0.20, 0.20, 0.22, 0.81, 0.9])\n",
    "        \n",
    "        i_array = np.array(range(N))\n",
    "        z_array = np.cos(i_array*2*math.pi/N/2) # np.cos(i_array*2*math.pi/N/2)\n",
    "        \n",
    "        print('z_array:',z_array)\n",
    "#         for j in range(len(z_array)):\n",
    "#             print(MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_array[j]], 1,sigma))\n",
    "        \n",
    "        \n",
    "        _Noise_label = np.ones((15000*T,10)) * 0.1\n",
    "\n",
    "        X_tilde,a,b = BACC_Enc_Data_v3(encoding_input_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde,a,b = BACC_Enc_Data_v3(encoding_label_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "\n",
    "        m = N # m is the number of received result @ master\n",
    "    #     print('number of results:',m)\n",
    "\n",
    "        for trial_idx in range(N_trials):\n",
    "            print('(T, sigma)=',T,sigma,') ',trial_idx,'-th Trial!!')\n",
    "\n",
    "            net_glob = CNNCifar(args=args)\n",
    "            net_glob.cuda()\n",
    "            net_glob.train()\n",
    "\n",
    "            # copy weights\n",
    "            w_glob = net_glob.state_dict()\n",
    "\n",
    "            for iter in range(N_epochs): #args.epochs\n",
    "                w_locals, loss_locals = [], []\n",
    "                idxs_users = np.random.choice(range(N), m, replace=False)\n",
    "                idxs_users = np.sort(idxs_users)\n",
    "                print('selected users:',idxs_users)\n",
    "                \n",
    "#                 coded_net = BACC_Enc_Model_withNoise_v4(net_glob.cuda(), N, K, T, 1, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "                dec_z_array = []\n",
    "                for idx in idxs_users: #for idx in range(N):\n",
    "            #         print(idx)\n",
    "                    local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[idx,:,:], label=y_tilde[idx,:,:])\n",
    "#                     w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "                    w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                    w_locals.append(copy.deepcopy(w))\n",
    "                    loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                    dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "                # update global weights\n",
    "                #w_glob = FedAvg(w_locals)\n",
    "                w_glob = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "\n",
    "                # copy weight to net_glob\n",
    "                net_glob.load_state_dict(w_glob)\n",
    "\n",
    "                # print loss\n",
    "            #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "            #     loss_train_arr.append(loss_train)\n",
    "\n",
    "                acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "                acc_test_arr_K4_G1_T0[N_idx][B_idx][trial_idx][iter] = acc_test\n",
    "                loss_test_arr_K4_G1_T0[N_idx][B_idx][trial_idx][iter] = loss_test\n",
    "                if iter % 1 ==0:\n",
    "                    print('Round {:3d}, Average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_test, acc_test))\n",
    "                #print(loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
