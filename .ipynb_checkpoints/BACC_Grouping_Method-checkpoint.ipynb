{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "\n",
    "import math\n",
    "\n",
    "from utils.sampling import mnist_iid, mnist_noniid, cifar_iid\n",
    "from utils.options import args_parser\n",
    "from models.Update import LocalUpdate\n",
    "from models.Nets import MLP, CNNMnist, CNNCifar, LeNet, CNNMnist2\n",
    "from models.Fed import FedAvg\n",
    "from models.Fed import FedQAvg, Quantization, Quantization_Finite, my_score, my_score_Finite\n",
    "from models.test import test_img\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "\n",
    "from utils.sampling import mnist_iid, mnist_noniid, cifar_iid\n",
    "from utils.options import args_parser\n",
    "from models.Update import LocalUpdate\n",
    "from models.Nets import MLP, CNNMnist, CNNCifar, LeNet, CNNMnist2, CNNMnist3\n",
    "from models.Fed import FedAvg\n",
    "from models.Fed import FedQAvg, Quantization, Quantization_Finite, my_score, my_score_Finite\n",
    "from models.test import test_img\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from utils.functions import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "class DatasetSplit(Dataset):\n",
    "    def __init__(self, dataset, idxs):\n",
    "        self.dataset = dataset\n",
    "        self.idxs = list(idxs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image, label = self.dataset[self.idxs[item]]\n",
    "        return image, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. G=2, N=4 (N_1=2, N_2=2), K=4 (K_1=2, K_2=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of X: (60000, 784)\n",
      "size of Y: (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "class my_argument:    \n",
    "    epochs = 200    #\"rounds of training\"\n",
    "    num_users = 15  # \"number of users: N\"\n",
    "    num_partition = 4 # \"number of users: K\"\n",
    "    frac = 0.5 #\"the fraction of clients: C\"\n",
    "    local_ep = 1 #\"the number of local epochs: E\"\n",
    "    local_bs = 200 #\"local batch size: B\"\n",
    "    bs=200 #\"test batch size\"\n",
    "    lr=0.01 #\"learning rate\"\n",
    "    momentum=0.5 # \"SGD momentum (default: 0.5)\"\n",
    "    split='user' # \"train-test split type, user or sample\"\n",
    "    opt='ADAM'\n",
    "    loss='Custom' # 'Custom' or 'Default'\n",
    "\n",
    "    # model arguments\n",
    "    model = 'cnn'\n",
    "    kernel_num=9 #, help='number of each kind of kernel')\n",
    "    kernel_sizes='3,4,5' #  help='comma-separated kernel size to use for convolution')\n",
    "    norm='None' #, help=\"batch_norm, layer_norm, or None\")\n",
    "    num_filters=32 #, help=\"number of filters for conv nets\")\n",
    "    max_pool='True' #help=\"Whether use max pooling rather than strided convolutions\")\n",
    "\n",
    "    # other arguments\n",
    "    dataset='mnist' #, help=\"name of dataset\")\n",
    "    iid=1\n",
    "    num_classes=10#, help=\"number of classes\")\n",
    "    num_channels=1#, help=\"number of channels of imges\")\n",
    "    gpu=1#, help=\"GPU ID, -1 for CPU\")\n",
    "    stopping_rounds=10#, help='rounds of early stopping')\n",
    "    verbose='False'#, help='verbose print')\n",
    "    seed=1#, help='random seed (default: 1)')\n",
    "    \n",
    "args = my_argument()\n",
    "\n",
    "args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "# load dataset and split users\n",
    "trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "dataset_train = datasets.MNIST('../data/mnist/', train=True, download=True, transform=trans_mnist)\n",
    "dataset_test = datasets.MNIST('../data/mnist/', train=False, download=True, transform=trans_mnist)\n",
    "\n",
    "dict_users = mnist_iid(dataset_train, args.num_partition)\n",
    "\n",
    "encoding_input_array_np = np.empty((len(dataset_train),28*28))\n",
    "encoding_label_array_np = np.empty((len(dataset_train),args.num_classes))\n",
    "print(\"size of X:\" ,encoding_input_array_np.shape)\n",
    "print(\"size of Y:\" ,encoding_label_array_np.shape)\n",
    "\n",
    "Size_submatrices = int(60000/args.num_partition)\n",
    "\n",
    "for i in range(args.num_partition):\n",
    "    \n",
    "    stt_pos = i*Size_submatrices\n",
    "    end_pos = (i+1)*Size_submatrices\n",
    "#     print(i,stt_pos,end_pos)\n",
    "    Temp_train = DataLoader(DatasetSplit(dataset_train, dict_users[i]), batch_size=Size_submatrices, shuffle=True)\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(Temp_train):\n",
    "        \n",
    "        images_np = images.detach().cpu().numpy()\n",
    "        encoding_input_array_np[stt_pos:end_pos,:] = np.reshape(images_np, (Size_submatrices,28*28))\n",
    "#         print(encoding_input_array_np[stt_pos:end_pos,:].shape)\n",
    "\n",
    "        onehot_labels = torch.nn.functional.one_hot(labels,num_classes=args.num_classes)\n",
    "        labels_np = onehot_labels.detach().cpu().numpy()\n",
    "#         print(labels_np.shape)\n",
    "        encoding_label_array_np[stt_pos:end_pos,:] = labels_np\n",
    "\n",
    "\n",
    "# print(labels_np[0:10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = 2\n",
    "N = 4 # N should be divisible by G\n",
    "K = 4 # N should be divisible by G\n",
    "\n",
    "size_per_group = int(60000/G)\n",
    "\n",
    "X_group = np.reshape(encoding_input_array_np, (G,size_per_group,28*28))\n",
    "y_group = np.reshape(encoding_label_array_np, (G,size_per_group,args.num_classes)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2 3\n",
      "##########################################\n",
      "###### 0 -th Trial!! ###########\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.2973 \n",
      "Accuracy: 2163/10000 (21.63%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.2949 \n",
      "Accuracy: 1899/10000 (18.99%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.8496 \n",
      "Accuracy: 7254/10000 (72.54%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3448 \n",
      "Accuracy: 9530/10000 (95.30%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3019 \n",
      "Accuracy: 9539/10000 (95.39%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2349 \n",
      "Accuracy: 9617/10000 (96.17%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2454 \n",
      "Accuracy: 9615/10000 (96.15%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2358 \n",
      "Accuracy: 9608/10000 (96.08%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2291 \n",
      "Accuracy: 9604/10000 (96.04%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2499 \n",
      "Accuracy: 9606/10000 (96.06%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2615 \n",
      "Accuracy: 9591/10000 (95.91%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2418 \n",
      "Accuracy: 9637/10000 (96.37%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2138 \n",
      "Accuracy: 9633/10000 (96.33%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2905 \n",
      "Accuracy: 9616/10000 (96.16%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2591 \n",
      "Accuracy: 9635/10000 (96.35%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2987 \n",
      "Accuracy: 9593/10000 (95.93%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2687 \n",
      "Accuracy: 9647/10000 (96.47%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2265 \n",
      "Accuracy: 9650/10000 (96.50%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2389 \n",
      "Accuracy: 9631/10000 (96.31%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2505 \n",
      "Accuracy: 9635/10000 (96.35%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2267 \n",
      "Accuracy: 9640/10000 (96.40%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2649 \n",
      "Accuracy: 9642/10000 (96.42%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2526 \n",
      "Accuracy: 9605/10000 (96.05%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2283 \n",
      "Accuracy: 9663/10000 (96.63%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2171 \n",
      "Accuracy: 9668/10000 (96.68%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2659 \n",
      "Accuracy: 9640/10000 (96.40%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2047 \n",
      "Accuracy: 9671/10000 (96.71%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.4531 \n",
      "Accuracy: 9568/10000 (95.68%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2314 \n",
      "Accuracy: 9604/10000 (96.04%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "N_i = int(N/G) # = 2\n",
    "K_i = int(K/G) # = 2\n",
    "T = 3\n",
    "sigma = 1\n",
    "Noise_Alloc = [0,2,4]\n",
    "m = N_i # number of selected workers (if there is no straggler, m=N_i)\n",
    "\n",
    "print(N_i,K_i,T)\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K_i+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K_i+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K_i+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "z_array = np.array([-0.81, 0.81])\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 30\n",
    "\n",
    "loss_test_arr_G2 = np.zeros((N_trials,N_epochs))\n",
    "acc_test_arr_G2  = np.zeros((N_trials,N_epochs))\n",
    "\n",
    "for trial_idx in range(N_trials):\n",
    "    \n",
    "    print('##########################################')\n",
    "    print('######',trial_idx,'-th Trial!! ###########')\n",
    "    \n",
    "    net_glob = CNNMnist2(args=args)\n",
    "    net_glob.cuda()\n",
    "    net_glob.train()\n",
    "    \n",
    "    # copy weights\n",
    "    w_glob = net_glob.state_dict()\n",
    "    \n",
    "    X_tilde = np.empty((N,Size_submatrices,28*28))\n",
    "    y_tilde = np.empty((N,Size_submatrices,10))\n",
    "    \n",
    "    for G_idx in range(G):\n",
    "        \n",
    "        _Noise_label = np.ones((size_per_group*T,10)) * 0.1\n",
    "        \n",
    "        X_tilde_tmp,a,b = BACC_Enc_Data_v3(X_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde_tmp,a,b = BACC_Enc_Data_v3(y_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "        \n",
    "        stt_pos = G_idx * N_i\n",
    "        end_pos = (G_idx+1) * N_i\n",
    "        \n",
    "        X_tilde[stt_pos:end_pos,:,:] = X_tilde_tmp\n",
    "        y_tilde[stt_pos:end_pos,:,:] = y_tilde_tmp\n",
    "        \n",
    "   \n",
    "\n",
    "    for iter in range(N_epochs): #args.epochs\n",
    "        \n",
    "        w_group_array = []\n",
    "        for G_idx in range(G):\n",
    "            w_locals, loss_locals = [], []\n",
    "            idxs_users = np.random.choice(range(N_i), m, replace=False)\n",
    "            idxs_users = np.sort(idxs_users)\n",
    "            print('selected users:',idxs_users)\n",
    "\n",
    "            coded_net = BACC_Enc_Model_withNoise_v3(net_glob.cuda(), N_i, K_i, T, 0.01, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "            dec_z_array = []\n",
    "            for idx in idxs_users: #for idx in range(N):\n",
    "        #         print(idx)\n",
    "                local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[G_idx*N_i+idx,:,:], label=y_tilde[G_idx*N_i+idx,:,:])\n",
    "                w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "#                 w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                w_locals.append(copy.deepcopy(w))\n",
    "                loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "            # update global weights\n",
    "            #w_glob = FedAvg(w_locals)\n",
    "            w_group = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "            \n",
    "            w_group_array.append(copy.deepcopy(w_group))\n",
    "        \n",
    "        w_glob = copy.deepcopy(w_group_array[0])\n",
    "        for k in w_glob.keys():\n",
    "            for G_idx in range(1,G):\n",
    "                w_glob[k] += w_group_array[G_idx][k]\n",
    "            w_glob[k] = torch.div(w_glob[k], len(w_group_array))\n",
    "        \n",
    "        # copy weight to net_glob\n",
    "        net_glob.load_state_dict(w_glob)\n",
    "\n",
    "        # print loss\n",
    "    #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "    #     loss_train_arr.append(loss_train)\n",
    "\n",
    "        acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "        acc_test_arr_G2[trial_idx][iter] = acc_test\n",
    "        loss_test_arr_G2[trial_idx][iter] = loss_test\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_group[G_idx,:,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. G=3, K=6 (K_i=2), N=6 (N_i=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of X: (60000, 784)\n",
      "size of Y: (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "class my_argument:    \n",
    "    epochs = 200    #\"rounds of training\"\n",
    "    num_users = 15  # \"number of users: N\"\n",
    "    num_partition = 6 # \"number of users: K\"\n",
    "    frac = 0.5 #\"the fraction of clients: C\"\n",
    "    local_ep = 1 #\"the number of local epochs: E\"\n",
    "    local_bs = 200 #\"local batch size: B\"\n",
    "    bs=200 #\"test batch size\"\n",
    "    lr=0.01 #\"learning rate\"\n",
    "    momentum=0.5 # \"SGD momentum (default: 0.5)\"\n",
    "    split='user' # \"train-test split type, user or sample\"\n",
    "    opt='ADAM'\n",
    "    loss='Custom' # 'Custom' or 'Default'\n",
    "\n",
    "    # model arguments\n",
    "    model = 'cnn'\n",
    "    kernel_num=9 #, help='number of each kind of kernel')\n",
    "    kernel_sizes='3,4,5' #  help='comma-separated kernel size to use for convolution')\n",
    "    norm='None' #, help=\"batch_norm, layer_norm, or None\")\n",
    "    num_filters=32 #, help=\"number of filters for conv nets\")\n",
    "    max_pool='True' #help=\"Whether use max pooling rather than strided convolutions\")\n",
    "\n",
    "    # other arguments\n",
    "    dataset='mnist' #, help=\"name of dataset\")\n",
    "    iid=1\n",
    "    num_classes=10#, help=\"number of classes\")\n",
    "    num_channels=1#, help=\"number of channels of imges\")\n",
    "    gpu=1#, help=\"GPU ID, -1 for CPU\")\n",
    "    stopping_rounds=10#, help='rounds of early stopping')\n",
    "    verbose='False'#, help='verbose print')\n",
    "    seed=1#, help='random seed (default: 1)')\n",
    "    \n",
    "args = my_argument()\n",
    "\n",
    "args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "# load dataset and split users\n",
    "trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "dataset_train = datasets.MNIST('../data/mnist/', train=True, download=True, transform=trans_mnist)\n",
    "dataset_test = datasets.MNIST('../data/mnist/', train=False, download=True, transform=trans_mnist)\n",
    "\n",
    "dict_users = mnist_iid(dataset_train, args.num_partition)\n",
    "\n",
    "encoding_input_array_np = np.empty((len(dataset_train),28*28))\n",
    "encoding_label_array_np = np.empty((len(dataset_train),args.num_classes))\n",
    "print(\"size of X:\" ,encoding_input_array_np.shape)\n",
    "print(\"size of Y:\" ,encoding_label_array_np.shape)\n",
    "\n",
    "Size_submatrices = int(60000/args.num_partition)\n",
    "\n",
    "for i in range(args.num_partition):\n",
    "    \n",
    "    stt_pos = i*Size_submatrices\n",
    "    end_pos = (i+1)*Size_submatrices\n",
    "#     print(i,stt_pos,end_pos)\n",
    "    Temp_train = DataLoader(DatasetSplit(dataset_train, dict_users[i]), batch_size=Size_submatrices, shuffle=True)\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(Temp_train):\n",
    "        \n",
    "        images_np = images.detach().cpu().numpy()\n",
    "        encoding_input_array_np[stt_pos:end_pos,:] = np.reshape(images_np, (Size_submatrices,28*28))\n",
    "#         print(encoding_input_array_np[stt_pos:end_pos,:].shape)\n",
    "\n",
    "        onehot_labels = torch.nn.functional.one_hot(labels,num_classes=args.num_classes)\n",
    "        labels_np = onehot_labels.detach().cpu().numpy()\n",
    "#         print(labels_np.shape)\n",
    "        encoding_label_array_np[stt_pos:end_pos,:] = labels_np\n",
    "\n",
    "\n",
    "# print(labels_np[0:10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = 3\n",
    "N = 6 # N should be divisible by G\n",
    "K = 6 # N should be divisible by G\n",
    "\n",
    "size_per_group = int(60000/G)\n",
    "\n",
    "X_group = np.reshape(encoding_input_array_np, (G,size_per_group,28*28))\n",
    "y_group = np.reshape(encoding_label_array_np, (G,size_per_group,args.num_classes)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2 3\n",
      "##########################################\n",
      "###### 0 -th Trial!! ###########\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 10000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 10000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 10000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 10000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 10000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 10000 \n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1027/10000 (10.27%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.2964 \n",
      "Accuracy: 2574/10000 (25.74%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.2028 \n",
      "Accuracy: 5098/10000 (50.98%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4135 \n",
      "Accuracy: 6138/10000 (61.38%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.8341 \n",
      "Accuracy: 8016/10000 (80.16%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.4538 \n",
      "Accuracy: 8951/10000 (89.51%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.4353 \n",
      "Accuracy: 9057/10000 (90.57%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.4178 \n",
      "Accuracy: 9011/10000 (90.11%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3867 \n",
      "Accuracy: 9143/10000 (91.43%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3976 \n",
      "Accuracy: 9128/10000 (91.28%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3632 \n",
      "Accuracy: 9188/10000 (91.88%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3798 \n",
      "Accuracy: 9189/10000 (91.89%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3676 \n",
      "Accuracy: 9186/10000 (91.86%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3669 \n",
      "Accuracy: 9174/10000 (91.74%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3746 \n",
      "Accuracy: 9148/10000 (91.48%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3653 \n",
      "Accuracy: 9212/10000 (92.12%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3733 \n",
      "Accuracy: 9189/10000 (91.89%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3773 \n",
      "Accuracy: 9161/10000 (91.61%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3742 \n",
      "Accuracy: 9180/10000 (91.80%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3901 \n",
      "Accuracy: 9140/10000 (91.40%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3975 \n",
      "Accuracy: 9138/10000 (91.38%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3835 \n",
      "Accuracy: 9136/10000 (91.36%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3650 \n",
      "Accuracy: 9167/10000 (91.67%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3753 \n",
      "Accuracy: 9104/10000 (91.04%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3893 \n",
      "Accuracy: 9137/10000 (91.37%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3831 \n",
      "Accuracy: 9179/10000 (91.79%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3602 \n",
      "Accuracy: 9173/10000 (91.73%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3802 \n",
      "Accuracy: 9144/10000 (91.44%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3609 \n",
      "Accuracy: 9185/10000 (91.85%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3711 \n",
      "Accuracy: 9164/10000 (91.64%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "N_i = int(N/G)\n",
    "K_i = int(K/G)\n",
    "T = 3\n",
    "sigma = 1\n",
    "Noise_Alloc = [0,2,4]\n",
    "m = N_i # number of selected workers (if there is no straggler, m=N_i)\n",
    "\n",
    "print(N_i,K_i,T)\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K_i+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K_i+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K_i+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "z_array = np.array([-0.81, 0.81])\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 30\n",
    "\n",
    "loss_test_arr_G3 = np.zeros((N_trials,N_epochs))\n",
    "acc_test_arr_G3  = np.zeros((N_trials,N_epochs))\n",
    "\n",
    "for trial_idx in range(N_trials):\n",
    "    \n",
    "    print('##########################################')\n",
    "    print('######',trial_idx,'-th Trial!! ###########')\n",
    "    \n",
    "    net_glob = CNNMnist2(args=args)\n",
    "    net_glob.cuda()\n",
    "    net_glob.train()\n",
    "    \n",
    "    # copy weights\n",
    "    w_glob = net_glob.state_dict()\n",
    "    \n",
    "    X_tilde = np.empty((N,Size_submatrices,28*28))\n",
    "    y_tilde = np.empty((N,Size_submatrices,10))\n",
    "    \n",
    "    for G_idx in range(G):\n",
    "        \n",
    "        _Noise_label = np.ones((size_per_group*T,10)) * 0.1\n",
    "        \n",
    "        X_tilde_tmp,a,b = BACC_Enc_Data_v3(X_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde_tmp,a,b = BACC_Enc_Data_v3(y_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "        \n",
    "        stt_pos = G_idx * N_i\n",
    "        end_pos = (G_idx+1) * N_i\n",
    "        \n",
    "        X_tilde[stt_pos:end_pos,:,:] = X_tilde_tmp\n",
    "        y_tilde[stt_pos:end_pos,:,:] = y_tilde_tmp\n",
    "        \n",
    "   \n",
    "\n",
    "    for iter in range(N_epochs): #args.epochs\n",
    "        \n",
    "        w_group_array = []\n",
    "        for G_idx in range(G):\n",
    "            w_locals, loss_locals = [], []\n",
    "            idxs_users = np.random.choice(range(N_i), m, replace=False)\n",
    "            idxs_users = np.sort(idxs_users)\n",
    "            print('selected users:',idxs_users)\n",
    "\n",
    "            coded_net = BACC_Enc_Model_withNoise_v3(net_glob.cuda(), N_i, K_i, T, 0.01, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "            dec_z_array = []\n",
    "            for idx in idxs_users: #for idx in range(N):\n",
    "        #         print(idx)\n",
    "                local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[G_idx*N_i+idx,:,:], label=y_tilde[G_idx*N_i+idx,:,:])\n",
    "                w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "                w_locals.append(copy.deepcopy(w))\n",
    "                loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "            # update global weights\n",
    "            #w_glob = FedAvg(w_locals)\n",
    "            w_group = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "            \n",
    "            w_group_array.append(copy.deepcopy(w_group))\n",
    "        \n",
    "        w_glob = copy.deepcopy(w_group_array[0])\n",
    "        for k in w_glob.keys():\n",
    "            for G_idx in range(1,G):\n",
    "                w_glob[k] += w_group_array[G_idx][k]\n",
    "            w_glob[k] = torch.div(w_glob[k], len(w_group_array))\n",
    "        \n",
    "        # copy weight to net_glob\n",
    "        net_glob.load_state_dict(w_glob)\n",
    "\n",
    "        # print loss\n",
    "    #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "    #     loss_train_arr.append(loss_train)\n",
    "\n",
    "        acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "        acc_test_arr_G3[trial_idx][iter] = acc_test\n",
    "        loss_test_arr_G3[trial_idx][iter] = loss_test\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. K=2, Without Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of X: (60000, 784)\n",
      "size of Y: (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "class my_argument:    \n",
    "    epochs = 200    #\"rounds of training\"\n",
    "    num_users = 15  # \"number of users: N\"\n",
    "    num_partition = 2 # \"number of users: K\"\n",
    "    frac = 0.5 #\"the fraction of clients: C\"\n",
    "    local_ep = 1 #\"the number of local epochs: E\"\n",
    "    local_bs = 200 #\"local batch size: B\"\n",
    "    bs=200 #\"test batch size\"\n",
    "    lr=0.01 #\"learning rate\"\n",
    "    momentum=0.5 # \"SGD momentum (default: 0.5)\"\n",
    "    split='user' # \"train-test split type, user or sample\"\n",
    "    opt='ADAM'\n",
    "    loss='Custom' # 'Custom' or 'Default'\n",
    "\n",
    "    # model arguments\n",
    "    model = 'cnn'\n",
    "    kernel_num=9 #, help='number of each kind of kernel')\n",
    "    kernel_sizes='3,4,5' #  help='comma-separated kernel size to use for convolution')\n",
    "    norm='None' #, help=\"batch_norm, layer_norm, or None\")\n",
    "    num_filters=32 #, help=\"number of filters for conv nets\")\n",
    "    max_pool='True' #help=\"Whether use max pooling rather than strided convolutions\")\n",
    "\n",
    "    # other arguments\n",
    "    dataset='mnist' #, help=\"name of dataset\")\n",
    "    iid=1\n",
    "    num_classes=10#, help=\"number of classes\")\n",
    "    num_channels=1#, help=\"number of channels of imges\")\n",
    "    gpu=1#, help=\"GPU ID, -1 for CPU\")\n",
    "    stopping_rounds=10#, help='rounds of early stopping')\n",
    "    verbose='False'#, help='verbose print')\n",
    "    seed=1#, help='random seed (default: 1)')\n",
    "    \n",
    "args = my_argument()\n",
    "\n",
    "args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "# load dataset and split users\n",
    "trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "dataset_train = datasets.MNIST('../data/mnist/', train=True, download=True, transform=trans_mnist)\n",
    "dataset_test = datasets.MNIST('../data/mnist/', train=False, download=True, transform=trans_mnist)\n",
    "\n",
    "dict_users = mnist_iid(dataset_train, args.num_partition)\n",
    "\n",
    "encoding_input_array_np = np.empty((len(dataset_train),28*28))\n",
    "encoding_label_array_np = np.empty((len(dataset_train),args.num_classes))\n",
    "print(\"size of X:\" ,encoding_input_array_np.shape)\n",
    "print(\"size of Y:\" ,encoding_label_array_np.shape)\n",
    "\n",
    "Size_submatrices = int(60000/args.num_partition)\n",
    "\n",
    "for i in range(args.num_partition):\n",
    "    \n",
    "    stt_pos = i*Size_submatrices\n",
    "    end_pos = (i+1)*Size_submatrices\n",
    "#     print(i,stt_pos,end_pos)\n",
    "    Temp_train = DataLoader(DatasetSplit(dataset_train, dict_users[i]), batch_size=Size_submatrices, shuffle=True)\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(Temp_train):\n",
    "        \n",
    "        images_np = images.detach().cpu().numpy()\n",
    "        encoding_input_array_np[stt_pos:end_pos,:] = np.reshape(images_np, (Size_submatrices,28*28))\n",
    "#         print(encoding_input_array_np[stt_pos:end_pos,:].shape)\n",
    "\n",
    "        onehot_labels = torch.nn.functional.one_hot(labels,num_classes=args.num_classes)\n",
    "        labels_np = onehot_labels.detach().cpu().numpy()\n",
    "#         print(labels_np.shape)\n",
    "        encoding_label_array_np[stt_pos:end_pos,:] = labels_np\n",
    "\n",
    "\n",
    "# print(labels_np[0:10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z_array: [-0.81  0.81]\n",
      "0.4838626198316927\n",
      "0.48386261983169315\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 30000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 30000 \n",
      "\n",
      "(T, sigma)= 3 1 )  0 -th Trial!!\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.1504 \n",
      "Accuracy: 5550/10000 (55.50%)\n",
      "\n",
      "Round   0, Average loss 2.150 Test accuracy 55.500\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.7712 \n",
      "Accuracy: 8177/10000 (81.77%)\n",
      "\n",
      "Round   1, Average loss 0.771 Test accuracy 81.770\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2180 \n",
      "Accuracy: 9692/10000 (96.92%)\n",
      "\n",
      "Round   2, Average loss 0.218 Test accuracy 96.920\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1758 \n",
      "Accuracy: 9740/10000 (97.40%)\n",
      "\n",
      "Round   3, Average loss 0.176 Test accuracy 97.400\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1905 \n",
      "Accuracy: 9728/10000 (97.28%)\n",
      "\n",
      "Round   4, Average loss 0.191 Test accuracy 97.280\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1868 \n",
      "Accuracy: 9750/10000 (97.50%)\n",
      "\n",
      "Round   5, Average loss 0.187 Test accuracy 97.500\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1771 \n",
      "Accuracy: 9752/10000 (97.52%)\n",
      "\n",
      "Round   6, Average loss 0.177 Test accuracy 97.520\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1665 \n",
      "Accuracy: 9737/10000 (97.37%)\n",
      "\n",
      "Round   7, Average loss 0.166 Test accuracy 97.370\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1977 \n",
      "Accuracy: 9736/10000 (97.36%)\n",
      "\n",
      "Round   8, Average loss 0.198 Test accuracy 97.360\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1733 \n",
      "Accuracy: 9732/10000 (97.32%)\n",
      "\n",
      "Round   9, Average loss 0.173 Test accuracy 97.320\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1734 \n",
      "Accuracy: 9756/10000 (97.56%)\n",
      "\n",
      "Round  10, Average loss 0.173 Test accuracy 97.560\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1853 \n",
      "Accuracy: 9765/10000 (97.65%)\n",
      "\n",
      "Round  11, Average loss 0.185 Test accuracy 97.650\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3139 \n",
      "Accuracy: 9485/10000 (94.85%)\n",
      "\n",
      "Round  12, Average loss 0.314 Test accuracy 94.850\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1789 \n",
      "Accuracy: 9738/10000 (97.38%)\n",
      "\n",
      "Round  13, Average loss 0.179 Test accuracy 97.380\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2365 \n",
      "Accuracy: 9737/10000 (97.37%)\n",
      "\n",
      "Round  14, Average loss 0.236 Test accuracy 97.370\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2020 \n",
      "Accuracy: 9748/10000 (97.48%)\n",
      "\n",
      "Round  15, Average loss 0.202 Test accuracy 97.480\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1793 \n",
      "Accuracy: 9749/10000 (97.49%)\n",
      "\n",
      "Round  16, Average loss 0.179 Test accuracy 97.490\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1965 \n",
      "Accuracy: 9742/10000 (97.42%)\n",
      "\n",
      "Round  17, Average loss 0.196 Test accuracy 97.420\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1861 \n",
      "Accuracy: 9749/10000 (97.49%)\n",
      "\n",
      "Round  18, Average loss 0.186 Test accuracy 97.490\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3015 \n",
      "Accuracy: 9566/10000 (95.66%)\n",
      "\n",
      "Round  19, Average loss 0.302 Test accuracy 95.660\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1838 \n",
      "Accuracy: 9762/10000 (97.62%)\n",
      "\n",
      "Round  20, Average loss 0.184 Test accuracy 97.620\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1723 \n",
      "Accuracy: 9744/10000 (97.44%)\n",
      "\n",
      "Round  21, Average loss 0.172 Test accuracy 97.440\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2225 \n",
      "Accuracy: 9748/10000 (97.48%)\n",
      "\n",
      "Round  22, Average loss 0.223 Test accuracy 97.480\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1853 \n",
      "Accuracy: 9747/10000 (97.47%)\n",
      "\n",
      "Round  23, Average loss 0.185 Test accuracy 97.470\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1962 \n",
      "Accuracy: 9755/10000 (97.55%)\n",
      "\n",
      "Round  24, Average loss 0.196 Test accuracy 97.550\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2181 \n",
      "Accuracy: 9743/10000 (97.43%)\n",
      "\n",
      "Round  25, Average loss 0.218 Test accuracy 97.430\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2300 \n",
      "Accuracy: 9750/10000 (97.50%)\n",
      "\n",
      "Round  26, Average loss 0.230 Test accuracy 97.500\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1930 \n",
      "Accuracy: 9749/10000 (97.49%)\n",
      "\n",
      "Round  27, Average loss 0.193 Test accuracy 97.490\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1894 \n",
      "Accuracy: 9753/10000 (97.53%)\n",
      "\n",
      "Round  28, Average loss 0.189 Test accuracy 97.530\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2037 \n",
      "Accuracy: 9753/10000 (97.53%)\n",
      "\n",
      "Round  29, Average loss 0.204 Test accuracy 97.530\n",
      "z_array: [-0.81 -0.22  0.22  0.81]\n",
      "0.4838626198316927\n",
      "0.46852880899669475\n",
      "0.468528808996694\n",
      "0.48386261983169315\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 30000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 30000 \n",
      "\n",
      "(T, sigma)= 3 1 )  0 -th Trial!!\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.2978 \n",
      "Accuracy: 4081/10000 (40.81%)\n",
      "\n",
      "Round   0, Average loss 2.298 Test accuracy 40.810\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.2801 \n",
      "Accuracy: 3493/10000 (34.93%)\n",
      "\n",
      "Round   1, Average loss 2.280 Test accuracy 34.930\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.2261 \n",
      "Accuracy: 7954/10000 (79.54%)\n",
      "\n",
      "Round   2, Average loss 2.226 Test accuracy 79.540\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.1914 \n",
      "Accuracy: 7929/10000 (79.29%)\n",
      "\n",
      "Round   3, Average loss 2.191 Test accuracy 79.290\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.1369 \n",
      "Accuracy: 6894/10000 (68.94%)\n",
      "\n",
      "Round   4, Average loss 2.137 Test accuracy 68.940\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.0815 \n",
      "Accuracy: 8826/10000 (88.26%)\n",
      "\n",
      "Round   5, Average loss 2.081 Test accuracy 88.260\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.0964 \n",
      "Accuracy: 8591/10000 (85.91%)\n",
      "\n",
      "Round   6, Average loss 2.096 Test accuracy 85.910\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.1099 \n",
      "Accuracy: 9409/10000 (94.09%)\n",
      "\n",
      "Round   7, Average loss 2.110 Test accuracy 94.090\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.1391 \n",
      "Accuracy: 8650/10000 (86.50%)\n",
      "\n",
      "Round   8, Average loss 2.139 Test accuracy 86.500\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.0060 \n",
      "Accuracy: 9606/10000 (96.06%)\n",
      "\n",
      "Round   9, Average loss 2.006 Test accuracy 96.060\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9598 \n",
      "Accuracy: 9456/10000 (94.56%)\n",
      "\n",
      "Round  10, Average loss 1.960 Test accuracy 94.560\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9703 \n",
      "Accuracy: 9485/10000 (94.85%)\n",
      "\n",
      "Round  11, Average loss 1.970 Test accuracy 94.850\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9826 \n",
      "Accuracy: 9690/10000 (96.90%)\n",
      "\n",
      "Round  12, Average loss 1.983 Test accuracy 96.900\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.0128 \n",
      "Accuracy: 9711/10000 (97.11%)\n",
      "\n",
      "Round  13, Average loss 2.013 Test accuracy 97.110\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8374 \n",
      "Accuracy: 9722/10000 (97.22%)\n",
      "\n",
      "Round  14, Average loss 1.837 Test accuracy 97.220\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9075 \n",
      "Accuracy: 9455/10000 (94.55%)\n",
      "\n",
      "Round  15, Average loss 1.908 Test accuracy 94.550\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9255 \n",
      "Accuracy: 9717/10000 (97.17%)\n",
      "\n",
      "Round  16, Average loss 1.925 Test accuracy 97.170\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8705 \n",
      "Accuracy: 9569/10000 (95.69%)\n",
      "\n",
      "Round  17, Average loss 1.871 Test accuracy 95.690\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8759 \n",
      "Accuracy: 9661/10000 (96.61%)\n",
      "\n",
      "Round  18, Average loss 1.876 Test accuracy 96.610\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9110 \n",
      "Accuracy: 9700/10000 (97.00%)\n",
      "\n",
      "Round  19, Average loss 1.911 Test accuracy 97.000\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8551 \n",
      "Accuracy: 9721/10000 (97.21%)\n",
      "\n",
      "Round  20, Average loss 1.855 Test accuracy 97.210\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.0106 \n",
      "Accuracy: 9707/10000 (97.07%)\n",
      "\n",
      "Round  21, Average loss 2.011 Test accuracy 97.070\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.0318 \n",
      "Accuracy: 9667/10000 (96.67%)\n",
      "\n",
      "Round  22, Average loss 2.032 Test accuracy 96.670\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9992 \n",
      "Accuracy: 9299/10000 (92.99%)\n",
      "\n",
      "Round  23, Average loss 1.999 Test accuracy 92.990\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8857 \n",
      "Accuracy: 9527/10000 (95.27%)\n",
      "\n",
      "Round  24, Average loss 1.886 Test accuracy 95.270\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8113 \n",
      "Accuracy: 9700/10000 (97.00%)\n",
      "\n",
      "Round  25, Average loss 1.811 Test accuracy 97.000\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8391 \n",
      "Accuracy: 9736/10000 (97.36%)\n",
      "\n",
      "Round  26, Average loss 1.839 Test accuracy 97.360\n",
      "selected users: [0 1 2 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.9353 \n",
      "Accuracy: 9652/10000 (96.52%)\n",
      "\n",
      "Round  27, Average loss 1.935 Test accuracy 96.520\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.7971 \n",
      "Accuracy: 9755/10000 (97.55%)\n",
      "\n",
      "Round  28, Average loss 1.797 Test accuracy 97.550\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9200 \n",
      "Accuracy: 9682/10000 (96.82%)\n",
      "\n",
      "Round  29, Average loss 1.920 Test accuracy 96.820\n",
      "z_array: [-0.9  -0.81 -0.22  0.22  0.81  0.9 ]\n",
      "0.039572349708934106\n",
      "0.4838626198316927\n",
      "0.46852880899669475\n",
      "0.468528808996694\n",
      "0.48386261983169315\n",
      "0.039572349708934106\n",
      "@BACC_Enc: N,K,T, m_i= 6 2 3 30000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 6 2 3 30000 \n",
      "\n",
      "(T, sigma)= 3 1 )  0 -th Trial!!\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 2.2223 \n",
      "Accuracy: 6257/10000 (62.57%)\n",
      "\n",
      "Round   1, Average loss 2.222 Test accuracy 62.570\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 2.2161 \n",
      "Accuracy: 6335/10000 (63.35%)\n",
      "\n",
      "Round   2, Average loss 2.216 Test accuracy 63.350\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 1.6638 \n",
      "Accuracy: 8681/10000 (86.81%)\n",
      "\n",
      "Round   3, Average loss 1.664 Test accuracy 86.810\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 1.3783 \n",
      "Accuracy: 9617/10000 (96.17%)\n",
      "\n",
      "Round   4, Average loss 1.378 Test accuracy 96.170\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 1.2237 \n",
      "Accuracy: 9644/10000 (96.44%)\n",
      "\n",
      "Round   5, Average loss 1.224 Test accuracy 96.440\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.8223 \n",
      "Accuracy: 9638/10000 (96.38%)\n",
      "\n",
      "Round   6, Average loss 0.822 Test accuracy 96.380\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.7091 \n",
      "Accuracy: 9713/10000 (97.13%)\n",
      "\n",
      "Round   7, Average loss 0.709 Test accuracy 97.130\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.6716 \n",
      "Accuracy: 9713/10000 (97.13%)\n",
      "\n",
      "Round   8, Average loss 0.672 Test accuracy 97.130\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.6693 \n",
      "Accuracy: 9678/10000 (96.78%)\n",
      "\n",
      "Round   9, Average loss 0.669 Test accuracy 96.780\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.5860 \n",
      "Accuracy: 9650/10000 (96.50%)\n",
      "\n",
      "Round  10, Average loss 0.586 Test accuracy 96.500\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.5678 \n",
      "Accuracy: 9689/10000 (96.89%)\n",
      "\n",
      "Round  11, Average loss 0.568 Test accuracy 96.890\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.6501 \n",
      "Accuracy: 9699/10000 (96.99%)\n",
      "\n",
      "Round  12, Average loss 0.650 Test accuracy 96.990\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.5906 \n",
      "Accuracy: 9678/10000 (96.78%)\n",
      "\n",
      "Round  13, Average loss 0.591 Test accuracy 96.780\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.6501 \n",
      "Accuracy: 9738/10000 (97.38%)\n",
      "\n",
      "Round  14, Average loss 0.650 Test accuracy 97.380\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.5508 \n",
      "Accuracy: 9722/10000 (97.22%)\n",
      "\n",
      "Round  15, Average loss 0.551 Test accuracy 97.220\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.5475 \n",
      "Accuracy: 9593/10000 (95.93%)\n",
      "\n",
      "Round  16, Average loss 0.547 Test accuracy 95.930\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.5397 \n",
      "Accuracy: 9471/10000 (94.71%)\n",
      "\n",
      "Round  17, Average loss 0.540 Test accuracy 94.710\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.8024 \n",
      "Accuracy: 9654/10000 (96.54%)\n",
      "\n",
      "Round  18, Average loss 0.802 Test accuracy 96.540\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.6738 \n",
      "Accuracy: 9694/10000 (96.94%)\n",
      "\n",
      "Round  19, Average loss 0.674 Test accuracy 96.940\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.5364 \n",
      "Accuracy: 9693/10000 (96.93%)\n",
      "\n",
      "Round  20, Average loss 0.536 Test accuracy 96.930\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.6852 \n",
      "Accuracy: 9707/10000 (97.07%)\n",
      "\n",
      "Round  21, Average loss 0.685 Test accuracy 97.070\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.6254 \n",
      "Accuracy: 9715/10000 (97.15%)\n",
      "\n",
      "Round  22, Average loss 0.625 Test accuracy 97.150\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.5700 \n",
      "Accuracy: 9576/10000 (95.76%)\n",
      "\n",
      "Round  23, Average loss 0.570 Test accuracy 95.760\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.7501 \n",
      "Accuracy: 9319/10000 (93.19%)\n",
      "\n",
      "Round  24, Average loss 0.750 Test accuracy 93.190\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.8517 \n",
      "Accuracy: 9267/10000 (92.67%)\n",
      "\n",
      "Round  25, Average loss 0.852 Test accuracy 92.670\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.8802 \n",
      "Accuracy: 9491/10000 (94.91%)\n",
      "\n",
      "Round  26, Average loss 0.880 Test accuracy 94.910\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.8818 \n",
      "Accuracy: 8799/10000 (87.99%)\n",
      "\n",
      "Round  27, Average loss 0.882 Test accuracy 87.990\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.8674 \n",
      "Accuracy: 9460/10000 (94.60%)\n",
      "\n",
      "Round  28, Average loss 0.867 Test accuracy 94.600\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.6362 \n",
      "Accuracy: 9709/10000 (97.09%)\n",
      "\n",
      "Round  29, Average loss 0.636 Test accuracy 97.090\n",
      "z_array: [-0.9  -0.81 -0.22 -0.16  0.16  0.22  0.81  0.9 ]\n",
      "0.039572349708934106\n",
      "0.4838626198316927\n",
      "0.46852880899669475\n",
      "0.23252578774407598\n",
      "0.23252578774407545\n",
      "0.468528808996694\n",
      "0.48386261983169315\n",
      "0.039572349708934106\n",
      "@BACC_Enc: N,K,T, m_i= 8 2 3 30000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 8 2 3 30000 \n",
      "\n",
      "(T, sigma)= 3 1 )  0 -th Trial!!\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 2.2079 \n",
      "Accuracy: 4164/10000 (41.64%)\n",
      "\n",
      "Round   0, Average loss 2.208 Test accuracy 41.640\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.6987 \n",
      "Accuracy: 7686/10000 (76.86%)\n",
      "\n",
      "Round   1, Average loss 1.699 Test accuracy 76.860\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.5383 \n",
      "Accuracy: 9540/10000 (95.40%)\n",
      "\n",
      "Round   2, Average loss 0.538 Test accuracy 95.400\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.5397 \n",
      "Accuracy: 9462/10000 (94.62%)\n",
      "\n",
      "Round   3, Average loss 0.540 Test accuracy 94.620\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.6978 \n",
      "Accuracy: 9447/10000 (94.47%)\n",
      "\n",
      "Round   4, Average loss 0.698 Test accuracy 94.470\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.3977 \n",
      "Accuracy: 9543/10000 (95.43%)\n",
      "\n",
      "Round   5, Average loss 0.398 Test accuracy 95.430\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.8793 \n",
      "Accuracy: 9627/10000 (96.27%)\n",
      "\n",
      "Round   6, Average loss 0.879 Test accuracy 96.270\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.2739 \n",
      "Accuracy: 9606/10000 (96.06%)\n",
      "\n",
      "Round   7, Average loss 0.274 Test accuracy 96.060\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.2858 \n",
      "Accuracy: 9585/10000 (95.85%)\n",
      "\n",
      "Round   8, Average loss 0.286 Test accuracy 95.850\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.4189 \n",
      "Accuracy: 9596/10000 (95.96%)\n",
      "\n",
      "Round   9, Average loss 0.419 Test accuracy 95.960\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.4356 \n",
      "Accuracy: 9599/10000 (95.99%)\n",
      "\n",
      "Round  10, Average loss 0.436 Test accuracy 95.990\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.3830 \n",
      "Accuracy: 9478/10000 (94.78%)\n",
      "\n",
      "Round  11, Average loss 0.383 Test accuracy 94.780\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.3993 \n",
      "Accuracy: 9545/10000 (95.45%)\n",
      "\n",
      "Round  12, Average loss 0.399 Test accuracy 95.450\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.4343 \n",
      "Accuracy: 9518/10000 (95.18%)\n",
      "\n",
      "Round  13, Average loss 0.434 Test accuracy 95.180\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.2578 \n",
      "Accuracy: 9606/10000 (96.06%)\n",
      "\n",
      "Round  14, Average loss 0.258 Test accuracy 96.060\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.2952 \n",
      "Accuracy: 9710/10000 (97.10%)\n",
      "\n",
      "Round  15, Average loss 0.295 Test accuracy 97.100\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.3339 \n",
      "Accuracy: 9661/10000 (96.61%)\n",
      "\n",
      "Round  16, Average loss 0.334 Test accuracy 96.610\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.5458 \n",
      "Accuracy: 9503/10000 (95.03%)\n",
      "\n",
      "Round  17, Average loss 0.546 Test accuracy 95.030\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.4201 \n",
      "Accuracy: 9622/10000 (96.22%)\n",
      "\n",
      "Round  18, Average loss 0.420 Test accuracy 96.220\n",
      "selected users: [0 1 2 3 4 5 6 7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.5977 \n",
      "Accuracy: 9224/10000 (92.24%)\n",
      "\n",
      "Round  19, Average loss 0.598 Test accuracy 92.240\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.3295 \n",
      "Accuracy: 9644/10000 (96.44%)\n",
      "\n",
      "Round  20, Average loss 0.330 Test accuracy 96.440\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.3735 \n",
      "Accuracy: 9571/10000 (95.71%)\n",
      "\n",
      "Round  21, Average loss 0.373 Test accuracy 95.710\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.3915 \n",
      "Accuracy: 9354/10000 (93.54%)\n",
      "\n",
      "Round  22, Average loss 0.391 Test accuracy 93.540\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.4134 \n",
      "Accuracy: 9209/10000 (92.09%)\n",
      "\n",
      "Round  23, Average loss 0.413 Test accuracy 92.090\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.4039 \n",
      "Accuracy: 9634/10000 (96.34%)\n",
      "\n",
      "Round  24, Average loss 0.404 Test accuracy 96.340\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.4398 \n",
      "Accuracy: 9725/10000 (97.25%)\n",
      "\n",
      "Round  25, Average loss 0.440 Test accuracy 97.250\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.4958 \n",
      "Accuracy: 9551/10000 (95.51%)\n",
      "\n",
      "Round  26, Average loss 0.496 Test accuracy 95.510\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.4113 \n",
      "Accuracy: 9572/10000 (95.72%)\n",
      "\n",
      "Round  27, Average loss 0.411 Test accuracy 95.720\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.2503 \n",
      "Accuracy: 9627/10000 (96.27%)\n",
      "\n",
      "Round  28, Average loss 0.250 Test accuracy 96.270\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.3805 \n",
      "Accuracy: 9541/10000 (95.41%)\n",
      "\n",
      "Round  29, Average loss 0.381 Test accuracy 95.410\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "K = 2\n",
    "T = 3\n",
    "sigma = 1\n",
    "Noise_Alloc = [0,2,4]\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "\n",
    "N_array = [2,4,6,8]\n",
    "B_array = [0.5]\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 30\n",
    "\n",
    "\n",
    "\n",
    "loss_test_arr_v1 = np.zeros((len(N_array),len(B_array),N_trials,N_epochs))\n",
    "acc_test_arr_v1  = np.zeros((len(N_array),len(B_array),N_trials,N_epochs))\n",
    "\n",
    "for N_idx in range(len(N_array)):\n",
    "    \n",
    "    N = N_array[N_idx]\n",
    "    \n",
    "    \n",
    "           \n",
    "        \n",
    "    # print(\"alpha_array: \",alpha_array,'\\n')\n",
    "    \n",
    "    \n",
    "    for B_idx in range(len(B_array)):\n",
    "        \n",
    "        B = B_array[B_idx]\n",
    "        z_array = []\n",
    "#         while(len(z_array)<N):\n",
    "#             z_tmp = np.random.uniform(-1,1,1)\n",
    "#             MIS_tmp = MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_tmp], 1,sigma)\n",
    "#             if MIS_tmp < B and MIS_tmp > 0.1:\n",
    "#                 z_array.append(z_tmp[0])\n",
    "#         \n",
    "#         z_array = np.sort(z_array)\n",
    "\n",
    "        if N==2:\n",
    "            z_array = np.array([-0.81, 0.81])\n",
    "        elif N ==4:\n",
    "            z_array = np.array([-0.81, -0.22, 0.22, 0.81])\n",
    "        elif N ==5:\n",
    "            z_array = np.array([-0.81, -0.22, 0, 0.22, 0.81])\n",
    "        elif N ==6:\n",
    "            z_array = np.array([-0.9, -0.81, -0.22, 0.22, 0.81, 0.9])\n",
    "        elif N ==7:\n",
    "            z_array = np.array([-0.9, -0.82, -0.21, 0, 0.21, 0.82, 0.9])\n",
    "        else:\n",
    "            z_array = np.array([-0.9, -0.81, -0.22, -0.16, 0.16, 0.22, 0.81, 0.9])\n",
    "#             z_array = np.array([-0.9, -0.81, -0.22, -0.20, 0.20, 0.22, 0.81, 0.9])\n",
    "            \n",
    "        print('z_array:',z_array)\n",
    "        for j in range(len(z_array)):\n",
    "            print(MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_array[j]], 1,sigma))\n",
    "        \n",
    "        \n",
    "        _Noise_label = np.ones((30000*T,10)) * 0.1\n",
    "\n",
    "        X_tilde,a,b = BACC_Enc_Data_v3(encoding_input_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde,a,b = BACC_Enc_Data_v3(encoding_label_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "\n",
    "        m = N # m is the number of received result @ master\n",
    "    #     print('number of results:',m)\n",
    "\n",
    "        for trial_idx in range(N_trials):\n",
    "            print('(T, sigma)=',T,sigma,') ',trial_idx,'-th Trial!!')\n",
    "\n",
    "            net_glob = CNNMnist2(args=args)\n",
    "            net_glob.cuda()\n",
    "            net_glob.train()\n",
    "\n",
    "            # copy weights\n",
    "            w_glob = net_glob.state_dict()\n",
    "\n",
    "            for iter in range(N_epochs): #args.epochs\n",
    "                w_locals, loss_locals = [], []\n",
    "                idxs_users = np.random.choice(range(N), m, replace=False)\n",
    "                idxs_users = np.sort(idxs_users)\n",
    "                print('selected users:',idxs_users)\n",
    "                \n",
    "                coded_net = BACC_Enc_Model_withNoise_v3(net_glob.cuda(), N, K, T, 0.01, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "                dec_z_array = []\n",
    "                for idx in idxs_users: #for idx in range(N):\n",
    "            #         print(idx)\n",
    "                    local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[idx,:,:], label=y_tilde[idx,:,:])\n",
    "                    w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "                    w_locals.append(copy.deepcopy(w))\n",
    "                    loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                    dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "                # update global weights\n",
    "                #w_glob = FedAvg(w_locals)\n",
    "                w_glob = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "\n",
    "                # copy weight to net_glob\n",
    "                net_glob.load_state_dict(w_glob)\n",
    "\n",
    "                # print loss\n",
    "            #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "            #     loss_train_arr.append(loss_train)\n",
    "\n",
    "                acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "                acc_test_arr_v1[N_idx][B_idx][trial_idx][iter] = acc_test\n",
    "                loss_test_arr_v1[N_idx][B_idx][trial_idx][iter] = loss_test\n",
    "                if iter % 1 ==0:\n",
    "                    print('Round {:3d}, Average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_test, acc_test))\n",
    "                #print(loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEGCAYAAABhMDI9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOydZ5QdxZmwn+qbw+QoaYJyQBJIQgiJKIEBZ8CsMc5rs84B25i1vYuN1+vdZT8nMPYaY2wTDescwAQhJJJQQAEJzYzy5KRJN4e+3fX96DtJcydKMxKz9ZzTp7qrU1XPnX676k1CSolCoVAoFL1oZ7oBCoVCoTi7UIJBoVAoFINQgkGhUCgUg1CCQaFQKBSDUIJBoVAoFIOwn+kGnAqFhYVy9uzZEzo3Eong8/lOb4POMNOtT9OtPzD9+jTd+gPTr0+Z+rNr164OKWXRcOe8qQXD7Nmzee211yZ07pYtW1i/fv3pbdAZZrr1abr1B6Zfn6Zbf2D69SlTf4QQdSOdM2lTSUKIXwkh2oUQbwyoyxdCbBRCHE6Xeel6IYT4sRDiiBBinxBi1WS1S6FQKBQjM5k6hgeAt55U93Vgk5RyAbApvQ3wNmBBevkk8LNJbJdCoVAoRmDSBIOU8kWg66Tqa4EH0+sPAtcNqH9IWmwDcoUQMyarbQqFQqEYHjGZITGEELOBJ6SUy9LbPVLK3AH7u6WUeUKIJ4A7pZQvp+s3AV+TUg5RIAghPok1qqCkpOT8xx9/fEJtC4fD+P3+CZ17tjLd+jTd+gPTr0/TrT8w/fqUqT8bNmzYJaVcPdw5Z4vyWWSoyyixpJT3AfcBrF69Wk5USTTdFEww/fo03foD069P060/MP36NJH+TLUfQ1vvFFG6bE/XNwLlA44rA5qnuG0KhUKhYOoFw1+Bj6bXPwr8ZUD9R9LWSWuBgJSyZYrbplAoFAomcSpJCPEYsB4oFEI0AncAdwK/FULcDNQD700f/nfg7cARIAp8bLLaNZ3QDZNo0iCaTBFJGDSETKpbgkgJEkmv+siUMl0HUkqEELjsGm6HDZddG7Rut029M3zKMIkkDMLJFJFEinDCKne1pYjsayFlmuiGJGWY6KZVpgyJblqlKSVXnVPC0pk5U9724TBNq326IdFTJrphkjRMTkRN6jujgPU36qX3bzVw/tSuCWyaGFBqaBrYNa2vXtMEUkoMU5IyB5dG37aJYUqy3Q7yfM7T1sfXG3rYWKfTuqMej9OG22Etnt7FqfVtux02hADDlJjS+h2a0vptmuaAddn/uxWit7RWeuebhQCR3jJk+veQ7qtumOmyv+8pw7qgw6bhtAucNhtOu4bDJnDaNZw2Lb1tLbopiSZTA55ff2n2bZuYve1kcHt7a6x2WpzcZ8O0+mlKiSEl0jRw9hzHFu8inj2HlLtg4AUzMrvAS3G2e+J/wBGYNMEgpXz/MLuuzHCsBD43WW2ZSgxT0hlO0BqM0xZM0BaM9y2twQTtwTjtoQSGKXHYrJfywB+pw9b/Q3XaNIQQxHTrxd8rAKLJFJGkQTJlDm3AKy+dUvttmsBtlzg9PdhdHQgtZe0Qg4rB6wIENmzSh036sOPHjh9NONAEaEJYpWb9O8d1k0haCITiKRKZ+tHLnt1javePNx3mQ2srufWqReR4HRmPMUwj3UfbgMoU6BFIRiEZQSbDvFpTz993HyMei+IghRMdRJykPUzMHiZmCxOxRwnZY4TscSKaSVz6MKUTaTowTTvStCOlA0wHSDuOpMacjgSJVAn/uek4PfZipMgshOeKZpaIel4xl9JD1pj6PxY0ARsWFXPTmgo2LCoa/iNAj0MynF6s52I9owjJWJj9x5vYe7SJnp4eioVBwyEHCRzEsZPATgwbcU0jITTiQiOBICE0QsJFAA9xYQdhAiYIAyFMwLDqhAGGh1R0HpiT89I7GYeRojLUyvyeRuaEWmjJLeCnT72OzxEnnxB5IkS+CJFLmHwRIg9rWyI4KMupNiuokRXUmBV0MLaPkyyirNCOsEocZqV2hJXaYXJEtG9/t/RzVM7kqDmTo3KGtS5n0iCLMbBh8x7mX668hpsvXjIpz+RsUT6/6dl8sJ1/+eP+vpf+QDQBhX4XpTluyvK8rKrMw6EJkoZJImV9VSZTRrq0vixD8RTJlIkpJT6XnSy3ndJsN16XDb/Ljtdpx+e04XVZpcdp41BNNcuWLk1/aAhE+qUsSH+9pL+0TGndJ5yM0xypozVWT1u8ns5EA116A8FUCylSpE7xmWi4sacFhQ1/n+Bwef3k2f2U2f1kubLIcvjJcWWT584mz5NNoS+HPI+Xqn17WHX+eUSMABG9h3AqQDjVQzDZTTDZQyDZTSDRTUesi6aeIH9oi/Gn35hkezWcdolu6qTMFLphlSaWAHJKiUeC1zTxmgYeKfGaMl2aeKTE55QkPBp1djtNDjtdNtugvnlMk1mpFEtSJvlGiogtQoNnLgFnPqZMkhuIMLupizkNEeY2xilrS2Hvk39PkbJBNNuLkVeEVlCGkV+EzPFRHNvFzMhWXFlJbF7BifzzqS++grqiDQSdJdZXbO8IwbC+XIVIjypsvaMLbdBoQ9MEbjNKW/3r7Dj0BI/9vo2nPEFm5SXxeGKEzTBdeoQuM0GXTNGtQar3K33AV7FIj2cEQL61SCAhBLoQJITAHOUrVwO8Y/ntCDuzvctZ4F/Dgqw15DpLAfpGvr3rff20ib6RlMMm0qWGkBIznEKPpkilJHoshmxqQjQ2YGtpwt7ajKOj3bqo0JAa2Lq6cB57nZyibgrKusgu7MEhkiSd2SSduejOPJKuOdhMnfnBGm6Iv9TXnrBzFh2+5XS7FxJwzSNoLyNqKyLL1cUssZ/C4Ovkdu3FHzyKQCIRRHIWEip8F21Fq9A9hXhCx/EEjrEgcJRzg2/gjG/pey773V5+VFjETockGm0B/nsMT3P8KMFwmnh0Wx3JlMlnLp9HSY6bkiwXJdluSnPcFPicE5qikVJyqPsQgUSAkB4iokcIJUOEk2HCepjOZIh6PUIoGCKSjNAje9jfmIcmNOzCjiY0NC29jsAfSlHUoONsl7QbAU6YQRJ2g4RDkHRICrMLOSdvCSWl11CWN5uynHJ8Dh+aDYQmEFqm0hrq66ZOIBGgO9FNIB6gJ9FDT6Knvy4RoCfRQk+ih65kGGlIMIBE5r47NSdCChLPZT7ArtnJd+WT78kn15VLyawiYkk40BShs92g1J3NxUUzKYqnsLe3Ywsk0FJ5YC/B0AQpASkhSQlJQppEUikS0iAiTKJ2iakZOO025rhdnCe95HqzyPPlUpCVT3F2EXlZ+djdLuwuG9TvIvK3nxCp7yaWSBGNZKFHPRi2mZhuP9rcMtrWzYT8Ipo7arHbAoS7jmJ2N5IVCVJUu5fifVFcCYkBNJAPgGdOIdmzTrAs70cscD1AOO8iwoWXEfIsJ5zwEe5OEO5JkNJN6J0ulBJME5nS0Q2dpKETNw10E1LCyVz7UlLaQlKBJJ0nkqS0JKaWxKbpFNtNyuzgcmjYbHYkGvGUIBQ3iOogpYbH6SDb48bjsJPTEsHdEkL6vRheF4bHjelxIp1ONCmwSRBSoElhDQZMw1qMFMJMIYwUDFyXEoFAFyZdeV4anCHazWPUO/aTm+1nSdlCVleuZEXFMnxZHjRNYOgmoe44oU5rCXZGCXXFCaS3wz2JYewby62lFGsZDhNot4Siy2vH6XPi8thxmnY0myAhdRKpBIlIkkRcImUmwdhJF9DAIgqdXkoLl1M610/Jsrn4F63EHU1ha2nB19yCDKVwLbwW15w5CEd65Bvrprb+Ze6pfohnAzXkofHPcXhfyeQFiJhUP4bJZvXq1fJsiJUUTaZY+Z2N3HRBOf927bLTck2An+79Kfe+fm/GfS6bC7/DT5YzC7/Dj8/pI9AdICc3B5I6BY06RQ1ucjqycUfywCwi7i4l6crNeL1TQSDToxHZv21pNPrrpGmtp6wXvRQ26wut7whOKgVSmgiHQLhtaF4nDqcdl9OBy+XE6XRgd2jYHBo2u0YimiIaSBAJJAn1xC2hcxIOp4bLZ/2zyfRccSQ9MtMAd3oqT0prv5Eyh3mpjB+b3WprMp7KeE1D6MTtYXQthF1LkJ+y44y4SOBHd2YPOd5jC5GVDb6iXBw2A2JdiHgXItYFyRBdpqAzaUdGNXLDEk9SIoVGZP5M5IyZ2PBi6A56ggahSApSJk4hcAsNkZ6Tl4CZbqymCew2zZoOFCCTCUjEQQiENMA0rb+xNK2Rqt2GcDjQnHZsLifC5cLmdlnHD5iU79cjANJAmCmM7mbiEZ0Y+ehGZp2IRGJzgZkUg56nEODPc5NVYC2uQAv63x7Hpem4587BNasYl6sbV3w/9tgxNJuGqFiDWHg1YvY6hN3Bjm2vsXTxucS7wwT2vEHo9SpizR2k7B5kaTmUlmH68zGlwO214/LacXkd/aXPjtNu4ow24eg5jK3rGB2hAtqCxZzocdMVdWNgjUCdyQA5geNkB4+TEzyOO95Jyu5F9+RC+XwSJTM5Zk9wTEbQXdlU+hZQQDHJiMmF185l0YUjSTWLYWIlvSn8GN7UvHy4A3ckyDsbdhLZFsG1YD72goJTumZ3vJuH9j/MNZ7reGv523FrbtzCg0tz48SFhoZpSFKhCInGJhKNLbQcayNp+AlruSTSAiCpgeHXyXLEmJlnp6DSSW5FARgGZiKBTCQwEwnMpI6MJzATScxkEpm01qWuYyR0zGQKQ09hJlOYqd7SxEwZmKZpvehJv/BtNrDZQUuvazakzQ6mjjTiCKFZLwEkaA6Ewwl2N0LTSCssEAISoTDEEpiaA9PpQeQVInLyMLzZROwmhm6S0q3S5bXj9Qtm+JuYZ+zCZzSRctl4Mj6fo6lybprv4gJXGLOnm6TTzSstCTa3RIk63Fy2YjbXXbyInKJctKwsbH4/OBzora1ED1QTqTpE9NAxYkdrSbR3YmoODM0BuYXYKmZjn1WOd/5svIvm4cp24jjwKPbXH8Bh07Gv+yfsl3wWzWNFt9y8eTNrV19M9PBeoi/cT6y1mah3MbHyt3PCrKC1o4NQT5R2kSRY1IWUtVRGEsxp7iKnpRF3opu82V7yKqJkcQB70Jrw02MaHeGZHO7yoNXGWNZtzVsl/E4c569gxqVXE9m6lfDfHyPvQx+i5OtfQ9itf3/TlLxytIPHdzbw7IFW9JT1pl1RkctHL6rk7ctn4LKn/76pFC133EHg6T+S+973cnD95Vx+xRUY3d0kjx0jcewYyWPHSRw7SvLwcfSmpj7NunPuXApu/jg573oXwjmCElxK2PYzePbzGLkLiL3z18Rd5XR1B9nfWEVN8xEa21uQCQ2HR3DR4jVcsngNOQVefHkubOnReffvfkfr//s27mVLKf/KddiP/RWO/BiiKZixAlZ8HJb9A/gG/696CwWVywqAAri0EngHycYmAn/9C4E//S/6Sw1oXi+eFSuQySRmLIYZjQ4qdV1HH3BNQXpwomloxaXEZ51DMH8BPVkz6Mpexon4iszPIm4VFakoDj2Ckx7snhD+HDeOgJeRhzsTR40YTgO3/e51Ch69j2trnu+rs+Xl4Zo3D+eC+bjmz8c1f4ElMPLzx3TNu3bdxa5nallb/+4xt0Mzk2RpEXJzIH9WNkVLZlJ0biU5RT6ENvLc76kgDQOp6wiHA3HSXHwfqST8eCXklMHHn4b2aqj+m7W07beOmXEeLHm3tRQtZMuWLVyyfDnRHTuIvLqNyPZt6HX1ANjy8/GtXol3xTl4F8+E/X8muWsjyYAkaZtNMlVEsj1IqrV1UDMMhxObnhy9U3Y7pPq1LM7KSlxLluBesgT3ksW4Fi/GUVw8/PndtbDxDqj6M2SXwVX/Bstu4NVnfs+6yDOw/3fgK4YrboeVH7KE6AB0U+dAxwG2tWxje8t2Xj/xOsXtSS6phvWHHBS0x5GahmdxBZGeALbmbgAiLmicn0P22otY+baPkHvOuWmBa/2d2r/3fboeeADf5Zcx6wc/xOYfHI65M5xgU3U7i2dkcW7Z4NGlGYvR9OWvEN6yhcLPfpbCL3yeF154YcT/IzMeJ1lXR/yNA3Q98giJ6mrsxcXk/+M/knvjjUPuP4jal+F3/wh6DK77Hzjn2r5dhmmwvXU7d+++m6rOKpbkL+HW1bdy4YwLkVLS+fP7OHHXXfiWV1K2uh4t1gT+Ujj3RljxASgeXmk70rtBSkls925afvsbkgcP4c3KQ/N6EV4PmteL5vFaZXpbeKzSUVyMfcZMHCXF/VNEA4iFkrQeDxLoCrOzZxtPtvyVbjq4eN5aPjH3PRS1hInXVJOoOUj8YA3J47XM+M53yL3hPcM/vxH6M9qIQQmGU8QwJWv+4zl+uPkuKor8FH3xiySPHCFx5AiJw1ZphsN9x9vy8nDNn48tPx/hcqK5XAiXe9B6XDP4SdXPqeAWfHIWK+yvox8/SqqxAWHoaNLAXpiPZ8F8PIsW4F2yCM+SRew8uJ8NV2w45T5NCrsegL/dAh/6A8x/y+B9nUeh5glLSDTutOoKF9HsmM3MolyIB/oWvSNApDZKtFkSaXORig0VRLa8PJyVlTgrK3BUVuKoqOSlmJv/3hehUbdxxfw8bruknHk+MEMhjFAYMxzCCIUwwxHMcAgzHMY+YwbuJUtwLVw08gtsJGpfgae/Dq37oGQZxolD2DQN1n0eLvkSuMZmdRRLxdjTtodtrdvY3ryNSE0166oNzj8CnX44MtdN8SUb2HDlzSwuOmfEa3U/9hit3/0PXPPnU37vz3DMGD0sWaq7m8ZPf4bYvn2U3vEt8m66CRjf/5GUksjLr9B5//1Et29Hy84m7/3vJ//DH8JeWJj5pEAT/PYj0PQaXPwluPJbg4SoKU2eOv4UP979Y5ojzVwy42JuecGL/MNTZM+TzFzVgpi9Fi79Csy70hrJjsJIfToeOM79++/n78f+jhCC377zt8zPmz+m/o/G1uatfPPlb9Iea+fSWZdyy6pbWJS/KOOxZiIBponm8Yx6XSUYxsHpEgw7a7v46D3P8/u/30HRZz5N0Re/MGi/lJJUe3taSBwmefQoiSNHMYJBZO9UTjKJjMeRicGK1pfX/Qf53QdZ3v4E7uXL8Cxbni6XZfxHOu2u/DVPwsyVkD3z1K5j6HDP+eAtgE88P7J9dqDJum/1X9Eb9+Lw54M7J+MiXdkku1NEj3agzTwH58KlOCsqsOVkNhkMxnXagwnmF09xHBzTgL2/ga330KaVUvKBn0Ju+ejnjUAgEeC11tfYe2Ivi/IX8ZaKt+C2j928M/zSyzR96UsIr4fy//kZnuXD68b0pibq/+kT6E1NzPz+98i++uq+fRP9zcX276fzF/cT2rgR4XCQ857rKfj4x3FWVAw9OJWAp74Gu34Nc9fDDb8aMv2TMBI8vveXmP/5P6w9YHDwvBQXvXUuxev/FWZfOqpPwEAy9ammq4Zf7PsFG+s24rK5uH7B9Tx9/Glm+GfwyNsfwaFlNpEeK62RVm746w0UeYq4fe3trC4d9p09biYiGJBSvmmX888/X06UzZs3T/jcgfzHk1Xypo99T1YtWizDr247pWuZpim7Am3y8vsvkN/67W3yJ5/aJHf/+YA0TXNM55+uPkkppTz2opR3ZEt53xVSGsapXWvPo9a1qp8c12mntT9nCWdTn2IHD8rDG66Q1eetkMGNGzMfU3NQHrr0MllzwRoZ2blzyP5T7U/82DHZfPs3ZfWy5bJqyTmy4UtfktE9ezL/5nc9JOV3iqT84VIpm3b310e7pfH0d2XdhgWyatFi+ftPrZIrHjhXXvDIBfIne34iI8nIuNo0sE972vbIzz73WbnsgWVy7aNr5V277pId0Q4ppZRPH39aLntgmbx3770T6XofhmnIm5++WV7wyAWyNlB7StfKRKa/EfCaHOHdqpTPp4CUko1VbXwk2YRwOPCsOO+UrieE4OEjj9Nlj3PNzA+xhw5mLJ/V5/k5ZZgGPPMNcHitIfzuB2D1xyd+rZd+ACXLYdHbTmszFaeGe+FCZv/2f2n43Odo/MIXKf7qV8n/+Mf6fm+RHTto/Nzn0bxeKh95GPfChae9Da45c5jx79+h8Aufp/vhh+l+7HFCTz2NvbSUrCuuIOstV+K94AJrXn7Vh6HkHPjfj8Avr4G3/heEWkm98HMaNjqIdzuZces/seQTt7Im2MDde+7m3tfv5XcHf8cHlnyAyuxKSn2llHhLKPIUDXZ0HICUkm0t2/jFvl+wo3UHua5cPr/i87x/yfvJHmAhds3sa3iu7jnu3Xcv68vXDzvtMxoPVz3M9tbtfHvdt6nMrpzQNU43SjCcAkdPRDjeEWH5iSN4VqxAc5+ap2ZPvIdHqx/l6tlXY+v0I7ROCsrOQPjfvY9C63644Zew+0F47tuw+J3gH0HZOhwH/gSdR+C9D45rOK+YGuyFhVQ++CDNX/8G7d/7Hsm6Okq/eTuh5zfTfNttOMrLqfjFfThmnuJ04ig4iospvvVWCj75SULPbSK06Tl6/vhHun/zG7SsLPyXX07WlVfgu/RSbJ96AX7/MXjyK+gRG/WvVqCHTcp+8iOyrrQCK5Rnl/P9y7/PR875CD947Qfcs+eeQfezCRuFnsI+QdFb+hw+ft36a+rq6yj2FHPb6tv4h4X/gNeR2S3vXy/8V3a27uRfX/5XHnvHYzhs45tSOth1kLt3380V5VfwngWjK5KnCiUYToGNVW34kjF89UfxvvOzp3y9h6oeIpaK8alzP0XNw0HyZ/pwOIex8pks4kHY9O9QtgaW3QCl58LPLoJnvwnv+fn4rmWa8OL3oWixZWn0fxw9mcA0Un3xqs4WNLebWT/8AScqKui87z5i+/aROHgQz3nnUX7vz7Dlnn7fl+GwZWWRe/115F5/HWYsRuTVVwlt2kT4+c0En3gC4XDgXbeWrA3vx7lgLc33/h0zoVPxq//Bu3rolPm5Refy4NseJJAI0BpppS3aRmuktW+9LdLGoe5DvNj4InHDsg0tsBfwzbXf5Lr51+G0jRxbKtedy7fWfYtbNt/Cz/f9nM+v/PyY+xpPxfn6S18nx5XDty/69ln1m1CC4RTYWNXKO2gBKfFeuOaUrhVIBPhNzW+4qvIq5ufO58W6l5mzYhhLjcnk5R9CpB3e/7j1hV+0EC6+BV76Pqz8IMy5bOzXqnkCTlTDe+4HbXTP72QsSqC9jUB7G8ETbTTt28eLzbXWzrSRhBxgLDFwvS/QmmbFlxJCswYoonc7vWgaNrsdm8NhLXaHtW23tjW7HXt63eF24/R4cXm9ON2ePrPP4ZBSEgsF6WltIdDWQk9bKz1tLfS0ttDT1kI00APA67+8B6fbg8Pjwen2DFh3p9e9OD0eXB4vLq8Pl8+Hy+vD6fXi9vpwetPbbne/KaqUpPQkqUQCPRFHTyT61q3SMmzwZGfjzcnFm52Ly+cb9NyKv/JlnJWVtNxxB/7165n1wx+MyerldGIaBlKa2OwONI/Hmk664gqkYRDbs4fQpucJbdpE67/9OwD2oiIqH/kV7kUjT+PkuHLIceVknO7R43HCPd20dzRyorOJtqNtrAhX0rRvn/V70jQ0m80qNQ1Ns9ZtDgcun49Liy/iXXPeyf3772dDxQaWFiwdtZ9SSu7a9n3ammr51rlfp2NfNfWBAKlkAofLjcPtHlQ63el1tweHy4Xd6ZpUQaIEwwQ5EUqwp6GHT8SaEE4nnvNOTb/wUNVDRPQInz7v04Q648QjOsWVQz1eJ5XuWnj1p3DuTVB2fn/9ZV+FN34PT94Kn34F7P1fUfFImPr9e+loqMNmd2B3OrE5nNgdDmwv3oVdLsWemo39wD5sDic2h4NooCctAFoJnmi31k+0EQ8FB7dH0zixf5e1LkRfRM1+d9n+KJsSCaa0osqaEilNS3CcRqs7p8eTFhS+/nWPF4mkp62VQFsLyVis/wQhyMovJLeklLmr1pBTXMLx2lrKZpSSjMXQ43GS8WjfejTQk16PkYxFMVIjR6sSQsPp8WAaBnoyMe6+ajY73uxsPDm5eLNz0gIjB/fttxHxZ9H+/LNp4WpFSupdF2kPRCHgxKHD7DPi6f2i/28yQBj3/r2SsRiJSJh4JEw8HCYeDvVvR8IkImHr+Q14bjklpeSWzCCnuMQqP/tpim77KskjR4jt3o3/sssGTXNJKdHjMeLhMLFwiHg4ZK0HA0QCPUQD3UQDPen1HqI9PeiJ+JBnU/vM38b1LAttNm60zeJPL3yN3UXzcPv8aYHux+X1kIhGrfsFeogGA4S6u3CkUtzALA688DAHxnU36+/wlps/y3lXTY7eTgmGCbKpug0pobKh2tIvuFwTvlYgEbB0C5VXsyBvAUd2WfmLiitPX2TNMbHxW6DZLVvxgTg88Pbvw6P/gHzlbtrLr+f43l3Uvr6L5kM1SHO46Khea6n5t4x7bXY72UXFZBeVUDJ3HjnFpeQUl5BTVEJ2cQnbd+1mw4ZT98uwLC2s8BamYWCkUhgp3Vr0FIau92+n+rf1eJxENEoyFu0rk7EoyWiURLoMdXaAlOQUl1C2eGn6ZTaj72VmP8nDN75lC5eO0bwzlUySiEb6l0iERDTat52MRkjEomiaDYfbjd3pwuFy4XC5sadLh9OFw219YQLEgkGiwR6igYD1kgwG+l5W3S3NRAM9pJLDBLAahvoXN47reLvDicvvx+3z4/b7ySosoqhyTvpl6gckgbZWetpaOb7nNSI93YPOd3l9fb8V45FDfUKmVwiYxjACVQg8/iy8Obn4cnOZMX+RJQxzcvHl5OLNtUZSu/fsYdXKlUjTtLz6TQPTMJG96+l6Q08O+LtEON52mFdrX8YjepiZdBLp6SYRCZOIxXB5PHjS9/GVFLPnxGY0n5t/WvtZcvKK0u3IweFyWyO++IAlESeZLvV4DD2RQI/HKJ4zd1zPfVx/o0m78jTnueo2FnpMxNHDeN92ahHDH656mIge4VPnfQqA9togml1QMOv0KZ5Nw+BE3XGaDlYT7GjHm52DP78AX24e/rx8fMFDuA78BbHhXyBn1qBzo8EAde1OaiNXUnvfc0RTLwBQMtIJqAkAACAASURBVHc+F173Xmafdz6l8xdaeQHS0xmph99LKtJF6sbHMQyTVFK39ulJPNk55BSV4M/LH3F65nQNla0vV0tXo9lsQ17WZyt2pxO704kvN29K76sn4tZoJZ3Xg7QJIzBoXUrJ1q1bWbduLdK0YmPRG8ivNx9Ir1AGHG4Pbp9/3M9fj8cJtFuCwipbCLS10tnUgM3hwOP3U1hWgdufhTsryyr9ftz+LDy+9HpWNt7sHLThPPMH4G1oonTegnG1EeAS4MjWO3joyJ956G0PcV7R0FkEKSVf2vwltjV18Ng7Hss4teX2nfl800owTIBoMsVLhzu41Wd9LfrWTFy/0DtauKryKhbmWeaA7fVBCmf5sdknnjQnFg7RcriG5oM1NB+qpuXIQVLpeWabw4Gh60POsYuL8XUexLfpNnx5eXizc2g/foyWo4dAStw+H7P9IebMKWH2p3+FN8MLy+5w4GreBl2vwTvvgnkTM+FTnDkcLjeOMQ6AnT4/WfmTqwtzuN0UVsymsGL2pN7ndPDV1V9la/NWbn/5dn73rt8NcTr805E/8XzD83x19VcnbN46FSjBMAFeOtxBImVyQfdxhMuF+xT0C49UP0JYD/Pp8z4NWFE9T9SFWLhmfMGxQp0ddNTs59mD+2g+VENnoxVTSGgaxbPnsnzD1cxcuJiZi5aQVVCEHo8R7u4m0tNFeM9fiWx7lMjcdxGxFxHu7qKzoZ6GQA95M2ex7ob3M2fl+ZTMnY+2/V545l+g+SXIzWBpJCW8+D3InmXFpFEo/g+R5cziOxd9h09u/CT37LmH2y64rW9fXbCOO3fcyYWlF/Lhcz58Bls5OkowTICNVW1ku+1k79mHfeVKtAlOTQQSAR6pemTQaKGnPUoyblA8e+z6hWighwe/+jkS0Qhun5+Zi5aw5JL1zFy4mNJ5C3Fk8K9werzke7zkF2TD3x+E8yrg5jtH9zVY8ynY+5gV/2fehqGxfmpfhvpX4W3fA/vE9S4KxZuVdTPXcePCG3m46mGurLiSVSWr0E2db7z0DRyag+9e8l20YbL3nS2c3a07CzFMyfM17by1wkvy0CG8ay6Y8LV6RwufOvdTfXXtdSGAcVkk7fzbH0nGYiy67iY+e/9vuP5rd3Dh9TdSvvTcjEJhEC//CMJtcM1/jc0BzWaHd/4Igs2w5c6h+1/8f+AvsbxUFYr/o9y6+lZm+mdy+yu3E9Wj3LfvPvZ37Odb675FqW9yQmWfTpRgGCe76rrpiiR5q2H5L/guvHBC1wkmgzxa9ShvqXjLoLnG9rogdqdGXulYEiBao4W9zz7J4lXLKMhzjmprP4juOtj6E1h+I5SPQ8CVXwDnf9SKmd/6Rn99/XY4/iJc9AXLkkmh+D+K1+Hl3y/+dxpCDXzlha9w3777ePe8d3PN7GvOdNPGhBIM4+S56jYcNsGC5oMItxv38uUTus4jVY8Q0kN9uoVe2mtDFJVnoY0xFejOv/0RI5lkbeAhLtz+GSu0dU/D2Brx3B1Wbs633DHe5sOVd4AnD574suXhDJZuwVsw8bhKCsU04oLSC/jgkg/yStMrzPDN4BtrvnGmmzRmlGAYB71B89bOLUDfvQvvqonpF4LJII9UPcKVFVcOGi2YhklHQ2jM00h9o4VzF5HvCNGdd64V3vnHK60XdqBx+JPrXrXiGF18i5U8Z7x48+Hq70LjDtjzEDTthiMbYd3nwDnB3AUKxTTjllW38L5F7+OH63+I33nmzVDHihIM4+DoiTDHOyK8vdxN4uBBvBM0U3206tGMo4Xu1igp3aRojI5trz3xJ4ykzoWLHCBsVJ3zz/DFPdb8/u6HLQHx5K1WjoOBmKalPM6aCRd/cUJ9AOC8m6DyEitT2cZvgTsXLvjExK+nUEwzPHYPt6+9nXMKRk6edLahBMM4eLaqDYCLotZUjXfN+PULhmnwSPUjbCjfwOL8xYP2tdVaISHG4vEcDQbY88wTLL74MgoCe2DmCgy7x/r6f+eP4Iu7LXPRXQ/Aj1fAk1+1FMYA+x6Hlr3wlm+f2te9EPCOH0AyDLUvwdrPgHuKw3goFIrTjhIM42BjVRvLZmXj2r8X4fHgWTZ6sKyTqQvWEUwGubLiyiH7TtSFcLpt5BaPrnh+7W9/tEYL774emnZB5UWDD8itgHfdDV/YbX3Z7/o13L0C/v7P8Ny/wazzYfl7x93+IRQvhvVfh5xyuPBTox+vUCjOepRgGCPtoTh7G3q4akkp0R078K5ciZiAfuFApxUuK9PQsr0uSFFlFkIb2Ww0Ggyw95knWXTRpRTIVjCSUHlx5oPzKuHd98AXdlmJ0HfeD+FWeOudY4p4OiYuuw1ued1SRisUijc9SjCMkeer25ESrprpJHH4MN4JmqlWdVbhtrmZkzNnUL2hm3Q0hsekeH7tiT+hJxOsveEmqNtqVZaP0p682XDtTywB8ZG/QPmphQkfwjDZsBQKxZsPJRjGyMaqNmblephVXw0wYce2qs4qFuYvxK4NdjrvbA5jGnJUwRANBtj79BMsvugyCmaVQ90rULzUshIaC/lzrITqCoVCMQxKMIyBaDLFy0c6uOqcEmI7diK8XjzLlo37OqY0qemq4Zz8TNNIvR7PIyue+0YL77kJDB0adgzVLygUCsUpoATDGHjxkBU07+pzSojs2I531SorOfk4qQ3WEk1FM+sXaoO4fQ6yCoYPYTFotFBWDi37QI8owaBQKE4rSjCMgd6geSuzJckjRyfsv1DVWQUMp3gOUTw7a8QcBLsGjhYA6tP6BSUYFArFaUQJhlGwgua1sWFxMfpuK82kb4L5nas6q3DZXMzLnTeoXk8adLVERtQvRIMB9jz9BIvWXWqNFsBSPOfPhayzPyiXQqF486AEwyjsquumO6pz1TklRLZvR/N6cZ8zMS/Gqs4qFuUtGqJ47mgII01JUcXw+oXe0cK6G9KjBdO0BIMaLSgUitOMEgyjsLGqFYdNcPnCIqI7duJZff6E9Au9iuclBUuG7GtPezyXzM48YrC8nJ9MjxYqrMoT1RDvGd5/QaFQKCaIEgyj8Fx1O+vmFeIJ9ZA8enTCaTzrg/VE9AhLC4Z6S7fXB/HmOPHlZk5ss+vJP6Mn4v2jBej3X1AjBoVCcZpRgmEEeqJJjndEuGheAdGdOwEmR/FcO3xE1T7dwtpL+kcLYAmG7FmQWzmh9igUCsVwnBHBIIT4shDigBDiDSHEY0IItxBijhBiuxDisBDif4UQE8uXeRqpabV8CxaXZhHZsQPN5zsl/YJTczI3d+6g+mQsRU9bdFj/hd7RwtqBowUpLcFQsW5sWdcUCoViHEy5YBBCzAK+CKyWUi4DbMBNwH8DP5JSLgC6gZunum0nU9Nizf0vmZFNdPsOS79gn1ia7KquKhbmLcShDdZPnKhPO7Zl0C/EQsG+0UJh+YCRQdcxK96RmkZSKBSTwJmaSrIDHiGEHfACLcAVwO/T+x8ErjtDbevjYFuIXK+D/FiQ5PHj+CYQZhssxXN1Z3XGaaS2uuFDbe9//ln0eGzwaAEG6BeU4lmhUJx+Jvb5ewpIKZuEEN8H6oEY8CywC+iRUqbShzUCszKdL4T4JPBJgJKSErZs2TKhdoTD4VHP3XEwRqkbXnvoQXKBaruN1DDnZAUPMef4o5iaA8PmJWX3Ytg8pOweGjRJ2AyTU1vL6y13kbL7CPtnIzUHDTtNHD7Y/trWIdc89OJmPPmFvHH0OBw93le/qOaPFDiy2XqgBapax9WnNxPTrT8w/fo03foD069PE+nPlAsGIUQecC0wB+gBfge8LcOhMtP5Usr7gPsAVq9eLdevXz+hdmzZsoWRzjVNScvzz3Dj6nJmv7aNoN/PxR/84PBTSc9tgZ59ULIUEicgHIJEGIwEB31eKC7kquN/YUkyPSi68DPwtjt5+LmtVCzKYv36wbmjjVSK1391D8s3XD20na/fAvMuY/2GDePq05uN6dYfmH59mm79genXp4n0Z8oFA/AW4LiU8gSAEOKPwEVArhDCnh41lAHNZ6BtfTR2x4gmDRaXZln5F1avHlm/EGq1UmV++uXB9akEVTu/h+PIH5j/jxtBj8GrP4XdDxJf/RWCHXGWXjp0cNR27DCpRIKyc04K1hdogu5aWKOS4igUisnhTOgY6oG1QgivsAIDXQlUAZuBf0gf81HgL2egbX1Ut1pz/4sccZK1taObqQabIXvG0Hq7i6rgcRbmLcIxc6WlMN7wL6BHad/0JyCzfqGh6g0AypacJBjqX7XKynXj65BCoVCMkSkXDFLK7VhK5t3A/nQb7gO+BnxFCHEEKAB+OdVtG8jB1hBCwKzadP6F0eIjhVoga6hgkFIOVTyXLIV5V9C+z/JtyBQKo7FqPwVlFXizcwbvqHsFnFlQsnzIOQqFQnE6OBNTSUgp7wDuOKn6GHCa04pNnJrWIBX5Xozdm9Gys3EvXjzyCcEWmHfFkOrGUCMhPTTUIumiL9D+2nZyc1K4vINNWI1UiqaD1Sy9fOj1qHsVKi4E2xn50ykUiv8DKM/nYahpDQ3WL9hGSF2ZCEEylDHK6YGuYXI8z91Au7mYIvGG5bA2gPbjR9HjMcqWnDQqiHRaMZKU/4JCoZhElGDIQFw3qO2IsCTfRbKuDs/yUbK1hdImo1kzh+yq6qzCrtmZnzt/UH0kmCSi51Bi7IKjmwbta6jaD0D5yYrnPv2C8l9QKBSThxIMGTjcFsaUsFSEAXCUV4x8QjBtQJVB+VzVWcWC3AU4bYMjfPSm8izK6YGt9wza16dfyMkdfLG6rWBzwcyV4+iNQqFQjA8lGDLQa5E0O9kDgLO8bOQTQi1WedKIIaPiOU17bRAhoOjiq+DYFmi1RgmmYdBYUzXUGgksxXPZBWDPHIVVoVAoTgdKMGTgYGsIt0MjL3ACAEd5+cgn9AmGwTqGxnAjwWRw2FSeeTN8ONZ+FBw+y7eBfv1C+dKT9AuJELTuU/oFhUIx6SjBkIGa1iALS7JINTUivF5seXkjnxBsAVc2uPyDqntDbZ+cg0FKyYn6oBU4z5MHqz4M+38HweY+/cKQEUPDdpCmEgwKhWLSUYIhAwdbQywqyUJvaMRZVoYYLbR1qDmjD0Ov4nlB3oLBh3fFiYV0inv9F9Z+xnrpb/85DVX7yZ9Zhi/3JGFUtxU0O5SfNRa9CoVimqIEw0mcCCXoCCdZPCMbvbFx9GkksEYM41A8n0grnvuS8+TNhiXvxtz5a5pqDgwNgwGWYJhxHjh94+2SQqFQjAslGE7iYG9ynhI/yUZrxDAqoZbMiueuYRTPdUE0m6CwbMDU00VfoD1gkIzFKD/nJP2CHoemXWoaSaFQTAlKMJxETdoiaaFTR8Zio48YTDMdQG+w4rk50kwgERhW8Vwwy4/NMeDxl62m0XmutbpoyeATmnaBkVT+CwqFYkpQguEkalpDFPpd+LragDGYqkZOgDQge/CIYaQczyfqQxRlCpzHXPKcUfytJ0VordsKCKhYO/aOKBQKxQRRguEkDqZDYeiNjcBYTFXTzm0nKZ+rOquwi6GKZz1hkIimyCn0DKo3TYPGhhOU50l49SeDw2TUvWIF3vOMYh2lUCgUp4ERBYMQYoYQ4ktCiD8IIV4VQjwvhPixEOIaMaqpzpsPw5QcarMEQ7KhAQDHrIyJ5PoJpn0YsocKhnm583DZBjujxUJJADxZJ+V+rj1OMhalbPVl1tRR/bZ0o3Ro2AEVKsy2QqGYGoYVDEKIXwCPpI+5G/gY8BXgZax8zK8IIS6ZikZOFbWdERIpk0WllqmqvaQEzTWKl3Gfc1u/YJBSUtVZlXEaKRrsFQyDLZX6/Beu/gR48vvDZLTsAz2iFM8KhWLKGCl280+klK9nqN8L/FYI4QZGCSL05qKmxbJIWjIjG72hAcdo+gWwBIPQwFfcV9USaaEn0ZNRMPSOGLzZgwVDY/Ub5JbOIKu0DC64GV78PnQehfp0LmglGBQKxRQx7Ighk1AQQlQKIZak98ellIcms3FTzcHWIJqA+cW9pqpj9GHwlwzKjzCS4jkW0oHBIwbTNGisfqPfTHXNJ8HmsMJk1G2F/HkZQ3orFArFZDDmbC9CiK8BqwFTCBGTUv7jpLXqDFHdGmJOoQ+nNEi1teEYkw/DUK/nqs4qbMLGwryFQw6PZtAxnKirJRGJUNYrGPzFcO77YO+jVjTVc9498U4pFArFOBlJx/AZIcTA/auklO+VUr4PWDX5TZt6LIukbPSmJpBydFNVsHwYMpiqzsudh9vuHnJ4LJjE6bZhd/Qn/mnMlN953echFYdEQE0jKRSKKWUkq6QY8LQQ4m3p7U1pq6TNwKYRzntTEk6kqO+Kjs9UFaxcDAOmeUbyeAZLx5BJ8ZxbMoPswqL+yuLFsOBqa10JBoVCMYUMO5UkpXxACPFb4GtCiE8C3wQeA5xSys6pauBUcajNUjwvKs0i+fpOgNGnkvQYxHsGTSW1RdvoincNKxiiIX2QYJCmSVP1G8xfk8Ec9a13WnmkcyvH2RuFQqGYOKPpGMqBB4EE8F0gDtwx2Y06E/TFSCrNRn+iEeFyYS8qGvmkvsxt/VNJBzqHyfGcJhZKklvs7ds+UV9LPBIeGh8JoGAeFHxmHL1QKBSKU2dYwSCE+CXgAzxAlZTyY0KI1cCvhRAvSyn/a6oaORXUtATxOW2U5XlobrRMVUcPtz3Uh6GqswpNaBkVz2AJhhnzcvq2G6vT+oVMEVUVCoXiDDCSjmG1lPImKeW1wFsBpJSvSSnfAUwrM1WwYiQtLM1C0wTJhjGaqoZarfIkwTA3Zy4eu2fI4aYpiYUHTyU1HNhPTnEJ2YXFQ45XKBSKM8FIguG5tLL5ZeB/B+6QUv5hcps1tUgpqUlbJEkp085tY1Q8Q184jJE8ngHiYR1kvw+DNE0aq9+gbEmGaSSFQqE4Q4ykfL5VCJEPGFLKwBS2acppCyYIxHQWl2Zh9PRgRiJjNFVtsfI1u6yEO6Mpnk/2eu5orCceDg3N76xQKBRnkJH8GG4CuocTCkKI2UKIaWFHWZ3OwbC4NAu9N3jemLyem63RQloXMVyO515Odm5rODBMfmeFQqE4g4xklTQL2COE2AHsAk4AbmA+sB4IAl+b7AZOBQMtkpKbraimY3Zuy6B4XpS/KOPh/ZFVrRFDY/V+souKySkuOZXmKxQKxWllpFhJP8AKgfEnLLPVdwAXAZ3AzVLK66SUB6eklZNMTUuQGTlucrwO9Ia0c9to4bZhSDiMkRTPALGgFSfJm+209AtVb2Q2U1UoFIozyIh+DFLKlBDiVSnlU1PVoDNBTWuIRaVWRjW9qRFbYSGa1zvySVKmw2H0C4bqrmoumjn87Fo0lETTBC6Pnc7GOmKhoJpGUigUZx1jyeC2SwjxmBDi6klvzRlAN0yOngj3CQbLVHUM00jRTisPc5bl3NYebacj1jGs4hmsqSR3lgOhCRrS/gtK8axQKM42xiIYFgAPAZ8QQhwWQnxHCDFvkts1ZRw7EUE3JEtKLcuiMZuq9jm3lRLVo2yu3wwM7/EMVgC9Pv3Cgf1kFRSRXaT0CwqF4uxi1LDbUkoTeAp4SgixHngU+HJaKf0NKeWOyW3i5FKTtkhaVJqF1HX0lhZyhlE8R/UoxwLHONJzhKPHNnG0pIijb9xF82vfBMBj97AoL7PiGaw4Sd5sJ1JKGqrfYM55q0b3rlYoFIopZlTBIITIBT4IfAToBr6MpZA+H8vxbc5kNnCyqWkNYdcE84r86M2NYJqDTFUfr3mcFxtf5FjgGE3hpr56h7Axx2bjvIJl3FB8LvNy5rG0cClex/C6iVgoSW6Jh66mBmLBAGVqGkmhUJyFjCVRz07gN8CNUsq6AfXb0nmh39QcbA0xr8iP064RTvsw9JqqGqbBnTvupMhbxMrilVw//3rm585nXu48ynY9iv3F78HNP7KyrY2ClLJvKqnXf6FceTwrFIqzkLEIhkXp6aQhSCn/8zS3Z8qpaQlywZx8gH5T1bSOIZAMYEiDjy39GB9Y8oHBJ4ZbwVc0JqEAoCcMUrqJN8tJQ/Ub+AsKySlR6ToVCsXZx1iUz39PTycBIITIE0I8OYltmjICMZ3mQLzfVLWxAeFwYC+2Atp1xqy0E/me/KEnh1rHlYe5N9ezy2ejsWo/5UuWKf2CQqE4KxmLYCiVUvb0bkgpu4GZIxw/KkKIXCHE74UQNUKIaiHEOiFEvhBiY9ryaaMQIu9U7jEWej2eey2Skg2NOMrKEJr1WLriXQAUuAuGnhxsGZLScyR6vZ67m/YRDfSwYO3Fp9J0hUKhmDTGIhgMIUSfmY4QouI03Pdu4Gkp5WLgPKAa+DqwSUq5ACt16NdPw31G5OAAiyToNVXtt0jqHTFkFAwneT2PRjSYRErJkZ3PkFs6g3nnrzmFlisUCsXkMRbB8C3gFSHEr4UQvwZeBP5lojcUQmQDlwG/BJBSJtMjkmuxssWRLq+b6D3GSnVriGy3nRk5bgCSTU2DnNs642nB4DlJMKQSloPbOEcM0mims+Eoq95+LZpmO/UOKBQKxSQwFj+GJ4UQa4B1gAC+JqVsP4V7zsUKyPdrIcR5WAH6bgFKpJQt6Xu2CCEyZq5J55/+JEBJSQlbtmyZUCPC4TA7DsYo9cALL7yAiEQoDgSoTySpSV9zT/ceNDR2b909SB/gjrWxFqhpDtA6xvufOCBJxV/D5nLTpTkn3O7R+jQZ1z1TTLf+wPTr03TrD0y/Pk2kP2OxSgIr13M96eiqQoj5Usqt42veoHuuAr4gpdwuhLibcUwbSSnvA+4DWL16tVy/fv2EGrF582ZaY0muXzmL9euXETtwgFpg8frLyU5f8/lXnqdQL2TDhg2DT67fBtth8eoNLF4wtvs/e/QV6vWjXHj9jVxy1eREF9myZQsTfR5nI9OtPzD9+jTd+gPTr08T6c+oU0lCiI8DW4Hngf9Ol6diptoINEopt6e3f48lKNqEEDPS95wBnMqoZFQ6YpJwIsXiGb36BctU1TkgHEZnvHPoNBIMydw2Fpqqt4CwseKad064zQqFQjEVjEXH8GWs8Nu1UspLsTyeWyZ6QyllK9AghOiNHXElUAX8Ffhouu6jwF8meo+x0Bi2XDMWDzBVBXAM0DF0xbqGMVXtjZM0NsEQC4fobnoNf/5y/HkZrqdQKBRnEWOZSopLKWNCCIQQTinlASHE4lO87xeAR4UQTuAY8DEsIfVbIcTNWNNW7z3Fe4xIQ8gSDAtL+qOq2vLysPn9fcd0xjuZmzt36MmhFrC5wDM2i9p9G59CmjrFcy8/9YYrFArFJDMWwdCSdnD7G/CMEKILaDuVm0op92KNQk7mylO57nhoDJmU5XnIclueyydHVZVS0hnrHMGHoT+l50ikdJ09T/8Nu2s2eTNPh6WvQqFQTC5jsUp6d3r1m0KIK4Ec4E3v+dwYMjmnIrtvO9nYiGdZf9KciB4haSYz6xhCLX15GEaj5pUXiPR04/Bv6Mv1rFAoFGczI+oYhBA2IcTrvdtSyk1Syj9KKROT37TJI64btEZln35BplLozc2DRgy9Pgz57gw6gWDzmBTPUkp2Pfln8mdVoNkr8aZzMSgUCsXZzIiCQUppAFVCiDEkQH7zcKQ9jCnpt0hqbYNUqi+qKowQDkPK9IhhdMFQt38vHfW1LL7k7Qgh+pL0KBQKxdnMWHQMhUC1EOJVINJbKaV8z6S1apLpjZHUb5GUjqo6IA/DsAH04j2Qio9JMOx64k/4cvOYseACePYA3mw1laRQKM5+xiIY7pz0VkwxumFS7BXMLvBZ2xlMVYeNkxRMm6qOMpXUUV9L7eu7ufh9HyYZkwBqxKBQKN4UjEX5vGkqGjKV3LSmgtLoMew2ayYt2dAIdjuO0v78y71TSXnuk0xSQ2nntlGUz689+WfsThfnXfU2Dm63gtMqwaBQKN4MjCW1ZwiQA463AQkpZfbwZ7250BsacMycibD3P47OeCe5rlzs2kmPaAwjhkhPNzUvb2HZhqvxZGUTC51A0wQu71gjkCgUCsWZYywjhqzedSGEBrwHK1T2tCHZ2DgoqiowvA9DqNUq/cMn6dn7zBMYhsH577gWsCKrerIcKjGPQqF4UzCWkBh9SClNKeXvgasmqT1nhJOd28CaSsrsw9AMnnxwuDNfKxFn78anmHf+heTNsIy5oqEknmw1jaRQKN4cjGUq6d0DNjUsj+Vp8+lrhMMY3d2DTFXBmkpakr9k6AmjZG478MLzxENBVr+zP51ELJhUPgwKheJNw1gmvQfGLEoBtVhJdaYFmUxVwQqgN+yIYRhTVWma7P77nymdt4BZi5f21cdCOnmlvtPXaIVCoZhExqJj+PBUNORMkWxIm6oOGDEkjAQhPZTZ6znUCqXnZrzW0V076G5p5h23/HOfPkFK+f/bO/PwKKuz/39OJgsJCYQ9IWEPCATiAMqmIhQXBEtFESNogFqpgLVEiRXxp2itqMHyllfAttIX0FAQZelLrfZVIsgiYTGCBGsQgkIC2QhJSCaZ5fz+mIWE2Z7smeR8ritXZs6c5yzzJHPPue9zf48jxqBQKBS+gJbzGNbZRPTszzsIIf7asMNqPFydw1BY7ibr2WyE0ly3rqQju7YT1rkLA0bdcq39CjMmo0XFGBQKhc+gJfg83HYmMwBSystYz2RoERjP/4Rf+/bo2l3bfeuQw7jelVR6CZAuXUkXT3/Phe9OMvyeqfjprp3nXF5SCaBiDAqFwmfQYhj8hBDt7U+EEB2AFuMXcblV1Z2AniOHwXnFkHn4IH46f4b+7O5q5eUlRkAltykUCt9BS/D5v4CDQogtWBPd4oE3G3RUjYjxp/MEDRhQrcwhh3H9isHDyW15WWfoFBVNUEhItfKyYuuKQcUYFAqFr+B1xSCl/B+sxuAKUAI8JKVc38DjahSkxYLx/HmXW1XBxYrBVrydSQAAIABJREFUk2E4d5YuvZ1Pe3O4klSMQaFQ+Aha8hhuBk5JKY/bnocJIW6SUh5p8NE1MKbcXKTR6LRVtaC8gBD/EIL9g6tfUJwNfgEQUn0lUVZ8hdLLhXTp1cepD7thCA5VhkGhUPgGWmIMfwHKqjy/Cvy5YYbTuBhdbFUFT1nPtnMY/Kq/bXlZZwFcGoayEiOBwf7oAmqUZK5QKBRNhpYYg5+U0mJ/IqW0CCFahMO80sVWVbC6kmpyclveuTOAa8NQXlyp3Eg+jNFo5Pz58xgMhnppr3379pw6dape2moOtLT5QMuaU5s2bWql0abFMJwVQszHunKQwHys2c8+j/H8T+DnR0Bk9Q/7gvICeob1dL6g5CJ0G+xUnHvuLKEdOxHSrr3Tayq5zbc5f/48YWFh9O7du15EEEtKSggLC/Ne0UdoafOBljMnKSUFBQW0bVtz1QUt/o1fAxOBS7af24HHa9xTM6Typ/MEREYiAqp/cHt2JTlvVc07d9blagGsriS1VdV3MRgMdOrUSSnjKnwOIQSdOnVCVyWvSitaJDEuAdNrM7DmjitVVbPFTFFFkbMryVAMlaVOriST0UjhhZ/oO/xml32UF1cS1T/c5WsK30AZBYWvUtu/XS27koKAOUAs4NCallLOq1WPzYjK8+cJmzC+WtnlistYpMVDDkP1FUPB+R+xmM10dbFV1WK2YLhqVK4khULhU2hxJW0EegP3AoeAfkD9ROKakspKzPn5zqqqBjc6SQ7DUP2Anrxz7ncklZeqrGdF3RFC8Mwzzzier1ixgmXLlmm6Nj09nTFjxhAbG0tcXBxbtmzxes2yZcuIiopCr9czcOBA5s+fj8Vi8XqdneXLlxMTE8MNN9zAp59+6rLOnDlz6NOnD3q9Hr1eT3p6uub2FQ2PFsMwQEq5BCiVUq4DJgFDGnZYDY8uPx/AObmtvGZyGHnnzuIfGER4hPNuJUcOgzIMijoQFBTEtm3byLf9zdaEkJAQNm7cyMmTJ/nkk09YtGgRRUVFXq9LTEwkPT2djIwMTpw4wZ49ezT1l5GRwebNmx39LViwALPZ7LJucnIy6enppKeno9frazQvRcOiZVeS0fa7SAgxCGsAulfDDalxsBuGADc6Sc6upGzr7+uynvOyztClZ2/8/JwDPOXF1rdObVdtGbz8vyfJyC6uUxtms7laMHBw93a89PNYD1eAv78/8+bNY+XKlfzhD3+oUX8Dqsi9dO/ena5du5KXl0d4uLa4V2VlJQaDgQ4dOmiqv3PnTuLj4wkKCqJPnz7ExMSQlpbGmDFjajRuRdOiZcWwziac9xLwKfA98FaDjqoR0OXZDMP1R3qWu1FWLc6BNu0h8JoWkpTSy44kpZOkqB8WLlxISkoKV65cqVaekpLicMdU/Zk+3Xm/SFpaGpWVlfTr189rfytXrkSv1xMZGcmAAQMc3+iTk5Or9XPLLbeg1+t56qmnALhw4QI9qvxPRUdHc+HCBZd9LF26lLi4OBITE6moqND8XigaHi27kuxZzqmAi839vol/fj5+bduiu+6bU4GhgAC/AMICrtvH7GKraklBPoarpW4Ng3IltSy8fbPXQm33yLdr146EhARWrVpFcPA1qZZZs2Yxa9Ysr9fn5OTw6KOPsmHDBvz8vH8fTExMZPHixRiNRqZPn87mzZuJj48nKSmJpKQkt/ORUjq15WpnzPLly4mIiKCyspJ58+bxxhtv8OKLL3odl6JxaLU6Dbr8fAJ69HD6oy00FNKxTUfnP+aSHPeBZxc7ksBqGPx0gqAQLR47hcIzixYtYt26dVy9etVRpmXFUFxczJQpU3j11VcZPXp0jfoMCAhg0qRJ7N27F/C+YoiOjuYnm9QMWBMEu3d3zv2JjIxECEFQUBBz584lLS2tRuNSNCyt9hNLl59P4BDnb4AF5e7kMHKg38BqRXlZNimMnq5DLvbkNrUPXlEfdOzYkRkzZrBu3Tp++ctfAt5XDJWVlUybNo2EhAQefPDBaq8tWbKEkSNHMm3aNLfXSyk5cOCAw5XkbcUwdepUZs6cydNPP012djaZmZmMHDnSqd2cnBwiIyORUrJjxw6GDPH5/SwtCi1HezoZD1dlvoSU0rpiuG6rKlhdSU7xBYvZenrb9YHnc2cJ7xZJYHD1MxjsKDkMRX3zzDPP1Gh30gcffMDevXtZv36909bQEydOEBER4fI6e4xhyJAhmEwmFixYoKm/2NhYZsyYweDBg5k0aRKrV692BNsnT55MdrZ1E8esWbMYOnQoQ4cOJT8/nxdeeEHznBQNj5YP+DRguIYyn8GUl4cwGp1UVcHqSuof3r96YWkuSLNT1nPej+4Dz6AE9BT1Q2lpqeNxt27dKCsr81C7Oo888giPPPKIy9eMRqPL3ULLli3TnCfhiqVLl7J06VKn8o8//tjxePfu3bVuX9HwuDUMQoiuQCQQLIQYCtj9Ie0A11+RfQTjedeqqlJKqysp2PsBPZWGci5fzGHQbRPc9lNWUkmHyJoLWCkUjYG75DOFwtOKYQrwSyAaWM01w1AC/L8GHleDYjcM1+cwlBhLMFqMHrKerxmG/B+zQEq69HIdeJZSUq4E9BQKhQ/i1jDYjvT8HyHEDCnlB404pgbHePESUggCoqKqlbvPYbAlt1XJerbvSOrqxpVkNJgxGy0qxqBQKHwOLdtVuwoh2gEIId4RQqQJISbWtWMhhE4I8bUQYpfteR8hxCEhRKYQYosQosG+ane+R0+HJ/rhp6s+fY9nPQsdtO3iKMrNOkNQ27aEde6CK8rUWc8KhcJH0WIY5kkpi4UQd2F1K80H3qyHvn8LVD0m6Q1gpZSyP3AZeKwe+nBN3ndEXt4Lu1+pVuxeQO8ihHaDKrIX9oxnd1tRy0uUgJ5CofBNtBgGeyrjPcD/SCmParzOLUKIaKwxjHdtzwXwM+BDW5UNwH116cMjwx/lQvdJsP9PcHK7o9guoOfSlVRlR5LFYib/x3OedyTZVwzKMCgUCh9Dy3bVb4QQHwMDgKVCiFCuGYva8l/As4A9M6YTUCSlNNmenweiXF0ohJgHzAPr1r0vvviiVgO4GvEwoaVnCf3oCY6dLeZqaC+OFh1FIPjmq2/QiWurg5svnqYsJIqTtr4MRYUYKwwUlle67b/wtPUtSv/2CAFnGifBrbS0tNbvR3OkOcynffv2lJSU1Ft7ZrO5xu21a9eOJ598ktdeew2AVatWUVpayvPPP6+5jeLiYm6++Wbuvfde3nrLs9TZE088wf79+2nXrh0Gg4Hp06ezZMkSl3Wvn4+UkmeffZZ///vfhISEsHbtWpfKqZMnT+bixYsOeY8dO3bQpYtrt2xjU5t71JyRUtb8/0hK6fEH0AEjgY62552BYd6u89DevcAa2+PxwC6gC3C6Sp0ewAlvbY0YMULWltTUVCmvZEuZ3F/KP+mlLLssXznwihy3eZxz5dd6SPnPxY6n3x34Uq6YMUVe/CHTbftpu87It3/9uTQZzbUeY01JTU1ttL4ag+Ywn4yMjHptr7i4uMbXBAUFyd69e8u8vDwppZTJycnypZdeqlEbTz31lHz44YflwoULvdadPXu23Lp1q5RSyvLyctmnTx955swZl3Wvn88///lPOWnSJGmxWOTBgwflyJEjXV53++23y8OHD9doDo1Fbe5Rc+bYsWNOZcAR6eGzVYuInlkI0Re4E/gDEEzdXEm3AFOFEJOxngjXDusKIlwI4S+tq4ZoILsOfWijXSQ8uAE23Avb5lEQGekceK68ChVXqm1VzTt3FuHnR6do95qC5SVGgkL80fm3Wjmqlse/noOLJ+rURLDZBLoq/3YRQ+Ge1z1eUxfZbYCjR49y6dIlJk2axJEjR2p0rcFgPZNL64HyO3fuJCEhASEEo0ePpqioyCF/ofAdtEhivA1MAOzpk1eBd2rboZRyiZQyWkrZG4gHdkspZ2FVb7Urf80Gdta2jxrRawxMeh0yP6Xg0nHXgWe4zjCcoWP3aPwD3ccPyoorVeBZUW/UVnbbYrHwzDPPkJycXKP+kpKS0Ov1REdHEx8fT9euXQGr6qorEb3XX7cat5rIbs+dOxe9Xs/vf/97l6qsiqZDS4xhrJRyuBDiawApZWEDbSX9HbBZCPEq8DWwrgH6cM3Nv4ILxygsSCU27DqZDEcOwzXDkHvuLD0GeRb9UjpJLRAv3+y1UN7Isttr1qxh8uTJ1T6stZCcnMz06dMpLS1l4sSJHDhwgLFjx7Jy5cpq9Woru52SkkJUVBQlJSU88MADvPfeeyQkJNRojIqGQ9MJbkIIP2wBZyFEJ0D7AbAekFJ+AXxhe3wGayyj8REC7v0jBSkj6XTuEORnQmebXpIj69ma3FZeWkJpQb7HHUlgNQwdlRyGoh5ZtGgRw4cPZ+7cuY6ylJQUl6uBmJgYPvzwQw4ePMiXX37JmjVrKC0tpbKyktDQUMc3fG+EhoYyfvx49u3bx9ixY0lMTCQ1NdXxusViwc/Pj/j4eJ577jnNsttRtuTSsLAwZs6cSVpamjIMzQhPWkl2f/9q4COgixDiZWAG8HIjja/RMAjBVQGdpIDNs+DxzyEozGnFkJdlO4PBi2EoK6kkaoC24xAVCi3URnY7JSXF8Xj9+vUcOXLEYRQSEhJ48sknXcpi2zGZTBw6dIjf/OY3AF5XDFOnTuXtt98mPj6eQ4cO0b59e6f4gslkoqioiM6dO2M0Gtm1axd33HGHxndB0Rh4ijGkAUgpNwIvACuwJp49KKXc3Ahja1QcyW3D50JBJuxYAFJaYwyBoVYjgTW+AJ4Ng9lsoeKqiWCV9ayoZ2oqu+2J48ePuw0K22MMcXFxDB06lPvvv19Tm5MnT6Zv377ExMTw+OOPs2bNGsdr9m2rFRUV3H333cTFxaHX64mKiuLxxx+v+4QU9YYnV5LDMSilPAmcbPjhNB325LaOPcbAna/Av1+AfSuhJNtpR1Lb8A60DXe/GjCUWrOeQ1SMQVEP1EV2uypz5sxhzpw5gDWvoX///i5jD+vXr69V+2CNJ6xevdrla/ZzINq2bcvRo0dr3Yei4fFkGLoIIZ5296KU8o8NMJ4mw7FiCO4EY56E7K/h81cgpCN0u3bSW+45z2cwgDrrWdH8adeuHVu3bm3qYSiaKZ4Mgw4IpcrKoSVTTUBPCJj635D7HeSedASezSYjBT/9SO+4YR7bKiu2GQblSlIoFD6IJ8OQI6V8xcPrLQonnaTAthD/Pvx1InQdBEDhhfNYzCa69HZ9BoMdu4Ce0klSKBS+iKYYQ2ug0FBIaEAoQbqga4Ud+8LTGeDfBvB+BoOda64kFWNQKBS+hyfDUOczF3yJgvICZzkMgIBryUS5WWfQBQTQIdKlvp+DsuJK/PwFgcFa0kQUCoWieeF2u6qUsrAxB9LUFBgKnOW2ryPv3Fk69+iNn07nsV55SSUhYYFuz2pQKBSK5oxSeLNRaCh01kmqgpTScTiPN9RZz4r6RAjBM88843i+YsUKli1bpvn6H3/8kbvuuotBgwYxePBgsrKyPNZftmwZUVFR6PV6Bg4cyPz587FYtIsdLF++nJiYGG644QY+/fRTl3XmzJlDnz59HJpL9q2siuaBMgw23LqSbJReLqC8pFiTYbAK6Kn4gqJ+CAoKYtu2bbVObEtISCApKYlTp06RlpbmEMTzRGJiIunp6WRkZHDixAn27Nmjqa+MjAw2b97MyZMn+eSTT1iwYAFms9ll3eTkZNLT00lPT3d5ZoOi6VBOcMBkMVFUUeTRlaQ18AxWV1Kn7konqaXxRtobfFf4XZ3aMJvN6Kq4Igd2HMjvRv7O4zV1kd3OyMjAZDJx5513Albto5pQWVmJwWCgQwdt8i47d+4kPj6eoKAg+vTpQ0xMDGlpaYwZM6ZG/SqaFrViAIoqipBIjysGh0ZSb8+GQUqpXEmKeqe2stvff/894eHh3H///QwbNoykpCS33+CrsnLlSvR6PZGRkQwYMMDxjT45Odml7PZTTz0F1Ex2e+nSpcTFxZGYmEhFRUWt3hdFw6BWDHg467kKuefO0q5LN4JCPK8EjAYzZpNFGYYWiLdv9lq4XnROK7WV3TaZTHz55Zd8/fXX9OzZk4ceeoj169fz2GOPeewvMTGRxYsXYzQamT59Ops3byY+Pp6kpCSSkpLczker7Pby5cuJiIigsrKSefPm8cYbb/Diiy96HJOi8VArBq5lPXsKPmsNPNuznkPaqRiDon5ZtGgR69at4+rVq44ybyuG6Ohohg0bRt++ffH39+e+++7j2LFjmvsMCAhg0qRJ7N27F/C+YtAqux0ZGYkQgqCgIObOnUtaWlqt3hNFw6BWDFQR0HPjSjJWGCjKyWbg2Nu8tqV0khQNRW1kt2+++WYuX75MXl4eXbp0Yffu3dx0000ALFmyhJEjRzJt2jS310spOXDggMOV5G3FMHXqVGbOnMnTTz9NdnY2mZmZLmW97cd9SinZsWMHQ4Z4PvhK0bioFQPXCei5IP+nc0hp0bxVFZRhUDQMNZXd1ul0rFixgokTJzJ06FCklA6J6xMnThAREeHyOnuMYciQIZhMJhYsWKCpv9jYWGbMmMHgwYOZNGkSq1evdgTbJ0+eTHa29XyTWbNmMXToUIYOHUp+fj4vvPCC5jkpGh61YsDqSgr0CyQ0wPWOjWuH83jWSALrAT0AIUpAT1FP1FV2+8477+T48eNO5Uaj0eVuoWXLltUoT+J6li5dytKlS53KP/74Y8fj3bt317p9RcOjVgzYchiCO7rNVM49d5bA4BDad/G+/9vuSmoTqmIMiuaNu+QzhUIZBrxnPVsDz70Rft7frvLiSoJC/NH5q7dWoVD4JurTC89Zz9Ji0bwjCayuJBVfUCgUvowyDHgW0LuSewmjoVyzYSgvMar4gkKh8GlavWGQUnp0JV2TwvAeeAZrjEHpJCkUCl+m1RuG4spiTBaTW1dS7rkzCOFHp569NLWnXEkKhcLXafWGwZH17MaVlHfuLB0iuxMQGOTy9aqYzRYqrpqUK0lRr9RVdvvZZ58lNjaWQYMG8dRTT7mUrahKVUnsgQMH8vLLL2vuS0rJU089RUxMDHFxcW6zrMePH88NN9zgyKDOzc3V3Iei4Wn1hqGw3HNyW965s17PeLZjUMltigagLrLbBw4cYP/+/Rw/fpxvv/2Ww4cPa5LQriqJvWHDBs6ePaupv3/9619kZmaSmZnJX/7yF+bPn++2bkpKiqMPLVLgisaj1Se42VcMrlxJRRdzKM7LZcTkX2hqq0yd9dyiufjaa1ScqpvstslsprCK7HbQoIFEPP+8x2vqIrsthMBgMFBZWYmUEqPRSLdu3TRfbzAYAGjbVpuM/M6dO0lISEAIwejRoykqKnLIXyh8h1a/YnAoq7oIPv9w1Crs1Xe4s9aLK8rtAnpqxaCoZ2oruz1mzBgmTJhAZGQkkZGR3H333QwaNMhrf0lJSej1eqKjo4mPj3d8o09MTHQpovf6668DNZPdnjt3Lnq9nt///vde3VuKxqXVrxgKDYX4CT/Cg8KdXjtz7BCdonsSHqHt244S0GvZePtmr4XGlt0+ffo0p06d4vz584BVHmPv3r2MGzfOY3/JyclMnz6d0tJSJk6cyIEDBxg7diwrV670OB+tstspKSlERUVRUlLCAw88wHvvvUdCQoLHMSkaD7ViMBQQHhSOzk9Xrbyi7CrnT52k7whtqwWAMnuMQQWfFQ1AbWS3t2/fzujRowkNDSU0NJR77rmHr776SnOfoaGhjB8/nn379gHeVwxaZbejoqIACAsLY+bMmUp2u5nR6g1DYXmhy8Dz2fSjWMxm+ml0I4HVleTnLwhso/NeWaGoIVVlt+3MmjXLEcCt+vPhhx8C0LNnT/bs2YPJZMJoNLJnzx6HKykhIcHrB7LJZOLQoUP069cPsKquVu1n//79pKen89xzzwFW2e2NGzcipeSrr76iffv2TvEFk8nkCKQbjUZ27dqlZLebGa3eMBQYXMthnDmaRnBYOyIH3KC5rfKSSkLCAt2K8SkUdaWmstvTp0+nX79+DB06lBtvvJEbb7yRn//85wAcP37cbVDYHmOIi4tj6NCh3H///Zr6mzx5Mn379iUmJobHH3+cNWvWOF6zn+lQUVHB3XffTVxcHHq9nqioKIcUuKJ50OpjDAXlBcR1iatWZjGbOfv1EfqOGImfn/Zv/2XqrGdFA1AX2W2dTsef//xnp/Li4mL69+9fLVBsZ/369bUaJ1jjCatXr3b5Wnp6OmDd4XT06NFa96FoeFr9iqHQ4OxKuvCfDAxXS+l306gatVWusp4VPkK7du3YunVrUw9D0Uxp1Yah3FROmanMyZX0w9E0dP7+9I4bxk8ZhRRkl7pp4br2SirVWc8KhcLnadWuJHc5DGeOphE9eCgV5X78739bl7+Dbu3OqJ/3dSt3IaVUOkkKhaJF0OgrBiFEDyFEqhDilBDipBDit7byjkKI/xNCZNp+d2josbg667kw+wKXcy7Qb8RITh3IQQKDxkby3f4c3n/xIMc+PYfJaHZqq9JgxmKSyjAoFAqfpylcSSbgGSnlIGA0sFAIMRh4DvhcStkf+Nz2vEFxtWI4c/QQAH2GjeTU/mx6DOrIhEcHEf/iSKIGdODg9h/YtOwQp4/mVkvmcWQ9qxwGhULh4zS6YZBS5kgpj9kelwCngCjgF8AGW7UNwH0NPRZXK4YfjqbRpWdvruT5U3q5gsG3WJNzOkS0ZcqCOKYu0hPYxp9P//ot2986xqWsYkDpJCkUipaDaEqNEiFEb2AvMAT4UUoZXuW1y1JKJ3eSEGIeMA+gW7duIzZv3lyrvktLS9lv3s+uol38secfCRABmAzlfLN+DRHDRmE2jqUsDwZMFfjpquclSIvk8lnIPS4xV0D73hDcUXDxmKTv3YLgDk2Tx1BaWkpoaGiT9N0QNIf5tG/fnpiYmHprz2w2o9PVLAEyNzeX5557jiNHjhAeHk5AQACLFi1y5CM0FOvWrSM4OJiZM2e6rVOb+TzxxBOkpqZy/PhxgoKCKCgo4Pbbb+fbb7+tUTuLFy8mJSWFnJwcr3WPHDnCiy++SHZ2NmFhYXTr1o2XX36Z2NhYp7rXz+n48eMkJiZSUlKCTqdj8eLFPPDAAx77e+2119iwYQOdO3fGYDAwbtw43nrrLfw0nBsP8NZbb7Fx40Z0Oh1vvvkmd9xxh1OdJ554gv3799OuXTsA1q5dS1xcnFO9zMxMiouLq5VNmDDhqJTyJrcDkFI2yQ8QChwF7rc9L7ru9cve2hgxYoSsLampqfK1r16TY1LGOMoy9u6WK2ZMkT8cOy7XzN8t932Y6bGNijKjPLD9tFy7MFW+/evP5du//lyWXjbUekx1JTU1tcn6bgiaw3wyMjLqtb3i4uIa1bdYLHL06NFy7dq1jrKsrCy5atUqp7pGo7HO46spNZ2PlFLOnj1b9ujRQ65Zs0ZKKWVeXp7s1atXjdo4fPiwfOSRR2Tbtm291r148aLs1auX3L9/v6Psyy+/lNu3b3dZ//o5/ec//5Hff/+9lFLKCxcuyIiICHn58mWPfb700ksyOTlZSiml2WyWt9xyi9y9e7fXsUop5cmTJ2VcXJw0GAzyzJkzsm/fvtJkMjnVmz17tty6davX9o4dO+ZUBhyRHj5bm2RXkhAiAPgISJFSbrMVXxJCREopc4QQkUCDn9xxfQ7DD0fTCGkfTuHFtlgseQy+xbN4XmCwP2Pu60fsrd05uOMHruSWK1dSC+bLD74n/ydtW5fdcf230c49QrltxgC39Xfv3k1gYCBPPPGEo6xXr1785je/AazJaP/85z8xGAxcvXqVzz//nGeffZZ//etfCCF44YUXeOihh/jiiy9YsWIFu3btAuDJJ5/kpptuYs6cOfTu3ZuHHnqI1NRUADZt2kRMTAzLli0jNDSUxYsXM378eEaNGkVqaipFRUWsW7eO2267jbKyMh577DG+++47Bg0aRFZWFqtXr+amm9x/GQWr7tPKlStrlfFsNptJSkpi06ZNbN++3Wv9t99+m9mzZzN27FhH2a233qq5vwEDrt2f7t2707VrV/Ly8ggPdxbedEVlZSUGg4EOHbTtp9m5cyfx8fEEBQXRp08fYmJiSEtLY8yYMZrHXFca3TAIq17EOuCUlPKPVV76BzAbeN32e2dDj6WqHIbZZCTrm2P0HzWWUwcu0b1/OB0itGnQt+sczN2/Ulovivrn5MmTDB8+3GOdgwcPcvz4cTp27MhHH31Eeno633zzDfn5+dx8881elVTBmvCWlpbGxo0bWbRokcOAVMVkMpGWlsbHH3/Myy+/zGeffca7775Lhw4dHAcB2WUvvNGzZ09uvfVW3nvvvWousZKSEm677TaX12zatInBgwfz9ttvM3XqVM1nPJw8eZLZs2e7fT0lJYXk5GTHc4vFgp+fHzExMQ7NKTtpaWlUVlY6tKM8sXLlSt5//33OnTvHPffc43hvkpOTSUlJcao/btw4Vq1axYULFxg9erSj3JN0+dKlS3nllVeYOHEir7/+OkFB3k+a1EJTrBhuAR4FTggh0m1lz2M1CB8IIR4DfgQebOiBFJQX0C/ceoPPnzpJRdlVwiNiOZ1ezsh7+zR09wofw9M3e63UVnbbzsKFC9m3bx+BgYEcPnwYsEppd+xo/YKzb98+Hn74YXQ6Hd26deP222/n8OHDDj+0Ox5++GHH78TERJd17HpJI0aMICsrC7AaJfuxo0OGDHHp43bH888/z9SpU5kyZYqjLCwszCGd4Yrs7Gy2bt3KF198obmf6xk1ahTFxcXcdddd/OlPf3KSLnd3j3Jycnj00UfZsGGDplhBYmIiixcvxmg0Mn36dDZv3kx8fDxJSUkkJSW5vU5qlC5fvnw5ERERVFZWMm/ePN544w1efPFFr+PSQqMbBinlPsCrwSo7AAAPFUlEQVRddHZiY46l0FDIzW1uBqxJbbqAAC5f6kRQSAn9hnVpzKEoFC6JjY3lo48+cjxfvXo1+fn51Vw1VU9Xc/WhAtZT4CwWi+O5/WQ2O1U/eNyJQNq/jep0Okwmk8f+tBATE4Ner+eDDz5wlHlbMZw9e5bTp087NgSUlZURExPD6dOn3fYTGxvLsWPH+MUvrCcxHjp0iA8//NCxKtKyYiguLmbKlCm8+uqr1b7NayEgIIBJkyaxd+9e4uPjva4YtEqX21dMQUFBzJ07lxUrVtRoXJ5otZIYZmmmqKKITm06IaXkh2NpRA+M4+yJIgaMisA/UElnK5qen/3sZxgMBtauXeso8ySiN27cOLZs2YLZbCYvL4+9e/cycuRIevXqRUZGBhUVFVy5coXPP/+82nVbtmxx/K6JL3vMmDGOD/aMjAxOnDjheE2LrPfSpUurfaDZVwyufgYPHsyUKVO4ePEiWVlZZGVlERIS4jAK27dvZ8mSJU59LFy4kPXr13PgwAFHWdX38HrpcruUuN0oVFZWMm3aNBISEnjwweqOjCVLlniNc0gpOXDggMP9lJSU5HJ+q1atAqzS5Zs3b6aiooKzZ8+SmZnJyJHO8v/23VhSSnbs2FGv0uWtVhKj1GwNInYK7kThhZ+4cukikf0nYDkvib3V2TorFE2BEIIdO3aQmJjIm2++SZcuXWjbti1vvPGGy/rTpk3j4MGD3HjjjQghePPNN4mIiABgxowZxMXF0b9/f4YNG1btuoqKCkaNGoXFYuHvf/+75vH96le/4sknnyQuLo5hw4YRFxdH+/btAc+y3nZiY2MZPnw4x44d09ynO3744QeXLrOIiAi2bNnC7373Oy5cuEDXrl3p3LmzZrfLBx98wN69eykoKHAoz65fvx69Xs+JEyeYOnWqy+vsMQaj0UhcXBwLFizQ1F9sbCwzZsxg8ODB+Pv7s3r1aseGhcmTJ/Puu+/SvXt3Zs2aRV5eHlJK9Ho977zzjqb2NeFpy1Jz/6nLdtX3Pn1PDlk/RH6W9Zk8tGOrXDFjity49N9y6+uHa91mU9MctnfWJ81hPk29XbUx6NWrl8zLy6vVtZcvX5bl5eVSSilPnz4te/XqJSsqKuSVK1fk9OnT63OYXpk1a5bMzc2tczs1uUd33XVXnftraHxmu2pzoMRcAkDH4I58d/RTOnTvTXG+jhH3qNWCQqGVsrIy7rjjDoxGI1JK1q5dS2BgIIGBgY0u6/3+++83an8An376aaP32Ri0esMQagwk+/tTdOs3EZNFR8yIrk08MoWicbHvMKoNYWFhHDlypP4Go2gWtNrgs90wFH+XBVJSXBBB/5HdCGzTam2lQqFQAK3ZMFhKCNIFcSH9G4LatsciuzgE8xQKhaI103oNg7mEzgGdyDr+Nf5BMXTuEUbXXrVPPFIoFIqWQqv1m5SYS+h1pR1GQzn49yD21u5uE3sUCoWiNdGqVwwROf4IXQCBwb0YMLJbUw9JoXDJpUuXmDlzJn379mXEiBGMGTNGk3hcXXnnnXfYuHFjvbc7Z84coqKiqKioACA/P5/evXtrvl5KydKlSxkwYACDBg1yJIZ5IjMzk3vvvZd+/foxYsQIJkyYwN69ezX1ZzAYGDlyJDfeeCOxsbG89NJLXq9ZtmwZUVFR6PV6Bg4cyPz586tlnntj+fLlxMTEcMMNN7jd+TRnzhz69OmDXq9Hr9d7lBKpKa16xRD6YzA6/17E3BRFUIhSRVU0P6SU3HfffcyePZtNmzYBcO7cOf7xj3841TWZTPj719+/dFVF1/pGp9Pxt7/9jfnz59f42vXr1/PTTz/x3Xff4efnR26uZyFmg8HAlClTWLFihSMZ7dtvv+XIkSOaBAaDgoLYvXs3oaGhGI1Gbr31Vu655x6v0hh2rSSLxcK4cePYs2cPEyZM8NpfRkYGmzdv5uTJk2RnZ3PHHXfw/fffuzz3Ijk5menTp3tts6a0SsNgkRZ0RQb8SishpA+DVaazQgOp6/9C7rkzdWrDbDKj87/2D961V18mzJnntr6S3XZm7dq1bNq0ySFk17Wr5y3mKSkpjBkzplqG8pAhQzRLSAghHAdGGY1GjEZjjdzOSnbbRyipLCE61yoI1jEqlsh+7Zt4RAqFa5Ts9jXssts//PADW7ZsYfv27XTp0oVVq1bRv39/t315ew9TU1OrKcraRfRCQkIc+kpms5kRI0Zw+vRpFi5cyKhRo7zOUclu+xgF5QX0zmmP0EUwdPwAFXRWaMLTN3utKNnta9RGdhusuk5t2rThyJEjbNu2jV/+8pd8+eWXmvudNm0amZmZDBgwgG3btjFhwoRqfbq6RzqdjvT0dIqKipg2bRrffvut1xWHkt32MXJyz9GhxA9dSF9uGB3R1MNRKNyiZLevYV8xREdHO85cnjZtGnPnzvXYT2xsbLVA8/bt2zly5AiLFy8GtK0Y7ISHhzN+/Hg++eQTza4oJbvtI5w79jUC6DooluDQwKYejkLhFiW7XV12G+C+++5j9+7dAOzZs8dx9GZaWhoJCQlOfcycOZP9+/dXC9hXfQ/tK4brZbftRiEvL4+ioiIAysvL+eyzzxg4cCCgZLdbFKWF/vgFDibu7hubeigKhUeU7LYzzz33HLNmzWLlypWEhoby7rvvAvDjjz8SHBzsVD84OJhdu3bx9NNPs2jRIrp160ZYWBgvvPCCpv5ycnKYPXs2ZrMZi8XCjBkzuPfeewGU7HZz/Kmt7PaOzz6XLy/9mzSZTLW6vrnSHGSq65PmMB8lu+2Z5iS7vXjxYvnNN9/UuR0lu91KZbd/MfFntNd94XJfsEKh0E5zkt2uejxnY6FktxUKRYtEyW4rrqdVBp8Vipog67DzRqFoSmr7t6sMg0LhgTZt2lBQUKCMg8LnkFJSUFCA2Wyu8bXKlaRQeCA6Oprz58+Tl5dXL+0ZDAbatGlTL201B1rafKBlzalNmzZcvXq1xtcpw6BQeCAgIIA+ffrUW3tffPGF01ZRX6alzQda3pzOnTtX42uUK0mhUCgU1VCGQaFQKBTVUIZBoVAoFNUQvrzbQgiRB9TcgWalM5Bfj8NpDrS0ObW0+UDLm1NLmw+0vDm5mk8vKWUXdxf4tGGoC0KII1JKz6eJ+BgtbU4tbT7Q8ubU0uYDLW9OtZmPciUpFAqFohrKMCgUCoWiGq3ZMPylqQfQALS0ObW0+UDLm1NLmw+0vDnVeD6tNsagUCgUCte05hWDQqFQKFygDINCoVAoqtEqDYMQYpIQ4j9CiNNCiOeaejx1RQiRJYQ4IYRIF0L4pDi+EOJvQohcIcS3Vco6CiH+TwiRafvdoSnHWBPczGeZEOKC7T6lCyEmN+UYa4oQoocQIlUIcUoIcVII8VtbuU/eJw/z8dn7JIRoI4RIE0J8Y5vTy7byPkKIQ7Z7tEUI4fGw+1YXYxBC6IDvgTuB88Bh4GEpZUaTDqwOCCGygJuklD6blCOEGAeUAhullENsZW8ChVLK120GvIOU8ndNOU6tuJnPMqBUSrmiKcdWW4QQkUCklPKYECIMOArcB8zBB++Th/nMwEfvkxBCAG2llKVCiABgH/Bb4Glgm5RysxDiHeAbKeVad+20xhXDSOC0lPKMlLIS2Az8oonH1OqRUu4FCq8r/gWwwfZ4A9Z/Wp/AzXx8GilljpTymO1xCXAKiMJH75OH+fgstiOdS21PA2w/EvgZ8KGt3Os9ao2GIQr4qcrz8/j4HwPWG/9vIcRRIcS8ph5MPdJNSpkD1n9ioGsTj6c+eFIIcdzmavIJl4srhBC9gWHAIVrAfbpuPuDD90kIoRNCpAO5wP8BPwBFUkqTrYrXz7zWaBiEizJf96fdIqUcDtwDLLS5MRTNj7VAP0AP5ABvNe1waocQIhT4CFgkpSxu6vHUFRfz8en7JKU0Syn1QDRWD8kgV9U8tdEaDcN5oEeV59FAdhONpV6QUmbbfucC27H+MbQELtn8wHZ/cG4Tj6dOSCkv2f5pLcBf8cH7ZPNbfwSkSCm32Yp99j65mk9LuE8AUsoi4AtgNBAuhLAfzOb1M681GobDQH9blD4QiAf+0cRjqjVCiLa2wBlCiLbAXcC3nq/yGf4BzLY9ng3sbMKx1Bn7h6eNafjYfbIFNtcBp6SUf6zykk/eJ3fz8eX7JIToIoQItz0OBu7AGjtJBabbqnm9R61uVxKAbfvZfwE64G9Syj808ZBqjRCiL9ZVAliPat3ki/MRQvwdGI9VIvgS8BKwA/gA6An8CDwopfSJgK6b+YzH6p6QQBbwa7tv3hcQQtwKfAmcACy24uex+uV97j55mM/D+Oh9EkLEYQ0u67B+8f9ASvmK7XNiM9AR+Bp4REpZ4bad1mgYFAqFQuGe1uhKUigUCoUHlGFQKBQKRTWUYVAoFApFNZRhUCgUCkU1lGFQKBQKRTWUYVAoGhEhxHghxK6mHodC4QllGBQKhUJRDWUYFAoXCCEesenapwsh/mwTJisVQrwlhDgmhPhcCNHFVlcvhPjKJrq23S66JoSIEUJ8ZtPGPyaE6GdrPlQI8aEQ4jshRIotAxchxOtCiAxbOz4n+axoOSjDoFBchxBiEPAQVnFCPWAGZgFtgWM2wcI9WLOZATYCv5NSxmHNorWXpwCrpZQ3AmOxCrKBVcVzETAY6AvcIoToiFV+IdbWzqsNO0uFwj3KMCgUzkwERgCHbfLFE7F+gFuALbY67wO3CiHaA+FSyj228g3AOJt+VZSUcjuAlNIgpSyz1UmTUp63ibSlA72BYsAAvCuEuB+w11UoGh1lGBQKZwSwQUqpt/3cIKVc5qKeJz0ZV/Ludqpq1JgBf5tW/kisSp/3AZ/UcMwKRb2hDINC4cznwHQhRFdwnGncC+v/i12hciawT0p5BbgshLjNVv4osMem639eCHGfrY0gIUSIuw5tZwK0l1J+jNXNpG+IiSkUWvD3XkWhaF1IKTOEEC9gPRXPDzACC4GrQKwQ4ihwBWscAqwyxu/YPvjPAHNt5Y8CfxZCvGJr40EP3YYBO4UQbbCuNhLreVoKhWaUuqpCoREhRKmUMrSpx6FQNDTKlaRQKBSKaqgVg0KhUCiqoVYMCoVCoaiGMgwKhUKhqIYyDAqFQqGohjIMCoVCoaiGMgwKhUKhqMb/B/+LhjHcxAv0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "B_sel = 0\n",
    "\n",
    "plt.plot(acc_test_arr_v1[0,B_sel,0,0:30],label='N=2, B='+str(B_array[B_sel]))\n",
    "plt.plot(acc_test_arr_v1[1,B_sel,0,0:30],label='N=4, B='+str(B_array[B_sel]))\n",
    "plt.plot(acc_test_arr_v1[2,B_sel,0,0:30],label='N=6, B='+str(B_array[B_sel]))\n",
    "plt.plot(acc_test_arr_v1[3,B_sel,0,0:30],label='N=8, B='+str(B_array[B_sel]))\n",
    "plt.plot(acc_test_arr_G2[0,0:30],label='Grouping, N=4, G=2, B='+str(B_array[B_sel]))\n",
    "plt.plot(acc_test_arr_G3[0,0:30],label='Grouping, N=6, G=3, B='+str(B_array[B_sel]))\n",
    "\n",
    "# plt.plot(acc_test_arr_v1[4,B_sel,0,0:30],label='N=7, B='+str(B_array[B_sel]))\n",
    "# plt.plot(acc_test_arr_v1[5,B_sel,0,0:30],label='N=8, B='+str(B_array[B_sel]))\n",
    "\n",
    "# plt.plot(plot_acc_v2[2,sigma_sel,:],label='T=10')\n",
    "# plt.plot(plot_acc_v2[3,sigma_sel,:],label='T=11')\n",
    "\n",
    "# plt.plot(acc_test_arr_uncoded,'r',label='uncoded')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "# plt.title(title_name)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Test accuracy(%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
