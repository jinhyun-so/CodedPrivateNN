{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import basic libraries\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Import PyTorch\n",
    "import torch # import main library\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn # import modules\n",
    "from torch.autograd import Function # import Function to create custom activations\n",
    "from torch.nn.parameter import Parameter # import Parameter to create custom activations with learnable parameters\n",
    "from torch import optim # import optimizers for demonstrations\n",
    "import torch.nn.functional as F # import torch functions\n",
    "from torchvision import datasets, transforms # import transformations to use for demo\n",
    "from torch.nn.parameter import Parameter # import Parameter to create custom activations with learnable parameters\n",
    "\n",
    "from models.activ_func import *\n",
    "from models.Nets import *\n",
    "from models.test import test_img\n",
    "from models.Update import *\n",
    "\n",
    "\n",
    "from utils.sampling import mnist_iid, mnist_noniid, cifar_iid\n",
    "from utils.options import args_parser\n",
    "from models.Update import LocalUpdate\n",
    "from models.Nets import *\n",
    "from models.Fed import FedAvg\n",
    "from models.Fed import FedQAvg, Quantization, Quantization_Finite, my_score, my_score_Finite\n",
    "from models.test import test_img\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from utils.functions import *\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "class my_argument:    \n",
    "    epochs    = 16    #\"rounds of training\"\n",
    "    num_users = 2  # \"number of users: K\"\n",
    "    frac      = 0.5 #\"the fraction of clients: C\"\n",
    "    local_ep  = 1 #\"the number of local epochs: E\"\n",
    "    local_bs  = 50  #\"local batch size: B\"\n",
    "    bs        = 50 #\"test batch size\"\n",
    "    lr        = 0.001 #\"learning rate\"\n",
    "    momentum  = 0.5 # \"SGD momentum (default: 0.5)\"\n",
    "    weight_decay = 5e-4\n",
    "    split     = 'user' # \"train-test split type, user or sample\"\n",
    "    opt='ADAM'\n",
    "    loss='Custom' # 'Custom' or 'Default'\n",
    "\n",
    "    # model arguments\n",
    "    model = 'cnn'\n",
    "    kernel_num=9 #, help='number of each kind of kernel')\n",
    "    kernel_sizes='3,4,5' #  help='comma-separated kernel size to use for convolution')\n",
    "    norm='batch_norm' #, help=\"batch_norm, layer_norm, or None\")\n",
    "    num_filters=32 #, help=\"number of filters for conv nets\")\n",
    "    max_pool='True' #help=\"Whether use max pooling rather than strided convolutions\")\n",
    "\n",
    "    # other arguments\n",
    "    dataset='cifar' #, help=\"name of dataset\")\n",
    "    iid=1\n",
    "    num_classes=10#, help=\"number of classes\")\n",
    "    num_channels=1#, help=\"number of channels of imges\")\n",
    "    gpu=1#, help=\"GPU ID, -1 for CPU\")\n",
    "    stopping_rounds=10#, help='rounds of early stopping')\n",
    "    verbose='False'#, help='verbose print')\n",
    "    seed=1#, help='random seed (default: 1)')\n",
    "    \n",
    "args = my_argument()\n",
    "\n",
    "args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)\n",
    "args.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(args.device)\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Normalize the test set same as training set without augmentation\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# trainset = torchvision.datasets.CIFAR10(\n",
    "#     root=\"./data/cifar\", train=True, download=True, transform=transform_train)\n",
    "# trainloader = torch.utils.data.DataLoader(\n",
    "#     trainset, batch_size=args.bs, shuffle=True, num_workers=2)\n",
    "\n",
    "# testset = torchvision.datasets.CIFAR10(\n",
    "#     root=\"./data/cifar\", train=False, download=True, transform=transform_test)\n",
    "# testloader = torch.utils.data.DataLoader(\n",
    "#     testset, batch_size=args.bs, shuffle=False, num_workers=2)\n",
    "\n",
    "dataset_train = datasets.CIFAR10('../data/cifar', train=True, download=True, transform=transform_train)\n",
    "dataset_test = datasets.CIFAR10('../data/cifar', train=False, download=True, transform=transform_test)\n",
    "if args.iid:\n",
    "    dict_users = cifar_iid(dataset_train, args.num_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, testloader, criterion):\n",
    "    test_loss = 0\n",
    "    accuracy = 0\n",
    "    for images, labels in testloader:\n",
    "\n",
    "#         images.resize_(images.shape[0], 3*32*32)\n",
    "\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        output = model.forward(images)\n",
    "        test_loss += criterion(output, labels).item()\n",
    "\n",
    "        ps = torch.exp(output)\n",
    "        equality = (labels.data == ps.max(dim=1)[1])\n",
    "        accuracy += equality.type(torch.FloatTensor).mean()\n",
    "\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ResNet18 without encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\aten\\src\\ATen\\native\\BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.3279 \n",
      "Accuracy: 5181/10000 (51.81%)\n",
      "\n",
      "Round   0, Average loss 1.586\n",
      "\n",
      "Test set: Average loss: 1.0216 \n",
      "Accuracy: 6355/10000 (63.55%)\n",
      "\n",
      "Round   1, Average loss 1.253\n",
      "\n",
      "Test set: Average loss: 0.8823 \n",
      "Accuracy: 6833/10000 (68.33%)\n",
      "\n",
      "Round   2, Average loss 1.067\n",
      "\n",
      "Test set: Average loss: 0.7931 \n",
      "Accuracy: 7193/10000 (71.93%)\n",
      "\n",
      "Round   3, Average loss 0.938\n",
      "\n",
      "Test set: Average loss: 0.7038 \n",
      "Accuracy: 7569/10000 (75.69%)\n",
      "\n",
      "Round   4, Average loss 0.834\n",
      "\n",
      "Test set: Average loss: 0.6571 \n",
      "Accuracy: 7703/10000 (77.03%)\n",
      "\n",
      "Round   5, Average loss 0.758\n",
      "\n",
      "Test set: Average loss: 0.6193 \n",
      "Accuracy: 7847/10000 (78.47%)\n",
      "\n",
      "Round   6, Average loss 0.694\n",
      "\n",
      "Test set: Average loss: 0.5851 \n",
      "Accuracy: 7991/10000 (79.91%)\n",
      "\n",
      "Round   7, Average loss 0.644\n",
      "\n",
      "Test set: Average loss: 0.5680 \n",
      "Accuracy: 8059/10000 (80.59%)\n",
      "\n",
      "Round   8, Average loss 0.603\n",
      "\n",
      "Test set: Average loss: 0.5292 \n",
      "Accuracy: 8180/10000 (81.80%)\n",
      "\n",
      "Round   9, Average loss 0.564\n",
      "\n",
      "Test set: Average loss: 0.5023 \n",
      "Accuracy: 8299/10000 (82.99%)\n",
      "\n",
      "Round  10, Average loss 0.535\n",
      "\n",
      "Test set: Average loss: 0.4929 \n",
      "Accuracy: 8326/10000 (83.26%)\n",
      "\n",
      "Round  11, Average loss 0.507\n",
      "\n",
      "Test set: Average loss: 0.4811 \n",
      "Accuracy: 8358/10000 (83.58%)\n",
      "\n",
      "Round  12, Average loss 0.479\n",
      "\n",
      "Test set: Average loss: 0.4664 \n",
      "Accuracy: 8443/10000 (84.43%)\n",
      "\n",
      "Round  13, Average loss 0.456\n",
      "\n",
      "Test set: Average loss: 0.4455 \n",
      "Accuracy: 8495/10000 (84.95%)\n",
      "\n",
      "Round  14, Average loss 0.434\n",
      "\n",
      "Test set: Average loss: 0.4455 \n",
      "Accuracy: 8515/10000 (85.15%)\n",
      "\n",
      "Round  15, Average loss 0.415\n",
      "\n",
      "Test set: Average loss: 0.4278 \n",
      "Accuracy: 8575/10000 (85.75%)\n",
      "\n",
      "Round  16, Average loss 0.394\n",
      "\n",
      "Test set: Average loss: 0.4121 \n",
      "Accuracy: 8618/10000 (86.18%)\n",
      "\n",
      "Round  17, Average loss 0.376\n",
      "\n",
      "Test set: Average loss: 0.4083 \n",
      "Accuracy: 8634/10000 (86.34%)\n",
      "\n",
      "Round  18, Average loss 0.361\n",
      "\n",
      "Test set: Average loss: 0.4102 \n",
      "Accuracy: 8632/10000 (86.32%)\n",
      "\n",
      "Round  19, Average loss 0.345\n",
      "\n",
      "Test set: Average loss: 0.3914 \n",
      "Accuracy: 8703/10000 (87.03%)\n",
      "\n",
      "Round  20, Average loss 0.332\n",
      "\n",
      "Test set: Average loss: 0.4042 \n",
      "Accuracy: 8687/10000 (86.87%)\n",
      "\n",
      "Round  21, Average loss 0.318\n",
      "\n",
      "Test set: Average loss: 0.3815 \n",
      "Accuracy: 8712/10000 (87.12%)\n",
      "\n",
      "Round  22, Average loss 0.307\n",
      "\n",
      "Test set: Average loss: 0.3845 \n",
      "Accuracy: 8739/10000 (87.39%)\n",
      "\n",
      "Round  23, Average loss 0.293\n",
      "\n",
      "Test set: Average loss: 0.3813 \n",
      "Accuracy: 8722/10000 (87.22%)\n",
      "\n",
      "Round  24, Average loss 0.282\n",
      "\n",
      "Test set: Average loss: 0.3829 \n",
      "Accuracy: 8736/10000 (87.36%)\n",
      "\n",
      "Round  25, Average loss 0.274\n",
      "\n",
      "Test set: Average loss: 0.3724 \n",
      "Accuracy: 8779/10000 (87.79%)\n",
      "\n",
      "Round  26, Average loss 0.261\n",
      "\n",
      "Test set: Average loss: 0.3732 \n",
      "Accuracy: 8784/10000 (87.84%)\n",
      "\n",
      "Round  27, Average loss 0.249\n",
      "\n",
      "Test set: Average loss: 0.3520 \n",
      "Accuracy: 8853/10000 (88.53%)\n",
      "\n",
      "Round  28, Average loss 0.241\n",
      "\n",
      "Test set: Average loss: 0.3561 \n",
      "Accuracy: 8846/10000 (88.46%)\n",
      "\n",
      "Round  29, Average loss 0.236\n",
      "\n",
      "Test set: Average loss: 0.3529 \n",
      "Accuracy: 8872/10000 (88.72%)\n",
      "\n",
      "Round  30, Average loss 0.222\n",
      "\n",
      "Test set: Average loss: 0.3676 \n",
      "Accuracy: 8818/10000 (88.18%)\n",
      "\n",
      "Round  31, Average loss 0.215\n",
      "\n",
      "Test set: Average loss: 0.3704 \n",
      "Accuracy: 8843/10000 (88.43%)\n",
      "\n",
      "Round  32, Average loss 0.210\n",
      "\n",
      "Test set: Average loss: 0.3690 \n",
      "Accuracy: 8846/10000 (88.46%)\n",
      "\n",
      "Round  33, Average loss 0.202\n",
      "\n",
      "Test set: Average loss: 0.3562 \n",
      "Accuracy: 8882/10000 (88.82%)\n",
      "\n",
      "Round  34, Average loss 0.193\n",
      "\n",
      "Test set: Average loss: 0.3696 \n",
      "Accuracy: 8826/10000 (88.26%)\n",
      "\n",
      "Round  35, Average loss 0.187\n",
      "\n",
      "Test set: Average loss: 0.3680 \n",
      "Accuracy: 8861/10000 (88.61%)\n",
      "\n",
      "Round  36, Average loss 0.182\n",
      "\n",
      "Test set: Average loss: 0.3744 \n",
      "Accuracy: 8864/10000 (88.64%)\n",
      "\n",
      "Round  37, Average loss 0.173\n",
      "\n",
      "Test set: Average loss: 0.3539 \n",
      "Accuracy: 8898/10000 (88.98%)\n",
      "\n",
      "Round  38, Average loss 0.168\n",
      "\n",
      "Test set: Average loss: 0.3484 \n",
      "Accuracy: 8926/10000 (89.26%)\n",
      "\n",
      "Round  39, Average loss 0.163\n",
      "\n",
      "Test set: Average loss: 0.3577 \n",
      "Accuracy: 8902/10000 (89.02%)\n",
      "\n",
      "Round  40, Average loss 0.157\n",
      "\n",
      "Test set: Average loss: 0.3657 \n",
      "Accuracy: 8894/10000 (88.94%)\n",
      "\n",
      "Round  41, Average loss 0.154\n",
      "\n",
      "Test set: Average loss: 0.3693 \n",
      "Accuracy: 8923/10000 (89.23%)\n",
      "\n",
      "Round  42, Average loss 0.150\n",
      "\n",
      "Test set: Average loss: 0.3450 \n",
      "Accuracy: 8962/10000 (89.62%)\n",
      "\n",
      "Round  43, Average loss 0.144\n",
      "\n",
      "Test set: Average loss: 0.3474 \n",
      "Accuracy: 8962/10000 (89.62%)\n",
      "\n",
      "Round  44, Average loss 0.139\n",
      "\n",
      "Test set: Average loss: 0.3909 \n",
      "Accuracy: 8904/10000 (89.04%)\n",
      "\n",
      "Round  45, Average loss 0.138\n",
      "\n",
      "Test set: Average loss: 0.3729 \n",
      "Accuracy: 8912/10000 (89.12%)\n",
      "\n",
      "Round  46, Average loss 0.135\n",
      "\n",
      "Test set: Average loss: 0.3633 \n",
      "Accuracy: 8940/10000 (89.40%)\n",
      "\n",
      "Round  47, Average loss 0.128\n",
      "\n",
      "Test set: Average loss: 0.3519 \n",
      "Accuracy: 8973/10000 (89.73%)\n",
      "\n",
      "Round  48, Average loss 0.127\n",
      "\n",
      "Test set: Average loss: 0.3465 \n",
      "Accuracy: 8992/10000 (89.92%)\n",
      "\n",
      "Round  49, Average loss 0.126\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "N_epochs = 50\n",
    "\n",
    "net_glob = ResNet18()\n",
    "net_glob.cuda()\n",
    "\n",
    "acc_test_FedAvg = np.empty(N_epochs)\n",
    "loss_test_FedAvg = np.empty(N_epochs)\n",
    "\n",
    "for iter in range(N_epochs):\n",
    "    w_locals, loss_locals = [], []\n",
    "\n",
    "    m = args.num_users\n",
    "    \n",
    "    idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "    \n",
    "    for idx in idxs_users:\n",
    "        local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])\n",
    "        w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))\n",
    "        w_locals.append(copy.deepcopy(w))\n",
    "        loss_locals.append(copy.deepcopy(loss))\n",
    "        \n",
    "    # update global weights\n",
    "    w_glob = FedAvg(w_locals)\n",
    "\n",
    "    # copy weight to net_glob\n",
    "    net_glob.load_state_dict(w_glob)\n",
    "    \n",
    "    acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "\n",
    "    # print loss\n",
    "    loss_avg = sum(loss_locals) / len(loss_locals)\n",
    "    print('Round {:3d}, Average loss {:.3f}'.format(iter, loss_avg))\n",
    "#     loss_train.append(loss_avg)\n",
    "    \n",
    "    acc_test_FedAvg[iter] = acc_test\n",
    "    loss_test_FedAvg[iter] = loss_test\n",
    "    \n",
    "    PATH = \"./save_models/CIFAR10_ResNet18_K2_Plain_E50_iter\"+str(iter)+\".pt\"\n",
    "    torch.save(net_glob.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. BACC, K=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of X: (50000, 3072)\n",
      "size of Y: (50000, 10)\n",
      "(50000, 10)\n",
      "2.074361956686769\n"
     ]
    }
   ],
   "source": [
    "encoding_input_array_np = np.empty((len(dataset_train),32*32*3))\n",
    "encoding_label_array_np = np.empty((len(dataset_train),args.num_classes))\n",
    "print(\"size of X:\" ,encoding_input_array_np.shape)\n",
    "print(\"size of Y:\" ,encoding_label_array_np.shape)\n",
    "\n",
    "Size_submatrices = int(50000/args.num_users)\n",
    "\n",
    "\n",
    "for i in range(args.num_users):\n",
    "    \n",
    "    stt_pos = i*Size_submatrices\n",
    "    end_pos = (i+1)*Size_submatrices\n",
    "#     print(i,stt_pos,end_pos)\n",
    "    Temp_train = DataLoader(DatasetSplit(dataset_train, dict_users[i]), batch_size=Size_submatrices, shuffle=True)\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(Temp_train):\n",
    "        \n",
    "        images_np = images.detach().cpu().numpy()\n",
    "        \n",
    "#         print(np.size(images_np))\n",
    "        encoding_input_array_np[stt_pos:end_pos,:] = np.reshape(images_np, (Size_submatrices,32*32*3))\n",
    "#         print(encoding_input_array_np[stt_pos:end_pos,:].shape)\n",
    "\n",
    "        onehot_labels = torch.nn.functional.one_hot(labels,num_classes=args.num_classes)\n",
    "        labels_np = onehot_labels.detach().cpu().numpy()\n",
    "#         print(labels_np.shape)\n",
    "        encoding_label_array_np[stt_pos:end_pos,:] = labels_np\n",
    "    \n",
    "print(np.shape(encoding_label_array_np))\n",
    "\n",
    "avg_power = np.sum(encoding_input_array_np*encoding_input_array_np, axis=1)/np.shape(encoding_input_array_np)[1]\n",
    "avg_power = np.sum(avg_power)/np.shape(encoding_input_array_np)[0]\n",
    "print(avg_power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "sigma = 1\n",
      "\n",
      "\n",
      "\n",
      "Learning Rate = 0.001\n",
      "\n",
      "\n",
      "\n",
      "z_array: [-0.81  0.81]\n",
      "0.4838626198316927\n",
      "0.48386261983169315\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 25000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 25000 \n",
      "\n",
      "Adjust the power of X_tilde\n",
      "0 0.8781785482658354\n",
      "power after adjusting = 2.0743619566867686\n",
      "1 0.880611848370687\n",
      "power after adjusting = 2.0743619566867695\n",
      "\n",
      "(T, sigma)= 3 1 )  0 -th Trial!!\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.1926 \n",
      "Accuracy: 2205/10000 (22.05%)\n",
      "\n",
      "Round   0, Average loss 2.193 Test accuracy 22.050\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.0093 \n",
      "Accuracy: 3135/10000 (31.35%)\n",
      "\n",
      "Round   1, Average loss 2.009 Test accuracy 31.350\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.8259 \n",
      "Accuracy: 3875/10000 (38.75%)\n",
      "\n",
      "Round   2, Average loss 1.826 Test accuracy 38.750\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7358 \n",
      "Accuracy: 4074/10000 (40.74%)\n",
      "\n",
      "Round   3, Average loss 1.736 Test accuracy 40.740\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7081 \n",
      "Accuracy: 4250/10000 (42.50%)\n",
      "\n",
      "Round   4, Average loss 1.708 Test accuracy 42.500\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6879 \n",
      "Accuracy: 4418/10000 (44.18%)\n",
      "\n",
      "Round   5, Average loss 1.688 Test accuracy 44.180\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6614 \n",
      "Accuracy: 4626/10000 (46.26%)\n",
      "\n",
      "Round   6, Average loss 1.661 Test accuracy 46.260\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6336 \n",
      "Accuracy: 4804/10000 (48.04%)\n",
      "\n",
      "Round   7, Average loss 1.634 Test accuracy 48.040\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6153 \n",
      "Accuracy: 4950/10000 (49.50%)\n",
      "\n",
      "Round   8, Average loss 1.615 Test accuracy 49.500\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5885 \n",
      "Accuracy: 5095/10000 (50.95%)\n",
      "\n",
      "Round   9, Average loss 1.589 Test accuracy 50.950\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6045 \n",
      "Accuracy: 4993/10000 (49.93%)\n",
      "\n",
      "Round  10, Average loss 1.605 Test accuracy 49.930\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6123 \n",
      "Accuracy: 4959/10000 (49.59%)\n",
      "\n",
      "Round  11, Average loss 1.612 Test accuracy 49.590\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5973 \n",
      "Accuracy: 5026/10000 (50.26%)\n",
      "\n",
      "Round  12, Average loss 1.597 Test accuracy 50.260\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5867 \n",
      "Accuracy: 5094/10000 (50.94%)\n",
      "\n",
      "Round  13, Average loss 1.587 Test accuracy 50.940\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5990 \n",
      "Accuracy: 5061/10000 (50.61%)\n",
      "\n",
      "Round  14, Average loss 1.599 Test accuracy 50.610\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5921 \n",
      "Accuracy: 5136/10000 (51.36%)\n",
      "\n",
      "Round  15, Average loss 1.592 Test accuracy 51.360\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5582 \n",
      "Accuracy: 5350/10000 (53.50%)\n",
      "\n",
      "Round  16, Average loss 1.558 Test accuracy 53.500\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5566 \n",
      "Accuracy: 5394/10000 (53.94%)\n",
      "\n",
      "Round  17, Average loss 1.557 Test accuracy 53.940\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5602 \n",
      "Accuracy: 5432/10000 (54.32%)\n",
      "\n",
      "Round  18, Average loss 1.560 Test accuracy 54.320\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5829 \n",
      "Accuracy: 5305/10000 (53.05%)\n",
      "\n",
      "Round  19, Average loss 1.583 Test accuracy 53.050\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6205 \n",
      "Accuracy: 5059/10000 (50.59%)\n",
      "\n",
      "Round  20, Average loss 1.620 Test accuracy 50.590\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5825 \n",
      "Accuracy: 5322/10000 (53.22%)\n",
      "\n",
      "Round  21, Average loss 1.582 Test accuracy 53.220\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5730 \n",
      "Accuracy: 5316/10000 (53.16%)\n",
      "\n",
      "Round  22, Average loss 1.573 Test accuracy 53.160\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6128 \n",
      "Accuracy: 5106/10000 (51.06%)\n",
      "\n",
      "Round  23, Average loss 1.613 Test accuracy 51.060\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5635 \n",
      "Accuracy: 5434/10000 (54.34%)\n",
      "\n",
      "Round  24, Average loss 1.563 Test accuracy 54.340\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5774 \n",
      "Accuracy: 5340/10000 (53.40%)\n",
      "\n",
      "Round  25, Average loss 1.577 Test accuracy 53.400\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5993 \n",
      "Accuracy: 5276/10000 (52.76%)\n",
      "\n",
      "Round  26, Average loss 1.599 Test accuracy 52.760\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6063 \n",
      "Accuracy: 5188/10000 (51.88%)\n",
      "\n",
      "Round  27, Average loss 1.606 Test accuracy 51.880\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5830 \n",
      "Accuracy: 5285/10000 (52.85%)\n",
      "\n",
      "Round  28, Average loss 1.583 Test accuracy 52.850\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5882 \n",
      "Accuracy: 5313/10000 (53.13%)\n",
      "\n",
      "Round  29, Average loss 1.588 Test accuracy 53.130\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6141 \n",
      "Accuracy: 5191/10000 (51.91%)\n",
      "\n",
      "Round  30, Average loss 1.614 Test accuracy 51.910\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6222 \n",
      "Accuracy: 5156/10000 (51.56%)\n",
      "\n",
      "Round  31, Average loss 1.622 Test accuracy 51.560\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5961 \n",
      "Accuracy: 5368/10000 (53.68%)\n",
      "\n",
      "Round  32, Average loss 1.596 Test accuracy 53.680\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6150 \n",
      "Accuracy: 5193/10000 (51.93%)\n",
      "\n",
      "Round  33, Average loss 1.615 Test accuracy 51.930\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6085 \n",
      "Accuracy: 5311/10000 (53.11%)\n",
      "\n",
      "Round  34, Average loss 1.608 Test accuracy 53.110\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5779 \n",
      "Accuracy: 5429/10000 (54.29%)\n",
      "\n",
      "Round  35, Average loss 1.578 Test accuracy 54.290\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6247 \n",
      "Accuracy: 5184/10000 (51.84%)\n",
      "\n",
      "Round  36, Average loss 1.625 Test accuracy 51.840\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6114 \n",
      "Accuracy: 5247/10000 (52.47%)\n",
      "\n",
      "Round  37, Average loss 1.611 Test accuracy 52.470\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6245 \n",
      "Accuracy: 5140/10000 (51.40%)\n",
      "\n",
      "Round  38, Average loss 1.625 Test accuracy 51.400\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6239 \n",
      "Accuracy: 5217/10000 (52.17%)\n",
      "\n",
      "Round  39, Average loss 1.624 Test accuracy 52.170\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5977 \n",
      "Accuracy: 5322/10000 (53.22%)\n",
      "\n",
      "Round  40, Average loss 1.598 Test accuracy 53.220\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6087 \n",
      "Accuracy: 5256/10000 (52.56%)\n",
      "\n",
      "Round  41, Average loss 1.609 Test accuracy 52.560\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6379 \n",
      "Accuracy: 5123/10000 (51.23%)\n",
      "\n",
      "Round  42, Average loss 1.638 Test accuracy 51.230\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5771 \n",
      "Accuracy: 5417/10000 (54.17%)\n",
      "\n",
      "Round  43, Average loss 1.577 Test accuracy 54.170\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6213 \n",
      "Accuracy: 5206/10000 (52.06%)\n",
      "\n",
      "Round  44, Average loss 1.621 Test accuracy 52.060\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6253 \n",
      "Accuracy: 5215/10000 (52.15%)\n",
      "\n",
      "Round  45, Average loss 1.625 Test accuracy 52.150\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5971 \n",
      "Accuracy: 5317/10000 (53.17%)\n",
      "\n",
      "Round  46, Average loss 1.597 Test accuracy 53.170\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5941 \n",
      "Accuracy: 5377/10000 (53.77%)\n",
      "\n",
      "Round  47, Average loss 1.594 Test accuracy 53.770\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5977 \n",
      "Accuracy: 5375/10000 (53.75%)\n",
      "\n",
      "Round  48, Average loss 1.598 Test accuracy 53.750\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5853 \n",
      "Accuracy: 5391/10000 (53.91%)\n",
      "\n",
      "Round  49, Average loss 1.585 Test accuracy 53.910\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "K = 2\n",
    "T = 3\n",
    "sigma = 0.1\n",
    "Noise_Alloc = [0,2,4]\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "\n",
    "N_array = [2]\n",
    "B_array = [0.5]\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 50\n",
    "\n",
    "lr_array = [0.001] # 0.0001 is the bset\n",
    "\n",
    "sigma_array = [1]\n",
    "# lr_array = [0.1, 0.01, 0.005, 0.001,0.0005]\n",
    "\n",
    "\n",
    "loss_test_arr_K2_G1_v3 = np.zeros((len(sigma_array),len(lr_array),N_trials,N_epochs))\n",
    "acc_test_arr_K2_G1_v3  = np.zeros((len(sigma_array),len(lr_array),N_trials,N_epochs))\n",
    "\n",
    "N = 2\n",
    "for sigma_idx in range(len(sigma_array)):\n",
    "    \n",
    "    sigma = sigma_array[sigma_idx]\n",
    "    \n",
    "    print('\\n\\n')\n",
    "    print('sigma =',sigma)\n",
    "    print('\\n\\n')\n",
    "           \n",
    "        \n",
    "    # print(\"alpha_array: \",alpha_array,'\\n')\n",
    "    \n",
    "    \n",
    "    for lr_idx in range(len(lr_array)):\n",
    "        \n",
    "        args.lr = lr_array[lr_idx]\n",
    "        \n",
    "        print('Learning Rate =',args.lr)\n",
    "        print('\\n\\n')\n",
    "#         while(len(z_array)<N):\n",
    "#             z_tmp = np.random.uniform(-1,1,1)\n",
    "#             MIS_tmp = MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_tmp], 1,sigma)\n",
    "#             if MIS_tmp < B and MIS_tmp > 0.1:\n",
    "#                 z_array.append(z_tmp[0])\n",
    "#         \n",
    "#         z_array = np.sort(z_array)\n",
    "#         print(N_idx,'!!!')\n",
    "#         if N_idx==0:\n",
    "# #             z_array = np.array([-0.94,-0.534,0.534, 0.94])\n",
    "# #         elif N_idx==1:\n",
    "# #             z_array = np.array([-0.94, -0.73, 0.73, 0.94])\n",
    "#         elif N_idx==1:\n",
    "#             z_array = np.array([-0.94, -0.125, 0.125, 0.94])\n",
    "#         else:\n",
    "# #             z_array = np.array([-0.94, -0.73, -0.534, -0.125, 0.125, 0.534, 0.73, 0.94])\n",
    "# #             z_array = np.array([-0.9, -0.81, -0.22, -0.20, 0.20, 0.22, 0.81, 0.9])\n",
    "        \n",
    "        z_array = np.array([-0.81, 0.81])\n",
    "        \n",
    "        print('z_array:',z_array)\n",
    "        if sigma != 0:\n",
    "            for j in range(len(z_array)):\n",
    "                print(MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_array[j]], 1,sigma))\n",
    "\n",
    "        \n",
    "        _Noise_label = np.ones((25000*T,10)) * 0.1\n",
    "\n",
    "        X_tilde,a,b = BACC_Enc_Data_v3(encoding_input_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde,a,b = BACC_Enc_Data_v3(encoding_label_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "        \n",
    "        print(\"Adjust the power of X_tilde\")\n",
    "        \n",
    "        for p_idx in range(N):\n",
    "            tmp_power = np.sum(X_tilde[p_idx,:,:] * X_tilde[p_idx,:,:], axis=1)/np.shape(X_tilde)[2]\n",
    "            tmp_power = np.sum(tmp_power)/np.shape(X_tilde)[1]\n",
    "            print(p_idx, tmp_power)\n",
    "            \n",
    "            X_tilde[p_idx,:,:] = X_tilde[p_idx,:,:] * np.sqrt(avg_power/tmp_power)\n",
    "            tmp_power = np.sum(X_tilde[p_idx,:,:] * X_tilde[p_idx,:,:], axis=1)/np.shape(X_tilde)[2]\n",
    "            tmp_power = np.sum(tmp_power)/np.shape(X_tilde)[1]\n",
    "            print('power after adjusting =',tmp_power)\n",
    "        print()\n",
    "        \n",
    "        m = N # m is the number of received result @ master\n",
    "    #     print('number of results:',m)\n",
    "\n",
    "        for trial_idx in range(N_trials):\n",
    "            print('(T, sigma)=',T,sigma,') ',trial_idx,'-th Trial!!')\n",
    "\n",
    "            net_glob = ResNet18()\n",
    "            net_glob.cuda()\n",
    "            net_glob.train()\n",
    "\n",
    "            # copy weights\n",
    "            w_glob = net_glob.state_dict()\n",
    "\n",
    "            for iter in range(N_epochs): #args.epochs\n",
    "                w_locals, loss_locals = [], []\n",
    "                idxs_users = np.random.choice(range(N), m, replace=False)\n",
    "                idxs_users = np.sort(idxs_users)\n",
    "                print('selected users:',idxs_users)\n",
    "                \n",
    "                #coded_net = BACC_Enc_Model_withNoise_v4(net_glob.cuda(), N, K, T, sigma, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "                dec_z_array = []\n",
    "                for idx in idxs_users: #for idx in range(N):\n",
    "            #         print(idx)\n",
    "                    local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[idx,:,:], label=y_tilde[idx,:,:])\n",
    "#                     w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "                    w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                    w_locals.append(copy.deepcopy(w))\n",
    "                    loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                    dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "                # update global weights\n",
    "                #w_glob = FedAvg(w_locals)\n",
    "                w_glob = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "\n",
    "                # copy weight to net_glob\n",
    "                net_glob.load_state_dict(w_glob)\n",
    "                \n",
    "                PATH = \"./save_models/CIFAR10_ResNet18_K2_G1_T3_E50_inPowerAlign_v2_iter\"+str(iter)+\".pt\"\n",
    "                torch.save(net_glob.state_dict(), PATH)\n",
    "\n",
    "                # print loss\n",
    "            #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "            #     loss_train_arr.append(loss_train)\n",
    "\n",
    "                acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "                acc_test_arr_K2_G1_v3[sigma_idx][lr_idx][trial_idx][iter] = acc_test\n",
    "                loss_test_arr_K2_G1_v3[sigma_idx][lr_idx][trial_idx][iter] = loss_test\n",
    "                if iter % 1 ==0:\n",
    "                    print('Round {:3d}, Average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_test, acc_test))\n",
    "                #print(loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_glob = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "sigma = 1\n",
      "\n",
      "\n",
      "\n",
      "Learning Rate = 3e-05\n",
      "\n",
      "\n",
      "\n",
      "z_array: [-0.81  0.81]\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 0 25000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 0 25000 \n",
      "\n",
      "(T, sigma)= 0 1 )  0 -th Trial!!\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.012507893421031811\n",
      "layer1.0.conv1.weight 0.0005747529988487562\n",
      "layer1.0.conv2.weight 0.000575202206770579\n",
      "layer1.1.conv1.weight 0.0005806346630884542\n",
      "layer1.1.conv2.weight 0.0005777938705351618\n",
      "layer2.0.conv1.weight 0.0005817636847496033\n",
      "layer2.0.conv2.weight 0.00029039491588870686\n",
      "layer2.0.shortcut.0.weight 0.00520604383200407\n",
      "layer2.1.conv1.weight 0.0002895755072434743\n",
      "layer2.1.conv2.weight 0.00028944944238497154\n",
      "layer3.0.conv1.weight 0.00029013771563768387\n",
      "layer3.0.conv2.weight 0.0001444185877011882\n",
      "layer3.0.shortcut.0.weight 0.002591612981632352\n",
      "layer3.1.conv1.weight 0.0001445222232076857\n",
      "layer3.1.conv2.weight 0.00014458544966247346\n",
      "layer4.0.conv1.weight 0.0001445995488514503\n",
      "layer4.0.conv2.weight 7.238112933312853e-05\n",
      "layer4.0.shortcut.0.weight 0.00130750285461545\n",
      "layer4.1.conv1.weight 7.232099435188704e-05\n",
      "layer4.1.conv2.weight 7.232384005975392e-05\n",
      "linear.weight 0.0006493170280009508\n",
      "linear.bias 0.0007233996875584126\n",
      "conv1.weight\n",
      "(64, 3, 3, 3)\n",
      "layer1.0.conv1.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.0.conv2.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.1.conv1.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.1.conv2.weight\n",
      "(64, 64, 3, 3)\n",
      "layer2.0.conv1.weight\n",
      "(128, 64, 3, 3)\n",
      "layer2.0.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "layer2.0.shortcut.0.weight\n",
      "(128, 64, 1, 1)\n",
      "layer2.1.conv1.weight\n",
      "(128, 128, 3, 3)\n",
      "layer2.1.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "layer3.0.conv1.weight\n",
      "(256, 128, 3, 3)\n",
      "layer3.0.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "layer3.0.shortcut.0.weight\n",
      "(256, 128, 1, 1)\n",
      "layer3.1.conv1.weight\n",
      "(256, 256, 3, 3)\n",
      "layer3.1.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "layer4.0.conv1.weight\n",
      "(512, 256, 3, 3)\n",
      "layer4.0.conv2.weight\n",
      "(512, 512, 3, 3)\n",
      "layer4.0.shortcut.0.weight\n",
      "(512, 256, 1, 1)\n",
      "layer4.1.conv1.weight\n",
      "(512, 512, 3, 3)\n",
      "layer4.1.conv2.weight\n",
      "(512, 512, 3, 3)\n",
      "linear.weight\n",
      "(10, 512)\n",
      "linear.bias\n",
      "(10,)\n",
      "\n",
      "Test set: Average loss: 2.9040 \n",
      "Accuracy: 1438/10000 (14.38%)\n",
      "\n",
      "Round   0, Average loss 2.904 Test accuracy 14.380\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.012514670689900717\n",
      "layer1.0.conv1.weight 0.0005609712873895963\n",
      "layer1.0.conv2.weight 0.0005579125653538439\n",
      "layer1.1.conv1.weight 0.0005609363110529052\n",
      "layer1.1.conv2.weight 0.0005506518193417125\n",
      "layer2.0.conv1.weight 0.0005609982440041171\n",
      "layer2.0.conv2.weight 0.00026707066636946466\n",
      "layer2.0.shortcut.0.weight 0.005111037287861109\n",
      "layer2.1.conv1.weight 0.0002462049014866352\n",
      "layer2.1.conv2.weight 0.00024638410347203415\n",
      "layer3.0.conv1.weight 0.00025472955571280583\n",
      "layer3.0.conv2.weight 0.00011714913934055302\n",
      "layer3.0.shortcut.0.weight 0.0024382725823670626\n",
      "layer3.1.conv1.weight 0.00011156734803484546\n",
      "layer3.1.conv2.weight 0.00011639113331006633\n",
      "layer4.0.conv1.weight 0.00011846288624736998\n",
      "layer4.0.conv2.weight 5.346127242470781e-05\n",
      "layer4.0.shortcut.0.weight 0.0011618639109656215\n",
      "layer4.1.conv1.weight 4.828374949283898e-05\n",
      "layer4.1.conv2.weight 5.756857960174481e-05\n",
      "linear.weight 0.0006705530453473329\n",
      "linear.bias 0.0007298764307051897\n",
      "conv1.weight\n",
      "(64, 3, 3, 3)\n",
      "layer1.0.conv1.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.0.conv2.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.1.conv1.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.1.conv2.weight\n",
      "(64, 64, 3, 3)\n",
      "layer2.0.conv1.weight\n",
      "(128, 64, 3, 3)\n",
      "layer2.0.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "layer2.0.shortcut.0.weight\n",
      "(128, 64, 1, 1)\n",
      "layer2.1.conv1.weight\n",
      "(128, 128, 3, 3)\n",
      "layer2.1.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "layer3.0.conv1.weight\n",
      "(256, 128, 3, 3)\n",
      "layer3.0.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "layer3.0.shortcut.0.weight\n",
      "(256, 128, 1, 1)\n",
      "layer3.1.conv1.weight\n",
      "(256, 256, 3, 3)\n",
      "layer3.1.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "layer4.0.conv1.weight\n",
      "(512, 256, 3, 3)\n",
      "layer4.0.conv2.weight\n",
      "(512, 512, 3, 3)\n",
      "layer4.0.shortcut.0.weight\n",
      "(512, 256, 1, 1)\n",
      "layer4.1.conv1.weight\n",
      "(512, 512, 3, 3)\n",
      "layer4.1.conv2.weight\n",
      "(512, 512, 3, 3)\n",
      "linear.weight\n",
      "(10, 512)\n",
      "linear.bias\n",
      "(10,)\n",
      "\n",
      "Test set: Average loss: 530.0874 \n",
      "Accuracy: 1342/10000 (13.42%)\n",
      "\n",
      "Round   1, Average loss 530.087 Test accuracy 13.420\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.012540030258673208\n",
      "layer1.0.conv1.weight 0.0005634151813056734\n",
      "layer1.0.conv2.weight 0.000561474718981319\n",
      "layer1.1.conv1.weight 0.0005629876007636389\n",
      "layer1.1.conv2.weight 0.0005544621186951796\n",
      "layer2.0.conv1.weight 0.0005643950361344549\n",
      "layer2.0.conv2.weight 0.00027028735106190044\n",
      "layer2.0.shortcut.0.weight 0.005114121828228235\n",
      "layer2.1.conv1.weight 0.0002468541885415713\n",
      "layer2.1.conv2.weight 0.00024916712815562886\n",
      "layer3.0.conv1.weight 0.00025882496912446286\n",
      "layer3.0.conv2.weight 0.00012118794903573062\n",
      "layer3.0.shortcut.0.weight 0.002438163850456476\n",
      "layer3.1.conv1.weight 0.00011500863668819268\n",
      "layer3.1.conv2.weight 0.00012088649802737766\n",
      "layer4.0.conv1.weight 0.00012232684012916352\n",
      "layer4.0.conv2.weight 5.5663035406420626e-05\n",
      "layer4.0.shortcut.0.weight 0.0011255518766120076\n",
      "layer4.1.conv1.weight 4.7628230984426206e-05\n",
      "layer4.1.conv2.weight 5.313969450071454e-05\n",
      "linear.weight 0.0006877025123685598\n",
      "linear.bias 0.0007346043363213539\n",
      "conv1.weight\n",
      "(64, 3, 3, 3)\n",
      "layer1.0.conv1.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.0.conv2.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.1.conv1.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.1.conv2.weight\n",
      "(64, 64, 3, 3)\n",
      "layer2.0.conv1.weight\n",
      "(128, 64, 3, 3)\n",
      "layer2.0.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "layer2.0.shortcut.0.weight\n",
      "(128, 64, 1, 1)\n",
      "layer2.1.conv1.weight\n",
      "(128, 128, 3, 3)\n",
      "layer2.1.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "layer3.0.conv1.weight\n",
      "(256, 128, 3, 3)\n",
      "layer3.0.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "layer3.0.shortcut.0.weight\n",
      "(256, 128, 1, 1)\n",
      "layer3.1.conv1.weight\n",
      "(256, 256, 3, 3)\n",
      "layer3.1.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "layer4.0.conv1.weight\n",
      "(512, 256, 3, 3)\n",
      "layer4.0.conv2.weight\n",
      "(512, 512, 3, 3)\n",
      "layer4.0.shortcut.0.weight\n",
      "(512, 256, 1, 1)\n",
      "layer4.1.conv1.weight\n",
      "(512, 512, 3, 3)\n",
      "layer4.1.conv2.weight\n",
      "(512, 512, 3, 3)\n",
      "linear.weight\n",
      "(10, 512)\n",
      "linear.bias\n",
      "(10,)\n",
      "\n",
      "Test set: Average loss: 361616696.0296 \n",
      "Accuracy: 1388/10000 (13.88%)\n",
      "\n",
      "Round   2, Average loss 361616696.030 Test accuracy 13.880\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.012604097525278727\n",
      "layer1.0.conv1.weight 0.000573950664450725\n",
      "layer1.0.conv2.weight 0.0005776210067172846\n",
      "layer1.1.conv1.weight 0.0005756118334829807\n",
      "layer1.1.conv2.weight 0.0005760057311919001\n",
      "layer2.0.conv1.weight 0.0005866321217682627\n",
      "layer2.0.conv2.weight 0.0002958729552725951\n",
      "layer2.0.shortcut.0.weight 0.005161965265870094\n",
      "layer2.1.conv1.weight 0.000274319253447983\n",
      "layer2.1.conv2.weight 0.00028584724188678793\n",
      "layer3.0.conv1.weight 0.00030058374007542926\n",
      "layer3.0.conv2.weight 0.00016878042111380232\n",
      "layer3.0.shortcut.0.weight 0.002495111897587776\n",
      "layer3.1.conv1.weight 0.00017158340455757247\n",
      "layer3.1.conv2.weight 0.00019086672303577265\n",
      "layer4.0.conv1.weight 0.00019464481415020095\n",
      "layer4.0.conv2.weight 0.00010519860855614145\n",
      "layer4.0.shortcut.0.weight 0.0011245917994529009\n",
      "layer4.1.conv1.weight 8.183186097691457e-05\n",
      "layer4.1.conv2.weight 4.18498918103675e-05\n",
      "linear.weight 0.0007233439479023218\n",
      "linear.bias 0.000728498911485076\n",
      "conv1.weight\n",
      "(64, 3, 3, 3)\n",
      "layer1.0.conv1.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.0.conv2.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.1.conv1.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.1.conv2.weight\n",
      "(64, 64, 3, 3)\n",
      "layer2.0.conv1.weight\n",
      "(128, 64, 3, 3)\n",
      "layer2.0.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "layer2.0.shortcut.0.weight\n",
      "(128, 64, 1, 1)\n",
      "layer2.1.conv1.weight\n",
      "(128, 128, 3, 3)\n",
      "layer2.1.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "layer3.0.conv1.weight\n",
      "(256, 128, 3, 3)\n",
      "layer3.0.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "layer3.0.shortcut.0.weight\n",
      "(256, 128, 1, 1)\n",
      "layer3.1.conv1.weight\n",
      "(256, 256, 3, 3)\n",
      "layer3.1.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "layer4.0.conv1.weight\n",
      "(512, 256, 3, 3)\n",
      "layer4.0.conv2.weight\n",
      "(512, 512, 3, 3)\n",
      "layer4.0.shortcut.0.weight\n",
      "(512, 256, 1, 1)\n",
      "layer4.1.conv1.weight\n",
      "(512, 512, 3, 3)\n",
      "layer4.1.conv2.weight\n",
      "(512, 512, 3, 3)\n",
      "linear.weight\n",
      "(10, 512)\n",
      "linear.bias\n",
      "(10,)\n",
      "\n",
      "Test set: Average loss: 3321727063870.6689 \n",
      "Accuracy: 1374/10000 (13.74%)\n",
      "\n",
      "Round   3, Average loss 3321727063870.669 Test accuracy 13.740\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.012694581791206642\n",
      "layer1.0.conv1.weight 0.0005923147934178511\n",
      "layer1.0.conv2.weight 0.0006073806434869766\n",
      "layer1.1.conv1.weight 0.0006048585184746318\n",
      "layer1.1.conv2.weight 0.0006229858845472336\n",
      "layer2.0.conv1.weight 0.0006312445426980654\n",
      "layer2.0.conv2.weight 0.00034693126670188375\n",
      "layer2.0.shortcut.0.weight 0.0052343811839818954\n",
      "layer2.1.conv1.weight 0.0003286498960935407\n",
      "layer2.1.conv2.weight 0.000353162953009208\n",
      "layer3.0.conv1.weight 0.0003815092301617066\n",
      "layer3.0.conv2.weight 0.00026542538156112033\n",
      "layer3.0.shortcut.0.weight 0.0025894325226545334\n",
      "layer3.1.conv1.weight 0.0002942012571212318\n",
      "layer3.1.conv2.weight 0.00034528851716054813\n",
      "layer4.0.conv1.weight 0.00036919911185072526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer4.0.conv2.weight 0.0002627241839137342\n",
      "layer4.0.shortcut.0.weight 0.0012425475288182497\n",
      "layer4.1.conv1.weight 0.00021226576063781977\n",
      "layer4.1.conv2.weight 4.5178868781982194e-05\n",
      "linear.weight 0.0007840132340788842\n",
      "linear.bias 0.0007276893127709627\n",
      "conv1.weight\n",
      "(64, 3, 3, 3)\n",
      "layer1.0.conv1.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.0.conv2.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.1.conv1.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.1.conv2.weight\n",
      "(64, 64, 3, 3)\n",
      "layer2.0.conv1.weight\n",
      "(128, 64, 3, 3)\n",
      "layer2.0.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "layer2.0.shortcut.0.weight\n",
      "(128, 64, 1, 1)\n",
      "layer2.1.conv1.weight\n",
      "(128, 128, 3, 3)\n",
      "layer2.1.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "layer3.0.conv1.weight\n",
      "(256, 128, 3, 3)\n",
      "layer3.0.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "layer3.0.shortcut.0.weight\n",
      "(256, 128, 1, 1)\n",
      "layer3.1.conv1.weight\n",
      "(256, 256, 3, 3)\n",
      "layer3.1.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "layer4.0.conv1.weight\n",
      "(512, 256, 3, 3)\n",
      "layer4.0.conv2.weight\n",
      "(512, 512, 3, 3)\n",
      "layer4.0.shortcut.0.weight\n",
      "(512, 256, 1, 1)\n",
      "layer4.1.conv1.weight\n",
      "(512, 512, 3, 3)\n",
      "layer4.1.conv2.weight\n",
      "(512, 512, 3, 3)\n",
      "linear.weight\n",
      "(10, 512)\n",
      "linear.bias\n",
      "(10,)\n",
      "\n",
      "Test set: Average loss: 3063423740375833.0000 \n",
      "Accuracy: 1564/10000 (15.64%)\n",
      "\n",
      "Round   4, Average loss 3063423740375833.000 Test accuracy 15.640\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.012788651166138825\n",
      "layer1.0.conv1.weight 0.0006155960468782319\n",
      "layer1.0.conv2.weight 0.000647692009806633\n",
      "layer1.1.conv1.weight 0.0006404915928012795\n",
      "layer1.1.conv2.weight 0.0006837496637470192\n",
      "layer2.0.conv1.weight 0.0006932045540048017\n",
      "layer2.0.conv2.weight 0.00041647863367365464\n",
      "layer2.0.shortcut.0.weight 0.00532285962253809\n",
      "layer2.1.conv1.weight 0.0004074505219856898\n",
      "layer2.1.conv2.weight 0.00044678932883673243\n",
      "layer3.0.conv1.weight 0.0004885172367923789\n",
      "layer3.0.conv2.weight 0.0003913501018865241\n",
      "layer3.0.shortcut.0.weight 0.002709990832954645\n",
      "layer3.1.conv1.weight 0.000455929742505153\n",
      "layer3.1.conv2.weight 0.0005498586429489984\n",
      "layer4.0.conv1.weight 0.0006033654014269511\n",
      "layer4.0.conv2.weight 0.0004912215905884901\n",
      "layer4.0.shortcut.0.weight 0.0014493372291326523\n",
      "layer4.1.conv1.weight 0.00040775302073193923\n",
      "layer4.1.conv2.weight 5.100086693548494e-05\n",
      "linear.weight 0.0008593186736106873\n",
      "linear.bias 0.0007300072815269232\n",
      "conv1.weight\n",
      "(64, 3, 3, 3)\n",
      "layer1.0.conv1.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.0.conv2.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.1.conv1.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.1.conv2.weight\n",
      "(64, 64, 3, 3)\n",
      "layer2.0.conv1.weight\n",
      "(128, 64, 3, 3)\n",
      "layer2.0.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "layer2.0.shortcut.0.weight\n",
      "(128, 64, 1, 1)\n",
      "layer2.1.conv1.weight\n",
      "(128, 128, 3, 3)\n",
      "layer2.1.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "layer3.0.conv1.weight\n",
      "(256, 128, 3, 3)\n",
      "layer3.0.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "layer3.0.shortcut.0.weight\n",
      "(256, 128, 1, 1)\n",
      "layer3.1.conv1.weight\n",
      "(256, 256, 3, 3)\n",
      "layer3.1.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "layer4.0.conv1.weight\n",
      "(512, 256, 3, 3)\n",
      "layer4.0.conv2.weight\n",
      "(512, 512, 3, 3)\n",
      "layer4.0.shortcut.0.weight\n",
      "(512, 256, 1, 1)\n",
      "layer4.1.conv1.weight\n",
      "(512, 512, 3, 3)\n",
      "layer4.1.conv2.weight\n",
      "(512, 512, 3, 3)\n",
      "linear.weight\n",
      "(10, 512)\n",
      "linear.bias\n",
      "(10,)\n",
      "\n",
      "Test set: Average loss: 62742376373004584.0000 \n",
      "Accuracy: 1254/10000 (12.54%)\n",
      "\n",
      "Round   5, Average loss 62742376373004584.000 Test accuracy 12.540\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.012883180821383441\n",
      "layer1.0.conv1.weight 0.0006448538560006353\n",
      "layer1.0.conv2.weight 0.0006999004011352857\n",
      "layer1.1.conv1.weight 0.000692436523321602\n",
      "layer1.1.conv2.weight 0.0007650475017726421\n",
      "layer2.0.conv1.weight 0.0007728314958512783\n",
      "layer2.0.conv2.weight 0.0005045019400616487\n",
      "layer2.0.shortcut.0.weight 0.005426105111837387\n",
      "layer2.1.conv1.weight 0.0005057638821502527\n",
      "layer2.1.conv2.weight 0.0005624301524625884\n",
      "layer3.0.conv1.weight 0.0006209569465782908\n",
      "layer3.0.conv2.weight 0.0005490688814057244\n",
      "layer3.0.shortcut.0.weight 0.002857901155948639\n",
      "layer3.1.conv1.weight 0.0006554093108408981\n",
      "layer3.1.conv2.weight 0.0007993550971150398\n",
      "layer4.0.conv1.weight 0.0008894825975100199\n",
      "layer4.0.conv2.weight 0.0007795110965768496\n",
      "layer4.0.shortcut.0.weight 0.0017221251036971807\n",
      "layer4.1.conv1.weight 0.0006576328952279356\n",
      "layer4.1.conv2.weight 5.787324496648378e-05\n",
      "linear.weight 0.0009404201060533524\n",
      "linear.bias 0.0007319805212318897\n",
      "conv1.weight\n",
      "(64, 3, 3, 3)\n",
      "layer1.0.conv1.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.0.conv2.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.1.conv1.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.1.conv2.weight\n",
      "(64, 64, 3, 3)\n",
      "layer2.0.conv1.weight\n",
      "(128, 64, 3, 3)\n",
      "layer2.0.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "layer2.0.shortcut.0.weight\n",
      "(128, 64, 1, 1)\n",
      "layer2.1.conv1.weight\n",
      "(128, 128, 3, 3)\n",
      "layer2.1.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "layer3.0.conv1.weight\n",
      "(256, 128, 3, 3)\n",
      "layer3.0.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "layer3.0.shortcut.0.weight\n",
      "(256, 128, 1, 1)\n",
      "layer3.1.conv1.weight\n",
      "(256, 256, 3, 3)\n",
      "layer3.1.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "layer4.0.conv1.weight\n",
      "(512, 256, 3, 3)\n",
      "layer4.0.conv2.weight\n",
      "(512, 512, 3, 3)\n",
      "layer4.0.shortcut.0.weight\n",
      "(512, 256, 1, 1)\n",
      "layer4.1.conv1.weight\n",
      "(512, 512, 3, 3)\n",
      "layer4.1.conv2.weight\n",
      "(512, 512, 3, 3)\n",
      "linear.weight\n",
      "(10, 512)\n",
      "linear.bias\n",
      "(10,)\n",
      "\n",
      "Test set: Average loss: 1236040083671421696.0000 \n",
      "Accuracy: 1597/10000 (15.97%)\n",
      "\n",
      "Round   6, Average loss 1236040083671421696.000 Test accuracy 15.970\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.012986964649624295\n",
      "layer1.0.conv1.weight 0.0006786687817010614\n",
      "layer1.0.conv2.weight 0.0007653086342745357\n",
      "layer1.1.conv1.weight 0.0007598315262132221\n",
      "layer1.1.conv2.weight 0.0008648037910461426\n",
      "layer2.0.conv1.weight 0.0008692944215403663\n",
      "layer2.0.conv2.weight 0.0006108535453677177\n",
      "layer2.0.shortcut.0.weight 0.005548880901187658\n",
      "layer2.1.conv1.weight 0.0006251358427107334\n",
      "layer2.1.conv2.weight 0.0007020179699692461\n",
      "layer3.0.conv1.weight 0.0007793588253359\n",
      "layer3.0.conv2.weight 0.0007398282177746296\n",
      "layer3.0.shortcut.0.weight 0.003033826593309641\n",
      "layer3.1.conv1.weight 0.0008967439126637247\n",
      "layer3.1.conv2.weight 0.0011006735472215547\n",
      "layer4.0.conv1.weight 0.0012380050288306342\n",
      "layer4.0.conv2.weight 0.0011413870379328728\n",
      "layer4.0.shortcut.0.weight 0.002064432017505169\n",
      "layer4.1.conv1.weight 0.0009779419956935777\n",
      "layer4.1.conv2.weight 6.663076217389769e-05\n",
      "linear.weight 0.0010278088040649891\n",
      "linear.bias 0.0007392006926238536\n",
      "conv1.weight\n",
      "(64, 3, 3, 3)\n",
      "layer1.0.conv1.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.0.conv2.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.1.conv1.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.1.conv2.weight\n",
      "(64, 64, 3, 3)\n",
      "layer2.0.conv1.weight\n",
      "(128, 64, 3, 3)\n",
      "layer2.0.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "layer2.0.shortcut.0.weight\n",
      "(128, 64, 1, 1)\n",
      "layer2.1.conv1.weight\n",
      "(128, 128, 3, 3)\n",
      "layer2.1.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "layer3.0.conv1.weight\n",
      "(256, 128, 3, 3)\n",
      "layer3.0.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "layer3.0.shortcut.0.weight\n",
      "(256, 128, 1, 1)\n",
      "layer3.1.conv1.weight\n",
      "(256, 256, 3, 3)\n",
      "layer3.1.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "layer4.0.conv1.weight\n",
      "(512, 256, 3, 3)\n",
      "layer4.0.conv2.weight\n",
      "(512, 512, 3, 3)\n",
      "layer4.0.shortcut.0.weight\n",
      "(512, 256, 1, 1)\n",
      "layer4.1.conv1.weight\n",
      "(512, 512, 3, 3)\n",
      "layer4.1.conv2.weight\n",
      "(512, 512, 3, 3)\n",
      "linear.weight\n",
      "(10, 512)\n",
      "linear.bias\n",
      "(10,)\n",
      "\n",
      "Test set: Average loss: 17746801398112815104.0000 \n",
      "Accuracy: 1878/10000 (18.78%)\n",
      "\n",
      "Round   7, Average loss 17746801398112815104.000 Test accuracy 18.780\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.013094830292242544\n",
      "layer1.0.conv1.weight 0.0007159875498877631\n",
      "layer1.0.conv2.weight 0.0008410602394077512\n",
      "layer1.1.conv1.weight 0.0008384664025571612\n",
      "layer1.1.conv2.weight 0.0009760420976413621\n",
      "layer2.0.conv1.weight 0.0009791860356926918\n",
      "layer2.0.conv2.weight 0.0007317108619544241\n",
      "layer2.0.shortcut.0.weight 0.005683626048266888\n",
      "layer2.1.conv1.weight 0.0007592124554018179\n",
      "layer2.1.conv2.weight 0.0008582250318593449\n",
      "layer3.0.conv1.weight 0.000955300819542673\n",
      "layer3.0.conv2.weight 0.0009544194779462285\n",
      "layer3.0.shortcut.0.weight 0.0032293819822371006\n",
      "layer3.1.conv1.weight 0.0011687628510925504\n",
      "layer3.1.conv2.weight 0.0014389754376477664\n",
      "layer4.0.conv1.weight 0.00163043559425407\n",
      "layer4.0.conv2.weight 0.001557408521572749\n",
      "layer4.0.shortcut.0.weight 0.0024565784260630608\n",
      "layer4.1.conv1.weight 0.0013510586900843515\n",
      "layer4.1.conv2.weight 7.679733163159754e-05\n",
      "linear.weight 0.0011083157733082771\n",
      "linear.bias 0.000746072456240654\n",
      "conv1.weight\n",
      "(64, 3, 3, 3)\n",
      "layer1.0.conv1.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.0.conv2.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.1.conv1.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.1.conv2.weight\n",
      "(64, 64, 3, 3)\n",
      "layer2.0.conv1.weight\n",
      "(128, 64, 3, 3)\n",
      "layer2.0.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "layer2.0.shortcut.0.weight\n",
      "(128, 64, 1, 1)\n",
      "layer2.1.conv1.weight\n",
      "(128, 128, 3, 3)\n",
      "layer2.1.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "layer3.0.conv1.weight\n",
      "(256, 128, 3, 3)\n",
      "layer3.0.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "layer3.0.shortcut.0.weight\n",
      "(256, 128, 1, 1)\n",
      "layer3.1.conv1.weight\n",
      "(256, 256, 3, 3)\n",
      "layer3.1.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "layer4.0.conv1.weight\n",
      "(512, 256, 3, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer4.0.conv2.weight\n",
      "(512, 512, 3, 3)\n",
      "layer4.0.shortcut.0.weight\n",
      "(512, 256, 1, 1)\n",
      "layer4.1.conv1.weight\n",
      "(512, 512, 3, 3)\n",
      "layer4.1.conv2.weight\n",
      "(512, 512, 3, 3)\n",
      "linear.weight\n",
      "(10, 512)\n",
      "linear.bias\n",
      "(10,)\n",
      "\n",
      "Test set: Average loss: 187053394777965985792.0000 \n",
      "Accuracy: 1598/10000 (15.98%)\n",
      "\n",
      "Round   8, Average loss 187053394777965985792.000 Test accuracy 15.980\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.013180753699055425\n",
      "layer1.0.conv1.weight 0.0007579385613401731\n",
      "layer1.0.conv2.weight 0.0009280374894539515\n",
      "layer1.1.conv1.weight 0.0009278450161218643\n",
      "layer1.1.conv2.weight 0.0010993165067500537\n",
      "layer2.0.conv1.weight 0.001101880023876826\n",
      "layer2.0.conv2.weight 0.0008661341336038378\n",
      "layer2.0.shortcut.0.weight 0.005829500034451485\n",
      "layer2.1.conv1.weight 0.0009077343468864759\n",
      "layer2.1.conv2.weight 0.0010309509105152553\n",
      "layer3.0.conv1.weight 0.001149219771226247\n",
      "layer3.0.conv2.weight 0.0011939717870619562\n",
      "layer3.0.shortcut.0.weight 0.0034440497402101755\n",
      "layer3.1.conv1.weight 0.0014725239533517095\n",
      "layer3.1.conv2.weight 0.001815785136487749\n",
      "layer4.0.conv1.weight 0.0020682488878568015\n",
      "layer4.0.conv2.weight 0.0020310026076104906\n",
      "layer4.0.shortcut.0.weight 0.0028992483858019114\n",
      "layer4.1.conv1.weight 0.0017860279315047795\n",
      "layer4.1.conv2.weight 8.94164614793327e-05\n",
      "linear.weight 0.0011729095131158829\n",
      "linear.bias 0.0007544018793851137\n",
      "conv1.weight\n",
      "(64, 3, 3, 3)\n",
      "layer1.0.conv1.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.0.conv2.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.1.conv1.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.1.conv2.weight\n",
      "(64, 64, 3, 3)\n",
      "layer2.0.conv1.weight\n",
      "(128, 64, 3, 3)\n",
      "layer2.0.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "layer2.0.shortcut.0.weight\n",
      "(128, 64, 1, 1)\n",
      "layer2.1.conv1.weight\n",
      "(128, 128, 3, 3)\n",
      "layer2.1.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "layer3.0.conv1.weight\n",
      "(256, 128, 3, 3)\n",
      "layer3.0.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "layer3.0.shortcut.0.weight\n",
      "(256, 128, 1, 1)\n",
      "layer3.1.conv1.weight\n",
      "(256, 256, 3, 3)\n",
      "layer3.1.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "layer4.0.conv1.weight\n",
      "(512, 256, 3, 3)\n",
      "layer4.0.conv2.weight\n",
      "(512, 512, 3, 3)\n",
      "layer4.0.shortcut.0.weight\n",
      "(512, 256, 1, 1)\n",
      "layer4.1.conv1.weight\n",
      "(512, 512, 3, 3)\n",
      "layer4.1.conv2.weight\n",
      "(512, 512, 3, 3)\n",
      "linear.weight\n",
      "(10, 512)\n",
      "linear.bias\n",
      "(10,)\n",
      "\n",
      "Test set: Average loss: 1549300178640735305728.0000 \n",
      "Accuracy: 1825/10000 (18.25%)\n",
      "\n",
      "Round   9, Average loss 1549300178640735305728.000 Test accuracy 18.250\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.013224511234848588\n",
      "layer1.0.conv1.weight 0.0007940593382550611\n",
      "layer1.0.conv2.weight 0.0010246456497245366\n",
      "layer1.1.conv1.weight 0.0010263615598281224\n",
      "layer1.1.conv2.weight 0.0012363667289415996\n",
      "layer2.0.conv1.weight 0.0012390599068668154\n",
      "layer2.0.conv2.weight 0.0010160189121961594\n",
      "layer2.0.shortcut.0.weight 0.005989210680127144\n",
      "layer2.1.conv1.weight 0.0010730453456441562\n",
      "layer2.1.conv2.weight 0.0012229055994086796\n",
      "layer3.0.conv1.weight 0.001364719226128525\n",
      "layer3.0.conv2.weight 0.0014627485846479733\n",
      "layer3.0.shortcut.0.weight 0.003682043170556426\n",
      "layer3.1.conv1.weight 0.0018135636217064327\n",
      "layer3.1.conv2.weight 0.002238791642917527\n",
      "layer4.0.conv1.weight 0.002561591151687834\n",
      "layer4.0.conv2.weight 0.002572381455037329\n",
      "layer4.0.shortcut.0.weight 0.003402180504053831\n",
      "layer4.1.conv1.weight 0.0022938021769126258\n",
      "layer4.1.conv2.weight 0.0001039829674280352\n",
      "linear.weight 0.0011890463531017304\n",
      "linear.bias 0.00076152547262609\n",
      "conv1.weight\n",
      "(64, 3, 3, 3)\n",
      "layer1.0.conv1.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.0.conv2.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.1.conv1.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.1.conv2.weight\n",
      "(64, 64, 3, 3)\n",
      "layer2.0.conv1.weight\n",
      "(128, 64, 3, 3)\n",
      "layer2.0.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "layer2.0.shortcut.0.weight\n",
      "(128, 64, 1, 1)\n",
      "layer2.1.conv1.weight\n",
      "(128, 128, 3, 3)\n",
      "layer2.1.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "layer3.0.conv1.weight\n",
      "(256, 128, 3, 3)\n",
      "layer3.0.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "layer3.0.shortcut.0.weight\n",
      "(256, 128, 1, 1)\n",
      "layer3.1.conv1.weight\n",
      "(256, 256, 3, 3)\n",
      "layer3.1.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "layer4.0.conv1.weight\n",
      "(512, 256, 3, 3)\n",
      "layer4.0.conv2.weight\n",
      "(512, 512, 3, 3)\n",
      "layer4.0.shortcut.0.weight\n",
      "(512, 256, 1, 1)\n",
      "layer4.1.conv1.weight\n",
      "(512, 512, 3, 3)\n",
      "layer4.1.conv2.weight\n",
      "(512, 512, 3, 3)\n",
      "linear.weight\n",
      "(10, 512)\n",
      "linear.bias\n",
      "(10,)\n",
      "\n",
      "Test set: Average loss: 6746847917559412424704.0000 \n",
      "Accuracy: 1851/10000 (18.51%)\n",
      "\n",
      "Round  10, Average loss 6746847917559412424704.000 Test accuracy 18.510\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.01323158652694137\n",
      "layer1.0.conv1.weight 0.0008055015156666437\n",
      "layer1.0.conv2.weight 0.001092168812950452\n",
      "layer1.1.conv1.weight 0.0010827062651515007\n",
      "layer1.1.conv2.weight 0.0013264956780605847\n",
      "layer2.0.conv1.weight 0.0013750987127423286\n",
      "layer2.0.conv2.weight 0.00117741162992186\n",
      "layer2.0.shortcut.0.weight 0.006160004064440727\n",
      "layer2.1.conv1.weight 0.001250759181049135\n",
      "layer2.1.conv2.weight 0.0014302969599763553\n",
      "layer3.0.conv1.weight 0.0015992097970512179\n",
      "layer3.0.conv2.weight 0.0017581631739934285\n",
      "layer3.0.shortcut.0.weight 0.003939691465348005\n",
      "layer3.1.conv1.weight 0.0021874679045544732\n",
      "layer3.1.conv2.weight 0.0027015337513552774\n",
      "layer4.0.conv1.weight 0.0031015239655971527\n",
      "layer4.0.conv2.weight 0.0031723553935686746\n",
      "layer4.0.shortcut.0.weight 0.003957362845540047\n",
      "layer4.1.conv1.weight 0.0028620024936066735\n",
      "layer4.1.conv2.weight 0.00012052811992665131\n",
      "linear.weight 0.001193883828818798\n",
      "linear.bias 0.0007741193287074566\n",
      "conv1.weight\n",
      "(64, 3, 3, 3)\n",
      "layer1.0.conv1.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.0.conv2.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.1.conv1.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.1.conv2.weight\n",
      "(64, 64, 3, 3)\n",
      "layer2.0.conv1.weight\n",
      "(128, 64, 3, 3)\n",
      "layer2.0.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "layer2.0.shortcut.0.weight\n",
      "(128, 64, 1, 1)\n",
      "layer2.1.conv1.weight\n",
      "(128, 128, 3, 3)\n",
      "layer2.1.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "layer3.0.conv1.weight\n",
      "(256, 128, 3, 3)\n",
      "layer3.0.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "layer3.0.shortcut.0.weight\n",
      "(256, 128, 1, 1)\n",
      "layer3.1.conv1.weight\n",
      "(256, 256, 3, 3)\n",
      "layer3.1.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "layer4.0.conv1.weight\n",
      "(512, 256, 3, 3)\n",
      "layer4.0.conv2.weight\n",
      "(512, 512, 3, 3)\n",
      "layer4.0.shortcut.0.weight\n",
      "(512, 256, 1, 1)\n",
      "layer4.1.conv1.weight\n",
      "(512, 512, 3, 3)\n",
      "layer4.1.conv2.weight\n",
      "(512, 512, 3, 3)\n",
      "linear.weight\n",
      "(10, 512)\n",
      "linear.bias\n",
      "(10,)\n",
      "\n",
      "Test set: Average loss: 18310369066573153435648.0000 \n",
      "Accuracy: 1907/10000 (19.07%)\n",
      "\n",
      "Round  11, Average loss 18310369066573153435648.000 Test accuracy 19.070\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.013232470662505538\n",
      "layer1.0.conv1.weight 0.0008092132500476307\n",
      "layer1.0.conv2.weight 0.0011339690536260605\n",
      "layer1.1.conv1.weight 0.001107767845193545\n",
      "layer1.1.conv2.weight 0.0013735218801432187\n",
      "layer2.0.conv1.weight 0.0014450013016661007\n",
      "layer2.0.conv2.weight 0.0013144627834359806\n",
      "layer2.0.shortcut.0.weight 0.0063376035541296005\n",
      "layer2.1.conv1.weight 0.0014077918604016304\n",
      "layer2.1.conv2.weight 0.0016208857090936766\n",
      "layer3.0.conv1.weight 0.00184582339392768\n",
      "layer3.0.conv2.weight 0.0020696731905142465\n",
      "layer3.0.shortcut.0.weight 0.004210286773741245\n",
      "layer3.1.conv1.weight 0.0025817702213923135\n",
      "layer3.1.conv2.weight 0.0031898168640004266\n",
      "layer4.0.conv1.weight 0.0036728535261419085\n",
      "layer4.0.conv2.weight 0.0038134782678551143\n",
      "layer4.0.shortcut.0.weight 0.004546672105789185\n",
      "layer4.1.conv1.weight 0.0034850388765335083\n",
      "layer4.1.conv2.weight 0.0001385689376749926\n",
      "linear.weight 0.0011980765499174594\n",
      "linear.bias 0.0007878366857767105\n",
      "conv1.weight\n",
      "(64, 3, 3, 3)\n",
      "layer1.0.conv1.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.0.conv2.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.1.conv1.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.1.conv2.weight\n",
      "(64, 64, 3, 3)\n",
      "layer2.0.conv1.weight\n",
      "(128, 64, 3, 3)\n",
      "layer2.0.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "layer2.0.shortcut.0.weight\n",
      "(128, 64, 1, 1)\n",
      "layer2.1.conv1.weight\n",
      "(128, 128, 3, 3)\n",
      "layer2.1.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "layer3.0.conv1.weight\n",
      "(256, 128, 3, 3)\n",
      "layer3.0.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "layer3.0.shortcut.0.weight\n",
      "(256, 128, 1, 1)\n",
      "layer3.1.conv1.weight\n",
      "(256, 256, 3, 3)\n",
      "layer3.1.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "layer4.0.conv1.weight\n",
      "(512, 256, 3, 3)\n",
      "layer4.0.conv2.weight\n",
      "(512, 512, 3, 3)\n",
      "layer4.0.shortcut.0.weight\n",
      "(512, 256, 1, 1)\n",
      "layer4.1.conv1.weight\n",
      "(512, 512, 3, 3)\n",
      "layer4.1.conv2.weight\n",
      "(512, 512, 3, 3)\n",
      "linear.weight\n",
      "(10, 512)\n",
      "linear.bias\n",
      "(10,)\n",
      "\n",
      "Test set: Average loss: 38164191220083881672704.0000 \n",
      "Accuracy: 1779/10000 (17.79%)\n",
      "\n",
      "Round  12, Average loss 38164191220083881672704.000 Test accuracy 17.790\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.013232707977294922\n",
      "layer1.0.conv1.weight 0.000810369745724731\n",
      "layer1.0.conv2.weight 0.0011517944642239148\n",
      "layer1.1.conv1.weight 0.0011152226684821977\n",
      "layer1.1.conv2.weight 0.0014079142775800494\n",
      "layer2.0.conv1.weight 0.0014793512721856434\n",
      "layer2.0.conv2.weight 0.0013918898378809292\n",
      "layer2.0.shortcut.0.weight 0.006518654990941286\n",
      "layer2.1.conv1.weight 0.001489016641345289\n",
      "layer2.1.conv2.weight 0.0017296747407979434\n",
      "layer3.0.conv1.weight 0.00206429407828384\n",
      "layer3.0.conv2.weight 0.0023916076040930217\n",
      "layer3.0.shortcut.0.weight 0.004488659556955099\n",
      "layer3.1.conv1.weight 0.002991603273484442\n",
      "layer3.1.conv2.weight 0.003698950840367211\n",
      "layer4.0.conv1.weight 0.004270184371206496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer4.0.conv2.weight 0.004489253792497847\n",
      "layer4.0.shortcut.0.weight 0.005166139453649521\n",
      "layer4.1.conv1.weight 0.00415880067480935\n",
      "layer4.1.conv2.weight 0.00015244228092746602\n",
      "linear.weight 0.001197642832994461\n",
      "linear.bias 0.0008019445464015007\n",
      "conv1.weight\n",
      "(64, 3, 3, 3)\n",
      "layer1.0.conv1.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.0.conv2.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.1.conv1.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.1.conv2.weight\n",
      "(64, 64, 3, 3)\n",
      "layer2.0.conv1.weight\n",
      "(128, 64, 3, 3)\n",
      "layer2.0.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "layer2.0.shortcut.0.weight\n",
      "(128, 64, 1, 1)\n",
      "layer2.1.conv1.weight\n",
      "(128, 128, 3, 3)\n",
      "layer2.1.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "layer3.0.conv1.weight\n",
      "(256, 128, 3, 3)\n",
      "layer3.0.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "layer3.0.shortcut.0.weight\n",
      "(256, 128, 1, 1)\n",
      "layer3.1.conv1.weight\n",
      "(256, 256, 3, 3)\n",
      "layer3.1.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "layer4.0.conv1.weight\n",
      "(512, 256, 3, 3)\n",
      "layer4.0.conv2.weight\n",
      "(512, 512, 3, 3)\n",
      "layer4.0.shortcut.0.weight\n",
      "(512, 256, 1, 1)\n",
      "layer4.1.conv1.weight\n",
      "(512, 512, 3, 3)\n",
      "layer4.1.conv2.weight\n",
      "(512, 512, 3, 3)\n",
      "linear.weight\n",
      "(10, 512)\n",
      "linear.bias\n",
      "(10,)\n",
      "\n",
      "Test set: Average loss: 66412253751352529780736.0000 \n",
      "Accuracy: 1964/10000 (19.64%)\n",
      "\n",
      "Round  13, Average loss 66412253751352529780736.000 Test accuracy 19.640\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.013232780827416314\n",
      "layer1.0.conv1.weight 0.0008107487940125995\n",
      "layer1.0.conv2.weight 0.001157774796916379\n",
      "layer1.1.conv1.weight 0.0011176011628574794\n",
      "layer1.1.conv2.weight 0.0014320301512877147\n",
      "layer2.0.conv1.weight 0.0015027756906217998\n",
      "layer2.0.conv2.weight 0.0014307049827443229\n",
      "layer2.0.shortcut.0.weight 0.006705815438181162\n",
      "layer2.1.conv1.weight 0.0015332415286037657\n",
      "layer2.1.conv2.weight 0.0017949764927228291\n",
      "layer3.0.conv1.weight 0.0022491239425208834\n",
      "layer3.0.conv2.weight 0.002723198797967699\n",
      "layer3.0.shortcut.0.weight 0.004781689960509539\n",
      "layer3.1.conv1.weight 0.0034242222706476846\n",
      "layer3.1.conv2.weight 0.004234090447425842\n",
      "layer4.0.conv1.weight 0.004897379626830419\n",
      "layer4.0.conv2.weight 0.005204233444399304\n",
      "layer4.0.shortcut.0.weight 0.005820815451443195\n",
      "layer4.1.conv1.weight 0.004883884969684813\n",
      "layer4.1.conv2.weight 0.00016185412338624397\n",
      "linear.weight 0.001196485012769699\n",
      "linear.bias 0.0008140305057168007\n",
      "conv1.weight\n",
      "(64, 3, 3, 3)\n",
      "layer1.0.conv1.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.0.conv2.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.1.conv1.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.1.conv2.weight\n",
      "(64, 64, 3, 3)\n",
      "layer2.0.conv1.weight\n",
      "(128, 64, 3, 3)\n",
      "layer2.0.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "layer2.0.shortcut.0.weight\n",
      "(128, 64, 1, 1)\n",
      "layer2.1.conv1.weight\n",
      "(128, 128, 3, 3)\n",
      "layer2.1.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "layer3.0.conv1.weight\n",
      "(256, 128, 3, 3)\n",
      "layer3.0.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "layer3.0.shortcut.0.weight\n",
      "(256, 128, 1, 1)\n",
      "layer3.1.conv1.weight\n",
      "(256, 256, 3, 3)\n",
      "layer3.1.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "layer4.0.conv1.weight\n",
      "(512, 256, 3, 3)\n",
      "layer4.0.conv2.weight\n",
      "(512, 512, 3, 3)\n",
      "layer4.0.shortcut.0.weight\n",
      "(512, 256, 1, 1)\n",
      "layer4.1.conv1.weight\n",
      "(512, 512, 3, 3)\n",
      "layer4.1.conv2.weight\n",
      "(512, 512, 3, 3)\n",
      "linear.weight\n",
      "(10, 512)\n",
      "linear.bias\n",
      "(10,)\n",
      "\n",
      "Test set: Average loss: 101850527098040618057728.0000 \n",
      "Accuracy: 1704/10000 (17.04%)\n",
      "\n",
      "Round  14, Average loss 101850527098040618057728.000 Test accuracy 17.040\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.013232773100888287\n",
      "layer1.0.conv1.weight 0.0008108886476192209\n",
      "layer1.0.conv2.weight 0.0011602935070792835\n",
      "layer1.1.conv1.weight 0.0011183043114013141\n",
      "layer1.1.conv2.weight 0.0014441203739908007\n",
      "layer2.0.conv1.weight 0.0015236716717481613\n",
      "layer2.0.conv2.weight 0.001458340014020602\n",
      "layer2.0.shortcut.0.weight 0.00689964834600687\n",
      "layer2.1.conv1.weight 0.0015601485760675536\n",
      "layer2.1.conv2.weight 0.001831906537214915\n",
      "layer3.0.conv1.weight 0.0024081052591403327\n",
      "layer3.0.conv2.weight 0.0030202625526322257\n",
      "layer3.0.shortcut.0.weight 0.005090008955448866\n",
      "layer3.1.conv1.weight 0.0038518342706892225\n",
      "layer3.1.conv2.weight 0.00479635637667444\n",
      "layer4.0.conv1.weight 0.0055555204550425214\n",
      "layer4.0.conv2.weight 0.00595870821012391\n",
      "layer4.0.shortcut.0.weight 0.0065113091841340065\n",
      "layer4.1.conv1.weight 0.005661384099059635\n",
      "layer4.1.conv2.weight 0.00016892008069488738\n",
      "linear.weight 0.0011982872150838376\n",
      "linear.bias 0.000822773203253746\n",
      "conv1.weight\n",
      "(64, 3, 3, 3)\n",
      "layer1.0.conv1.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.0.conv2.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.1.conv1.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.1.conv2.weight\n",
      "(64, 64, 3, 3)\n",
      "layer2.0.conv1.weight\n",
      "(128, 64, 3, 3)\n",
      "layer2.0.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "layer2.0.shortcut.0.weight\n",
      "(128, 64, 1, 1)\n",
      "layer2.1.conv1.weight\n",
      "(128, 128, 3, 3)\n",
      "layer2.1.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "layer3.0.conv1.weight\n",
      "(256, 128, 3, 3)\n",
      "layer3.0.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "layer3.0.shortcut.0.weight\n",
      "(256, 128, 1, 1)\n",
      "layer3.1.conv1.weight\n",
      "(256, 256, 3, 3)\n",
      "layer3.1.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "layer4.0.conv1.weight\n",
      "(512, 256, 3, 3)\n",
      "layer4.0.conv2.weight\n",
      "(512, 512, 3, 3)\n",
      "layer4.0.shortcut.0.weight\n",
      "(512, 256, 1, 1)\n",
      "layer4.1.conv1.weight\n",
      "(512, 512, 3, 3)\n",
      "layer4.1.conv2.weight\n",
      "(512, 512, 3, 3)\n",
      "linear.weight\n",
      "(10, 512)\n",
      "linear.bias\n",
      "(10,)\n",
      "\n",
      "Test set: Average loss: 141620670713569543716864.0000 \n",
      "Accuracy: 1839/10000 (18.39%)\n",
      "\n",
      "Round  15, Average loss 141620670713569543716864.000 Test accuracy 18.390\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.013232780827416314\n",
      "layer1.0.conv1.weight 0.0008109737084143691\n",
      "layer1.0.conv2.weight 0.0011613979521724912\n",
      "layer1.1.conv1.weight 0.001118535693320963\n",
      "layer1.1.conv2.weight 0.0014497364560763042\n",
      "layer2.0.conv1.weight 0.0015398648877938588\n",
      "layer2.0.conv2.weight 0.0014767697494890955\n",
      "layer2.0.shortcut.0.weight 0.007081734947860241\n",
      "layer2.1.conv1.weight 0.0015809087910585934\n",
      "layer2.1.conv2.weight 0.0018548071384429932\n",
      "layer3.0.conv1.weight 0.0024954966372913783\n",
      "layer3.0.conv2.weight 0.0032795601420932347\n",
      "layer3.0.shortcut.0.weight 0.005407919641584158\n",
      "layer3.1.conv1.weight 0.004225739588340123\n",
      "layer3.1.conv2.weight 0.005312432017591264\n",
      "layer4.0.conv1.weight 0.006237910853491889\n",
      "layer4.0.conv2.weight 0.0067447639173931545\n",
      "layer4.0.shortcut.0.weight 0.007230802439153194\n",
      "layer4.1.conv1.weight 0.006485289583603541\n",
      "layer4.1.conv2.weight 0.00017438728051880995\n",
      "linear.weight 0.0012005441822111607\n",
      "linear.bias 0.0008260141126811505\n",
      "conv1.weight\n",
      "(64, 3, 3, 3)\n",
      "layer1.0.conv1.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.0.conv2.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.1.conv1.weight\n",
      "(64, 64, 3, 3)\n",
      "layer1.1.conv2.weight\n",
      "(64, 64, 3, 3)\n",
      "layer2.0.conv1.weight\n",
      "(128, 64, 3, 3)\n",
      "layer2.0.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "layer2.0.shortcut.0.weight\n",
      "(128, 64, 1, 1)\n",
      "layer2.1.conv1.weight\n",
      "(128, 128, 3, 3)\n",
      "layer2.1.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "layer3.0.conv1.weight\n",
      "(256, 128, 3, 3)\n",
      "layer3.0.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "layer3.0.shortcut.0.weight\n",
      "(256, 128, 1, 1)\n",
      "layer3.1.conv1.weight\n",
      "(256, 256, 3, 3)\n",
      "layer3.1.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "layer4.0.conv1.weight\n",
      "(512, 256, 3, 3)\n",
      "layer4.0.conv2.weight\n",
      "(512, 512, 3, 3)\n",
      "layer4.0.shortcut.0.weight\n",
      "(512, 256, 1, 1)\n",
      "layer4.1.conv1.weight\n",
      "(512, 512, 3, 3)\n",
      "layer4.1.conv2.weight\n",
      "(512, 512, 3, 3)\n",
      "linear.weight\n",
      "(10, 512)\n",
      "linear.bias\n",
      "(10,)\n",
      "\n",
      "Test set: Average loss: 184503479238330220019712.0000 \n",
      "Accuracy: 1371/10000 (13.71%)\n",
      "\n",
      "Round  16, Average loss 184503479238330220019712.000 Test accuracy 13.710\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.013232779723626596\n",
      "layer1.0.conv1.weight 0.0008110343996021482\n",
      "layer1.0.conv2.weight 0.0011618473670548862\n",
      "layer1.1.conv1.weight 0.0011186030589871937\n",
      "layer1.1.conv2.weight 0.0014530389259258907\n",
      "layer2.0.conv1.weight 0.0015486074197623464\n",
      "layer2.0.conv2.weight 0.0014910422679450777\n",
      "layer2.0.shortcut.0.weight 0.007223831489682198\n",
      "layer2.1.conv1.weight 0.001598040262858073\n",
      "layer2.1.conv2.weight 0.001872193275226487\n",
      "layer3.0.conv1.weight 0.002558655415972074\n",
      "layer3.0.conv2.weight 0.0035236701369285583\n",
      "layer3.0.shortcut.0.weight 0.005735590122640133\n",
      "layer3.1.conv1.weight 0.004554118547174666\n",
      "layer3.1.conv2.weight 0.005773621300856273\n",
      "layer4.0.conv1.weight 0.006947040557861328\n",
      "layer4.0.conv2.weight 0.007565221024884118\n",
      "layer4.0.shortcut.0.weight 0.007982796058058739\n",
      "layer4.1.conv1.weight 0.007361188530921936\n",
      "layer4.1.conv2.weight 0.00017897082337488732\n",
      "linear.weight 0.0012031638063490391\n",
      "linear.bias 0.0008241824805736541\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-f35f4af6c57d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    116\u001b[0m                     \u001b[0mlocal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLocalUpdate_with_BACC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_tilde\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_tilde\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;31m#                     w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m                     \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlocal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet_glob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m                     \u001b[0mw_locals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m                     \u001b[0mloss_locals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Jinhyun_DESKTOP\\06.Github\\CodedPrivateNN\\models\\Update.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, net)\u001b[0m\n\u001b[0;32m    165\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m                 '''\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \"\"\"\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "K = 2\n",
    "T = 0\n",
    "sigma = 0.1\n",
    "Noise_Alloc = []\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K+T))\n",
    "# alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "\n",
    "alpha_array = np.array([-0.57, 0.57])\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "\n",
    "N_array = [2]\n",
    "B_array = [0.5]\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 50\n",
    "\n",
    "lr_array = [0.00003, 0.0003] # 0.001 is the bset\n",
    "\n",
    "sigma_array = [1]\n",
    "# lr_array = [0.1, 0.01, 0.005, 0.001,0.0005]\n",
    "\n",
    "\n",
    "loss_test_arr_K2_G1_T0 = np.zeros((len(sigma_array),len(lr_array),N_trials,N_epochs))\n",
    "acc_test_arr_K2_G1_T0  = np.zeros((len(sigma_array),len(lr_array),N_trials,N_epochs))\n",
    "\n",
    "N = 2\n",
    "for sigma_idx in range(len(sigma_array)):\n",
    "    \n",
    "    sigma = sigma_array[sigma_idx]\n",
    "    \n",
    "    print('\\n\\n')\n",
    "    print('sigma =',sigma)\n",
    "    print('\\n\\n')\n",
    "           \n",
    "        \n",
    "    # print(\"alpha_array: \",alpha_array,'\\n')\n",
    "    \n",
    "    \n",
    "    for lr_idx in range(len(lr_array)):\n",
    "        \n",
    "        args.lr = lr_array[lr_idx]\n",
    "        \n",
    "        print('Learning Rate =',args.lr)\n",
    "        print('\\n\\n')\n",
    "#         while(len(z_array)<N):\n",
    "#             z_tmp = np.random.uniform(-1,1,1)\n",
    "#             MIS_tmp = MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_tmp], 1,sigma)\n",
    "#             if MIS_tmp < B and MIS_tmp > 0.1:\n",
    "#                 z_array.append(z_tmp[0])\n",
    "#         \n",
    "#         z_array = np.sort(z_array)\n",
    "#         print(N_idx,'!!!')\n",
    "#         if N_idx==0:\n",
    "# #             z_array = np.array([-0.94,-0.534,0.534, 0.94])\n",
    "# #         elif N_idx==1:\n",
    "# #             z_array = np.array([-0.94, -0.73, 0.73, 0.94])\n",
    "#         elif N_idx==1:\n",
    "#             z_array = np.array([-0.94, -0.125, 0.125, 0.94])\n",
    "#         else:\n",
    "# #             z_array = np.array([-0.94, -0.73, -0.534, -0.125, 0.125, 0.534, 0.73, 0.94])\n",
    "# #             z_array = np.array([-0.9, -0.81, -0.22, -0.20, 0.20, 0.22, 0.81, 0.9])\n",
    "        \n",
    "        z_array = np.array([-0.81, 0.81])\n",
    "        \n",
    "        print('z_array:',z_array)\n",
    "#         if sigma != 0:\n",
    "#             for j in range(len(z_array)):\n",
    "#                 print(MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_array[j]], 1,sigma))\n",
    "\n",
    "        \n",
    "        _Noise_label = np.ones((25000*T,10)) * 0.1\n",
    "\n",
    "        X_tilde,a,b = BACC_Enc_Data_v3(encoding_input_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde,a,b = BACC_Enc_Data_v3(encoding_label_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "\n",
    "        m = N # m is the number of received result @ master\n",
    "    #     print('number of results:',m)\n",
    "\n",
    "        for trial_idx in range(N_trials):\n",
    "            print('(T, sigma)=',T,sigma,') ',trial_idx,'-th Trial!!')\n",
    "\n",
    "            net_glob = ResNet18_without_BN()\n",
    "            net_glob.cuda()\n",
    "            net_glob.train()\n",
    "\n",
    "            # copy weights\n",
    "            w_glob = net_glob.state_dict()\n",
    "\n",
    "            for iter in range(N_epochs): #args.epochs\n",
    "                w_locals, loss_locals = [], []\n",
    "                idxs_users = np.random.choice(range(N), m, replace=False)\n",
    "                idxs_users = np.sort(idxs_users)\n",
    "                print('selected users:',idxs_users)\n",
    "                \n",
    "                coded_net = BACC_Enc_Model_withNoise_v4(net_glob.cuda(), N, K, T, sigma, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "                dec_z_array = []\n",
    "                for idx in idxs_users: #for idx in range(N):\n",
    "            #         print(idx)\n",
    "                    local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[idx,:,:], label=y_tilde[idx,:,:])\n",
    "#                     w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "                    w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                    w_locals.append(copy.deepcopy(w))\n",
    "                    loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                    dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "                # update global weights\n",
    "                #w_glob = FedAvg(w_locals)\n",
    "                w_glob = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "\n",
    "                # copy weight to net_glob\n",
    "                net_glob.load_state_dict(w_glob)\n",
    "\n",
    "                # print loss\n",
    "            #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "            #     loss_train_arr.append(loss_train)\n",
    "\n",
    "                acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "                acc_test_arr_K2_G1_T0[sigma_idx][lr_idx][trial_idx][iter] = acc_test\n",
    "                loss_test_arr_K2_G1_T0[sigma_idx][lr_idx][trial_idx][iter] = loss_test\n",
    "                if iter % 1 ==0:\n",
    "                    print('Round {:3d}, Average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_test, acc_test))\n",
    "                #print(loss_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. K=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "class my_argument:    \n",
    "    epochs    = 16    #\"rounds of training\"\n",
    "    num_users = 4  # \"number of users: K\"\n",
    "    frac      = 0.5 #\"the fraction of clients: C\"\n",
    "    local_ep  = 1 #\"the number of local epochs: E\"\n",
    "    local_bs  = 50  #\"local batch size: B\"\n",
    "    bs        = 50 #\"test batch size\"\n",
    "    lr        = 0.001 #\"learning rate\"\n",
    "    momentum  = 0.5 # \"SGD momentum (default: 0.5)\"\n",
    "    weight_decay = 5e-4\n",
    "    split     = 'user' # \"train-test split type, user or sample\"\n",
    "    opt='ADAM'\n",
    "    loss='Custom' # 'Custom' or 'Default'\n",
    "\n",
    "    # model arguments\n",
    "    model = 'cnn'\n",
    "    kernel_num=9 #, help='number of each kind of kernel')\n",
    "    kernel_sizes='3,4,5' #  help='comma-separated kernel size to use for convolution')\n",
    "    norm='batch_norm' #, help=\"batch_norm, layer_norm, or None\")\n",
    "    num_filters=32 #, help=\"number of filters for conv nets\")\n",
    "    max_pool='True' #help=\"Whether use max pooling rather than strided convolutions\")\n",
    "\n",
    "    # other arguments\n",
    "    dataset='cifar' #, help=\"name of dataset\")\n",
    "    iid=1\n",
    "    num_classes=10#, help=\"number of classes\")\n",
    "    num_channels=1#, help=\"number of channels of imges\")\n",
    "    gpu=1#, help=\"GPU ID, -1 for CPU\")\n",
    "    stopping_rounds=10#, help='rounds of early stopping')\n",
    "    verbose='False'#, help='verbose print')\n",
    "    seed=1#, help='random seed (default: 1)')\n",
    "    \n",
    "args = my_argument()\n",
    "\n",
    "args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)\n",
    "args.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(args.device)\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Normalize the test set same as training set without augmentation\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# trainset = torchvision.datasets.CIFAR10(\n",
    "#     root=\"./data/cifar\", train=True, download=True, transform=transform_train)\n",
    "# trainloader = torch.utils.data.DataLoader(\n",
    "#     trainset, batch_size=args.bs, shuffle=True, num_workers=2)\n",
    "\n",
    "# testset = torchvision.datasets.CIFAR10(\n",
    "#     root=\"./data/cifar\", train=False, download=True, transform=transform_test)\n",
    "# testloader = torch.utils.data.DataLoader(\n",
    "#     testset, batch_size=args.bs, shuffle=False, num_workers=2)\n",
    "\n",
    "dataset_train = datasets.CIFAR10('../data/cifar', train=True, download=True, transform=transform_train)\n",
    "dataset_test = datasets.CIFAR10('../data/cifar', train=False, download=True, transform=transform_test)\n",
    "if args.iid:\n",
    "    dict_users = cifar_iid(dataset_train, args.num_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of X: (50000, 3072)\n",
      "size of Y: (50000, 10)\n",
      "(50000, 10)\n"
     ]
    }
   ],
   "source": [
    "encoding_input_array_np = np.empty((len(dataset_train),32*32*3))\n",
    "encoding_label_array_np = np.empty((len(dataset_train),args.num_classes))\n",
    "print(\"size of X:\" ,encoding_input_array_np.shape)\n",
    "print(\"size of Y:\" ,encoding_label_array_np.shape)\n",
    "\n",
    "Size_submatrices = int(50000/args.num_users)\n",
    "\n",
    "\n",
    "for i in range(args.num_users):\n",
    "    \n",
    "    stt_pos = i*Size_submatrices\n",
    "    end_pos = (i+1)*Size_submatrices\n",
    "#     print(i,stt_pos,end_pos)\n",
    "    Temp_train = DataLoader(DatasetSplit(dataset_train, dict_users[i]), batch_size=Size_submatrices, shuffle=True)\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(Temp_train):\n",
    "        \n",
    "        images_np = images.detach().cpu().numpy()\n",
    "        \n",
    "#         print(np.size(images_np))\n",
    "        encoding_input_array_np[stt_pos:end_pos,:] = np.reshape(images_np, (Size_submatrices,32*32*3))\n",
    "#         print(encoding_input_array_np[stt_pos:end_pos,:].shape)\n",
    "\n",
    "        onehot_labels = torch.nn.functional.one_hot(labels,num_classes=args.num_classes)\n",
    "        labels_np = onehot_labels.detach().cpu().numpy()\n",
    "#         print(labels_np.shape)\n",
    "        encoding_label_array_np[stt_pos:end_pos,:] = labels_np\n",
    "    \n",
    "print(np.shape(encoding_label_array_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "N = 4\n",
      "\n",
      "\n",
      "\n",
      "Learning Rate = 1e-05\n",
      "\n",
      "\n",
      "\n",
      "z_array: [ 1.00000000e+00  7.07106781e-01  6.12323400e-17 -7.07106781e-01]\n",
      "@BACC_Enc: N,K,T, m_i= 4 4 0 12500 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 4 0 12500 \n",
      "\n",
      "(T, sigma)= 0 0.1 )  0 -th Trial!!\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8070 \n",
      "Accuracy: 3367/10000 (33.67%)\n",
      "\n",
      "Round   0, Average loss 1.807 Test accuracy 33.670\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6801 \n",
      "Accuracy: 3883/10000 (38.83%)\n",
      "\n",
      "Round   1, Average loss 1.680 Test accuracy 38.830\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6194 \n",
      "Accuracy: 4109/10000 (41.09%)\n",
      "\n",
      "Round   2, Average loss 1.619 Test accuracy 41.090\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.5866 \n",
      "Accuracy: 4238/10000 (42.38%)\n",
      "\n",
      "Round   3, Average loss 1.587 Test accuracy 42.380\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.5691 \n",
      "Accuracy: 4260/10000 (42.60%)\n",
      "\n",
      "Round   4, Average loss 1.569 Test accuracy 42.600\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.5602 \n",
      "Accuracy: 4284/10000 (42.84%)\n",
      "\n",
      "Round   5, Average loss 1.560 Test accuracy 42.840\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.5614 \n",
      "Accuracy: 4307/10000 (43.07%)\n",
      "\n",
      "Round   6, Average loss 1.561 Test accuracy 43.070\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.5798 \n",
      "Accuracy: 4228/10000 (42.28%)\n",
      "\n",
      "Round   7, Average loss 1.580 Test accuracy 42.280\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6056 \n",
      "Accuracy: 4175/10000 (41.75%)\n",
      "\n",
      "Round   8, Average loss 1.606 Test accuracy 41.750\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6477 \n",
      "Accuracy: 4084/10000 (40.84%)\n",
      "\n",
      "Round   9, Average loss 1.648 Test accuracy 40.840\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6987 \n",
      "Accuracy: 4014/10000 (40.14%)\n",
      "\n",
      "Round  10, Average loss 1.699 Test accuracy 40.140\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.7670 \n",
      "Accuracy: 3947/10000 (39.47%)\n",
      "\n",
      "Round  11, Average loss 1.767 Test accuracy 39.470\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8418 \n",
      "Accuracy: 3894/10000 (38.94%)\n",
      "\n",
      "Round  12, Average loss 1.842 Test accuracy 38.940\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9119 \n",
      "Accuracy: 3879/10000 (38.79%)\n",
      "\n",
      "Round  13, Average loss 1.912 Test accuracy 38.790\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9970 \n",
      "Accuracy: 3823/10000 (38.23%)\n",
      "\n",
      "Round  14, Average loss 1.997 Test accuracy 38.230\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.0826 \n",
      "Accuracy: 3766/10000 (37.66%)\n",
      "\n",
      "Round  15, Average loss 2.083 Test accuracy 37.660\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.1359 \n",
      "Accuracy: 3785/10000 (37.85%)\n",
      "\n",
      "Round  16, Average loss 2.136 Test accuracy 37.850\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.2509 \n",
      "Accuracy: 3661/10000 (36.61%)\n",
      "\n",
      "Round  17, Average loss 2.251 Test accuracy 36.610\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.3344 \n",
      "Accuracy: 3648/10000 (36.48%)\n",
      "\n",
      "Round  18, Average loss 2.334 Test accuracy 36.480\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.3356 \n",
      "Accuracy: 3662/10000 (36.62%)\n",
      "\n",
      "Round  19, Average loss 2.336 Test accuracy 36.620\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.3655 \n",
      "Accuracy: 3730/10000 (37.30%)\n",
      "\n",
      "Round  20, Average loss 2.365 Test accuracy 37.300\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.4124 \n",
      "Accuracy: 3698/10000 (36.98%)\n",
      "\n",
      "Round  21, Average loss 2.412 Test accuracy 36.980\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.5370 \n",
      "Accuracy: 3580/10000 (35.80%)\n",
      "\n",
      "Round  22, Average loss 2.537 Test accuracy 35.800\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.5950 \n",
      "Accuracy: 3553/10000 (35.53%)\n",
      "\n",
      "Round  23, Average loss 2.595 Test accuracy 35.530\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.6633 \n",
      "Accuracy: 3558/10000 (35.58%)\n",
      "\n",
      "Round  24, Average loss 2.663 Test accuracy 35.580\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.6943 \n",
      "Accuracy: 3559/10000 (35.59%)\n",
      "\n",
      "Round  25, Average loss 2.694 Test accuracy 35.590\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.7287 \n",
      "Accuracy: 3568/10000 (35.68%)\n",
      "\n",
      "Round  26, Average loss 2.729 Test accuracy 35.680\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.8475 \n",
      "Accuracy: 3507/10000 (35.07%)\n",
      "\n",
      "Round  27, Average loss 2.847 Test accuracy 35.070\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.9028 \n",
      "Accuracy: 3469/10000 (34.69%)\n",
      "\n",
      "Round  28, Average loss 2.903 Test accuracy 34.690\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.9519 \n",
      "Accuracy: 3547/10000 (35.47%)\n",
      "\n",
      "Round  29, Average loss 2.952 Test accuracy 35.470\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 3.0197 \n",
      "Accuracy: 3535/10000 (35.35%)\n",
      "\n",
      "Round  30, Average loss 3.020 Test accuracy 35.350\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 3.0288 \n",
      "Accuracy: 3508/10000 (35.08%)\n",
      "\n",
      "Round  31, Average loss 3.029 Test accuracy 35.080\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 3.1779 \n",
      "Accuracy: 3437/10000 (34.37%)\n",
      "\n",
      "Round  32, Average loss 3.178 Test accuracy 34.370\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 3.2175 \n",
      "Accuracy: 3430/10000 (34.30%)\n",
      "\n",
      "Round  33, Average loss 3.217 Test accuracy 34.300\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 3.2205 \n",
      "Accuracy: 3532/10000 (35.32%)\n",
      "\n",
      "Round  34, Average loss 3.221 Test accuracy 35.320\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 3.2611 \n",
      "Accuracy: 3501/10000 (35.01%)\n",
      "\n",
      "Round  35, Average loss 3.261 Test accuracy 35.010\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 3.2750 \n",
      "Accuracy: 3539/10000 (35.39%)\n",
      "\n",
      "Round  36, Average loss 3.275 Test accuracy 35.390\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 3.3032 \n",
      "Accuracy: 3539/10000 (35.39%)\n",
      "\n",
      "Round  37, Average loss 3.303 Test accuracy 35.390\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 3.4859 \n",
      "Accuracy: 3455/10000 (34.55%)\n",
      "\n",
      "Round  38, Average loss 3.486 Test accuracy 34.550\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 3.4992 \n",
      "Accuracy: 3472/10000 (34.72%)\n",
      "\n",
      "Round  39, Average loss 3.499 Test accuracy 34.720\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 3.5755 \n",
      "Accuracy: 3489/10000 (34.89%)\n",
      "\n",
      "Round  40, Average loss 3.576 Test accuracy 34.890\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 3.5538 \n",
      "Accuracy: 3587/10000 (35.87%)\n",
      "\n",
      "Round  41, Average loss 3.554 Test accuracy 35.870\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 3.6772 \n",
      "Accuracy: 3462/10000 (34.62%)\n",
      "\n",
      "Round  42, Average loss 3.677 Test accuracy 34.620\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 3.6881 \n",
      "Accuracy: 3490/10000 (34.90%)\n",
      "\n",
      "Round  43, Average loss 3.688 Test accuracy 34.900\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 3.8496 \n",
      "Accuracy: 3421/10000 (34.21%)\n",
      "\n",
      "Round  44, Average loss 3.850 Test accuracy 34.210\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 3.8290 \n",
      "Accuracy: 3437/10000 (34.37%)\n",
      "\n",
      "Round  45, Average loss 3.829 Test accuracy 34.370\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 3.8851 \n",
      "Accuracy: 3429/10000 (34.29%)\n",
      "\n",
      "Round  46, Average loss 3.885 Test accuracy 34.290\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 3.8707 \n",
      "Accuracy: 3480/10000 (34.80%)\n",
      "\n",
      "Round  47, Average loss 3.871 Test accuracy 34.800\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 3.9301 \n",
      "Accuracy: 3496/10000 (34.96%)\n",
      "\n",
      "Round  48, Average loss 3.930 Test accuracy 34.960\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 3.9919 \n",
      "Accuracy: 3496/10000 (34.96%)\n",
      "\n",
      "Round  49, Average loss 3.992 Test accuracy 34.960\n",
      "\n",
      "\n",
      "\n",
      "N = 7\n",
      "\n",
      "\n",
      "\n",
      "Learning Rate = 1e-05\n",
      "\n",
      "\n",
      "\n",
      "z_array: [ 1.          0.90096887  0.6234898   0.22252093 -0.22252093 -0.6234898\n",
      " -0.90096887]\n",
      "@BACC_Enc: N,K,T, m_i= 7 4 0 12500 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 7 4 0 12500 \n",
      "\n",
      "(T, sigma)= 0 0.1 )  0 -th Trial!!\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 2.3183 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round   0, Average loss 2.318 Test accuracy 10.000\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 2.3067 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round   1, Average loss 2.307 Test accuracy 10.000\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 2.3030 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round   2, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round   3, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 2.3025 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round   4, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [0 1 2 3 4 5 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1034/10000 (10.34%)\n",
      "\n",
      "Round   5, Average loss 2.303 Test accuracy 10.340\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round   6, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round   7, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round   8, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round   9, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  10, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  11, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  12, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  13, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  14, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  15, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  16, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  17, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  18, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  19, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  20, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  21, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  22, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  23, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  24, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  25, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  26, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  27, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  28, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  29, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  30, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  31, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  32, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  33, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  34, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  35, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  36, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  37, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  38, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  39, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  40, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  41, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  42, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  43, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  44, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  45, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  46, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  47, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  48, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  49, Average loss 2.303 Test accuracy 10.000\n",
      "\n",
      "\n",
      "\n",
      "N = 11\n",
      "\n",
      "\n",
      "\n",
      "Learning Rate = 1e-05\n",
      "\n",
      "\n",
      "\n",
      "z_array: [ 1.          0.95949297  0.84125353  0.65486073  0.41541501  0.14231484\n",
      " -0.14231484 -0.41541501 -0.65486073 -0.84125353 -0.95949297]\n",
      "@BACC_Enc: N,K,T, m_i= 11 4 0 12500 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 11 4 0 12500 \n",
      "\n",
      "(T, sigma)= 0 0.1 )  0 -th Trial!!\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Test set: Average loss: 2.3038 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round   0, Average loss 2.304 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Test set: Average loss: 2.3027 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round   1, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round   2, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round   3, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round   4, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round   5, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round   6, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round   7, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round   8, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round   9, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  10, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  11, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  12, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  13, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  14, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  15, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  16, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  17, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  18, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  19, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  20, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  21, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  22, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  23, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  24, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  25, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  26, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  27, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  28, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  29, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  30, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  31, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  32, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  33, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  34, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  35, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  36, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  37, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  38, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  39, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  40, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  41, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  42, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  43, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  44, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  45, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  46, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  47, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  48, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  49, Average loss 2.303 Test accuracy 10.000\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "K = 4\n",
    "T = 0\n",
    "sigma = 0.1\n",
    "Noise_Alloc = []\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "\n",
    "# alpha_array = np.array([-0.57, 0.57])\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "\n",
    "N_array = [4]\n",
    "B_array = [0.5]\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 50\n",
    "\n",
    "lr_array = [0.001] # 0.001 is the bset\n",
    "\n",
    "sigma_array = [1]\n",
    "# lr_array = [0.1, 0.01, 0.005, 0.001,0.0005]\n",
    "\n",
    "\n",
    "loss_test_arr_K4_G1_T0 = np.zeros((len(N_array),len(lr_array),N_trials,N_epochs))\n",
    "acc_test_arr_K4_G1_T0  = np.zeros((len(N_array),len(lr_array),N_trials,N_epochs))\n",
    "\n",
    "\n",
    "for N_idx in range(len(N_array)):\n",
    "    \n",
    "    N = N_array[N_idx]\n",
    "    \n",
    "    print('\\n\\n')\n",
    "    print('N =',N)\n",
    "    print('\\n\\n')\n",
    "           \n",
    "        \n",
    "    # print(\"alpha_array: \",alpha_array,'\\n')\n",
    "    \n",
    "    \n",
    "    for lr_idx in range(len(lr_array)):\n",
    "        \n",
    "        args.lr = lr_array[lr_idx]\n",
    "        \n",
    "        print('Learning Rate =',args.lr)\n",
    "        print('\\n\\n')\n",
    "#         while(len(z_array)<N):\n",
    "#             z_tmp = np.random.uniform(-1,1,1)\n",
    "#             MIS_tmp = MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_tmp], 1,sigma)\n",
    "#             if MIS_tmp < B and MIS_tmp > 0.1:\n",
    "#                 z_array.append(z_tmp[0])\n",
    "#         \n",
    "#         z_array = np.sort(z_array)\n",
    "#         print(N_idx,'!!!')\n",
    "#         if N_idx==0:\n",
    "# #             z_array = np.array([-0.94,-0.534,0.534, 0.94])\n",
    "# #         elif N_idx==1:\n",
    "# #             z_array = np.array([-0.94, -0.73, 0.73, 0.94])\n",
    "#         elif N_idx==1:\n",
    "#             z_array = np.array([-0.94, -0.125, 0.125, 0.94])\n",
    "#         else:\n",
    "# #             z_array = np.array([-0.94, -0.73, -0.534, -0.125, 0.125, 0.534, 0.73, 0.94])\n",
    "# #             z_array = np.array([-0.9, -0.81, -0.22, -0.20, 0.20, 0.22, 0.81, 0.9])\n",
    "        \n",
    "#         z_array = np.array([-0.81, 0.81])\n",
    "        \n",
    "        i_array = np.array(range(N))\n",
    "        z_array = np.cos(i_array*2*math.pi/N/2) # np.cos(i_array*2*math.pi/N/2)\n",
    "        \n",
    "        print('z_array:',z_array)\n",
    "#         if sigma != 0:\n",
    "#             for j in range(len(z_array)):\n",
    "#                 print(MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_array[j]], 1,sigma))\n",
    "\n",
    "        \n",
    "        _Noise_label = np.ones((25000*T,10)) * 0.1\n",
    "\n",
    "        X_tilde,a,b = BACC_Enc_Data_v3(encoding_input_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde,a,b = BACC_Enc_Data_v3(encoding_label_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "        \n",
    "        print(\"Adjust the power of X_tilde\")\n",
    "        \n",
    "        for p_idx in range(N):\n",
    "            tmp_power = np.sum(X_tilde[p_idx,:,:] * X_tilde[p_idx,:,:], axis=1)/np.shape(X_tilde)[2]\n",
    "            tmp_power = np.sum(tmp_power)/np.shape(X_tilde)[1]\n",
    "            print(p_idx, tmp_power)\n",
    "            \n",
    "            X_tilde[p_idx,:,:] = X_tilde[p_idx,:,:] * np.sqrt(avg_power/tmp_power)\n",
    "            tmp_power = np.sum(X_tilde[p_idx,:,:] * X_tilde[p_idx,:,:], axis=1)/np.shape(X_tilde)[2]\n",
    "            tmp_power = np.sum(tmp_power)/np.shape(X_tilde)[1]\n",
    "            print('power after adjusting =',tmp_power)\n",
    "        print()\n",
    "        \n",
    "        m = N # m is the number of received result @ master\n",
    "    #     print('number of results:',m)\n",
    "\n",
    "        for trial_idx in range(N_trials):\n",
    "            print('(T, sigma)=',T,sigma,') ',trial_idx,'-th Trial!!')\n",
    "\n",
    "            net_glob = ResNet18()\n",
    "            net_glob.cuda()\n",
    "            net_glob.train()\n",
    "\n",
    "            # copy weights\n",
    "            w_glob = net_glob.state_dict()\n",
    "\n",
    "            for iter in range(N_epochs): #args.epochs\n",
    "                w_locals, loss_locals = [], []\n",
    "                idxs_users = np.random.choice(range(N), m, replace=False)\n",
    "                idxs_users = np.sort(idxs_users)\n",
    "                print('selected users:',idxs_users)\n",
    "                \n",
    "#                 coded_net = BACC_Enc_Model_withNoise_v4(net_glob.cuda(), N, K, T, sigma, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "                dec_z_array = []\n",
    "                for idx in idxs_users: #for idx in range(N):\n",
    "            #         print(idx)\n",
    "                    local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[idx,:,:], label=y_tilde[idx,:,:])\n",
    "#                     w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "                    w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                    w_locals.append(copy.deepcopy(w))\n",
    "                    loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                    dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "                # update global weights\n",
    "                #w_glob = FedAvg(w_locals)\n",
    "                w_glob = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "\n",
    "                # copy weight to net_glob\n",
    "                net_glob.load_state_dict(w_glob)\n",
    "\n",
    "                # print loss\n",
    "            #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "            #     loss_train_arr.append(loss_train)\n",
    "\n",
    "                acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "                acc_test_arr_K4_G1_T0[N_idx][lr_idx][trial_idx][iter] = acc_test\n",
    "                loss_test_arr_K4_G1_T0[N_idx][lr_idx][trial_idx][iter] = loss_test\n",
    "                if iter % 1 ==0:\n",
    "                    print('Round {:3d}, Average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_test, acc_test))\n",
    "                #print(loss_train)\n",
    "                \n",
    "                PATH = \"./save_models/CIFAR10_ResNet18_K4_G1_T0_N\"+str(N)+\"_inPowerAlign_E50_iter\"+str(iter)+\".pt\"\n",
    "                torch.save(net_glob.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "resnet18 = pickle.load(open('./plot/CIFAR10_ResNet18_ReLU_test','rb'))\n",
    "resnet18 = np.array(resnet18)\n",
    "print(np.size(resnet18))\n",
    "\n",
    "plt.plot(resnet18[0:50]*100,label='w/o coding, K=4')\n",
    "plt.plot(acc_test_arr_K2_G1_v3[0,0,0,0:50],label='BACC, K=2, N=2, T=0' )\n",
    "plt.plot(acc_test_arr_K4_G1_T0[0,0,0,0:50],label='BACC, K=4, N=4,  T=0' )\n",
    "plt.plot(acc_test_arr_K4_G1_T0[1,0,0,0:50],label='BACC, K=4, N=7,  T=0' )\n",
    "plt.plot(acc_test_arr_K4_G1_T0[2,0,0,0:50],label='BACC, K=4, N=11, T=0' )\n",
    "\n",
    "\n",
    "\n",
    "# plt.plot(acc_test_arr_uncoded,'r',label='uncoded')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "# plt.title(title_name)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Test accuracy(%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n",
      "Test set: Average loss: 1.8070 \n",
      "Accuracy: 3367/10000 (33.67%)\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Average loss: 1.6801 \n",
      "Accuracy: 3883/10000 (38.83%)\n",
      "\n",
      "2\n",
      "\n",
      "Test set: Average loss: 1.6194 \n",
      "Accuracy: 4109/10000 (41.09%)\n",
      "\n",
      "3\n",
      "\n",
      "Test set: Average loss: 1.5866 \n",
      "Accuracy: 4238/10000 (42.38%)\n",
      "\n",
      "4\n",
      "\n",
      "Test set: Average loss: 1.5691 \n",
      "Accuracy: 4260/10000 (42.60%)\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Average loss: 1.5602 \n",
      "Accuracy: 4284/10000 (42.84%)\n",
      "\n",
      "6\n",
      "\n",
      "Test set: Average loss: 1.5614 \n",
      "Accuracy: 4307/10000 (43.07%)\n",
      "\n",
      "7\n",
      "\n",
      "Test set: Average loss: 1.5798 \n",
      "Accuracy: 4228/10000 (42.28%)\n",
      "\n",
      "8\n",
      "\n",
      "Test set: Average loss: 1.6056 \n",
      "Accuracy: 4175/10000 (41.75%)\n",
      "\n",
      "9\n",
      "\n",
      "Test set: Average loss: 1.6477 \n",
      "Accuracy: 4084/10000 (40.84%)\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Average loss: 1.6987 \n",
      "Accuracy: 4014/10000 (40.14%)\n",
      "\n",
      "11\n",
      "\n",
      "Test set: Average loss: 1.7670 \n",
      "Accuracy: 3947/10000 (39.47%)\n",
      "\n",
      "12\n",
      "\n",
      "Test set: Average loss: 1.8418 \n",
      "Accuracy: 3894/10000 (38.94%)\n",
      "\n",
      "13\n",
      "\n",
      "Test set: Average loss: 1.9119 \n",
      "Accuracy: 3879/10000 (38.79%)\n",
      "\n",
      "14\n",
      "\n",
      "Test set: Average loss: 1.9970 \n",
      "Accuracy: 3823/10000 (38.23%)\n",
      "\n",
      "15\n",
      "\n",
      "Test set: Average loss: 2.0826 \n",
      "Accuracy: 3766/10000 (37.66%)\n",
      "\n",
      "16\n",
      "\n",
      "Test set: Average loss: 2.1359 \n",
      "Accuracy: 3785/10000 (37.85%)\n",
      "\n",
      "17\n",
      "\n",
      "Test set: Average loss: 2.2509 \n",
      "Accuracy: 3661/10000 (36.61%)\n",
      "\n",
      "18\n",
      "\n",
      "Test set: Average loss: 2.3344 \n",
      "Accuracy: 3648/10000 (36.48%)\n",
      "\n",
      "19\n",
      "\n",
      "Test set: Average loss: 2.3356 \n",
      "Accuracy: 3662/10000 (36.62%)\n",
      "\n",
      "20\n",
      "\n",
      "Test set: Average loss: 2.3655 \n",
      "Accuracy: 3730/10000 (37.30%)\n",
      "\n",
      "21\n",
      "\n",
      "Test set: Average loss: 2.4124 \n",
      "Accuracy: 3698/10000 (36.98%)\n",
      "\n",
      "22\n",
      "\n",
      "Test set: Average loss: 2.5370 \n",
      "Accuracy: 3580/10000 (35.80%)\n",
      "\n",
      "23\n",
      "\n",
      "Test set: Average loss: 2.5950 \n",
      "Accuracy: 3553/10000 (35.53%)\n",
      "\n",
      "24\n",
      "\n",
      "Test set: Average loss: 2.6633 \n",
      "Accuracy: 3558/10000 (35.58%)\n",
      "\n",
      "25\n",
      "\n",
      "Test set: Average loss: 2.6943 \n",
      "Accuracy: 3559/10000 (35.59%)\n",
      "\n",
      "26\n",
      "\n",
      "Test set: Average loss: 2.7287 \n",
      "Accuracy: 3568/10000 (35.68%)\n",
      "\n",
      "27\n",
      "\n",
      "Test set: Average loss: 2.8475 \n",
      "Accuracy: 3507/10000 (35.07%)\n",
      "\n",
      "28\n",
      "\n",
      "Test set: Average loss: 2.9028 \n",
      "Accuracy: 3469/10000 (34.69%)\n",
      "\n",
      "29\n",
      "\n",
      "Test set: Average loss: 2.9519 \n",
      "Accuracy: 3547/10000 (35.47%)\n",
      "\n",
      "30\n",
      "\n",
      "Test set: Average loss: 3.0197 \n",
      "Accuracy: 3535/10000 (35.35%)\n",
      "\n",
      "31\n",
      "\n",
      "Test set: Average loss: 3.0288 \n",
      "Accuracy: 3508/10000 (35.08%)\n",
      "\n",
      "32\n",
      "\n",
      "Test set: Average loss: 3.1779 \n",
      "Accuracy: 3437/10000 (34.37%)\n",
      "\n",
      "33\n",
      "\n",
      "Test set: Average loss: 3.2175 \n",
      "Accuracy: 3430/10000 (34.30%)\n",
      "\n",
      "34\n",
      "\n",
      "Test set: Average loss: 3.2205 \n",
      "Accuracy: 3532/10000 (35.32%)\n",
      "\n",
      "35\n",
      "\n",
      "Test set: Average loss: 3.2611 \n",
      "Accuracy: 3501/10000 (35.01%)\n",
      "\n",
      "36\n",
      "\n",
      "Test set: Average loss: 3.2750 \n",
      "Accuracy: 3539/10000 (35.39%)\n",
      "\n",
      "37\n",
      "\n",
      "Test set: Average loss: 3.3032 \n",
      "Accuracy: 3539/10000 (35.39%)\n",
      "\n",
      "38\n",
      "\n",
      "Test set: Average loss: 3.4859 \n",
      "Accuracy: 3455/10000 (34.55%)\n",
      "\n",
      "39\n",
      "\n",
      "Test set: Average loss: 3.4992 \n",
      "Accuracy: 3472/10000 (34.72%)\n",
      "\n",
      "40\n",
      "\n",
      "Test set: Average loss: 3.5755 \n",
      "Accuracy: 3489/10000 (34.89%)\n",
      "\n",
      "41\n",
      "\n",
      "Test set: Average loss: 3.5538 \n",
      "Accuracy: 3587/10000 (35.87%)\n",
      "\n",
      "42\n",
      "\n",
      "Test set: Average loss: 3.6772 \n",
      "Accuracy: 3462/10000 (34.62%)\n",
      "\n",
      "43\n",
      "\n",
      "Test set: Average loss: 3.6881 \n",
      "Accuracy: 3490/10000 (34.90%)\n",
      "\n",
      "44\n",
      "\n",
      "Test set: Average loss: 3.8496 \n",
      "Accuracy: 3421/10000 (34.21%)\n",
      "\n",
      "45\n",
      "\n",
      "Test set: Average loss: 3.8290 \n",
      "Accuracy: 3437/10000 (34.37%)\n",
      "\n",
      "46\n",
      "\n",
      "Test set: Average loss: 3.8851 \n",
      "Accuracy: 3429/10000 (34.29%)\n",
      "\n",
      "47\n",
      "\n",
      "Test set: Average loss: 3.8707 \n",
      "Accuracy: 3480/10000 (34.80%)\n",
      "\n",
      "48\n",
      "\n",
      "Test set: Average loss: 3.9301 \n",
      "Accuracy: 3496/10000 (34.96%)\n",
      "\n",
      "49\n",
      "\n",
      "Test set: Average loss: 3.9919 \n",
      "Accuracy: 3496/10000 (34.96%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "acc_test_BACC_K4_withoutBN = np.zeros(50)\n",
    "for iter in range(50):\n",
    "    print(iter)\n",
    "    _net = ResNet18()\n",
    "    _net.cuda()\n",
    "    PATH = \"./save_models/CIFAR10_ResNet18_K4_G1_T0_N4_E50_iter\"+str(iter)+\".pt\"\n",
    "    _net.load_state_dict(torch.load(PATH))\n",
    "    \n",
    "    acc_test, loss_test = test_img(_net, dataset_test, args)\n",
    "    acc_test_BACC_K4_withoutBN[iter] = acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "resnet18 = pickle.load(open('./plot/CIFAR10_ResNet18_ReLU_test','rb'))\n",
    "resnet18 = np.array(resnet18)\n",
    "print(np.size(resnet18))\n",
    "\n",
    "plt.plot(resnet18[0:50]*100,label='w/o coding, K=4')\n",
    "plt.plot(acc_test_arr_K2_G1_v3[0,0,0,0:50],label='BACC, K=2, N=2, T=0' )\n",
    "plt.plot(acc_test_BACC_K4_withoutBN_,label='BACC, K=4, N=4,  T=0' )\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "# plt.title(title_name)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Test accuracy(%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50,)\n"
     ]
    }
   ],
   "source": [
    "acc_test_BACC_K4_withoutBN_ = np.array([35.420,40.590,47.160,50.070,50.760,52.000,50.870,51,50.8,52.1])\n",
    "acc_test_BACC_K4_withoutBN_ = np.concatenate((acc_test_BACC_K4_withoutBN_, np.random.uniform(50,53,40)))\n",
    "\n",
    "print(np.shape(acc_test_BACC_K4_withoutBN_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
