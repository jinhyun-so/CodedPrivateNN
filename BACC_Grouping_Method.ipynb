{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "\n",
    "import math\n",
    "\n",
    "from utils.sampling import mnist_iid, mnist_noniid, cifar_iid\n",
    "from utils.options import args_parser\n",
    "from models.Update import LocalUpdate\n",
    "from models.Nets import MLP, CNNMnist, CNNCifar, LeNet, CNNMnist2\n",
    "from models.Fed import FedAvg\n",
    "from models.Fed import FedQAvg, Quantization, Quantization_Finite, my_score, my_score_Finite\n",
    "from models.test import test_img\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "\n",
    "from utils.sampling import mnist_iid, mnist_noniid, cifar_iid\n",
    "from utils.options import args_parser\n",
    "from models.Update import LocalUpdate\n",
    "from models.Nets import MLP, CNNMnist, CNNCifar, LeNet, CNNMnist2, CNNMnist3\n",
    "from models.Fed import FedAvg\n",
    "from models.Fed import FedQAvg, Quantization, Quantization_Finite, my_score, my_score_Finite\n",
    "from models.test import test_img\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from utils.functions import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "class DatasetSplit(Dataset):\n",
    "    def __init__(self, dataset, idxs):\n",
    "        self.dataset = dataset\n",
    "        self.idxs = list(idxs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image, label = self.dataset[self.idxs[item]]\n",
    "        return image, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. G=2, N=4 (N_1=2, N_2=2), K=4 (K_1=2, K_2=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of X: (60000, 784)\n",
      "size of Y: (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "class my_argument:    \n",
    "    epochs = 200    #\"rounds of training\"\n",
    "    num_users = 15  # \"number of users: N\"\n",
    "    num_partition = 4 # \"number of users: K\"\n",
    "    frac = 0.5 #\"the fraction of clients: C\"\n",
    "    local_ep = 1 #\"the number of local epochs: E\"\n",
    "    local_bs = 200 #\"local batch size: B\"\n",
    "    bs=200 #\"test batch size\"\n",
    "    lr=0.01 #\"learning rate\"\n",
    "    momentum=0.5 # \"SGD momentum (default: 0.5)\"\n",
    "    split='user' # \"train-test split type, user or sample\"\n",
    "    opt='ADAM'\n",
    "    loss='Custom' # 'Custom' or 'Default'\n",
    "\n",
    "    # model arguments\n",
    "    model = 'cnn'\n",
    "    kernel_num=9 #, help='number of each kind of kernel')\n",
    "    kernel_sizes='3,4,5' #  help='comma-separated kernel size to use for convolution')\n",
    "    norm='None' #, help=\"batch_norm, layer_norm, or None\")\n",
    "    num_filters=32 #, help=\"number of filters for conv nets\")\n",
    "    max_pool='True' #help=\"Whether use max pooling rather than strided convolutions\")\n",
    "\n",
    "    # other arguments\n",
    "    dataset='mnist' #, help=\"name of dataset\")\n",
    "    iid=1\n",
    "    num_classes=10#, help=\"number of classes\")\n",
    "    num_channels=1#, help=\"number of channels of imges\")\n",
    "    gpu=1#, help=\"GPU ID, -1 for CPU\")\n",
    "    stopping_rounds=10#, help='rounds of early stopping')\n",
    "    verbose='False'#, help='verbose print')\n",
    "    seed=1#, help='random seed (default: 1)')\n",
    "    \n",
    "args = my_argument()\n",
    "\n",
    "args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "# load dataset and split users\n",
    "trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "dataset_train = datasets.MNIST('../data/mnist/', train=True, download=True, transform=trans_mnist)\n",
    "dataset_test = datasets.MNIST('../data/mnist/', train=False, download=True, transform=trans_mnist)\n",
    "\n",
    "dict_users = mnist_iid(dataset_train, args.num_partition)\n",
    "\n",
    "encoding_input_array_np = np.empty((len(dataset_train),28*28))\n",
    "encoding_label_array_np = np.empty((len(dataset_train),args.num_classes))\n",
    "print(\"size of X:\" ,encoding_input_array_np.shape)\n",
    "print(\"size of Y:\" ,encoding_label_array_np.shape)\n",
    "\n",
    "Size_submatrices = int(60000/args.num_partition)\n",
    "\n",
    "for i in range(args.num_partition):\n",
    "    \n",
    "    stt_pos = i*Size_submatrices\n",
    "    end_pos = (i+1)*Size_submatrices\n",
    "#     print(i,stt_pos,end_pos)\n",
    "    Temp_train = DataLoader(DatasetSplit(dataset_train, dict_users[i]), batch_size=Size_submatrices, shuffle=True)\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(Temp_train):\n",
    "        \n",
    "        images_np = images.detach().cpu().numpy()\n",
    "        encoding_input_array_np[stt_pos:end_pos,:] = np.reshape(images_np, (Size_submatrices,28*28))\n",
    "#         print(encoding_input_array_np[stt_pos:end_pos,:].shape)\n",
    "\n",
    "        onehot_labels = torch.nn.functional.one_hot(labels,num_classes=args.num_classes)\n",
    "        labels_np = onehot_labels.detach().cpu().numpy()\n",
    "#         print(labels_np.shape)\n",
    "        encoding_label_array_np[stt_pos:end_pos,:] = labels_np\n",
    "\n",
    "\n",
    "# print(labels_np[0:10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = 2\n",
    "N = 4 # N should be divisible by G\n",
    "K = 4 # N should be divisible by G\n",
    "\n",
    "size_per_group = int(60000/G)\n",
    "\n",
    "X_group = np.reshape(encoding_input_array_np, (G,size_per_group,28*28))\n",
    "y_group = np.reshape(encoding_label_array_np, (G,size_per_group,args.num_classes)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2 3\n",
      "##########################################\n",
      "###### 0 -th Trial!! ###########\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 15000 \n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3024 \n",
      "Accuracy: 1207/10000 (12.07%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.2902 \n",
      "Accuracy: 3262/10000 (32.62%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7413 \n",
      "Accuracy: 6346/10000 (63.46%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.8561 \n",
      "Accuracy: 7913/10000 (79.13%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3443 \n",
      "Accuracy: 9438/10000 (94.38%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2913 \n",
      "Accuracy: 9534/10000 (95.34%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2846 \n",
      "Accuracy: 9577/10000 (95.77%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2664 \n",
      "Accuracy: 9601/10000 (96.01%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2251 \n",
      "Accuracy: 9593/10000 (95.93%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2496 \n",
      "Accuracy: 9541/10000 (95.41%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2409 \n",
      "Accuracy: 9605/10000 (96.05%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2638 \n",
      "Accuracy: 9601/10000 (96.01%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2503 \n",
      "Accuracy: 9601/10000 (96.01%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2227 \n",
      "Accuracy: 9636/10000 (96.36%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2835 \n",
      "Accuracy: 9587/10000 (95.87%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2355 \n",
      "Accuracy: 9634/10000 (96.34%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2502 \n",
      "Accuracy: 9619/10000 (96.19%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2178 \n",
      "Accuracy: 9638/10000 (96.38%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2421 \n",
      "Accuracy: 9627/10000 (96.27%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2318 \n",
      "Accuracy: 9628/10000 (96.28%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2415 \n",
      "Accuracy: 9639/10000 (96.39%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2228 \n",
      "Accuracy: 9662/10000 (96.62%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3347 \n",
      "Accuracy: 9592/10000 (95.92%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2119 \n",
      "Accuracy: 9646/10000 (96.46%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2470 \n",
      "Accuracy: 9645/10000 (96.45%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2941 \n",
      "Accuracy: 9606/10000 (96.06%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2180 \n",
      "Accuracy: 9651/10000 (96.51%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2774 \n",
      "Accuracy: 9644/10000 (96.44%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2453 \n",
      "Accuracy: 9636/10000 (96.36%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3982 \n",
      "Accuracy: 9592/10000 (95.92%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "N_i = int(N/G)\n",
    "K_i = int(K/G)\n",
    "T = 3\n",
    "sigma = 1\n",
    "Noise_Alloc = [0,2,4]\n",
    "m = N_i # number of selected workers (if there is no straggler, m=N_i)\n",
    "\n",
    "print(N_i,K_i,T)\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K_i+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K_i+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K_i+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "z_array = np.array([-0.81, 0.81])\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 30\n",
    "\n",
    "loss_test_arr_G2 = np.zeros((N_trials,N_epochs))\n",
    "acc_test_arr_G2  = np.zeros((N_trials,N_epochs))\n",
    "\n",
    "for trial_idx in range(N_trials):\n",
    "    \n",
    "    print('##########################################')\n",
    "    print('######',trial_idx,'-th Trial!! ###########')\n",
    "    \n",
    "    net_glob = CNNMnist2(args=args)\n",
    "    net_glob.cuda()\n",
    "    net_glob.train()\n",
    "    \n",
    "    # copy weights\n",
    "    w_glob = net_glob.state_dict()\n",
    "    \n",
    "    X_tilde = np.empty((N,Size_submatrices,28*28))\n",
    "    y_tilde = np.empty((N,Size_submatrices,10))\n",
    "    \n",
    "    for G_idx in range(G):\n",
    "        \n",
    "        _Noise_label = np.ones((size_per_group*T,10)) * 0.1\n",
    "        \n",
    "        X_tilde_tmp,a,b = BACC_Enc_Data_v3(X_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde_tmp,a,b = BACC_Enc_Data_v3(y_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "        \n",
    "        stt_pos = G_idx * N_i\n",
    "        end_pos = (G_idx+1) * N_i\n",
    "        \n",
    "        X_tilde[stt_pos:end_pos,:,:] = X_tilde_tmp\n",
    "        y_tilde[stt_pos:end_pos,:,:] = y_tilde_tmp\n",
    "        \n",
    "   \n",
    "\n",
    "    for iter in range(N_epochs): #args.epochs\n",
    "        \n",
    "        w_group_array = []\n",
    "        for G_idx in range(G):\n",
    "            w_locals, loss_locals = [], []\n",
    "            idxs_users = np.random.choice(range(N_i), m, replace=False)\n",
    "            idxs_users = np.sort(idxs_users)\n",
    "            print('selected users:',idxs_users)\n",
    "\n",
    "            coded_net = BACC_Enc_Model_withNoise_v3(net_glob.cuda(), N_i, K_i, T, 0.01, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "            dec_z_array = []\n",
    "            for idx in idxs_users: #for idx in range(N):\n",
    "        #         print(idx)\n",
    "                local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[G_idx*N_i+idx,:,:], label=y_tilde[G_idx*N_i+idx,:,:])\n",
    "                w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "                w_locals.append(copy.deepcopy(w))\n",
    "                loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "            # update global weights\n",
    "            #w_glob = FedAvg(w_locals)\n",
    "            w_group = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "            \n",
    "            w_group_array.append(copy.deepcopy(w_group))\n",
    "        \n",
    "        w_glob = copy.deepcopy(w_group_array[0])\n",
    "        for k in w_glob.keys():\n",
    "            for G_idx in range(1,G):\n",
    "                w_glob[k] += w_group_array[G_idx][k]\n",
    "            w_glob[k] = torch.div(w_glob[k], len(w_group_array))\n",
    "        \n",
    "        # copy weight to net_glob\n",
    "        net_glob.load_state_dict(w_glob)\n",
    "\n",
    "        # print loss\n",
    "    #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "    #     loss_train_arr.append(loss_train)\n",
    "\n",
    "        acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "        acc_test_arr_G2[trial_idx][iter] = acc_test\n",
    "        loss_test_arr_G2[trial_idx][iter] = loss_test\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_group[G_idx,:,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. G=3, K=6 (K_i=2), N=6 (N_i=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of X: (60000, 784)\n",
      "size of Y: (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "class my_argument:    \n",
    "    epochs = 200    #\"rounds of training\"\n",
    "    num_users = 15  # \"number of users: N\"\n",
    "    num_partition = 6 # \"number of users: K\"\n",
    "    frac = 0.5 #\"the fraction of clients: C\"\n",
    "    local_ep = 1 #\"the number of local epochs: E\"\n",
    "    local_bs = 200 #\"local batch size: B\"\n",
    "    bs=200 #\"test batch size\"\n",
    "    lr=0.01 #\"learning rate\"\n",
    "    momentum=0.5 # \"SGD momentum (default: 0.5)\"\n",
    "    split='user' # \"train-test split type, user or sample\"\n",
    "    opt='ADAM'\n",
    "    loss='Custom' # 'Custom' or 'Default'\n",
    "\n",
    "    # model arguments\n",
    "    model = 'cnn'\n",
    "    kernel_num=9 #, help='number of each kind of kernel')\n",
    "    kernel_sizes='3,4,5' #  help='comma-separated kernel size to use for convolution')\n",
    "    norm='None' #, help=\"batch_norm, layer_norm, or None\")\n",
    "    num_filters=32 #, help=\"number of filters for conv nets\")\n",
    "    max_pool='True' #help=\"Whether use max pooling rather than strided convolutions\")\n",
    "\n",
    "    # other arguments\n",
    "    dataset='mnist' #, help=\"name of dataset\")\n",
    "    iid=1\n",
    "    num_classes=10#, help=\"number of classes\")\n",
    "    num_channels=1#, help=\"number of channels of imges\")\n",
    "    gpu=1#, help=\"GPU ID, -1 for CPU\")\n",
    "    stopping_rounds=10#, help='rounds of early stopping')\n",
    "    verbose='False'#, help='verbose print')\n",
    "    seed=1#, help='random seed (default: 1)')\n",
    "    \n",
    "args = my_argument()\n",
    "\n",
    "args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "# load dataset and split users\n",
    "trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "dataset_train = datasets.MNIST('../data/mnist/', train=True, download=True, transform=trans_mnist)\n",
    "dataset_test = datasets.MNIST('../data/mnist/', train=False, download=True, transform=trans_mnist)\n",
    "\n",
    "dict_users = mnist_iid(dataset_train, args.num_partition)\n",
    "\n",
    "encoding_input_array_np = np.empty((len(dataset_train),28*28))\n",
    "encoding_label_array_np = np.empty((len(dataset_train),args.num_classes))\n",
    "print(\"size of X:\" ,encoding_input_array_np.shape)\n",
    "print(\"size of Y:\" ,encoding_label_array_np.shape)\n",
    "\n",
    "Size_submatrices = int(60000/args.num_partition)\n",
    "\n",
    "for i in range(args.num_partition):\n",
    "    \n",
    "    stt_pos = i*Size_submatrices\n",
    "    end_pos = (i+1)*Size_submatrices\n",
    "#     print(i,stt_pos,end_pos)\n",
    "    Temp_train = DataLoader(DatasetSplit(dataset_train, dict_users[i]), batch_size=Size_submatrices, shuffle=True)\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(Temp_train):\n",
    "        \n",
    "        images_np = images.detach().cpu().numpy()\n",
    "        encoding_input_array_np[stt_pos:end_pos,:] = np.reshape(images_np, (Size_submatrices,28*28))\n",
    "#         print(encoding_input_array_np[stt_pos:end_pos,:].shape)\n",
    "\n",
    "        onehot_labels = torch.nn.functional.one_hot(labels,num_classes=args.num_classes)\n",
    "        labels_np = onehot_labels.detach().cpu().numpy()\n",
    "#         print(labels_np.shape)\n",
    "        encoding_label_array_np[stt_pos:end_pos,:] = labels_np\n",
    "\n",
    "\n",
    "# print(labels_np[0:10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = 3\n",
    "N = 6 # N should be divisible by G\n",
    "K = 6 # N should be divisible by G\n",
    "\n",
    "size_per_group = int(60000/G)\n",
    "\n",
    "X_group = np.reshape(encoding_input_array_np, (G,size_per_group,28*28))\n",
    "y_group = np.reshape(encoding_label_array_np, (G,size_per_group,args.num_classes)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2 3\n",
      "##########################################\n",
      "###### 0 -th Trial!! ###########\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 10000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 10000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 10000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 10000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 10000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 10000 \n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1027/10000 (10.27%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.2964 \n",
      "Accuracy: 2574/10000 (25.74%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.2028 \n",
      "Accuracy: 5098/10000 (50.98%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4135 \n",
      "Accuracy: 6138/10000 (61.38%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.8341 \n",
      "Accuracy: 8016/10000 (80.16%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.4538 \n",
      "Accuracy: 8951/10000 (89.51%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.4353 \n",
      "Accuracy: 9057/10000 (90.57%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.4178 \n",
      "Accuracy: 9011/10000 (90.11%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3867 \n",
      "Accuracy: 9143/10000 (91.43%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3976 \n",
      "Accuracy: 9128/10000 (91.28%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3632 \n",
      "Accuracy: 9188/10000 (91.88%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3798 \n",
      "Accuracy: 9189/10000 (91.89%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3676 \n",
      "Accuracy: 9186/10000 (91.86%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3669 \n",
      "Accuracy: 9174/10000 (91.74%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3746 \n",
      "Accuracy: 9148/10000 (91.48%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3653 \n",
      "Accuracy: 9212/10000 (92.12%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3733 \n",
      "Accuracy: 9189/10000 (91.89%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3773 \n",
      "Accuracy: 9161/10000 (91.61%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3742 \n",
      "Accuracy: 9180/10000 (91.80%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3901 \n",
      "Accuracy: 9140/10000 (91.40%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3975 \n",
      "Accuracy: 9138/10000 (91.38%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3835 \n",
      "Accuracy: 9136/10000 (91.36%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3650 \n",
      "Accuracy: 9167/10000 (91.67%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3753 \n",
      "Accuracy: 9104/10000 (91.04%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3893 \n",
      "Accuracy: 9137/10000 (91.37%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3831 \n",
      "Accuracy: 9179/10000 (91.79%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3602 \n",
      "Accuracy: 9173/10000 (91.73%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3802 \n",
      "Accuracy: 9144/10000 (91.44%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3609 \n",
      "Accuracy: 9185/10000 (91.85%)\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3711 \n",
      "Accuracy: 9164/10000 (91.64%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "N_i = int(N/G)\n",
    "K_i = int(K/G)\n",
    "T = 3\n",
    "sigma = 1\n",
    "Noise_Alloc = [0,2,4]\n",
    "m = N_i # number of selected workers (if there is no straggler, m=N_i)\n",
    "\n",
    "print(N_i,K_i,T)\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K_i+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K_i+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K_i+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "z_array = np.array([-0.81, 0.81])\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 30\n",
    "\n",
    "loss_test_arr_G3 = np.zeros((N_trials,N_epochs))\n",
    "acc_test_arr_G3  = np.zeros((N_trials,N_epochs))\n",
    "\n",
    "for trial_idx in range(N_trials):\n",
    "    \n",
    "    print('##########################################')\n",
    "    print('######',trial_idx,'-th Trial!! ###########')\n",
    "    \n",
    "    net_glob = CNNMnist2(args=args)\n",
    "    net_glob.cuda()\n",
    "    net_glob.train()\n",
    "    \n",
    "    # copy weights\n",
    "    w_glob = net_glob.state_dict()\n",
    "    \n",
    "    X_tilde = np.empty((N,Size_submatrices,28*28))\n",
    "    y_tilde = np.empty((N,Size_submatrices,10))\n",
    "    \n",
    "    for G_idx in range(G):\n",
    "        \n",
    "        _Noise_label = np.ones((size_per_group*T,10)) * 0.1\n",
    "        \n",
    "        X_tilde_tmp,a,b = BACC_Enc_Data_v3(X_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde_tmp,a,b = BACC_Enc_Data_v3(y_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "        \n",
    "        stt_pos = G_idx * N_i\n",
    "        end_pos = (G_idx+1) * N_i\n",
    "        \n",
    "        X_tilde[stt_pos:end_pos,:,:] = X_tilde_tmp\n",
    "        y_tilde[stt_pos:end_pos,:,:] = y_tilde_tmp\n",
    "        \n",
    "   \n",
    "\n",
    "    for iter in range(N_epochs): #args.epochs\n",
    "        \n",
    "        w_group_array = []\n",
    "        for G_idx in range(G):\n",
    "            w_locals, loss_locals = [], []\n",
    "            idxs_users = np.random.choice(range(N_i), m, replace=False)\n",
    "            idxs_users = np.sort(idxs_users)\n",
    "            print('selected users:',idxs_users)\n",
    "\n",
    "            coded_net = BACC_Enc_Model_withNoise_v3(net_glob.cuda(), N_i, K_i, T, 0.01, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "            dec_z_array = []\n",
    "            for idx in idxs_users: #for idx in range(N):\n",
    "        #         print(idx)\n",
    "                local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[G_idx*N_i+idx,:,:], label=y_tilde[G_idx*N_i+idx,:,:])\n",
    "                w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "                w_locals.append(copy.deepcopy(w))\n",
    "                loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "            # update global weights\n",
    "            #w_glob = FedAvg(w_locals)\n",
    "            w_group = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "            \n",
    "            w_group_array.append(copy.deepcopy(w_group))\n",
    "        \n",
    "        w_glob = copy.deepcopy(w_group_array[0])\n",
    "        for k in w_glob.keys():\n",
    "            for G_idx in range(1,G):\n",
    "                w_glob[k] += w_group_array[G_idx][k]\n",
    "            w_glob[k] = torch.div(w_glob[k], len(w_group_array))\n",
    "        \n",
    "        # copy weight to net_glob\n",
    "        net_glob.load_state_dict(w_glob)\n",
    "\n",
    "        # print loss\n",
    "    #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "    #     loss_train_arr.append(loss_train)\n",
    "\n",
    "        acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "        acc_test_arr_G3[trial_idx][iter] = acc_test\n",
    "        loss_test_arr_G3[trial_idx][iter] = loss_test\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. K=2, Without Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of X: (60000, 784)\n",
      "size of Y: (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "class my_argument:    \n",
    "    epochs = 200    #\"rounds of training\"\n",
    "    num_users = 15  # \"number of users: N\"\n",
    "    num_partition = 2 # \"number of users: K\"\n",
    "    frac = 0.5 #\"the fraction of clients: C\"\n",
    "    local_ep = 1 #\"the number of local epochs: E\"\n",
    "    local_bs = 200 #\"local batch size: B\"\n",
    "    bs=200 #\"test batch size\"\n",
    "    lr=0.01 #\"learning rate\"\n",
    "    momentum=0.5 # \"SGD momentum (default: 0.5)\"\n",
    "    split='user' # \"train-test split type, user or sample\"\n",
    "    opt='ADAM'\n",
    "    loss='Custom' # 'Custom' or 'Default'\n",
    "\n",
    "    # model arguments\n",
    "    model = 'cnn'\n",
    "    kernel_num=9 #, help='number of each kind of kernel')\n",
    "    kernel_sizes='3,4,5' #  help='comma-separated kernel size to use for convolution')\n",
    "    norm='None' #, help=\"batch_norm, layer_norm, or None\")\n",
    "    num_filters=32 #, help=\"number of filters for conv nets\")\n",
    "    max_pool='True' #help=\"Whether use max pooling rather than strided convolutions\")\n",
    "\n",
    "    # other arguments\n",
    "    dataset='mnist' #, help=\"name of dataset\")\n",
    "    iid=1\n",
    "    num_classes=10#, help=\"number of classes\")\n",
    "    num_channels=1#, help=\"number of channels of imges\")\n",
    "    gpu=1#, help=\"GPU ID, -1 for CPU\")\n",
    "    stopping_rounds=10#, help='rounds of early stopping')\n",
    "    verbose='False'#, help='verbose print')\n",
    "    seed=1#, help='random seed (default: 1)')\n",
    "    \n",
    "args = my_argument()\n",
    "\n",
    "args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "# load dataset and split users\n",
    "trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "dataset_train = datasets.MNIST('../data/mnist/', train=True, download=True, transform=trans_mnist)\n",
    "dataset_test = datasets.MNIST('../data/mnist/', train=False, download=True, transform=trans_mnist)\n",
    "\n",
    "dict_users = mnist_iid(dataset_train, args.num_partition)\n",
    "\n",
    "encoding_input_array_np = np.empty((len(dataset_train),28*28))\n",
    "encoding_label_array_np = np.empty((len(dataset_train),args.num_classes))\n",
    "print(\"size of X:\" ,encoding_input_array_np.shape)\n",
    "print(\"size of Y:\" ,encoding_label_array_np.shape)\n",
    "\n",
    "Size_submatrices = int(60000/args.num_partition)\n",
    "\n",
    "for i in range(args.num_partition):\n",
    "    \n",
    "    stt_pos = i*Size_submatrices\n",
    "    end_pos = (i+1)*Size_submatrices\n",
    "#     print(i,stt_pos,end_pos)\n",
    "    Temp_train = DataLoader(DatasetSplit(dataset_train, dict_users[i]), batch_size=Size_submatrices, shuffle=True)\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(Temp_train):\n",
    "        \n",
    "        images_np = images.detach().cpu().numpy()\n",
    "        encoding_input_array_np[stt_pos:end_pos,:] = np.reshape(images_np, (Size_submatrices,28*28))\n",
    "#         print(encoding_input_array_np[stt_pos:end_pos,:].shape)\n",
    "\n",
    "        onehot_labels = torch.nn.functional.one_hot(labels,num_classes=args.num_classes)\n",
    "        labels_np = onehot_labels.detach().cpu().numpy()\n",
    "#         print(labels_np.shape)\n",
    "        encoding_label_array_np[stt_pos:end_pos,:] = labels_np\n",
    "\n",
    "\n",
    "# print(labels_np[0:10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z_array: [-0.81  0.81]\n",
      "0.4838626198316927\n",
      "0.48386261983169315\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 30000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 30000 \n",
      "\n",
      "(T, sigma)= 3 1 )  0 -th Trial!!\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.1504 \n",
      "Accuracy: 5550/10000 (55.50%)\n",
      "\n",
      "Round   0, Average loss 2.150 Test accuracy 55.500\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.7712 \n",
      "Accuracy: 8177/10000 (81.77%)\n",
      "\n",
      "Round   1, Average loss 0.771 Test accuracy 81.770\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2180 \n",
      "Accuracy: 9692/10000 (96.92%)\n",
      "\n",
      "Round   2, Average loss 0.218 Test accuracy 96.920\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1758 \n",
      "Accuracy: 9740/10000 (97.40%)\n",
      "\n",
      "Round   3, Average loss 0.176 Test accuracy 97.400\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1905 \n",
      "Accuracy: 9728/10000 (97.28%)\n",
      "\n",
      "Round   4, Average loss 0.191 Test accuracy 97.280\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1868 \n",
      "Accuracy: 9750/10000 (97.50%)\n",
      "\n",
      "Round   5, Average loss 0.187 Test accuracy 97.500\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1771 \n",
      "Accuracy: 9752/10000 (97.52%)\n",
      "\n",
      "Round   6, Average loss 0.177 Test accuracy 97.520\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1665 \n",
      "Accuracy: 9737/10000 (97.37%)\n",
      "\n",
      "Round   7, Average loss 0.166 Test accuracy 97.370\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1977 \n",
      "Accuracy: 9736/10000 (97.36%)\n",
      "\n",
      "Round   8, Average loss 0.198 Test accuracy 97.360\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1733 \n",
      "Accuracy: 9732/10000 (97.32%)\n",
      "\n",
      "Round   9, Average loss 0.173 Test accuracy 97.320\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1734 \n",
      "Accuracy: 9756/10000 (97.56%)\n",
      "\n",
      "Round  10, Average loss 0.173 Test accuracy 97.560\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1853 \n",
      "Accuracy: 9765/10000 (97.65%)\n",
      "\n",
      "Round  11, Average loss 0.185 Test accuracy 97.650\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3139 \n",
      "Accuracy: 9485/10000 (94.85%)\n",
      "\n",
      "Round  12, Average loss 0.314 Test accuracy 94.850\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1789 \n",
      "Accuracy: 9738/10000 (97.38%)\n",
      "\n",
      "Round  13, Average loss 0.179 Test accuracy 97.380\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2365 \n",
      "Accuracy: 9737/10000 (97.37%)\n",
      "\n",
      "Round  14, Average loss 0.236 Test accuracy 97.370\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2020 \n",
      "Accuracy: 9748/10000 (97.48%)\n",
      "\n",
      "Round  15, Average loss 0.202 Test accuracy 97.480\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1793 \n",
      "Accuracy: 9749/10000 (97.49%)\n",
      "\n",
      "Round  16, Average loss 0.179 Test accuracy 97.490\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1965 \n",
      "Accuracy: 9742/10000 (97.42%)\n",
      "\n",
      "Round  17, Average loss 0.196 Test accuracy 97.420\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1861 \n",
      "Accuracy: 9749/10000 (97.49%)\n",
      "\n",
      "Round  18, Average loss 0.186 Test accuracy 97.490\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3015 \n",
      "Accuracy: 9566/10000 (95.66%)\n",
      "\n",
      "Round  19, Average loss 0.302 Test accuracy 95.660\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1838 \n",
      "Accuracy: 9762/10000 (97.62%)\n",
      "\n",
      "Round  20, Average loss 0.184 Test accuracy 97.620\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1723 \n",
      "Accuracy: 9744/10000 (97.44%)\n",
      "\n",
      "Round  21, Average loss 0.172 Test accuracy 97.440\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2225 \n",
      "Accuracy: 9748/10000 (97.48%)\n",
      "\n",
      "Round  22, Average loss 0.223 Test accuracy 97.480\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1853 \n",
      "Accuracy: 9747/10000 (97.47%)\n",
      "\n",
      "Round  23, Average loss 0.185 Test accuracy 97.470\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1962 \n",
      "Accuracy: 9755/10000 (97.55%)\n",
      "\n",
      "Round  24, Average loss 0.196 Test accuracy 97.550\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2181 \n",
      "Accuracy: 9743/10000 (97.43%)\n",
      "\n",
      "Round  25, Average loss 0.218 Test accuracy 97.430\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2300 \n",
      "Accuracy: 9750/10000 (97.50%)\n",
      "\n",
      "Round  26, Average loss 0.230 Test accuracy 97.500\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1930 \n",
      "Accuracy: 9749/10000 (97.49%)\n",
      "\n",
      "Round  27, Average loss 0.193 Test accuracy 97.490\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1894 \n",
      "Accuracy: 9753/10000 (97.53%)\n",
      "\n",
      "Round  28, Average loss 0.189 Test accuracy 97.530\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.2037 \n",
      "Accuracy: 9753/10000 (97.53%)\n",
      "\n",
      "Round  29, Average loss 0.204 Test accuracy 97.530\n",
      "z_array: [-0.81 -0.22  0.22  0.81]\n",
      "0.4838626198316927\n",
      "0.46852880899669475\n",
      "0.468528808996694\n",
      "0.48386261983169315\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 30000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 30000 \n",
      "\n",
      "(T, sigma)= 3 1 )  0 -th Trial!!\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.2978 \n",
      "Accuracy: 4081/10000 (40.81%)\n",
      "\n",
      "Round   0, Average loss 2.298 Test accuracy 40.810\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.2801 \n",
      "Accuracy: 3493/10000 (34.93%)\n",
      "\n",
      "Round   1, Average loss 2.280 Test accuracy 34.930\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.2261 \n",
      "Accuracy: 7954/10000 (79.54%)\n",
      "\n",
      "Round   2, Average loss 2.226 Test accuracy 79.540\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.1914 \n",
      "Accuracy: 7929/10000 (79.29%)\n",
      "\n",
      "Round   3, Average loss 2.191 Test accuracy 79.290\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.1369 \n",
      "Accuracy: 6894/10000 (68.94%)\n",
      "\n",
      "Round   4, Average loss 2.137 Test accuracy 68.940\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.0815 \n",
      "Accuracy: 8826/10000 (88.26%)\n",
      "\n",
      "Round   5, Average loss 2.081 Test accuracy 88.260\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.0964 \n",
      "Accuracy: 8591/10000 (85.91%)\n",
      "\n",
      "Round   6, Average loss 2.096 Test accuracy 85.910\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.1099 \n",
      "Accuracy: 9409/10000 (94.09%)\n",
      "\n",
      "Round   7, Average loss 2.110 Test accuracy 94.090\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.1391 \n",
      "Accuracy: 8650/10000 (86.50%)\n",
      "\n",
      "Round   8, Average loss 2.139 Test accuracy 86.500\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.0060 \n",
      "Accuracy: 9606/10000 (96.06%)\n",
      "\n",
      "Round   9, Average loss 2.006 Test accuracy 96.060\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9598 \n",
      "Accuracy: 9456/10000 (94.56%)\n",
      "\n",
      "Round  10, Average loss 1.960 Test accuracy 94.560\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9703 \n",
      "Accuracy: 9485/10000 (94.85%)\n",
      "\n",
      "Round  11, Average loss 1.970 Test accuracy 94.850\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9826 \n",
      "Accuracy: 9690/10000 (96.90%)\n",
      "\n",
      "Round  12, Average loss 1.983 Test accuracy 96.900\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.0128 \n",
      "Accuracy: 9711/10000 (97.11%)\n",
      "\n",
      "Round  13, Average loss 2.013 Test accuracy 97.110\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8374 \n",
      "Accuracy: 9722/10000 (97.22%)\n",
      "\n",
      "Round  14, Average loss 1.837 Test accuracy 97.220\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9075 \n",
      "Accuracy: 9455/10000 (94.55%)\n",
      "\n",
      "Round  15, Average loss 1.908 Test accuracy 94.550\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9255 \n",
      "Accuracy: 9717/10000 (97.17%)\n",
      "\n",
      "Round  16, Average loss 1.925 Test accuracy 97.170\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8705 \n",
      "Accuracy: 9569/10000 (95.69%)\n",
      "\n",
      "Round  17, Average loss 1.871 Test accuracy 95.690\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8759 \n",
      "Accuracy: 9661/10000 (96.61%)\n",
      "\n",
      "Round  18, Average loss 1.876 Test accuracy 96.610\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9110 \n",
      "Accuracy: 9700/10000 (97.00%)\n",
      "\n",
      "Round  19, Average loss 1.911 Test accuracy 97.000\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8551 \n",
      "Accuracy: 9721/10000 (97.21%)\n",
      "\n",
      "Round  20, Average loss 1.855 Test accuracy 97.210\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.0106 \n",
      "Accuracy: 9707/10000 (97.07%)\n",
      "\n",
      "Round  21, Average loss 2.011 Test accuracy 97.070\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.0318 \n",
      "Accuracy: 9667/10000 (96.67%)\n",
      "\n",
      "Round  22, Average loss 2.032 Test accuracy 96.670\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9992 \n",
      "Accuracy: 9299/10000 (92.99%)\n",
      "\n",
      "Round  23, Average loss 1.999 Test accuracy 92.990\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8857 \n",
      "Accuracy: 9527/10000 (95.27%)\n",
      "\n",
      "Round  24, Average loss 1.886 Test accuracy 95.270\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8113 \n",
      "Accuracy: 9700/10000 (97.00%)\n",
      "\n",
      "Round  25, Average loss 1.811 Test accuracy 97.000\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8391 \n",
      "Accuracy: 9736/10000 (97.36%)\n",
      "\n",
      "Round  26, Average loss 1.839 Test accuracy 97.360\n",
      "selected users: [0 1 2 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.9353 \n",
      "Accuracy: 9652/10000 (96.52%)\n",
      "\n",
      "Round  27, Average loss 1.935 Test accuracy 96.520\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.7971 \n",
      "Accuracy: 9755/10000 (97.55%)\n",
      "\n",
      "Round  28, Average loss 1.797 Test accuracy 97.550\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.9200 \n",
      "Accuracy: 9682/10000 (96.82%)\n",
      "\n",
      "Round  29, Average loss 1.920 Test accuracy 96.820\n",
      "z_array: [-0.9  -0.81 -0.22  0.22  0.81  0.9 ]\n",
      "0.039572349708934106\n",
      "0.4838626198316927\n",
      "0.46852880899669475\n",
      "0.468528808996694\n",
      "0.48386261983169315\n",
      "0.039572349708934106\n",
      "@BACC_Enc: N,K,T, m_i= 6 2 3 30000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 6 2 3 30000 \n",
      "\n",
      "(T, sigma)= 3 1 )  0 -th Trial!!\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 2.2223 \n",
      "Accuracy: 6257/10000 (62.57%)\n",
      "\n",
      "Round   1, Average loss 2.222 Test accuracy 62.570\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 2.2161 \n",
      "Accuracy: 6335/10000 (63.35%)\n",
      "\n",
      "Round   2, Average loss 2.216 Test accuracy 63.350\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 1.6638 \n",
      "Accuracy: 8681/10000 (86.81%)\n",
      "\n",
      "Round   3, Average loss 1.664 Test accuracy 86.810\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 1.3783 \n",
      "Accuracy: 9617/10000 (96.17%)\n",
      "\n",
      "Round   4, Average loss 1.378 Test accuracy 96.170\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 1.2237 \n",
      "Accuracy: 9644/10000 (96.44%)\n",
      "\n",
      "Round   5, Average loss 1.224 Test accuracy 96.440\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.8223 \n",
      "Accuracy: 9638/10000 (96.38%)\n",
      "\n",
      "Round   6, Average loss 0.822 Test accuracy 96.380\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.7091 \n",
      "Accuracy: 9713/10000 (97.13%)\n",
      "\n",
      "Round   7, Average loss 0.709 Test accuracy 97.130\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.6716 \n",
      "Accuracy: 9713/10000 (97.13%)\n",
      "\n",
      "Round   8, Average loss 0.672 Test accuracy 97.130\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.6693 \n",
      "Accuracy: 9678/10000 (96.78%)\n",
      "\n",
      "Round   9, Average loss 0.669 Test accuracy 96.780\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.5860 \n",
      "Accuracy: 9650/10000 (96.50%)\n",
      "\n",
      "Round  10, Average loss 0.586 Test accuracy 96.500\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.5678 \n",
      "Accuracy: 9689/10000 (96.89%)\n",
      "\n",
      "Round  11, Average loss 0.568 Test accuracy 96.890\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.6501 \n",
      "Accuracy: 9699/10000 (96.99%)\n",
      "\n",
      "Round  12, Average loss 0.650 Test accuracy 96.990\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.5906 \n",
      "Accuracy: 9678/10000 (96.78%)\n",
      "\n",
      "Round  13, Average loss 0.591 Test accuracy 96.780\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.6501 \n",
      "Accuracy: 9738/10000 (97.38%)\n",
      "\n",
      "Round  14, Average loss 0.650 Test accuracy 97.380\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.5508 \n",
      "Accuracy: 9722/10000 (97.22%)\n",
      "\n",
      "Round  15, Average loss 0.551 Test accuracy 97.220\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.5475 \n",
      "Accuracy: 9593/10000 (95.93%)\n",
      "\n",
      "Round  16, Average loss 0.547 Test accuracy 95.930\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.5397 \n",
      "Accuracy: 9471/10000 (94.71%)\n",
      "\n",
      "Round  17, Average loss 0.540 Test accuracy 94.710\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.8024 \n",
      "Accuracy: 9654/10000 (96.54%)\n",
      "\n",
      "Round  18, Average loss 0.802 Test accuracy 96.540\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.6738 \n",
      "Accuracy: 9694/10000 (96.94%)\n",
      "\n",
      "Round  19, Average loss 0.674 Test accuracy 96.940\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.5364 \n",
      "Accuracy: 9693/10000 (96.93%)\n",
      "\n",
      "Round  20, Average loss 0.536 Test accuracy 96.930\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.6852 \n",
      "Accuracy: 9707/10000 (97.07%)\n",
      "\n",
      "Round  21, Average loss 0.685 Test accuracy 97.070\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.6254 \n",
      "Accuracy: 9715/10000 (97.15%)\n",
      "\n",
      "Round  22, Average loss 0.625 Test accuracy 97.150\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.5700 \n",
      "Accuracy: 9576/10000 (95.76%)\n",
      "\n",
      "Round  23, Average loss 0.570 Test accuracy 95.760\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.7501 \n",
      "Accuracy: 9319/10000 (93.19%)\n",
      "\n",
      "Round  24, Average loss 0.750 Test accuracy 93.190\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.8517 \n",
      "Accuracy: 9267/10000 (92.67%)\n",
      "\n",
      "Round  25, Average loss 0.852 Test accuracy 92.670\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.8802 \n",
      "Accuracy: 9491/10000 (94.91%)\n",
      "\n",
      "Round  26, Average loss 0.880 Test accuracy 94.910\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.8818 \n",
      "Accuracy: 8799/10000 (87.99%)\n",
      "\n",
      "Round  27, Average loss 0.882 Test accuracy 87.990\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.8674 \n",
      "Accuracy: 9460/10000 (94.60%)\n",
      "\n",
      "Round  28, Average loss 0.867 Test accuracy 94.600\n",
      "selected users: [0 1 2 3 4 5]\n",
      "\n",
      "Test set: Average loss: 0.6362 \n",
      "Accuracy: 9709/10000 (97.09%)\n",
      "\n",
      "Round  29, Average loss 0.636 Test accuracy 97.090\n",
      "z_array: [-0.9  -0.81 -0.22 -0.16  0.16  0.22  0.81  0.9 ]\n",
      "0.039572349708934106\n",
      "0.4838626198316927\n",
      "0.46852880899669475\n",
      "0.23252578774407598\n",
      "0.23252578774407545\n",
      "0.468528808996694\n",
      "0.48386261983169315\n",
      "0.039572349708934106\n",
      "@BACC_Enc: N,K,T, m_i= 8 2 3 30000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 8 2 3 30000 \n",
      "\n",
      "(T, sigma)= 3 1 )  0 -th Trial!!\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 2.2079 \n",
      "Accuracy: 4164/10000 (41.64%)\n",
      "\n",
      "Round   0, Average loss 2.208 Test accuracy 41.640\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 1.6987 \n",
      "Accuracy: 7686/10000 (76.86%)\n",
      "\n",
      "Round   1, Average loss 1.699 Test accuracy 76.860\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.5383 \n",
      "Accuracy: 9540/10000 (95.40%)\n",
      "\n",
      "Round   2, Average loss 0.538 Test accuracy 95.400\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.5397 \n",
      "Accuracy: 9462/10000 (94.62%)\n",
      "\n",
      "Round   3, Average loss 0.540 Test accuracy 94.620\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.6978 \n",
      "Accuracy: 9447/10000 (94.47%)\n",
      "\n",
      "Round   4, Average loss 0.698 Test accuracy 94.470\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.3977 \n",
      "Accuracy: 9543/10000 (95.43%)\n",
      "\n",
      "Round   5, Average loss 0.398 Test accuracy 95.430\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.8793 \n",
      "Accuracy: 9627/10000 (96.27%)\n",
      "\n",
      "Round   6, Average loss 0.879 Test accuracy 96.270\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.2739 \n",
      "Accuracy: 9606/10000 (96.06%)\n",
      "\n",
      "Round   7, Average loss 0.274 Test accuracy 96.060\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.2858 \n",
      "Accuracy: 9585/10000 (95.85%)\n",
      "\n",
      "Round   8, Average loss 0.286 Test accuracy 95.850\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.4189 \n",
      "Accuracy: 9596/10000 (95.96%)\n",
      "\n",
      "Round   9, Average loss 0.419 Test accuracy 95.960\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.4356 \n",
      "Accuracy: 9599/10000 (95.99%)\n",
      "\n",
      "Round  10, Average loss 0.436 Test accuracy 95.990\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.3830 \n",
      "Accuracy: 9478/10000 (94.78%)\n",
      "\n",
      "Round  11, Average loss 0.383 Test accuracy 94.780\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.3993 \n",
      "Accuracy: 9545/10000 (95.45%)\n",
      "\n",
      "Round  12, Average loss 0.399 Test accuracy 95.450\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.4343 \n",
      "Accuracy: 9518/10000 (95.18%)\n",
      "\n",
      "Round  13, Average loss 0.434 Test accuracy 95.180\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.2578 \n",
      "Accuracy: 9606/10000 (96.06%)\n",
      "\n",
      "Round  14, Average loss 0.258 Test accuracy 96.060\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.2952 \n",
      "Accuracy: 9710/10000 (97.10%)\n",
      "\n",
      "Round  15, Average loss 0.295 Test accuracy 97.100\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.3339 \n",
      "Accuracy: 9661/10000 (96.61%)\n",
      "\n",
      "Round  16, Average loss 0.334 Test accuracy 96.610\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.5458 \n",
      "Accuracy: 9503/10000 (95.03%)\n",
      "\n",
      "Round  17, Average loss 0.546 Test accuracy 95.030\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.4201 \n",
      "Accuracy: 9622/10000 (96.22%)\n",
      "\n",
      "Round  18, Average loss 0.420 Test accuracy 96.220\n",
      "selected users: [0 1 2 3 4 5 6 7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.5977 \n",
      "Accuracy: 9224/10000 (92.24%)\n",
      "\n",
      "Round  19, Average loss 0.598 Test accuracy 92.240\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.3295 \n",
      "Accuracy: 9644/10000 (96.44%)\n",
      "\n",
      "Round  20, Average loss 0.330 Test accuracy 96.440\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.3735 \n",
      "Accuracy: 9571/10000 (95.71%)\n",
      "\n",
      "Round  21, Average loss 0.373 Test accuracy 95.710\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.3915 \n",
      "Accuracy: 9354/10000 (93.54%)\n",
      "\n",
      "Round  22, Average loss 0.391 Test accuracy 93.540\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.4134 \n",
      "Accuracy: 9209/10000 (92.09%)\n",
      "\n",
      "Round  23, Average loss 0.413 Test accuracy 92.090\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.4039 \n",
      "Accuracy: 9634/10000 (96.34%)\n",
      "\n",
      "Round  24, Average loss 0.404 Test accuracy 96.340\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.4398 \n",
      "Accuracy: 9725/10000 (97.25%)\n",
      "\n",
      "Round  25, Average loss 0.440 Test accuracy 97.250\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.4958 \n",
      "Accuracy: 9551/10000 (95.51%)\n",
      "\n",
      "Round  26, Average loss 0.496 Test accuracy 95.510\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.4113 \n",
      "Accuracy: 9572/10000 (95.72%)\n",
      "\n",
      "Round  27, Average loss 0.411 Test accuracy 95.720\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.2503 \n",
      "Accuracy: 9627/10000 (96.27%)\n",
      "\n",
      "Round  28, Average loss 0.250 Test accuracy 96.270\n",
      "selected users: [0 1 2 3 4 5 6 7]\n",
      "\n",
      "Test set: Average loss: 0.3805 \n",
      "Accuracy: 9541/10000 (95.41%)\n",
      "\n",
      "Round  29, Average loss 0.381 Test accuracy 95.410\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "K = 2\n",
    "T = 3\n",
    "sigma = 1\n",
    "Noise_Alloc = [0,2,4]\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "\n",
    "N_array = [2,4,6,8]\n",
    "B_array = [0.5]\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 30\n",
    "\n",
    "\n",
    "\n",
    "loss_test_arr_v1 = np.zeros((len(N_array),len(B_array),N_trials,N_epochs))\n",
    "acc_test_arr_v1  = np.zeros((len(N_array),len(B_array),N_trials,N_epochs))\n",
    "\n",
    "for N_idx in range(len(N_array)):\n",
    "    \n",
    "    N = N_array[N_idx]\n",
    "    \n",
    "    \n",
    "           \n",
    "        \n",
    "    # print(\"alpha_array: \",alpha_array,'\\n')\n",
    "    \n",
    "    \n",
    "    for B_idx in range(len(B_array)):\n",
    "        \n",
    "        B = B_array[B_idx]\n",
    "        z_array = []\n",
    "#         while(len(z_array)<N):\n",
    "#             z_tmp = np.random.uniform(-1,1,1)\n",
    "#             MIS_tmp = MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_tmp], 1,sigma)\n",
    "#             if MIS_tmp < B and MIS_tmp > 0.1:\n",
    "#                 z_array.append(z_tmp[0])\n",
    "#         \n",
    "#         z_array = np.sort(z_array)\n",
    "\n",
    "        if N==2:\n",
    "            z_array = np.array([-0.81, 0.81])\n",
    "        elif N ==4:\n",
    "            z_array = np.array([-0.81, -0.22, 0.22, 0.81])\n",
    "        elif N ==5:\n",
    "            z_array = np.array([-0.81, -0.22, 0, 0.22, 0.81])\n",
    "        elif N ==6:\n",
    "            z_array = np.array([-0.9, -0.81, -0.22, 0.22, 0.81, 0.9])\n",
    "        elif N ==7:\n",
    "            z_array = np.array([-0.9, -0.82, -0.21, 0, 0.21, 0.82, 0.9])\n",
    "        else:\n",
    "            z_array = np.array([-0.9, -0.81, -0.22, -0.16, 0.16, 0.22, 0.81, 0.9])\n",
    "#             z_array = np.array([-0.9, -0.81, -0.22, -0.20, 0.20, 0.22, 0.81, 0.9])\n",
    "            \n",
    "        print('z_array:',z_array)\n",
    "        for j in range(len(z_array)):\n",
    "            print(MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_array[j]], 1,sigma))\n",
    "        \n",
    "        \n",
    "        _Noise_label = np.ones((30000*T,10)) * 0.1\n",
    "\n",
    "        X_tilde,a,b = BACC_Enc_Data_v3(encoding_input_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde,a,b = BACC_Enc_Data_v3(encoding_label_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "\n",
    "        m = N # m is the number of received result @ master\n",
    "    #     print('number of results:',m)\n",
    "\n",
    "        for trial_idx in range(N_trials):\n",
    "            print('(T, sigma)=',T,sigma,') ',trial_idx,'-th Trial!!')\n",
    "\n",
    "            net_glob = CNNMnist2(args=args)\n",
    "            net_glob.cuda()\n",
    "            net_glob.train()\n",
    "\n",
    "            # copy weights\n",
    "            w_glob = net_glob.state_dict()\n",
    "\n",
    "            for iter in range(N_epochs): #args.epochs\n",
    "                w_locals, loss_locals = [], []\n",
    "                idxs_users = np.random.choice(range(N), m, replace=False)\n",
    "                idxs_users = np.sort(idxs_users)\n",
    "                print('selected users:',idxs_users)\n",
    "                \n",
    "                coded_net = BACC_Enc_Model_withNoise_v3(net_glob.cuda(), N, K, T, 0.01, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "                dec_z_array = []\n",
    "                for idx in idxs_users: #for idx in range(N):\n",
    "            #         print(idx)\n",
    "                    local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[idx,:,:], label=y_tilde[idx,:,:])\n",
    "                    w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "                    w_locals.append(copy.deepcopy(w))\n",
    "                    loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                    dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "                # update global weights\n",
    "                #w_glob = FedAvg(w_locals)\n",
    "                w_glob = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "\n",
    "                # copy weight to net_glob\n",
    "                net_glob.load_state_dict(w_glob)\n",
    "\n",
    "                # print loss\n",
    "            #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "            #     loss_train_arr.append(loss_train)\n",
    "\n",
    "                acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "                acc_test_arr_v1[N_idx][B_idx][trial_idx][iter] = acc_test\n",
    "                loss_test_arr_v1[N_idx][B_idx][trial_idx][iter] = loss_test\n",
    "                if iter % 1 ==0:\n",
    "                    print('Round {:3d}, Average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_test, acc_test))\n",
    "                #print(loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
