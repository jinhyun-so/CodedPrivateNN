{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "\n",
    "import math\n",
    "\n",
    "from utils.sampling import mnist_iid, mnist_noniid, cifar_iid\n",
    "from utils.options import args_parser\n",
    "from models.Update import LocalUpdate\n",
    "from models.Nets import MLP, CNNMnist, CNNCifar, LeNet, CNNMnist2\n",
    "from models.Fed import FedAvg\n",
    "from models.Fed import FedQAvg, Quantization, Quantization_Finite, my_score, my_score_Finite\n",
    "from models.test import test_img\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "\n",
    "from utils.sampling import mnist_iid, mnist_noniid, cifar_iid\n",
    "from utils.options import args_parser\n",
    "from models.Update import LocalUpdate\n",
    "from models.Nets import MLP, CNNMnist, CNNCifar, LeNet, CNNMnist2, CNNMnist3\n",
    "from models.Fed import FedAvg\n",
    "from models.Fed import FedQAvg, Quantization, Quantization_Finite, my_score, my_score_Finite\n",
    "from models.test import test_img\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from utils.functions import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "class DatasetSplit(Dataset):\n",
    "    def __init__(self, dataset, idxs):\n",
    "        self.dataset = dataset\n",
    "        self.idxs = list(idxs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image, label = self.dataset[self.idxs[item]]\n",
    "        return image, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of X: (60000, 784)\n",
      "size of Y: (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "class my_argument:    \n",
    "    epochs = 200    #\"rounds of training\"\n",
    "    num_users = 15  # \"number of users: N\"\n",
    "    num_partition = 6 # \"number of users: K\"\n",
    "    frac = 0.5 #\"the fraction of clients: C\"\n",
    "    local_ep = 1 #\"the number of local epochs: E\"\n",
    "    local_bs = 200 #\"local batch size: B\"\n",
    "    bs=200 #\"test batch size\"\n",
    "    lr=0.01 #\"learning rate\"\n",
    "    momentum=0.5 # \"SGD momentum (default: 0.5)\"\n",
    "    split='user' # \"train-test split type, user or sample\"\n",
    "    opt='ADAM'\n",
    "    loss='Custom' # 'Custom' or 'Default'\n",
    "\n",
    "    # model arguments\n",
    "    model = 'cnn'\n",
    "    kernel_num=9 #, help='number of each kind of kernel')\n",
    "    kernel_sizes='3,4,5' #  help='comma-separated kernel size to use for convolution')\n",
    "    norm='None' #, help=\"batch_norm, layer_norm, or None\")\n",
    "    num_filters=32 #, help=\"number of filters for conv nets\")\n",
    "    max_pool='True' #help=\"Whether use max pooling rather than strided convolutions\")\n",
    "\n",
    "    # other arguments\n",
    "    dataset='mnist' #, help=\"name of dataset\")\n",
    "    iid=1\n",
    "    num_classes=10#, help=\"number of classes\")\n",
    "    num_channels=1#, help=\"number of channels of imges\")\n",
    "    gpu=1#, help=\"GPU ID, -1 for CPU\")\n",
    "    stopping_rounds=10#, help='rounds of early stopping')\n",
    "    verbose='False'#, help='verbose print')\n",
    "    seed=1#, help='random seed (default: 1)')\n",
    "    \n",
    "args = my_argument()\n",
    "\n",
    "args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "# load dataset and split users\n",
    "trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "dataset_train = datasets.MNIST('../data/mnist/', train=True, download=True, transform=trans_mnist)\n",
    "dataset_test = datasets.MNIST('../data/mnist/', train=False, download=True, transform=trans_mnist)\n",
    "\n",
    "dict_users = mnist_iid(dataset_train, args.num_partition)\n",
    "\n",
    "encoding_input_array_np = np.empty((len(dataset_train),28*28))\n",
    "encoding_label_array_np = np.empty((len(dataset_train),args.num_classes))\n",
    "print(\"size of X:\" ,encoding_input_array_np.shape)\n",
    "print(\"size of Y:\" ,encoding_label_array_np.shape)\n",
    "\n",
    "Size_submatrices = int(60000/args.num_partition)\n",
    "\n",
    "for i in range(args.num_partition):\n",
    "    \n",
    "    stt_pos = i*Size_submatrices\n",
    "    end_pos = (i+1)*Size_submatrices\n",
    "#     print(i,stt_pos,end_pos)\n",
    "    Temp_train = DataLoader(DatasetSplit(dataset_train, dict_users[i]), batch_size=Size_submatrices, shuffle=True)\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(Temp_train):\n",
    "        \n",
    "        images_np = images.detach().cpu().numpy()\n",
    "        encoding_input_array_np[stt_pos:end_pos,:] = np.reshape(images_np, (Size_submatrices,28*28))\n",
    "#         print(encoding_input_array_np[stt_pos:end_pos,:].shape)\n",
    "\n",
    "        onehot_labels = torch.nn.functional.one_hot(labels,num_classes=args.num_classes)\n",
    "        labels_np = onehot_labels.detach().cpu().numpy()\n",
    "#         print(labels_np.shape)\n",
    "        encoding_label_array_np[stt_pos:end_pos,:] = labels_np\n",
    "\n",
    "\n",
    "# print(labels_np[0:10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "371 128 96 78 327\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.9904231284161811 0.002545988916433775\n",
      "-0.9840348954471423 0.04568003220792001\n",
      "-0.9819451588330403 0.07563641161113017\n",
      "-0.9810861386834353 0.0908592108947527\n",
      "-0.9773599210161321 0.1801797307937816\n",
      "-0.9666355668346698 0.7354190442336326\n",
      "-0.9622589171126521 1.1365473047906514\n",
      "-0.9613183850556295 1.2389602118079723\n",
      "-0.9599456749886062 1.399564909478382\n",
      "-0.9590180134967923 1.5158887581506482\n",
      "-0.9575891080038901 1.7079852371845186\n",
      "-0.9561936585770487 1.9115739891980268\n",
      "-0.9543679699143783 2.2037704659742126\n",
      "-0.951463187769173 2.737506936512943\n",
      "-0.949455388077479 3.1657029088817357\n",
      "-0.9469672329851415 3.7834948509544737\n",
      "-0.94502274430461 4.356284407748436\n",
      "-0.9407530777561179 6.101814274120889\n",
      "-0.9380888764015067 7.993337972566525\n",
      "-0.9371679572108156 9.05245721134913\n",
      "-0.93714936164548 9.078129969539567\n",
      "-0.9351596835849159 16.93395976759343\n",
      "-0.934868399697222 16.85648390663707\n",
      "-0.933553528402197 10.287181768295074\n",
      "-0.93230483506727 8.547438326014959\n",
      "-0.9306512159656071 7.2270353504721925\n",
      "-0.9304061725181321 7.077272947456262\n",
      "-0.9285422123910514 6.156243902794919\n",
      "-0.9284706475452524 6.126745836086189\n",
      "-0.926091397231277 5.304569159524076\n",
      "-0.9240282259123225 4.764882996798698\n",
      "-0.9239775921042477 4.753069499807569\n",
      "-0.9218189900785929 4.298267308256091\n",
      "-0.918016648961208 3.6677820447396265\n",
      "-0.9140216977187852 3.154722070086179\n",
      "-0.9118838371578009 2.9225247008089044\n",
      "-0.9086775590671679 2.6147230445256078\n",
      "-0.9051661426068391 2.3207358369215383\n",
      "-0.9044789846637624 2.267525091167424\n",
      "-0.9028607334735599 2.14706267681658\n",
      "-0.9018155660238425 2.0726173700670296\n",
      "-0.9017529352765625 2.0682351501685305\n",
      "-0.8992809820446872 1.901898106849485\n",
      "-0.8988571865251784 1.8746060100339117\n",
      "-0.8987947665098965 1.8706148531796851\n",
      "-0.8976487597235991 1.7986084605882267\n",
      "-0.893749042696961 1.570175393526057\n",
      "-0.8934323636943777 1.5526618489170305\n",
      "-0.8929453746505238 1.5260135222648863\n",
      "-0.8921952041039565 1.4856237563386614\n",
      "-0.8909897401786475 1.422346996117031\n",
      "-0.8849176066977433 1.1315823485413727\n",
      "-0.883607492752386 1.0745012686589348\n",
      "-0.8814121439467837 0.9830311458863608\n",
      "-0.8810393847726823 0.9680074102279154\n",
      "-0.8773444965293582 0.8267843419511306\n",
      "-0.8705891325032475 0.602994169538691\n",
      "-0.8668009873980751 0.4958873780485744\n",
      "-0.8638130295218869 0.42025579656391726\n",
      "-0.8631199449826694 0.4037948545549337\n",
      "-0.8616646247223445 0.37052872055514907\n",
      "-0.8605909977180126 0.34709871587261626\n",
      "-0.8596849282372705 0.3280480006221594\n",
      "-0.8551948050154758 0.2430963594523323\n",
      "-0.8499071894805574 0.16209818172154994\n",
      "-0.8487402154325023 0.14681889418310934\n",
      "-0.846594795417767 0.12104299371943285\n",
      "-0.8453045192449964 0.1069388282273511\n",
      "-0.845076449583539 0.10455209586355176\n",
      "-0.8447520598828713 0.10121166342875396\n",
      "-0.8396548470745322 0.05674445839199171\n",
      "-0.8372466721598528 0.04067160903522765\n",
      "-0.8325077979580553 0.017417014713024922\n",
      "-0.8312716067022772 0.013054151015498247\n",
      "-0.8312537318881814 0.012995981334665039\n",
      "-0.8308861382476656 0.011830383371214911\n",
      "-0.8293815973339644 0.0076596071302451095\n",
      "-0.828642021767731 0.005955398169020435\n",
      "-0.8253497598698498 0.001014569743556195\n",
      "-0.8245215699909543 0.0004258705522058272\n",
      "-0.8215643690029988 0.0003549918590533305\n",
      "-0.8162074674227904 0.007794032692028207\n",
      "-0.8159321625689397 0.008424893483399568\n",
      "-0.8127923668882775 0.017249897540747412\n",
      "-0.8122514249294226 0.019066690859222437\n",
      "-0.8113630504228662 0.022235118155196014\n",
      "-0.8105880683607782 0.02518483004422279\n",
      "-0.8104776443581028 0.0256190992754115\n",
      "-0.8059243797144708 0.04650008771119135\n",
      "-0.8054328070786845 0.049095981027878224\n",
      "-0.8042248464943298 0.05575305883529662\n",
      "-0.8041611911077859 0.0561147885551441\n",
      "-0.8036739684356426 0.058919597700356104\n",
      "-0.8030890419042229 0.06237106540003054\n",
      "-0.8019539779809648 0.06933002664380651\n",
      "-0.8001160280725672 0.0813269306755082\n",
      "-0.7977501704801473 0.09809223049076682\n",
      "-0.7968070328233019 0.10519077007730936\n",
      "-0.7963403712511714 0.10879080282705819\n",
      "-0.7958494742672948 0.1126405673438981\n",
      "-0.7945960175911151 0.12276329471753313\n",
      "-0.7940177931363772 0.12757521727431664\n",
      "-0.7892021598106185 0.17118585993952753\n",
      "-0.7879774432308879 0.1833011081800356\n",
      "-0.7863175003120804 0.20040078141131565\n",
      "-0.7852610448703119 0.2116966160266723\n",
      "-0.784533719630583 0.21966254582901745\n",
      "-0.7837626085677347 0.22827836108599445\n",
      "-0.7793123032271692 0.2815217340887398\n",
      "-0.7766984486732706 0.31569951998187407\n",
      "-0.775395587958343 0.333574038508281\n",
      "-0.7741276083814144 0.3515214287245766\n",
      "-0.7735230038405245 0.36027420995477727\n",
      "-0.7725258632582515 0.3749894137185056\n",
      "-0.7724295786214894 0.3764289483564346\n",
      "-0.7552772800921701 0.6916369800271573\n",
      "-0.7552497829327798 0.6922471112773564\n",
      "-0.7546722651094258 0.7051484161359549\n",
      "-0.7544361634719448 0.7104707509380831\n",
      "-0.7528506725811264 0.7469451876501917\n",
      "-0.7505555262341803 0.8020705630083593\n",
      "-0.7499692258805941 0.816606924643033\n",
      "-0.7498562908240003 0.8194286399640631\n",
      "-0.7482152677614256 0.861232726291579\n",
      "-0.7479235010812884 0.8688247638196133\n",
      "-0.7448098034415702 0.9529494642418002\n",
      "-0.7445418932919416 0.9604596692302358\n",
      "-0.7439165831425094 0.9781610094487004\n",
      "-0.7437361363484825 0.9833143014961161\n",
      "-0.7405926977125972 1.076424210389164\n",
      "-0.7373633790786773 1.1789640292444037\n",
      "-0.7311131665497788 1.3991476675706653\n",
      "-0.7307661353362165 1.4122753750213564\n",
      "-0.7286395816367213 1.4949355095760268\n",
      "-0.726946234008679 1.563572911479474\n",
      "-0.725639067670848 1.618336826116883\n",
      "-0.7241227131777175 1.6838818296993838\n",
      "-0.7238191016093873 1.6972728662649335\n",
      "-0.7198425739769181 1.8813262311285877\n",
      "-0.718090982709602 1.9677960937803345\n",
      "-0.7178802996995504 1.9784318356886401\n",
      "-0.7168761251278946 2.0298387825327002\n",
      "-0.7165582785790297 2.0463600951790304\n",
      "-0.7162543909898018 2.0622699685364445\n",
      "-0.7129934896687886 2.2403055177459867\n",
      "-0.7083162827526726 2.5212623442603426\n",
      "-0.7074749069308783 2.5753466381528676\n",
      "-0.7062691341017759 2.6549015453065135\n",
      "-0.7060263478364155 2.6712206958198452\n",
      "-0.7039409093478051 2.8157488910822437\n",
      "-0.7037404897704596 2.83006415673707\n",
      "-0.7011664000412825 3.0210413595612255\n",
      "-0.7001982319602209 3.0964912614079836\n",
      "-0.6992444699179585 3.172894665077289\n",
      "-0.6960019801606028 3.4494706331361717\n",
      "-0.6937943836177498 3.6544840463488772\n",
      "-0.6934483628611301 3.6879871222655694\n",
      "-0.6934434930016833 3.6884614270916356\n",
      "-0.6922635415903962 3.8057376884281977\n",
      "-0.6906954200520405 3.9692959916486013\n",
      "-0.6896733345132917 4.081048328140918\n",
      "-0.6868492215460378 4.413890624048662\n",
      "-0.6844028988316475 4.735912512968022\n",
      "-0.6837285861772779 4.831165399748927\n",
      "-0.6829167706566832 4.950031376201902\n",
      "-0.6793872692957639 5.530103447781289\n",
      "-0.6778997285012158 5.812888129679127\n",
      "-0.6699632273131642 8.070744757166064\n",
      "-0.6692190073489266 8.406669287012278\n",
      "-0.6600684691034817 10.447775628206523\n",
      "-0.6586477917632128 9.353306770172134\n",
      "-0.6529239663617132 7.007904114452234\n",
      "-0.6493844347876268 6.168248051600161\n",
      "-0.6477439603072372 5.852116026052165\n",
      "-0.6465269117245975 5.6393177132605015\n",
      "-0.6464952850471015 5.634005455162172\n",
      "-0.6434052356472102 5.159954306990227\n",
      "-0.6369927690250221 4.3846350483300185\n",
      "-0.6368452515564611 4.369247878729058\n",
      "-0.6360766414373469 4.290525316962859\n",
      "-0.6352936687620434 4.212728663844577\n",
      "-0.6307795015559963 3.8052012925277996\n",
      "-0.6248554107505966 3.3542153138085204\n",
      "-0.6217281349196615 3.1455987241393775\n",
      "-0.6208380665779156 3.089339418514136\n",
      "-0.6206326414798171 3.0765378535988748\n",
      "-0.6204831282523122 3.0672628607311063\n",
      "-0.6175299068728715 2.8910020753638426\n",
      "-0.613781309076268 2.6844811182060035\n",
      "-0.6122882889226167 2.6069968150928142\n",
      "-0.6122616748661069 2.6056385434030207\n",
      "-0.6070650907865638 2.3545669225638863\n",
      "-0.6009156214374529 2.0892587363278317\n",
      "-0.6002040704959859 2.0604999809702855\n",
      "-0.5970842128587326 1.9386840089711013\n",
      "-0.5964193617712408 1.9135866649736744\n",
      "-0.5931418264006916 1.7940057175200388\n",
      "-0.5929262047809836 1.78637128792186\n",
      "-0.5889824643106643 1.6514853257735798\n",
      "-0.5838887060125266 1.4896388859586869\n",
      "-0.5779355997921842 1.3163006077753832\n",
      "-0.5733517789463733 1.1932948197946769\n",
      "-0.5694483327527624 1.0951310918212909\n",
      "-0.5687914083002878 1.0791772154184331\n",
      "-0.567793115514694 1.0552366262421187\n",
      "-0.5629903467930755 0.9450234796650745\n",
      "-0.5571715656841176 0.8219534979351008\n",
      "-0.5548018979739731 0.7749455559223152\n",
      "-0.5524835970954727 0.7306311769054984\n",
      "-0.5459310305680962 0.6139931801458144\n",
      "-0.5458683288794497 0.6129367535707945\n",
      "-0.5422018187472655 0.5530668984494092\n",
      "-0.5416518792896401 0.5444060048867483\n",
      "-0.5414928151175618 0.5419162475148196\n",
      "-0.5398210810229251 0.5161621368310448\n",
      "-0.5358017696559048 0.4572763306193677\n",
      "-0.5341751602364708 0.43463896227347104\n",
      "-0.5318106605573536 0.4029324946487317\n",
      "-0.5310016521270278 0.39240650547093453\n",
      "-0.526239154049484 0.33370390238593295\n",
      "-0.5259740309351935 0.33059738182345005\n",
      "-0.5248897760859934 0.31806721791681736\n",
      "-0.521992877716082 0.28594867140236\n",
      "-0.5215413301185898 0.2811184563302929\n",
      "-0.5177846941560689 0.24274250603315162\n",
      "-0.5176175864467449 0.24110960345347604\n",
      "-0.5175250989771951 0.24020854544414433\n",
      "-0.5173275191698967 0.23829003698355075\n",
      "-0.5164119178949618 0.2295131586537708\n",
      "-0.5108139474269602 0.17984800810657364\n",
      "-0.510280231358976 0.17546513126978355\n",
      "-0.5087532662818903 0.16325774214816632\n",
      "-0.5070339819525147 0.15009609862839568\n",
      "-0.4979791140815122 0.09065529772755573\n",
      "-0.49444951680064486 0.07181866316933845\n",
      "-0.4902442401278517 0.052414326104167866\n",
      "-0.4879127646911914 0.04304355477959956\n",
      "-0.4873243585279099 0.040832198823153275\n",
      "-0.4839130563119043 0.029212436033721954\n",
      "-0.4804729322034582 0.01953104757221101\n",
      "-0.47990950372347396 0.018136853699114486\n",
      "-0.4778717557311991 0.013537948425325413\n",
      "-0.4728914867642018 0.005172277080107043\n",
      "-0.47235185318652184 0.004506679362193653\n",
      "-0.4712672482134723 0.0033093371470955877\n",
      "-0.4711606378650097 0.003201732871582572\n",
      "-0.4663567517902234 0.00020433932133324855\n",
      "-0.4548849083925015 0.007273706297355838\n",
      "-0.4546161331433056 0.007673545765496144\n",
      "-0.45352374514547256 0.009407113368936534\n",
      "-0.45310967801550706 0.010109642091447381\n",
      "-0.447248596136975 0.022712951952431195\n",
      "-0.44636126249258345 0.025051379073048167\n",
      "-0.44630847822770625 0.02519403307087114\n",
      "-0.44396123408503296 0.03193990809110327\n",
      "-0.4418607467092899 0.038642416116663664\n",
      "-0.4415167141483842 0.03980003937638158\n",
      "-0.4374518847017965 0.05475148396853629\n",
      "-0.43721693851747534 0.055687471534153064\n",
      "-0.436999170972715 0.05656203113504275\n",
      "-0.4328615790107313 0.07446131381889441\n",
      "-0.4280234371949545 0.09849437122722364\n",
      "-0.4257087200930605 0.1111845768973641\n",
      "-0.42565383842543625 0.11149488334894173\n",
      "-0.42467604070679554 0.11709687317539356\n",
      "-0.41812593753003213 0.15824303723947514\n",
      "-0.41072987491412105 0.21244857959812924\n",
      "-0.4103010461801073 0.21584998559330942\n",
      "-0.4088912286987283 0.2272359960543415\n",
      "-0.40761192317214334 0.23783999143166698\n",
      "-0.4047624143370663 0.26240158525139906\n",
      "-0.40334209579578917 0.2751365225576308\n",
      "-0.4010851163888338 0.2960583125768653\n",
      "-0.39879019821088124 0.3182080227558447\n",
      "-0.3970910045761764 0.3351872642369688\n",
      "-0.39576699214948796 0.34876439541832094\n",
      "-0.3948821461432688 0.35800943345141883\n",
      "-0.39102502324714816 0.39994295557825255\n",
      "-0.38796131373756304 0.4351918548100595\n",
      "-0.3807523203264409 0.5252406849601114\n",
      "-0.3805656040729184 0.5277104186611956\n",
      "-0.37980433093065713 0.537853878915006\n",
      "-0.37966626391861036 0.5397063041615741\n",
      "-0.37802846139406454 0.5619825026172431\n",
      "-0.377249478039722 0.5727750461479172\n",
      "-0.376128011076174 0.588539027519073\n",
      "-0.37395631169326204 0.6198374788892499\n",
      "-0.3711415964132374 0.6619541690487597\n",
      "-0.3709137887725342 0.6654410519350825\n",
      "-0.367606328399831 0.7174191098091944\n",
      "-0.36711366126451583 0.7253818399703891\n",
      "-0.36624514270896924 0.7395610621242062\n",
      "-0.36533166174333265 0.7546714402728524\n",
      "-0.3647886439340968 0.7637505007797352\n",
      "-0.3647218888650894 0.7648716309404908\n",
      "-0.3557964596010834 0.9250699945844046\n",
      "-0.35237848118707715 0.9921412225780804\n",
      "-0.3522737409497745 0.9942495217341873\n",
      "-0.35226912633800644 0.9943424821679381\n",
      "-0.35128109268274965 1.0143901505287127\n",
      "-0.3489548361724897 1.0627407275075624\n",
      "-0.3459616146304647 1.1274031460415421\n",
      "-0.3445005409506803 1.1600021878787359\n",
      "-0.3437611750560441 1.176764495757814\n",
      "-0.34216369167278793 1.213603376602907\n",
      "-0.33983908772949833 1.2687671605721556\n",
      "-0.3395685472586707 1.2753098155068638\n",
      "-0.3389048523819509 1.2914705960707522\n",
      "-0.3349472493329593 1.3911779394549757\n",
      "-0.33432689079910793 1.4073414822215864\n",
      "-0.3336852275192912 1.4242170822951594\n",
      "-0.3333424795810218 1.4332972883593689\n",
      "-0.33296781893100436 1.4432759456897668\n",
      "-0.327288030325146 1.6016000786301579\n",
      "-0.3267847130802104 1.6162942415535153\n",
      "-0.32488110949070936 1.6728980973061687\n",
      "-0.3241290924668474 1.6957171729411653\n",
      "-0.3220055962319972 1.7615969713439201\n",
      "-0.3177702745038635 1.8996876262109317\n",
      "-0.3131022320356649 2.063071611872294\n",
      "-0.3114426553990659 2.124217238366898\n",
      "-0.3110897521824221 2.1374376723778368\n",
      "-0.30992867597675233 2.181487597668192\n",
      "-0.30785165034142925 2.262471044167906\n",
      "-0.30735164572491724 2.282398059550951\n",
      "-0.3068573128273968 2.302268167777752\n",
      "-0.301828862200888 2.5144829426011377\n",
      "-0.30132803449842727 2.5366833047937263\n",
      "-0.29407347834257624 2.882801821915101\n",
      "-0.282344797809313 3.564182846160121\n",
      "-0.27873139070021513 3.8144946896074123\n",
      "-0.27381703843668803 4.196088455780294\n",
      "-0.2735945346280475 4.2146674376985285\n",
      "-0.2702578807228313 4.509115332374465\n",
      "-0.2672350472426559 4.805276802822094\n",
      "-0.26712041811093945 4.8171392944057665\n",
      "-0.26606389815097153 4.928866915272201\n",
      "-0.2626632543315155 5.32141445795039\n",
      "-0.2608346455444317 5.557078010626485\n",
      "-0.2599803388762003 5.674173846620419\n",
      "-0.25862610406609 5.870145106232808\n",
      "-0.25548302513878585 6.383981908790039\n",
      "-0.25495350106987646 6.480320394375893\n",
      "-0.25384155253860197 6.693718036180018\n",
      "-0.25165076304175527 7.1667061757349115\n",
      "-0.2469676198235371 8.547833100332966\n",
      "-0.24676743693754388 8.624487379861995\n",
      "-0.24426700056630524 9.806208042197738\n",
      "-0.24037061545404503 14.271383284860526\n",
      "-0.23973129217443723 16.959700182264825\n",
      "-0.2391991999146783 20.631096173241797\n",
      "-0.23852673944710623 15.111856457090937\n",
      "-0.23803864029630328 13.722765753579937\n",
      "-0.2367256676840359 11.683988702994201\n",
      "-0.23568751693521262 10.712694220065805\n",
      "-0.23432811869171966 9.796222888055919\n",
      "-0.22910816727235628 7.736981375209303\n",
      "-0.22841311753029436 7.547939864670897\n",
      "-0.22465744469543814 6.699696837529417\n",
      "-0.22462858698529375 6.694068656898629\n",
      "-0.22459639284255117 6.687802940324817\n",
      "-0.2211458729673017 6.085915202549449\n",
      "-0.2199672877150598 5.90663647971231\n",
      "-0.21521181297668468 5.281299053817963\n",
      "-0.2140694236092 5.149899095262653\n",
      "-0.2065570529669183 4.413806373146899\n",
      "-0.20477287453016402 4.264719975460162\n",
      "-0.20261676362790704 4.094896325185893\n",
      "-0.2024721981783819 4.083884624065243\n",
      "-0.20152550935342717 4.012871984445207\n",
      "-0.199231557315678 3.8482504232811285\n",
      "-0.1972581098709465 3.714366944097657\n",
      "-0.1939444772896386 3.5038392224857238\n",
      "-0.19309854865628795 3.452707408880144\n",
      "-0.1930079248544596 3.4472889091348975\n",
      "-0.19096882605251708 3.3282758343807886\n",
      "-0.18820775058846806 3.1754316462047134\n",
      "-0.1864081733214762 3.080531386982283\n",
      "-0.18445557629438247 2.9814435977642577\n",
      "-0.1772919154200716 2.648430363174012\n",
      "-0.1759478365759044 2.5907104806320485\n",
      "-0.17558949237426047 2.575552956196313\n",
      "-0.1753023783664167 2.5634772100498555\n",
      "-0.173579621893031 2.492276106894975\n",
      "-0.16614706602098206 2.2077084986919027\n",
      "-0.16385488605957854 2.12663429165217\n",
      "-0.16299471262994025 2.0969521472070722\n",
      "-0.1625530485822151 2.0818643829144197\n",
      "-0.15996753539128816 1.9955573255534267\n",
      "-0.15512628294733055 1.842679636612185\n",
      "-0.15130461280851426 1.7294012388193747\n",
      "-0.15069468205362258 1.7118884442562408\n",
      "-0.14942734249173406 1.6759792471186834\n",
      "-0.14804970716195065 1.63766405591914\n",
      "-0.1451674096021236 1.559840465082967\n",
      "-0.14260597077192982 1.4932278333906748\n",
      "-0.1407519993814521 1.4464452389148927\n",
      "-0.14030475445779467 1.435334560530856\n",
      "-0.13899858538343723 1.403267536386491\n",
      "-0.1380694141355694 1.3807967918410748\n",
      "-0.1318813694822718 1.2380547569727078\n",
      "-0.13172180365534536 1.2345269031140136\n",
      "-0.1280441372946035 1.1552424928340297\n",
      "-0.12577996528933233 1.108305844714511\n",
      "-0.12573235672848315 1.1073338239382287\n",
      "-0.12547709214057234 1.1021324560065717\n",
      "-0.12436709155068582 1.079716280367028\n",
      "-0.122948517497532 1.0515398279336958\n",
      "-0.12223878369449337 1.037638356799422\n",
      "-0.12006317558874002 0.9958228614156931\n",
      "-0.11997457221034002 0.9941450673805232\n",
      "-0.118057430739821 0.9583161653068442\n",
      "-0.11703590654961271 0.9395906971324863\n",
      "-0.11580473970459959 0.9173542590839252\n",
      "-0.11455065531540387 0.8950717474718627\n",
      "-0.10989508350447252 0.8155098984865609\n",
      "-0.10696536752439689 0.7679094475218272\n",
      "-0.10310115657974595 0.7079218794538412\n",
      "-0.0975469466371619 0.6270394899435882\n",
      "-0.09312196261660755 0.5669104965069884\n",
      "-0.09207093225276441 0.5531711702762495\n",
      "-0.08973586509440756 0.5233748098835449\n",
      "-0.0855463604960216 0.47237830020681154\n",
      "-0.08412888767904159 0.4558245576104639\n",
      "-0.08235472143000355 0.4355942337449893\n",
      "-0.07760683862627582 0.38407434909690363\n",
      "-0.0750967800933704 0.35834295283182155\n",
      "-0.07096934785228659 0.31823280386528824\n",
      "-0.07049503716521532 0.31379592902269093\n",
      "-0.0683230077144874 0.29392504942126185\n",
      "-0.06628412200342515 0.27593303038043937\n",
      "-0.06626551807553249 0.2757717796820204\n",
      "-0.061131720735059236 0.23326014890802735\n",
      "-0.05897205243912862 0.21654115974112043\n",
      "-0.05883262976898007 0.21548516832582038\n",
      "-0.05741967521190161 0.20494197846033024\n",
      "-0.04944089880885971 0.1507264724832716\n",
      "-0.04753648728579951 0.13909532397761812\n",
      "-0.047334613570816364 0.1378914482446654\n",
      "-0.04489753875333702 0.12379424948116434\n",
      "-0.04463971768210784 0.1223498081024713\n",
      "-0.04153893738677583 0.10567460794317349\n",
      "-0.039833720038886966 0.09704863293465261\n",
      "-0.03773862258327987 0.08697389805288773\n",
      "-0.037412212201770156 0.08545595008196898\n",
      "-0.03481056006518779 0.0738517695703475\n",
      "-0.03466415250423216 0.07322476496137048\n",
      "-0.032670089825757254 0.06495937294325094\n",
      "-0.030128827826248816 0.055162571310271136\n",
      "-0.02800494362513506 0.04760333601306466\n",
      "-0.02120377551665098 0.02720219404992979\n",
      "-0.0205796599335355 0.02561801216070492\n",
      "-0.01945928086382942 0.022894750604861836\n",
      "-0.019450847078669176 0.022874837473741413\n",
      "-0.017535562622468026 0.01857913206201908\n",
      "-0.017300005439854216 0.01808191039985662\n",
      "-0.011525508675322316 0.008012687524031389\n",
      "-0.010811771200530895 0.007049936222483248\n",
      "-0.0074888192660385045 0.003380371456026997\n",
      "-0.004092432570407256 0.0010091055090967172\n",
      "-0.002596244749968557 0.00040609064621917616\n",
      "9.556869117521849e-05 5.502181213827029e-07\n",
      "0.0028471238451461822 0.0004883713107901068\n",
      "0.005075233029292692 0.0015521121830591915\n",
      "0.007379099351808582 0.003281992867637621\n",
      "0.011009804847376348 0.0073108650123358205\n",
      "0.011787179621769006 0.008381143014456378\n",
      "0.012241192515743071 0.009040166001474033\n",
      "0.013260558355738583 0.010611113967164161\n",
      "0.01579567118054248 0.015066784513710239\n",
      "0.02078138437213739 0.026124783888533575\n",
      "0.021141340966448707 0.027041551235548657\n",
      "0.02286676947714894 0.03165861930906668\n",
      "0.02313524856661786 0.03241022385109614\n",
      "0.023889036055112278 0.03456832065402661\n",
      "0.024079844418186003 0.035125815991418806\n",
      "0.02732883862814184 0.045316367370304325\n",
      "0.030629436139934185 0.0570274695667357\n",
      "0.03145338001077547 0.060166215111399995\n",
      "0.031557386116802144 0.06056855934450046\n",
      "0.03171219970235728 0.06117000362857127\n",
      "0.032058433039927836 0.06252616653676994\n",
      "0.03318533583103034 0.06704618512031972\n",
      "0.03327051477768039 0.06739444629185526\n",
      "0.03458729045746578 0.07289670155916114\n",
      "0.03751329862187203 0.0859245609956069\n",
      "0.040320633991066224 0.09947258041916017\n",
      "0.04125720107370823 0.10422292264382599\n",
      "0.045037492012434566 0.12458208250459586\n",
      "0.04509847144527179 0.12492617644785109\n",
      "0.04691553492824019 0.1354099814877149\n",
      "0.050248583360090304 0.1558098477186154\n",
      "0.05109915485985894 0.16126061603200506\n",
      "0.05217753872067865 0.1683157282485783\n",
      "0.05643459214888069 0.19776141627606064\n",
      "0.05757858382201819 0.20611335609823053\n",
      "0.05867715654523775 0.21431093222242378\n",
      "0.06424296216633052 0.2585531416129071\n",
      "0.06654926933226757 0.2782369254546993\n",
      "0.06782549758236289 0.28947613241419784\n",
      "0.06906735586979784 0.30065244177515243\n",
      "0.06907072775472822 0.3006831113811658\n",
      "0.06981760507063783 0.3075198750253214\n",
      "0.07032103907677256 0.31217710146363\n",
      "0.07144399612835439 0.32270812105887225\n",
      "0.07319425112059541 0.3395177249241\n",
      "0.0745361682373118 0.35273546319749083\n",
      "0.07584683238627288 0.36592464219224863\n",
      "0.07623844050543216 0.3699193295369253\n",
      "0.07733533263560699 0.3812414586671621\n",
      "0.0788966985424242 0.39769863584745685\n",
      "0.07889779134548447 0.39771029537037267\n",
      "0.0838482795359643 0.4525888324783195\n",
      "0.08489043643040972 0.4646747567372094\n",
      "0.08708334907511106 0.49072500288957493\n",
      "0.09160163574530222 0.5471024830972919\n",
      "0.0916938476893312 0.548291711364905\n",
      "0.09495058738563333 0.5913072651680297\n",
      "0.09522183487270985 0.5949799235211305\n",
      "0.09647935974630117 0.6121900647380172\n",
      "0.09653547420481279 0.6129650987595937\n",
      "0.099887827763387 0.6603782791231227\n",
      "0.10063610772325982 0.671263880502214\n",
      "0.1018162799185307 0.6886606592956084\n",
      "0.10262319776898599 0.7007175874892921\n",
      "0.10364833693900777 0.7162271326557963\n",
      "0.10380003540039895 0.7185405800629042\n",
      "0.1040110293674621 0.7217662133102727\n",
      "0.1044211609581589 0.7280626280394812\n",
      "0.10530433414905915 0.7417402736428617\n",
      "0.10592715455891288 0.7514842311133518\n",
      "0.10912527396946126 0.8028215366726691\n",
      "0.10919123791152141 0.8039036928124341\n",
      "0.11185744883253124 0.8484484269894019\n",
      "0.11197813466412043 0.8505022971333734\n",
      "0.1148674619659229 0.9006660083845679\n",
      "0.11539090614790615 0.9099605127219079\n",
      "0.1156784129278634 0.9150929674228241\n",
      "0.11711744988412631 0.9410762372529466\n",
      "0.11790386855334067 0.9554851404365854\n",
      "0.11835729887611435 0.9638609444543385\n",
      "0.12219650371957314 1.0368143026428283\n",
      "0.12376895347068828 1.067771766705637\n",
      "0.12392326927700315 1.0708444062245177\n",
      "0.12999142939619368 1.196743737550039\n",
      "0.1306707379934493 1.2114737830009064\n",
      "0.13445218421124716 1.2959330534656608\n",
      "0.13848924580319255 1.3909151134074946\n",
      "0.1386484028498891 1.3947658823100653\n",
      "0.13897546417705064 1.402704969202715\n",
      "0.13938199312606958 1.4126218873375818\n",
      "0.14200059118249708 1.4778220949269474\n",
      "0.14246040244775826 1.4895117970550826\n",
      "0.1444380139027701 1.5406330204832148\n",
      "0.14553623990663955 1.5696266831279293\n",
      "0.1469808312990908 1.6084401337047334\n",
      "0.14713865645505497 1.6127278888329628\n",
      "0.1488863621100467 1.6608450554452676\n",
      "0.15073494625816242 1.7130398767902515\n",
      "0.15106489018639468 1.7225000563979527\n",
      "0.15538025359950858 1.850431501801188\n",
      "0.1562955954378591 1.8786099116152775\n",
      "0.160510449933279 2.0134000804101677\n",
      "0.16074079967571153 2.021014856353388\n",
      "0.16170116012002622 2.053050736153274\n",
      "0.16187064448505217 2.0587533054304954\n",
      "0.1664096686870853 2.2171855970908965\n",
      "0.16792065399312506 2.2724964272129085\n",
      "0.169207154237905 2.320666374162804\n",
      "0.17730905236543348 2.649175282384812\n",
      "0.17880390173409078 2.7150515895845992\n",
      "0.18233546342729712 2.8781199616297073\n",
      "0.1825355486240945 2.887689752004705\n",
      "0.1833258256892687 2.9258522664769457\n",
      "0.1885161414367027 3.1920566074982033\n",
      "0.18935362979308556 3.237759240149436\n",
      "0.1912675662156642 3.3453726232159218\n",
      "0.19747734994563726 3.728910049688281\n",
      "0.20134862561071887 3.999810149757474\n",
      "0.20503167156008728 4.285842344439152\n",
      "0.2060088683916803 4.367120374035006\n",
      "0.20768265064406388 4.512259466231429\n",
      "0.21277208623306842 5.0078685250107675\n",
      "0.21383856550155755 5.124085681141811\n",
      "0.2160552637156521 5.382468707028418\n",
      "0.21709221346732033 5.512110795246849\n",
      "0.21827128526982387 5.667236668575488\n",
      "0.21976339083343954 5.876750082947836\n",
      "0.22295392222427668 6.385267927530759\n",
      "0.22646091340227703 7.075647751686589\n",
      "0.22883830533855853 7.662087665385068\n",
      "0.2339010148659657 9.559701787854456\n",
      "0.23594796025017684 10.92730707187355\n",
      "0.24147805519151455 12.199316388920021\n",
      "0.2415345079965041 12.124896849571615\n",
      "0.2437787040069772 10.106246586960916\n",
      "0.24396855520219285 9.98586677628883\n",
      "0.2442405369507592 9.821695543128296\n",
      "0.2461206760398722 8.887002101097798\n",
      "0.24749271813258478 8.355903580229636\n",
      "0.2502273997608482 7.521418823719918\n",
      "0.25071774208323583 7.394254810418459\n",
      "0.2516110314638158 7.176040035584177\n",
      "0.25305384027702504 6.85502456212432\n",
      "0.2648520618160004 5.062655097016612\n",
      "0.2656220963484286 4.976918836432452\n",
      "0.26604587179996386 4.930811758210441\n",
      "0.2754828043301658 4.060789275412806\n",
      "0.27580839576580374 4.035094059494668\n",
      "0.27680583669424585 3.95782480983261\n",
      "0.2780533725988914 3.864119270470135\n",
      "0.2833774278886567 3.496681230439107\n",
      "0.28541500348223203 3.3682456142812893\n",
      "0.2858947497751303 3.3388788463841843\n",
      "0.286049458673979 3.3294770173055332\n",
      "0.28729967534420453 3.2546929763657886\n",
      "0.28877334289509604 3.1691674595997754\n",
      "0.29478265211810006 2.8467874235381565\n",
      "0.2979589451109341 2.6914843106342743\n",
      "0.2987337941074304 2.655013960809325\n",
      "0.29959770870273505 2.614973583517012\n",
      "0.3004980999625635 2.573922771483873\n",
      "0.30153685204472436 2.5274023481553027\n",
      "0.3019336009371125 2.5098656725982167\n",
      "0.30207641324579027 2.503584100742461\n",
      "0.3044077986748823 2.4032868693095\n",
      "0.3051748284618079 2.371187437523524\n",
      "0.3058728663082717 2.3423485221459446\n",
      "0.30699533844561766 2.296703067641382\n",
      "0.30978581512073333 2.1869670714823775\n",
      "0.3106824810349611 2.1527917601457522\n",
      "0.32485072653594793 1.6738149427389297\n",
      "0.325044875092241 1.6679635921358757\n",
      "0.32608928472780985 1.636782746892764\n",
      "0.32938703769258426 1.5415120460676546\n",
      "0.33071634301833264 1.5044282750253508\n",
      "0.3311191738461039 1.4933355073194687\n",
      "0.33134275093912624 1.4872076239484164\n",
      "0.33186713047280336 1.4729152417252314\n",
      "0.3333825830341237 1.4322324649908567\n",
      "0.33372615177405596 1.4231359844503721\n",
      "0.3396038773407406 1.2744539337778102\n",
      "0.3404696691669371 1.253617552491044\n",
      "0.3424987410428313 1.2058057258164128\n",
      "0.34469901702579975 1.1555331061722842\n",
      "0.34674219931577 1.1102685072968494\n",
      "0.3508440540543716 1.023349897440647\n",
      "0.3510552612297295 1.0190128413907413\n",
      "0.35285531291178773 0.9825835237232086\n",
      "0.35805191803386105 0.8825993888921487\n",
      "0.35968819309693134 0.852648670839848\n",
      "0.36348212123760093 0.7858940614676088\n",
      "0.3673880516716006 0.7209398569413683\n",
      "0.3683107887879764 0.7061335560360252\n",
      "0.36886935586754177 0.697268519026922\n",
      "0.3693770312329834 0.6892746256982586\n",
      "0.3696148274473898 0.6855509341266514\n",
      "0.3710644229740252 0.663134079163564\n",
      "0.38250177710348265 0.5024439419754863\n",
      "0.3861148059014512 0.4572906342153241\n",
      "0.3875859791551062 0.4396311930745742\n",
      "0.3884984263325242 0.42888538542212046\n",
      "0.3890184470298317 0.4228313168489532\n",
      "0.3901456015044802 0.4098824171275876\n",
      "0.3919225611407875 0.3899450831357474\n",
      "0.392900782684793 0.37921534485826786\n",
      "0.3952610341127645 0.35403384124931697\n",
      "0.39573917333598185 0.34905295294386257\n",
      "0.4013953302293738 0.29313238869014857\n",
      "0.4036863638044432 0.27201935402281924\n",
      "0.4057493383225126 0.25374635164635906\n",
      "0.40581721434882567 0.25315688856858465\n",
      "0.4068711090530248 0.24409977336234379\n",
      "0.40817245975419336 0.23316176447367679\n",
      "0.40829716152834394 0.23212780492257526\n",
      "0.4083997399653805 0.23127913171783607\n",
      "0.4099012369506243 0.2190471702638439\n",
      "0.4101208746787963 0.21728767203760835\n",
      "0.4137004673338567 0.1896711408571567\n",
      "0.41480682269821334 0.18153536772793663\n",
      "0.4148662377445975 0.18110373996996484\n",
      "0.41510435723752215 0.17937928987456314\n",
      "0.4161592176686215 0.17184369161542268\n",
      "0.4176340551794564 0.1615900309009851\n",
      "0.41950847753339127 0.1490294528056848\n",
      "0.4200100442271988 0.1457573136739928\n",
      "0.4220869170442536 0.13260482849723784\n",
      "0.4266115664869927 0.1061425367407814\n",
      "0.42791214509897646 0.09908678978139285\n",
      "0.4279215047366458 0.09903689869535227\n",
      "0.4286815259514094 0.09502780670901187\n",
      "0.4307774681779246 0.08440236299297744\n",
      "0.4360417883340524 0.06048690910081652\n",
      "0.436800424780855 0.05736608641516592\n",
      "0.4392872318178196 0.04770982485536699\n",
      "0.44584152667094945 0.026473354970261584\n",
      "0.4471501026406359 0.022966955818023136\n",
      "0.44991945255504584 0.016355664541577185\n",
      "0.4524902854338888 0.011207045273567721\n",
      "0.4541998735576056 0.008313610117113254\n",
      "0.4674722696799143 0.0005798694132265167\n",
      "0.4687699175136524 0.0012595100933337064\n",
      "0.4695630154610011 0.001804253157645923\n",
      "0.47075299388740777 0.002806885054775086\n",
      "0.4755899754576576 0.00920368465521213\n",
      "0.4770645105740525 0.011906744485386126\n",
      "0.4773382510470048 0.0124478421275829\n",
      "0.4782032990408931 0.014239116106903668\n",
      "0.47842825955045654 0.014725259332684683\n",
      "0.47906345849206855 0.01614337454292756\n",
      "0.48142676288686426 0.022013378277897074\n",
      "0.48811477704661943 0.04381695855234051\n",
      "0.4890825474447651 0.04762317476805903\n",
      "0.49262644330762684 0.06300638874798273\n",
      "0.4944651986350732 0.07189715047208123\n",
      "0.49534177419249836 0.07635774235435622\n",
      "0.495352555350564 0.07641350320340833\n",
      "0.4963742154970039 0.08179723286695544\n",
      "0.49968712582533414 0.1006267136514924\n",
      "0.5060691474981469 0.1429776980406858\n",
      "0.5066532327938247 0.14726412611841763\n",
      "0.5090123270234492 0.16529431374217524\n",
      "0.5098976812626417 0.1723607278202993\n",
      "0.5101237826903122 0.17419180928171885\n",
      "0.5108596011223208 0.18022572271885615\n",
      "0.5111146421646526 0.1823439522766453\n",
      "0.5119221235951494 0.18914193442338817\n",
      "0.5152019779056816 0.21819983638317805\n",
      "0.5157035136557346 0.2228501372966672\n",
      "0.5166727435379781 0.23199441026039144\n",
      "0.5188589279332452 0.25338928253537896\n",
      "0.519646337497758 0.2613589072809299\n",
      "0.5214123870826233 0.2797477942299663\n",
      "0.52226573578416 0.2888903185297695\n",
      "0.5252459266241631 0.3221522403487952\n",
      "0.5254256469142506 0.32422505070652097\n",
      "0.5266819569391148 0.3389298747802812\n",
      "0.5269850004936376 0.3425335341970838\n",
      "0.5273996719171725 0.3475004546487921\n",
      "0.5277515907923354 0.3517482979132454\n",
      "0.5321902686157909 0.40792791132016626\n",
      "0.5324224768210304 0.4110014415103249\n",
      "0.5426047073521529 0.5594642142869072\n",
      "0.5433164106029837 0.5708736804618628\n",
      "0.5437852330750841 0.5784655984531915\n",
      "0.5442241261365857 0.5856279131173169\n",
      "0.5493371899478232 0.6730646359838186\n",
      "0.5525074490995565 0.7310788161718826\n",
      "0.5529238240857672 0.7389206686513267\n",
      "0.5582964016494991 0.844886052203351\n",
      "0.5646213380491598 0.9815471937284748\n",
      "0.5651008397259205 0.9924596203744515\n",
      "0.5663027300903174 1.020166126109156\n",
      "0.5665225921341823 1.0252897329774986\n",
      "0.5666533662972246 1.028345423087546\n",
      "0.5670149082531264 1.0368250662409106\n",
      "0.5672195946541592 1.0416465749390231\n",
      "0.5677661276703403 1.0545944592990086\n",
      "0.5742619696883857 1.2170370618385493\n",
      "0.5753016057744667 1.2445633267870488\n",
      "0.5761673328604291 1.2678219888562834\n",
      "0.5767443854411787 1.2834976670112923\n",
      "0.5796914383992655 1.3657652327487517\n",
      "0.5798257563228622 1.3696048634027098\n",
      "0.5807829363179504 1.3972003468791483\n",
      "0.5814817362724711 1.417608317030811\n",
      "0.5823958305666685 1.4446428297598228\n",
      "0.5853401486654333 1.5344173858540198\n",
      "0.5870782072550362 1.5894220727332886\n",
      "0.5877888176290198 1.6123580441281273\n",
      "0.5878378593182876 1.6139506917842767\n",
      "0.5895511009470233 1.670394997680448\n",
      "0.5898123958850663 1.6791435769393233\n",
      "0.5898509563191583 1.6804378308024235\n",
      "0.5919471162160252 1.7520522005684176\n",
      "0.5935136182100691 1.8072352961559317\n",
      "0.5940852596850017 1.8277400636215226\n",
      "0.595573894034088 1.8820897281736662\n",
      "0.5960037603824317 1.898045948536934\n",
      "0.5973516983424547 1.9488643710644438\n",
      "0.5994170774088479 2.0291240815311067\n",
      "0.6009526235786056 2.09076455694154\n",
      "0.6019206570158764 2.1305275798947227\n",
      "0.6024944795741796 2.154438557059403\n",
      "0.6031686431128445 2.182862230290126\n",
      "0.6037088483613064 2.205901512714295\n",
      "0.6059187807238808 2.3026874834687963\n",
      "0.605987729484242 2.305774756821356\n",
      "0.6100653962587861 2.4961881403383224\n",
      "0.610621927369116 2.5234397657641447\n",
      "0.6111514462974414 2.5496686544345994\n",
      "0.6118617361286518 2.585321654950847\n",
      "0.612089444929028 2.596867631463818\n",
      "0.6121977751066956 2.602380575501327\n",
      "0.6129029200228693 2.6385859796179214\n",
      "0.6140897898388118 2.700813470035068\n",
      "0.6179194435282194 2.913525425180718\n",
      "0.6186992869373587 2.959261691861644\n",
      "0.6213265264842669 3.120052703087796\n",
      "0.6216662995973321 3.14164792738232\n",
      "0.6243611605978185 3.3200421988017466\n",
      "0.6264567560577101 3.4682978307841674\n",
      "0.6276040408619068 3.553383189649018\n",
      "0.6289418957021555 3.6564124088530554\n",
      "0.6305328528845286 3.7847121607349625\n",
      "0.6366729258333681 4.351388168718865\n",
      "0.637092132472971 4.3950511930868785\n",
      "0.6383644787076501 4.532263943821744\n",
      "0.6413300540725988 4.883209927244416\n",
      "0.6424884978323171 5.0341192975236035\n",
      "0.6429568561445025 5.097662276001608\n",
      "0.6435164146394023 5.175628654943423\n",
      "0.6438423952825829 5.222125918636018\n",
      "0.6439828617641932 5.242414823576482\n",
      "0.6469914250042117 5.718571312551542\n",
      "0.6485799864986992 6.008630038663202\n",
      "0.6501444545243378 6.328146308921745\n",
      "0.6535808130643614 7.196321042389052\n",
      "0.6541154627783883 7.359722610237817\n",
      "0.6550963044096301 7.686945899750917\n",
      "0.6572691091582401 8.585865398733956\n",
      "0.6580099883494335 8.972299470695653\n",
      "0.6591224732085252 9.674300052155182\n",
      "0.6633104903727323 18.47722376252621\n",
      "0.663356219185252 17.848275922596827\n",
      "0.6642421728708279 13.321691386999774\n",
      "0.665643077279374 10.972957131527728\n",
      "0.6673238237567074 9.490273774506848\n",
      "0.6677486220474551 9.21024175316423\n",
      "0.6691438568662744 8.442822838508906\n",
      "0.6716754801293578 7.41808428455787\n",
      "0.6727222141448985 7.080111909417734\n",
      "0.677459314440443 5.902018738955346\n",
      "0.6777781399427101 5.837230497195313\n",
      "0.6805010687532049 5.334612245102215\n",
      "0.6828633818763121 4.958017044320794\n",
      "0.6876588463446744 4.314575059277689\n",
      "0.6913868864421766 3.8960417868661525\n",
      "0.6915798240698554 3.875925650111937\n",
      "0.6928059065091918 3.7512415286620047\n",
      "0.6951374284326852 3.5280196757062154\n",
      "0.6958641161285852 3.461852158990661\n",
      "0.6981912457722883 3.259780741162372\n",
      "0.6996310382479349 3.1416730646761257\n",
      "0.7004236870469911 3.0787353805279047\n",
      "0.701362715797138 3.0059925699704646\n",
      "0.7030274188043371 2.881627666425819\n",
      "0.7053371001384043 2.718114112598333\n",
      "0.7085832917282238 2.5043369857803865\n",
      "0.7125349935941883 2.2664578663969346\n",
      "0.715087932372318 2.124394513262351\n",
      "0.7158626028832895 2.082948180461364\n",
      "0.7191322614341586 1.9159763547480992\n",
      "0.7196232307145844 1.8919665138815185\n",
      "0.7225901124718073 1.7524139664079683\n",
      "0.7237537797830584 1.7001657873019753\n",
      "0.7257088214580791 1.6153743583261173\n",
      "0.7258502806617155 1.6093805524435834\n",
      "0.7259708292354208 1.6042875320105918\n",
      "0.7279831672669843 1.5212393344301776\n",
      "0.7283983215322571 1.504559217274635\n",
      "0.7332777833235287 1.3194722634858622\n",
      "0.7345319301146289 1.2750025014917155\n",
      "0.7355932909126011 1.238306808791736\n",
      "0.7376412731957804 1.1698536205663175\n",
      "0.7376809448474697 1.1685575285798804\n",
      "0.7403842236595697 1.0828281201141943\n",
      "0.741117482116836 1.0604322032584155\n",
      "0.7432760106919218 0.9965472137905182\n",
      "0.7436108092983307 0.986905442352449\n",
      "0.7501401069856983 0.8123507594007743\n",
      "0.7505738357883331 0.8016196312225655\n",
      "0.7538543474640014 0.7237064067390169\n",
      "0.7553248526984526 0.6905822806636971\n",
      "0.75720063289866 0.6498768554931679\n",
      "0.7617006756292153 0.5589421158053531\n",
      "0.763554532886279 0.5240934982612449\n",
      "0.7637277619877467 0.5209117973110154\n",
      "0.765669654446046 0.48609608188664066\n",
      "0.7698809592508202 0.41575158689258174\n",
      "0.7725818172250503 0.37415437107246097\n",
      "0.773154108310717 0.36567732741754705\n",
      "0.7735425713624506 0.3599889404372347\n",
      "0.7764172923637691 0.3195088577679582\n",
      "0.7767276891243888 0.31530485064286135\n",
      "0.7795661932366822 0.2783189858953781\n",
      "0.782899993482189 0.23812645877472438\n",
      "0.7851340650802787 0.21307617248940808\n",
      "0.7856165312364511 0.2078595012084731\n",
      "0.7904803619761902 0.158988893994254\n",
      "0.7909248620152292 0.15485346539803582\n",
      "0.7910537636242196 0.15366441138448214\n",
      "0.7912813937221144 0.15157579857014872\n",
      "0.7915736431989451 0.14891513516464638\n",
      "0.7916184710914871 0.14850909014512417\n",
      "0.7926354453165991 0.1394450357403435\n",
      "0.7933626564706455 0.13313618275131428\n",
      "0.7943611717957488 0.12470679875072628\n",
      "0.7956769616335087 0.11400876570843244\n",
      "0.7971584857005398 0.1025178536966388\n",
      "0.7975216780741885 0.09979023548080063\n",
      "0.7992417090941686 0.08734932104015561\n",
      "0.7994931482892087 0.08559656892353125\n",
      "0.8015352720536366 0.07198392502021898\n",
      "0.8051442076368633 0.05065057468096723\n",
      "0.8052809355257557 0.04991125065092916\n",
      "0.8059580799565662 0.046324530868009625\n",
      "0.8065847901469683 0.04311621066257099\n",
      "0.8074034172147471 0.03908739411947029\n",
      "0.8086757166197549 0.03319294805712934\n",
      "0.8101375858316646 0.02697825066438896\n",
      "0.811924275028886 0.02020687784304015\n",
      "0.8125930082833805 0.017909492325330328\n",
      "0.8137343613638415 0.014291716886877788\n",
      "0.8178408045628367 0.004539136587694629\n",
      "0.8251223719258953 0.0008274686614124949\n",
      "0.8260688793813309 0.0017347099308916715\n",
      "0.8277587370943 0.004211294129136133\n",
      "0.8295406371350378 0.00805554709834623\n",
      "0.8341368773220168 0.024214993250551306\n",
      "0.8399084722081511 0.058614489866136925\n",
      "0.8404849377650505 0.06299385541693747\n",
      "0.8421915863317564 0.07703075524904864\n",
      "0.8448697568270083 0.10241631368085516\n",
      "0.8456088974748095 0.11017352512151361\n",
      "0.8463784832348702 0.11860644843316229\n",
      "0.8464707384269048 0.11964203528321504\n",
      "0.8466193095880858 0.12132096936011394\n",
      "0.8471364902899954 0.12727363041813733\n",
      "0.8495270665278654 0.15702182885124216\n",
      "0.8504407118209916 0.16938690694363742\n",
      "0.852174244418985 0.19441331456907923\n",
      "0.8566998697750372 0.26985317947039483\n",
      "0.8604627476437905 0.34436221124190436\n",
      "0.8624885235025772 0.38914689875494307\n",
      "0.8636483174987128 0.41630739846130144\n",
      "0.8658755180254094 0.47164271639673416\n",
      "0.8670354766217887 0.5021485489819627\n",
      "0.8700355212255284 0.5865396272583102\n",
      "0.870762144760552 0.6081935921094614\n",
      "0.8719307188483756 0.6440293439759636\n",
      "0.8740037785838763 0.7107170883239025\n",
      "0.8746699165943255 0.7330043840820991\n",
      "0.8824377587205814 1.0251225287546812\n",
      "0.8840117549699711 1.0919124311515895\n",
      "0.8901449806469299 1.379160150142323\n",
      "0.8928069737945963 1.5185021821386981\n",
      "0.8970510045953746 1.761976346911147\n",
      "0.9020324691301913 2.0878616053126513\n",
      "0.9040343083567581 2.2337627734200662\n",
      "0.9055536147948142 2.351316011963311\n",
      "0.9067999501755222 2.452659442638715\n",
      "0.9081516228755009 2.56811069273013\n",
      "0.9086375751670448 2.6111444502342414\n",
      "0.9128112617381887 3.0202511724694316\n",
      "0.9136797579956788 3.115892575984129\n",
      "0.9142279896825001 3.1784843458876817\n",
      "0.9155614127277611 3.3386044184292434\n",
      "0.92014760545438 3.999220042759636\n",
      "0.9220493560204399 4.3427448737666055\n",
      "0.9226929541365934 4.471777175911894\n",
      "0.9236401696088063 4.675831720815239\n",
      "0.9238876916465115 4.732240277801473\n",
      "0.9271951918687891 5.652256229525992\n",
      "0.9272424902148182 5.668320213984287\n",
      "0.9273512937154826 5.705676563956932\n",
      "0.9283164744354426 6.064351629026801\n",
      "0.9288486765355279 6.286624633774166\n",
      "0.9305213771759107 7.14661452080343\n",
      "0.9329993375460999 9.378470199154883\n",
      "0.934833975724469 16.25365956491745\n",
      "0.9379807468157946 8.10036637312172\n",
      "0.9382315923103051 7.857509369978746\n",
      "0.9463145644615452 3.9654742904062816\n",
      "0.9473726311642992 3.6750523801132604\n",
      "0.9484801265104987 3.394922843412941\n",
      "0.9499617519231454 3.052471520811751\n",
      "0.9519505758510158 2.6414025023900956\n",
      "0.9535987665259829 2.3363689441381097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.953671633298314 2.3235529424344445\n",
      "0.9540220323398028 2.262675012729333\n",
      "0.9555601310592956 2.0095231125905384\n",
      "0.955770805950217 1.976555626669604\n",
      "0.9559148077806445 1.954249262940429\n",
      "0.9571384738708655 1.7719512837632683\n",
      "0.9571808277801219 1.765868206077496\n",
      "0.9588791122489926 1.5338643919601298\n",
      "0.9597707640836168 1.4210076469955135\n",
      "0.9640733275759785 0.9556670033653697\n",
      "0.9650764497558149 0.8646953202858207\n",
      "0.9696555294397105 0.5241217654653044\n",
      "0.9717879105521774 0.4032312395715541\n",
      "0.9807165001791225 0.0979778037643968\n",
      "0.9827457044466505 0.06302717455442429\n",
      "0.9874649190325822 0.014827638685087739\n",
      "0.9878798443126815 0.012395802186275\n",
      "0.9898030700720033 0.004202379372326719\n",
      "0.993669753144766 0.00040337006600140894\n",
      "0.9966678036813599 0.006203579257266869\n",
      "0.9997876469590101 0.01794094489732098\n"
     ]
    }
   ],
   "source": [
    "K = 6\n",
    "T = 7\n",
    "Noise_Alloc = [0,2,4,6,8,10,12]\n",
    "sigma = 1\n",
    "\n",
    "N = 12\n",
    "\n",
    "j_array = np.array(range(K+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K+T)))\n",
    "\n",
    "i_array = np.array(range(N))\n",
    "beta_array = np.cos(i_array*2*math.pi/(N-1)/2) # np.cos(i_array*2*math.pi/N/2)\n",
    "# print(\"z_array: \",z_array,'\\n')\n",
    "\n",
    "N = 1000\n",
    "z_array = np.random.uniform(-1,1,N) #np.cos(i_array[1:]*2*math.pi/(K+T)/2) # np.cos(i_array*2*math.pi/N/2)\n",
    "z_array = np.sort(z_array)\n",
    "MIS_array = np.zeros((N))\n",
    "MIS_LCC_array = np.zeros((N))\n",
    "# print(z_array)\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "B = [0.5, 1, 1.5, 2]\n",
    "\n",
    "z_array_0 = []\n",
    "z_array_1 = []\n",
    "z_array_2 = []\n",
    "z_array_3 = []\n",
    "z_array_4 = []\n",
    "\n",
    "for j in range(len(z_array)):\n",
    "    MIS_array[j] = MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_array[j]], 1,sigma)\n",
    "    MIS_LCC_array[j] = MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_array[j]], 1,sigma, _is_LCC=True)\n",
    "    \n",
    "    if MIS_array[j] < B[0]:\n",
    "        z_array_0.append(z_array[j])\n",
    "    elif MIS_array[j] < B[1]:\n",
    "        z_array_1.append(z_array[j])\n",
    "    elif MIS_array[j] < B[2]:\n",
    "        z_array_2.append(z_array[j])\n",
    "    elif MIS_array[j] < B[3]:\n",
    "        z_array_3.append(z_array[j])\n",
    "    else:\n",
    "        z_array_4.append(z_array[j])\n",
    "#     print('(beta index, MIS) = ',j,',',MIS_array[j])\n",
    "#     print()\n",
    "\n",
    "\n",
    "\n",
    "print(len(z_array_0),len(z_array_1),len(z_array_2),len(z_array_3),len(z_array_4))\n",
    "\n",
    "\n",
    "plt.plot(z_array, MIS_array, label='Mutual Information Security, BACC')\n",
    "plt.plot(z_array, MIS_LCC_array, label='Mutual Information Security, LCC')\n",
    "plt.plot(alpha_array[Signal_Alloc],0*np.ones(len(Signal_Alloc)),'g*',label='alpha_i, for X')\n",
    "plt.plot(alpha_array[Noise_Alloc],0*np.ones(len(Noise_Alloc)),'r*',label='alpha_i, for N')\n",
    "# plt.plot(beta_array,0*np.ones(len(beta_array)),'b.',label='beta_i')\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('MIS')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "    \n",
    "for i in range(len(z_array)):\n",
    "    print(z_array[i],MIS_array[i])\n",
    "    \n",
    "# print(alpha_array[Signal_Alloc])\n",
    "# print(alpha_array[Noise_Alloc])\n",
    "# print(alpha_array)\n",
    "\n",
    "# plt.plot((2*j_array[Signal_Alloc]+1)/(K+T),alpha_array[Signal_Alloc],'g*',label='alpha_i, for X')\n",
    "# plt.plot((2*j_array[Noise_Alloc]+1)/(K+T),alpha_array[Noise_Alloc],'r*',label='alpha_i, for N')\n",
    "# plt.plot(2*i_array[1:]/(K+T), z_array,'b.',label='beta_i')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49114422 0.49586111 0.49792726 0.48895618 0.49606435 0.4897194\n",
      " 0.4897194  0.49606435 0.48895618 0.49792726 0.49586111 0.49114422]\n"
     ]
    }
   ],
   "source": [
    "z_array_ = np.array([-0.9702, -0.8668, -0.765, -0.538, -0.383, -0.087, 0.087,0.383, 0.538, 0.765,0.8668, 0.9702])\n",
    "MIS_array_ = np.zeros(len(z_array_))\n",
    "for j in range(len(z_array_)):\n",
    "    MIS_array_[j] = MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_array_[j]], 1,sigma)\n",
    "\n",
    "print(MIS_array_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N,K,T,Z_array: 6 6 7 [-0.9702 -0.765  -0.383   0.383   0.765   0.9702]\n",
      "z_array: [-0.9702 -0.765  -0.383   0.383   0.765   0.9702]\n",
      "0.49114422108359246\n",
      "0.497927255191399\n",
      "0.4960643470070548\n",
      "0.496064347007058\n",
      "0.4979272551914008\n",
      "0.49114422108359496\n",
      "@BACC_Enc: N,K,T, m_i= 6 6 7 10000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 6 6 7 10000 \n",
      "\n",
      "(T, sigma)= 7 1 )  0 -th Trial!!\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.013568487167358399\n",
      "conv1.bias 0.012439791113138199\n",
      "conv2.weight 0.0004147003963589668\n",
      "conv2.bias 0.00045963723096065223\n",
      "fc1.weight 0.0003242381615564227\n",
      "fc1.bias 0.0004071409814059734\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1103/10000 (11.03%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 11.030\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0010383100807666778\n",
      "conv1.bias 0.005525953136384487\n",
      "conv2.weight 0.0011569742113351823\n",
      "conv2.bias 0.0022692824713885784\n",
      "fc1.weight 0.0002222033217549324\n",
      "fc1.bias 0.0016450196504592895\n",
      "\n",
      "Test set: Average loss: 2.2905 \n",
      "Accuracy: 2009/10000 (20.09%)\n",
      "\n",
      "Round   1, Average loss 2.291 Test accuracy 20.090\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.000246691033244133\n",
      "conv1.bias 0.007237417623400688\n",
      "conv2.weight 0.00025859029963612554\n",
      "conv2.bias 0.003041739109903574\n",
      "fc1.weight 0.0002878074301406741\n",
      "fc1.bias 0.0014450762420892716\n",
      "\n",
      "Test set: Average loss: 2.2769 \n",
      "Accuracy: 2934/10000 (29.34%)\n",
      "\n",
      "Round   2, Average loss 2.277 Test accuracy 29.340\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0012379450350999833\n",
      "conv1.bias 0.003665669821202755\n",
      "conv2.weight 0.0003343518078327179\n",
      "conv2.bias 0.0035742998588830233\n",
      "fc1.weight 0.0007827669382095337\n",
      "fc1.bias 0.0021547969430685044\n",
      "\n",
      "Test set: Average loss: 2.2918 \n",
      "Accuracy: 2078/10000 (20.78%)\n",
      "\n",
      "Round   3, Average loss 2.292 Test accuracy 20.780\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0003698790818452835\n",
      "conv1.bias 0.011292134411633015\n",
      "conv2.weight 0.0002162267081439495\n",
      "conv2.bias 0.00396499689668417\n",
      "fc1.weight 0.0003994722384959459\n",
      "fc1.bias 0.0009430006146430969\n",
      "\n",
      "Test set: Average loss: 2.2280 \n",
      "Accuracy: 6207/10000 (62.07%)\n",
      "\n",
      "Round   4, Average loss 2.228 Test accuracy 62.070\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.00031990379095077516\n",
      "conv1.bias 0.013955106027424335\n",
      "conv2.weight 0.00015574351884424685\n",
      "conv2.bias 0.004342664033174515\n",
      "fc1.weight 0.00035518903750926257\n",
      "fc1.bias 0.0008205934427678585\n",
      "\n",
      "Test set: Average loss: 1.5935 \n",
      "Accuracy: 6593/10000 (65.93%)\n",
      "\n",
      "Round   5, Average loss 1.594 Test accuracy 65.930\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0007413546741008759\n",
      "conv1.bias 0.018095146864652634\n",
      "conv2.weight 0.0001767655462026596\n",
      "conv2.bias 0.003706754185259342\n",
      "fc1.weight 0.0005466998554766178\n",
      "fc1.bias 0.0016889460384845734\n",
      "\n",
      "Test set: Average loss: 0.9132 \n",
      "Accuracy: 7550/10000 (75.50%)\n",
      "\n",
      "Round   6, Average loss 0.913 Test accuracy 75.500\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0011603507399559022\n",
      "conv1.bias 0.020017676055431366\n",
      "conv2.weight 0.00023511456325650215\n",
      "conv2.bias 0.0037100445479154587\n",
      "fc1.weight 0.00038789401296526196\n",
      "fc1.bias 0.0030488910153508185\n",
      "\n",
      "Test set: Average loss: 0.9188 \n",
      "Accuracy: 7772/10000 (77.72%)\n",
      "\n",
      "Round   7, Average loss 0.919 Test accuracy 77.720\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0011330546438694\n",
      "conv1.bias 0.012591935694217682\n",
      "conv2.weight 0.00034886576235294343\n",
      "conv2.bias 0.003470436204224825\n",
      "fc1.weight 0.0005214142613112927\n",
      "fc1.bias 0.001885920949280262\n",
      "\n",
      "Test set: Average loss: 0.8175 \n",
      "Accuracy: 8007/10000 (80.07%)\n",
      "\n",
      "Round   8, Average loss 0.818 Test accuracy 80.070\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0012599383294582367\n",
      "conv1.bias 0.015216724015772343\n",
      "conv2.weight 0.0003755698353052139\n",
      "conv2.bias 0.003593204077333212\n",
      "fc1.weight 0.0005777834914624691\n",
      "fc1.bias 0.002686852216720581\n",
      "\n",
      "Test set: Average loss: 0.5734 \n",
      "Accuracy: 8654/10000 (86.54%)\n",
      "\n",
      "Round   9, Average loss 0.573 Test accuracy 86.540\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.001682247519493103\n",
      "conv1.bias 0.017605237662792206\n",
      "conv2.weight 0.0003053640387952328\n",
      "conv2.bias 0.003492932766675949\n",
      "fc1.weight 0.0004537393338978291\n",
      "fc1.bias 0.0027056463062763215\n",
      "\n",
      "Test set: Average loss: 0.7803 \n",
      "Accuracy: 7650/10000 (76.50%)\n",
      "\n",
      "Round  10, Average loss 0.780 Test accuracy 76.500\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0015907683968544007\n",
      "conv1.bias 0.01824970357120037\n",
      "conv2.weight 0.0003711047023534775\n",
      "conv2.bias 0.0035366942174732685\n",
      "fc1.weight 0.0006382214836776256\n",
      "fc1.bias 0.0010662835091352462\n",
      "\n",
      "Test set: Average loss: 0.5626 \n",
      "Accuracy: 8701/10000 (87.01%)\n",
      "\n",
      "Round  11, Average loss 0.563 Test accuracy 87.010\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0018846589326858522\n",
      "conv1.bias 0.016332145780324936\n",
      "conv2.weight 0.0003202741220593452\n",
      "conv2.bias 0.0033205836080014706\n",
      "fc1.weight 0.0005268601235002279\n",
      "fc1.bias 0.0029787756502628326\n",
      "\n",
      "Test set: Average loss: 0.6602 \n",
      "Accuracy: 8108/10000 (81.08%)\n",
      "\n",
      "Round  12, Average loss 0.660 Test accuracy 81.080\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0017964832484722137\n",
      "conv1.bias 0.018540970981121063\n",
      "conv2.weight 0.00039220109581947324\n",
      "conv2.bias 0.0034626550041139126\n",
      "fc1.weight 0.000762553233653307\n",
      "fc1.bias 0.0013386872597038746\n",
      "\n",
      "Test set: Average loss: 0.5820 \n",
      "Accuracy: 8728/10000 (87.28%)\n",
      "\n",
      "Round  13, Average loss 0.582 Test accuracy 87.280\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0016038936376571655\n",
      "conv1.bias 0.018397673964500427\n",
      "conv2.weight 0.0003938792645931244\n",
      "conv2.bias 0.0031784384045749903\n",
      "fc1.weight 0.0005705499090254307\n",
      "fc1.bias 0.0028280241414904593\n",
      "\n",
      "Test set: Average loss: 0.6038 \n",
      "Accuracy: 8464/10000 (84.64%)\n",
      "\n",
      "Round  14, Average loss 0.604 Test accuracy 84.640\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0018619486689567565\n",
      "conv1.bias 0.019884392619132996\n",
      "conv2.weight 0.00029477700591087343\n",
      "conv2.bias 0.003017311217263341\n",
      "fc1.weight 0.0005165738984942436\n",
      "fc1.bias 0.0014529317617416383\n",
      "\n",
      "Test set: Average loss: 0.6746 \n",
      "Accuracy: 8160/10000 (81.60%)\n",
      "\n",
      "Round  15, Average loss 0.675 Test accuracy 81.600\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0015378136932849883\n",
      "conv1.bias 0.019117027521133423\n",
      "conv2.weight 0.00036455847322940826\n",
      "conv2.bias 0.003166862763464451\n",
      "fc1.weight 0.0006556165870279073\n",
      "fc1.bias 0.0011592024937272071\n",
      "\n",
      "Test set: Average loss: 0.5505 \n",
      "Accuracy: 8716/10000 (87.16%)\n",
      "\n",
      "Round  16, Average loss 0.550 Test accuracy 87.160\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0019055990874767303\n",
      "conv1.bias 0.018946845084428787\n",
      "conv2.weight 0.000377063974738121\n",
      "conv2.bias 0.00321508152410388\n",
      "fc1.weight 0.00046941759064793586\n",
      "fc1.bias 0.002367394231259823\n",
      "\n",
      "Test set: Average loss: 0.6243 \n",
      "Accuracy: 8229/10000 (82.29%)\n",
      "\n",
      "Round  17, Average loss 0.624 Test accuracy 82.290\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0016086077690124511\n",
      "conv1.bias 0.01748562604188919\n",
      "conv2.weight 0.00042424678802490236\n",
      "conv2.bias 0.0032632658258080482\n",
      "fc1.weight 0.0007117222994565964\n",
      "fc1.bias 0.0011886392720043659\n",
      "\n",
      "Test set: Average loss: 0.5559 \n",
      "Accuracy: 8692/10000 (86.92%)\n",
      "\n",
      "Round  18, Average loss 0.556 Test accuracy 86.920\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.001696295440196991\n",
      "conv1.bias 0.019318614155054092\n",
      "conv2.weight 0.0004472777247428894\n",
      "conv2.bias 0.0032224946189671755\n",
      "fc1.weight 0.0005140310153365135\n",
      "fc1.bias 0.0018523335456848145\n",
      "\n",
      "Test set: Average loss: 0.6009 \n",
      "Accuracy: 8434/10000 (84.34%)\n",
      "\n",
      "Round  19, Average loss 0.601 Test accuracy 84.340\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0015927451848983766\n",
      "conv1.bias 0.018823426216840744\n",
      "conv2.weight 0.0003924503177404404\n",
      "conv2.bias 0.003232500981539488\n",
      "fc1.weight 0.0007739220280200243\n",
      "fc1.bias 0.001165486965328455\n",
      "\n",
      "Test set: Average loss: 0.5426 \n",
      "Accuracy: 8716/10000 (87.16%)\n",
      "\n",
      "Round  20, Average loss 0.543 Test accuracy 87.160\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.001462552696466446\n",
      "conv1.bias 0.0176846943795681\n",
      "conv2.weight 0.0004985882341861725\n",
      "conv2.bias 0.003338852897286415\n",
      "fc1.weight 0.000510583259165287\n",
      "fc1.bias 0.0018228242173790933\n",
      "\n",
      "Test set: Average loss: 0.5458 \n",
      "Accuracy: 8812/10000 (88.12%)\n",
      "\n",
      "Round  21, Average loss 0.546 Test accuracy 88.120\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0013989898562431335\n",
      "conv1.bias 0.021303590387105942\n",
      "conv2.weight 0.0003955953568220139\n",
      "conv2.bias 0.0031669079326093197\n",
      "fc1.weight 0.0007151613011956215\n",
      "fc1.bias 0.0015613818541169167\n",
      "\n",
      "Test set: Average loss: 0.6119 \n",
      "Accuracy: 8707/10000 (87.07%)\n",
      "\n",
      "Round  22, Average loss 0.612 Test accuracy 87.070\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.001086352840065956\n",
      "conv1.bias 0.018438655883073807\n",
      "conv2.weight 0.0006064872816205025\n",
      "conv2.bias 0.00336405448615551\n",
      "fc1.weight 0.0005863168276846409\n",
      "fc1.bias 0.0020791146904230117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.5450 \n",
      "Accuracy: 8780/10000 (87.80%)\n",
      "\n",
      "Round  23, Average loss 0.545 Test accuracy 87.800\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0011764369904994965\n",
      "conv1.bias 0.018424436450004578\n",
      "conv2.weight 0.00048601847141981124\n",
      "conv2.bias 0.0030252975411713123\n",
      "fc1.weight 0.0007991677150130272\n",
      "fc1.bias 0.0021514132618904113\n",
      "\n",
      "Test set: Average loss: 0.5446 \n",
      "Accuracy: 8812/10000 (88.12%)\n",
      "\n",
      "Round  24, Average loss 0.545 Test accuracy 88.120\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0014754223823547363\n",
      "conv1.bias 0.01926979422569275\n",
      "conv2.weight 0.00041424836963415145\n",
      "conv2.bias 0.0031531895510852337\n",
      "fc1.weight 0.0005116318818181754\n",
      "fc1.bias 0.0017448499798774718\n",
      "\n",
      "Test set: Average loss: 0.5526 \n",
      "Accuracy: 8746/10000 (87.46%)\n",
      "\n",
      "Round  25, Average loss 0.553 Test accuracy 87.460\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0014549927413463593\n",
      "conv1.bias 0.018835678696632385\n",
      "conv2.weight 0.0004300336167216301\n",
      "conv2.bias 0.0032102172262966633\n",
      "fc1.weight 0.0006059605628252029\n",
      "fc1.bias 0.002041472867131233\n",
      "\n",
      "Test set: Average loss: 0.5459 \n",
      "Accuracy: 8773/10000 (87.73%)\n",
      "\n",
      "Round  26, Average loss 0.546 Test accuracy 87.730\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.001740257441997528\n",
      "conv1.bias 0.02155904844403267\n",
      "conv2.weight 0.0003417903184890747\n",
      "conv2.bias 0.003216553945094347\n",
      "fc1.weight 0.0005245140753686428\n",
      "fc1.bias 0.002240009605884552\n",
      "\n",
      "Test set: Average loss: 0.5525 \n",
      "Accuracy: 8785/10000 (87.85%)\n",
      "\n",
      "Round  27, Average loss 0.553 Test accuracy 87.850\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0012018655240535736\n",
      "conv1.bias 0.021959085017442703\n",
      "conv2.weight 0.0004348946362733841\n",
      "conv2.bias 0.0033477479591965675\n",
      "fc1.weight 0.0004580809734761715\n",
      "fc1.bias 0.002669219672679901\n",
      "\n",
      "Test set: Average loss: 0.5256 \n",
      "Accuracy: 8780/10000 (87.80%)\n",
      "\n",
      "Round  28, Average loss 0.526 Test accuracy 87.800\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0014887316524982452\n",
      "conv1.bias 0.020866189152002335\n",
      "conv2.weight 0.00041340194642543795\n",
      "conv2.bias 0.0032328381203114986\n",
      "fc1.weight 0.0004789593163877726\n",
      "fc1.bias 0.002548482455313206\n",
      "\n",
      "Test set: Average loss: 0.5361 \n",
      "Accuracy: 8797/10000 (87.97%)\n",
      "\n",
      "Round  29, Average loss 0.536 Test accuracy 87.970\n",
      "N,K,T,Z_array: 6 6 7 [-0.9702 -0.765  -0.087   0.087   0.765   0.9702]\n",
      "z_array: [-0.9702 -0.765  -0.087   0.087   0.765   0.9702]\n",
      "0.49114422108359246\n",
      "0.497927255191399\n",
      "0.4897194008225054\n",
      "0.48971940082250653\n",
      "0.4979272551914008\n",
      "0.49114422108359496\n",
      "@BACC_Enc: N,K,T, m_i= 6 6 7 10000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 6 6 7 10000 \n",
      "\n",
      "(T, sigma)= 7 1 )  0 -th Trial!!\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.013115813732147217\n",
      "conv1.bias 0.013067955151200294\n",
      "conv2.weight 0.00041644595563411713\n",
      "conv2.bias 0.0003475059929769486\n",
      "fc1.weight 0.0003282411489635706\n",
      "fc1.bias 0.000565385865047574\n",
      "\n",
      "Test set: Average loss: 2.3025 \n",
      "Accuracy: 1115/10000 (11.15%)\n",
      "\n",
      "Round   0, Average loss 2.302 Test accuracy 11.150\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0008581215143203735\n",
      "conv1.bias 0.006116770673543215\n",
      "conv2.weight 0.0014807662367820739\n",
      "conv2.bias 0.002591043245047331\n",
      "fc1.weight 0.00040824022144079206\n",
      "fc1.bias 0.0013328444212675095\n",
      "\n",
      "Test set: Average loss: 2.2676 \n",
      "Accuracy: 3027/10000 (30.27%)\n",
      "\n",
      "Round   1, Average loss 2.268 Test accuracy 30.270\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0005844859033823013\n",
      "conv1.bias 0.0036407927982509136\n",
      "conv2.weight 0.00043644413352012635\n",
      "conv2.bias 0.004024764522910118\n",
      "fc1.weight 0.00026566677261143925\n",
      "fc1.bias 0.0014070401899516582\n",
      "\n",
      "Test set: Average loss: 1.5533 \n",
      "Accuracy: 6379/10000 (63.79%)\n",
      "\n",
      "Round   2, Average loss 1.553 Test accuracy 63.790\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0005825530365109444\n",
      "conv1.bias 0.007704365067183971\n",
      "conv2.weight 0.0003650897741317749\n",
      "conv2.bias 0.003508280962705612\n",
      "fc1.weight 0.0015425204299390316\n",
      "fc1.bias 0.004491864889860153\n",
      "\n",
      "Test set: Average loss: 0.9423 \n",
      "Accuracy: 7582/10000 (75.82%)\n",
      "\n",
      "Round   3, Average loss 0.942 Test accuracy 75.820\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0007467235624790191\n",
      "conv1.bias 0.012911681085824966\n",
      "conv2.weight 0.0004149192199110985\n",
      "conv2.bias 0.0033507419284433126\n",
      "fc1.weight 0.0011097601614892482\n",
      "fc1.bias 0.002104731649160385\n",
      "\n",
      "Test set: Average loss: 0.5412 \n",
      "Accuracy: 8877/10000 (88.77%)\n",
      "\n",
      "Round   4, Average loss 0.541 Test accuracy 88.770\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0008075518906116486\n",
      "conv1.bias 0.015431050211191177\n",
      "conv2.weight 0.000576733872294426\n",
      "conv2.bias 0.003515855176374316\n",
      "fc1.weight 0.0008362394757568837\n",
      "fc1.bias 0.0040556170046329495\n",
      "\n",
      "Test set: Average loss: 0.5931 \n",
      "Accuracy: 8463/10000 (84.63%)\n",
      "\n",
      "Round   5, Average loss 0.593 Test accuracy 84.630\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0008761876821517944\n",
      "conv1.bias 0.01287783868610859\n",
      "conv2.weight 0.0006257887929677963\n",
      "conv2.bias 0.0032516312785446644\n",
      "fc1.weight 0.0017557866871356965\n",
      "fc1.bias 0.003992599621415138\n",
      "\n",
      "Test set: Average loss: 0.6832 \n",
      "Accuracy: 8105/10000 (81.05%)\n",
      "\n",
      "Round   6, Average loss 0.683 Test accuracy 81.050\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0006852782517671585\n",
      "conv1.bias 0.012473130598664284\n",
      "conv2.weight 0.0007959702610969544\n",
      "conv2.bias 0.0033494234085083008\n",
      "fc1.weight 0.0007238111924380064\n",
      "fc1.bias 0.003098742850124836\n",
      "\n",
      "Test set: Average loss: 0.5988 \n",
      "Accuracy: 8549/10000 (85.49%)\n",
      "\n",
      "Round   7, Average loss 0.599 Test accuracy 85.490\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0008165697008371353\n",
      "conv1.bias 0.0171237513422966\n",
      "conv2.weight 0.0006085120514035225\n",
      "conv2.bias 0.003375319531187415\n",
      "fc1.weight 0.0007958708330988884\n",
      "fc1.bias 0.0025543496012687683\n",
      "\n",
      "Test set: Average loss: 0.4636 \n",
      "Accuracy: 8969/10000 (89.69%)\n",
      "\n",
      "Round   8, Average loss 0.464 Test accuracy 89.690\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0007472319900989532\n",
      "conv1.bias 0.013418326154351234\n",
      "conv2.weight 0.000800073966383934\n",
      "conv2.bias 0.003479140345007181\n",
      "fc1.weight 0.0008922994136810302\n",
      "fc1.bias 0.0020867906510829925\n",
      "\n",
      "Test set: Average loss: 0.5123 \n",
      "Accuracy: 8724/10000 (87.24%)\n",
      "\n",
      "Round   9, Average loss 0.512 Test accuracy 87.240\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0006437493860721588\n",
      "conv1.bias 0.013683146797120571\n",
      "conv2.weight 0.0009236218780279159\n",
      "conv2.bias 0.003387301228940487\n",
      "fc1.weight 0.0009808510541915894\n",
      "fc1.bias 0.0021239623427391054\n",
      "\n",
      "Test set: Average loss: 0.6371 \n",
      "Accuracy: 8302/10000 (83.02%)\n",
      "\n",
      "Round  10, Average loss 0.637 Test accuracy 83.020\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0006452398002147674\n",
      "conv1.bias 0.0127263143658638\n",
      "conv2.weight 0.0009541831910610199\n",
      "conv2.bias 0.0031155410688370466\n",
      "fc1.weight 0.0012497588060796261\n",
      "fc1.bias 0.0016489671543240547\n",
      "\n",
      "Test set: Average loss: 0.7240 \n",
      "Accuracy: 7982/10000 (79.82%)\n",
      "\n",
      "Round  11, Average loss 0.724 Test accuracy 79.820\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0006453192234039307\n",
      "conv1.bias 0.0107964426279068\n",
      "conv2.weight 0.0011550602316856384\n",
      "conv2.bias 0.0034126397222280502\n",
      "fc1.weight 0.0013424133881926536\n",
      "fc1.bias 0.0019592737779021265\n",
      "\n",
      "Test set: Average loss: 0.5581 \n",
      "Accuracy: 8673/10000 (86.73%)\n",
      "\n",
      "Round  12, Average loss 0.558 Test accuracy 86.730\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.000657903105020523\n",
      "conv1.bias 0.010473622009158134\n",
      "conv2.weight 0.001257966011762619\n",
      "conv2.bias 0.0029149861074984074\n",
      "fc1.weight 0.001779339462518692\n",
      "fc1.bias 0.003002222068607807\n",
      "\n",
      "Test set: Average loss: 0.5650 \n",
      "Accuracy: 8490/10000 (84.90%)\n",
      "\n",
      "Round  13, Average loss 0.565 Test accuracy 84.900\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.000804005116224289\n",
      "conv1.bias 0.012460611760616302\n",
      "conv2.weight 0.0012158603221178056\n",
      "conv2.bias 0.0026866942644119263\n",
      "fc1.weight 0.0018362756818532944\n",
      "fc1.bias 0.002550286054611206\n",
      "\n",
      "Test set: Average loss: 0.5299 \n",
      "Accuracy: 8631/10000 (86.31%)\n",
      "\n",
      "Round  14, Average loss 0.530 Test accuracy 86.310\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.000840749740600586\n",
      "conv1.bias 0.013705635443329811\n",
      "conv2.weight 0.0010182950645685196\n",
      "conv2.bias 0.002674545394256711\n",
      "fc1.weight 0.0013736467808485031\n",
      "fc1.bias 0.0028463220223784447\n",
      "\n",
      "Test set: Average loss: 0.5557 \n",
      "Accuracy: 8713/10000 (87.13%)\n",
      "\n",
      "Round  15, Average loss 0.556 Test accuracy 87.130\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0008124689757823944\n",
      "conv1.bias 0.012727228924632072\n",
      "conv2.weight 0.0010738016664981842\n",
      "conv2.bias 0.0030644983053207397\n",
      "fc1.weight 0.0012214640155434608\n",
      "fc1.bias 0.0020450834184885026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.5205 \n",
      "Accuracy: 8556/10000 (85.56%)\n",
      "\n",
      "Round  16, Average loss 0.521 Test accuracy 85.560\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0006944460421800613\n",
      "conv1.bias 0.008997960016131401\n",
      "conv2.weight 0.0014434333145618438\n",
      "conv2.bias 0.0030706708785146475\n",
      "fc1.weight 0.001347850076854229\n",
      "fc1.bias 0.00138432327657938\n",
      "\n",
      "Test set: Average loss: 0.5095 \n",
      "Accuracy: 8789/10000 (87.89%)\n",
      "\n",
      "Round  17, Average loss 0.509 Test accuracy 87.890\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0006730212271213532\n",
      "conv1.bias 0.010269971564412117\n",
      "conv2.weight 0.001519549787044525\n",
      "conv2.bias 0.0026848213747143745\n",
      "fc1.weight 0.0019847191870212555\n",
      "fc1.bias 0.002699158526957035\n",
      "\n",
      "Test set: Average loss: 0.6868 \n",
      "Accuracy: 8168/10000 (81.68%)\n",
      "\n",
      "Round  18, Average loss 0.687 Test accuracy 81.680\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.000717766135931015\n",
      "conv1.bias 0.01033158227801323\n",
      "conv2.weight 0.0014057190716266633\n",
      "conv2.bias 0.0026055616326630116\n",
      "fc1.weight 0.0018374767154455186\n",
      "fc1.bias 0.001604926772415638\n",
      "\n",
      "Test set: Average loss: 0.7144 \n",
      "Accuracy: 8072/10000 (80.72%)\n",
      "\n",
      "Round  19, Average loss 0.714 Test accuracy 80.720\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0006270625442266464\n",
      "conv1.bias 0.009212292730808258\n",
      "conv2.weight 0.001759946048259735\n",
      "conv2.bias 0.0027088639326393604\n",
      "fc1.weight 0.0018604781478643417\n",
      "fc1.bias 0.0012277576141059398\n",
      "\n",
      "Test set: Average loss: 0.8130 \n",
      "Accuracy: 7963/10000 (79.63%)\n",
      "\n",
      "Round  20, Average loss 0.813 Test accuracy 79.630\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0009161920845508576\n",
      "conv1.bias 0.011108627542853355\n",
      "conv2.weight 0.001109941229224205\n",
      "conv2.bias 0.002531536854803562\n",
      "fc1.weight 0.0014956301078200341\n",
      "fc1.bias 0.002360871434211731\n",
      "\n",
      "Test set: Average loss: 0.7510 \n",
      "Accuracy: 8165/10000 (81.65%)\n",
      "\n",
      "Round  21, Average loss 0.751 Test accuracy 81.650\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0009080444276332855\n",
      "conv1.bias 0.013690578751266003\n",
      "conv2.weight 0.0009675461798906327\n",
      "conv2.bias 0.0027270144782960415\n",
      "fc1.weight 0.0013206076808273792\n",
      "fc1.bias 0.0028204819187521934\n",
      "\n",
      "Test set: Average loss: 0.6834 \n",
      "Accuracy: 8206/10000 (82.06%)\n",
      "\n",
      "Round  22, Average loss 0.683 Test accuracy 82.060\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0007428954541683197\n",
      "conv1.bias 0.012103479355573654\n",
      "conv2.weight 0.0009475830942392349\n",
      "conv2.bias 0.0032538101077079773\n",
      "fc1.weight 0.0012930991128087045\n",
      "fc1.bias 0.0020555149763822555\n",
      "\n",
      "Test set: Average loss: 0.7072 \n",
      "Accuracy: 8241/10000 (82.41%)\n",
      "\n",
      "Round  23, Average loss 0.707 Test accuracy 82.410\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.000694008469581604\n",
      "conv1.bias 0.010958852246403694\n",
      "conv2.weight 0.0012078028917312622\n",
      "conv2.bias 0.0033806331921368837\n",
      "fc1.weight 0.0013298537582159043\n",
      "fc1.bias 0.0017811300233006478\n",
      "\n",
      "Test set: Average loss: 0.6987 \n",
      "Accuracy: 8225/10000 (82.25%)\n",
      "\n",
      "Round  24, Average loss 0.699 Test accuracy 82.250\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0008374081552028656\n",
      "conv1.bias 0.012534362263977528\n",
      "conv2.weight 0.001230313628911972\n",
      "conv2.bias 0.0029457658529281616\n",
      "fc1.weight 0.0015046068467199803\n",
      "fc1.bias 0.002312519401311874\n",
      "\n",
      "Test set: Average loss: 0.6794 \n",
      "Accuracy: 8272/10000 (82.72%)\n",
      "\n",
      "Round  25, Average loss 0.679 Test accuracy 82.720\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0009178499877452851\n",
      "conv1.bias 0.011634411290287971\n",
      "conv2.weight 0.0012599314749240876\n",
      "conv2.bias 0.002996944822371006\n",
      "fc1.weight 0.0014216949231922627\n",
      "fc1.bias 0.0028553007170557974\n",
      "\n",
      "Test set: Average loss: 0.6865 \n",
      "Accuracy: 8271/10000 (82.71%)\n",
      "\n",
      "Round  26, Average loss 0.687 Test accuracy 82.710\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0009266649186611176\n",
      "conv1.bias 0.011611800640821457\n",
      "conv2.weight 0.0009680069983005523\n",
      "conv2.bias 0.00313572003506124\n",
      "fc1.weight 0.0011537264101207257\n",
      "fc1.bias 0.002431723289191723\n",
      "\n",
      "Test set: Average loss: 0.6751 \n",
      "Accuracy: 8092/10000 (80.92%)\n",
      "\n",
      "Round  27, Average loss 0.675 Test accuracy 80.920\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0008770002424716949\n",
      "conv1.bias 0.013772954232990742\n",
      "conv2.weight 0.0007344631850719452\n",
      "conv2.bias 0.0036095096729695797\n",
      "fc1.weight 0.0009640508331358432\n",
      "fc1.bias 0.0025140995159745215\n",
      "\n",
      "Test set: Average loss: 0.5478 \n",
      "Accuracy: 8652/10000 (86.52%)\n",
      "\n",
      "Round  28, Average loss 0.548 Test accuracy 86.520\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0006302113831043243\n",
      "conv1.bias 0.01049641240388155\n",
      "conv2.weight 0.0008240272849798203\n",
      "conv2.bias 0.0032406800892204046\n",
      "fc1.weight 0.0008969493210315704\n",
      "fc1.bias 0.002519350126385689\n",
      "\n",
      "Test set: Average loss: 0.5367 \n",
      "Accuracy: 8888/10000 (88.88%)\n",
      "\n",
      "Round  29, Average loss 0.537 Test accuracy 88.880\n",
      "N,K,T,Z_array: 6 6 7 [-0.9702 -0.8668 -0.383   0.383   0.8668  0.9702]\n",
      "z_array: [-0.9702 -0.8668 -0.383   0.383   0.8668  0.9702]\n",
      "0.49114422108359246\n",
      "0.49586111464393723\n",
      "0.4960643470070548\n",
      "0.496064347007058\n",
      "0.49586111464393606\n",
      "0.49114422108359496\n",
      "@BACC_Enc: N,K,T, m_i= 6 6 7 10000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 6 6 7 10000 \n",
      "\n",
      "(T, sigma)= 7 1 )  0 -th Trial!!\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.013411548137664795\n",
      "conv1.bias 0.010944221168756485\n",
      "conv2.weight 0.00041487269103527067\n",
      "conv2.bias 0.0003726833965629339\n",
      "fc1.weight 0.00032665473408997057\n",
      "fc1.bias 0.00026944344863295556\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0007258961349725724\n",
      "conv1.bias 0.006960548460483551\n",
      "conv2.weight 0.0011123929917812346\n",
      "conv2.bias 0.0025164815597236156\n",
      "fc1.weight 0.00034879092127084734\n",
      "fc1.bias 0.0013933797366917134\n",
      "\n",
      "Test set: Average loss: 2.2805 \n",
      "Accuracy: 1766/10000 (17.66%)\n",
      "\n",
      "Round   1, Average loss 2.281 Test accuracy 17.660\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.00040710486471652986\n",
      "conv1.bias 0.010296311229467392\n",
      "conv2.weight 0.0004976442456245422\n",
      "conv2.bias 0.004838391672819853\n",
      "fc1.weight 0.00030245049856603146\n",
      "fc1.bias 0.0023400992155075072\n",
      "\n",
      "Test set: Average loss: 2.2980 \n",
      "Accuracy: 1293/10000 (12.93%)\n",
      "\n",
      "Round   2, Average loss 2.298 Test accuracy 12.930\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0006115869805216789\n",
      "conv1.bias 0.006741219200193882\n",
      "conv2.weight 0.0002672269195318222\n",
      "conv2.bias 0.003473330521956086\n",
      "fc1.weight 0.0007824420928955079\n",
      "fc1.bias 0.002425641193985939\n",
      "\n",
      "Test set: Average loss: 2.1364 \n",
      "Accuracy: 4014/10000 (40.14%)\n",
      "\n",
      "Round   3, Average loss 2.136 Test accuracy 40.140\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.000723966360092163\n",
      "conv1.bias 0.012257276102900505\n",
      "conv2.weight 0.0002736600302159786\n",
      "conv2.bias 0.004754933062940836\n",
      "fc1.weight 0.0003206943394616246\n",
      "fc1.bias 0.0010522302240133286\n",
      "\n",
      "Test set: Average loss: 2.1220 \n",
      "Accuracy: 4176/10000 (41.76%)\n",
      "\n",
      "Round   4, Average loss 2.122 Test accuracy 41.760\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0005903533101081848\n",
      "conv1.bias 0.0068789017386734486\n",
      "conv2.weight 0.000492757149040699\n",
      "conv2.bias 0.003553156740963459\n",
      "fc1.weight 0.001209819782525301\n",
      "fc1.bias 0.0015104686841368674\n",
      "\n",
      "Test set: Average loss: 1.1119 \n",
      "Accuracy: 7039/10000 (70.39%)\n",
      "\n",
      "Round   5, Average loss 1.112 Test accuracy 70.390\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0011066843569278717\n",
      "conv1.bias 0.019554395228624344\n",
      "conv2.weight 0.0003269724920392036\n",
      "conv2.bias 0.004508620128035545\n",
      "fc1.weight 0.0003682475071400404\n",
      "fc1.bias 0.0021864566951990128\n",
      "\n",
      "Test set: Average loss: 1.1209 \n",
      "Accuracy: 6874/10000 (68.74%)\n",
      "\n",
      "Round   6, Average loss 1.121 Test accuracy 68.740\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0009675179421901703\n",
      "conv1.bias 0.013202039524912834\n",
      "conv2.weight 0.0004200446978211403\n",
      "conv2.bias 0.0036638129968196154\n",
      "fc1.weight 0.000956436712294817\n",
      "fc1.bias 0.002652997709810734\n",
      "\n",
      "Test set: Average loss: 0.7097 \n",
      "Accuracy: 8242/10000 (82.42%)\n",
      "\n",
      "Round   7, Average loss 0.710 Test accuracy 82.420\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.001016290783882141\n",
      "conv1.bias 0.014372758567333221\n",
      "conv2.weight 0.0004869353026151657\n",
      "conv2.bias 0.0036061909049749374\n",
      "fc1.weight 0.0005767063703387976\n",
      "fc1.bias 0.003642700985074043\n",
      "\n",
      "Test set: Average loss: 0.8173 \n",
      "Accuracy: 7523/10000 (75.23%)\n",
      "\n",
      "Round   8, Average loss 0.817 Test accuracy 75.230\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0011399269104003906\n",
      "conv1.bias 0.015038138255476952\n",
      "conv2.weight 0.00035068515688180924\n",
      "conv2.bias 0.0030213939025998116\n",
      "fc1.weight 0.000666290195658803\n",
      "fc1.bias 0.003112429939210415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.7114 \n",
      "Accuracy: 7811/10000 (78.11%)\n",
      "\n",
      "Round   9, Average loss 0.711 Test accuracy 78.110\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0008666618168354035\n",
      "conv1.bias 0.014259738847613335\n",
      "conv2.weight 0.00043633460998535154\n",
      "conv2.bias 0.0031710690818727016\n",
      "fc1.weight 0.000868310034275055\n",
      "fc1.bias 0.004149564355611801\n",
      "\n",
      "Test set: Average loss: 0.4783 \n",
      "Accuracy: 8968/10000 (89.68%)\n",
      "\n",
      "Round  10, Average loss 0.478 Test accuracy 89.680\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0012009069323539734\n",
      "conv1.bias 0.015999801456928253\n",
      "conv2.weight 0.0003370234370231628\n",
      "conv2.bias 0.0033077418338507414\n",
      "fc1.weight 0.0005838020704686641\n",
      "fc1.bias 0.0045231029391288756\n",
      "\n",
      "Test set: Average loss: 0.9071 \n",
      "Accuracy: 7186/10000 (71.86%)\n",
      "\n",
      "Round  11, Average loss 0.907 Test accuracy 71.860\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0011247414350509643\n",
      "conv1.bias 0.011721398681402206\n",
      "conv2.weight 0.0007903504371643066\n",
      "conv2.bias 0.0033103711903095245\n",
      "fc1.weight 0.0009556887671351433\n",
      "fc1.bias 0.0012824758887290955\n",
      "\n",
      "Test set: Average loss: 0.9512 \n",
      "Accuracy: 7713/10000 (77.13%)\n",
      "\n",
      "Round  12, Average loss 0.951 Test accuracy 77.130\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0008402650058269501\n",
      "conv1.bias 0.01032976619899273\n",
      "conv2.weight 0.0005929425358772277\n",
      "conv2.bias 0.003002587705850601\n",
      "fc1.weight 0.00048387148417532443\n",
      "fc1.bias 0.0017705444246530534\n",
      "\n",
      "Test set: Average loss: 0.7009 \n",
      "Accuracy: 8188/10000 (81.88%)\n",
      "\n",
      "Round  13, Average loss 0.701 Test accuracy 81.880\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0008791586011648178\n",
      "conv1.bias 0.014124337583780289\n",
      "conv2.weight 0.0006525678187608719\n",
      "conv2.bias 0.003334777895361185\n",
      "fc1.weight 0.0007928656414151192\n",
      "fc1.bias 0.0016778895631432533\n",
      "\n",
      "Test set: Average loss: 0.7511 \n",
      "Accuracy: 7724/10000 (77.24%)\n",
      "\n",
      "Round  14, Average loss 0.751 Test accuracy 77.240\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0015723563730716706\n",
      "conv1.bias 0.015424933284521103\n",
      "conv2.weight 0.00027644846588373187\n",
      "conv2.bias 0.003009933978319168\n",
      "fc1.weight 0.0004595732316374779\n",
      "fc1.bias 0.002169826067984104\n",
      "\n",
      "Test set: Average loss: 0.7938 \n",
      "Accuracy: 7653/10000 (76.53%)\n",
      "\n",
      "Round  15, Average loss 0.794 Test accuracy 76.530\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0011415764689445495\n",
      "conv1.bias 0.010602150112390518\n",
      "conv2.weight 0.00041992731392383577\n",
      "conv2.bias 0.002895117737352848\n",
      "fc1.weight 0.0007810859940946102\n",
      "fc1.bias 0.0029106996953487396\n",
      "\n",
      "Test set: Average loss: 0.8308 \n",
      "Accuracy: 7500/10000 (75.00%)\n",
      "\n",
      "Round  16, Average loss 0.831 Test accuracy 75.000\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.001134227067232132\n",
      "conv1.bias 0.012018878012895584\n",
      "conv2.weight 0.0003827676177024841\n",
      "conv2.bias 0.003007688093930483\n",
      "fc1.weight 0.0008482007309794426\n",
      "fc1.bias 0.0031786464154720307\n",
      "\n",
      "Test set: Average loss: 0.6965 \n",
      "Accuracy: 7927/10000 (79.27%)\n",
      "\n",
      "Round  17, Average loss 0.697 Test accuracy 79.270\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0010503672808408737\n",
      "conv1.bias 0.013373893685638905\n",
      "conv2.weight 0.00036332279443740845\n",
      "conv2.bias 0.003250565379858017\n",
      "fc1.weight 0.0007648198399692774\n",
      "fc1.bias 0.0032023489475250244\n",
      "\n",
      "Test set: Average loss: 0.8334 \n",
      "Accuracy: 7562/10000 (75.62%)\n",
      "\n",
      "Round  18, Average loss 0.833 Test accuracy 75.620\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0010553981363773347\n",
      "conv1.bias 0.014058484695851803\n",
      "conv2.weight 0.0004188720881938934\n",
      "conv2.bias 0.0030751056037843227\n",
      "fc1.weight 0.000799462478607893\n",
      "fc1.bias 0.001945909485220909\n",
      "\n",
      "Test set: Average loss: 0.6395 \n",
      "Accuracy: 8295/10000 (82.95%)\n",
      "\n",
      "Round  19, Average loss 0.639 Test accuracy 82.950\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0011876954138278961\n",
      "conv1.bias 0.013809232972562313\n",
      "conv2.weight 0.0003512515872716904\n",
      "conv2.bias 0.003128393553197384\n",
      "fc1.weight 0.0006469900719821454\n",
      "fc1.bias 0.004517891258001327\n",
      "\n",
      "Test set: Average loss: 0.8183 \n",
      "Accuracy: 7847/10000 (78.47%)\n",
      "\n",
      "Round  20, Average loss 0.818 Test accuracy 78.470\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0013338634371757507\n",
      "conv1.bias 0.014853563159704208\n",
      "conv2.weight 0.00030826132744550706\n",
      "conv2.bias 0.0028442363254725933\n",
      "fc1.weight 0.0005748569499701261\n",
      "fc1.bias 0.002956005372107029\n",
      "\n",
      "Test set: Average loss: 0.7030 \n",
      "Accuracy: 8074/10000 (80.74%)\n",
      "\n",
      "Round  21, Average loss 0.703 Test accuracy 80.740\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0012174621224403381\n",
      "conv1.bias 0.010496238246560097\n",
      "conv2.weight 0.0004414839670062065\n",
      "conv2.bias 0.0029011727310717106\n",
      "fc1.weight 0.0007284754887223243\n",
      "fc1.bias 0.002902012690901756\n",
      "\n",
      "Test set: Average loss: 0.8123 \n",
      "Accuracy: 7537/10000 (75.37%)\n",
      "\n",
      "Round  22, Average loss 0.812 Test accuracy 75.370\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0010201834887266158\n",
      "conv1.bias 0.012932123616337776\n",
      "conv2.weight 0.00045786406844854354\n",
      "conv2.bias 0.0030450711492449045\n",
      "fc1.weight 0.0006693842355161905\n",
      "fc1.bias 0.003893962875008583\n",
      "\n",
      "Test set: Average loss: 0.7857 \n",
      "Accuracy: 7611/10000 (76.11%)\n",
      "\n",
      "Round  23, Average loss 0.786 Test accuracy 76.110\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.001129075735807419\n",
      "conv1.bias 0.014081638306379318\n",
      "conv2.weight 0.0005798391252756119\n",
      "conv2.bias 0.003350219689309597\n",
      "fc1.weight 0.0009080300107598305\n",
      "fc1.bias 0.002814442850649357\n",
      "\n",
      "Test set: Average loss: 0.8713 \n",
      "Accuracy: 7757/10000 (77.57%)\n",
      "\n",
      "Round  24, Average loss 0.871 Test accuracy 77.570\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0008058610558509826\n",
      "conv1.bias 0.011727615259587765\n",
      "conv2.weight 0.0005376377329230308\n",
      "conv2.bias 0.00330053037032485\n",
      "fc1.weight 0.0005443970672786236\n",
      "fc1.bias 0.003740723803639412\n",
      "\n",
      "Test set: Average loss: 0.9693 \n",
      "Accuracy: 7240/10000 (72.40%)\n",
      "\n",
      "Round  25, Average loss 0.969 Test accuracy 72.400\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0011239070445299149\n",
      "conv1.bias 0.01293045561760664\n",
      "conv2.weight 0.00046661101281642915\n",
      "conv2.bias 0.003189526731148362\n",
      "fc1.weight 0.0006837807595729828\n",
      "fc1.bias 0.0021025879308581353\n",
      "\n",
      "Test set: Average loss: 1.0504 \n",
      "Accuracy: 6859/10000 (68.59%)\n",
      "\n",
      "Round  26, Average loss 1.050 Test accuracy 68.590\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0013627311587333679\n",
      "conv1.bias 0.013814468868076801\n",
      "conv2.weight 0.00038551893085241317\n",
      "conv2.bias 0.003132601734250784\n",
      "fc1.weight 0.0008736203424632549\n",
      "fc1.bias 0.0030507152900099755\n",
      "\n",
      "Test set: Average loss: 0.8550 \n",
      "Accuracy: 7469/10000 (74.69%)\n",
      "\n",
      "Round  27, Average loss 0.855 Test accuracy 74.690\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0014549656212329864\n",
      "conv1.bias 0.014322414994239807\n",
      "conv2.weight 0.0003547518327832222\n",
      "conv2.bias 0.0031277593225240707\n",
      "fc1.weight 0.0007061298005282879\n",
      "fc1.bias 0.0034250982105731966\n",
      "\n",
      "Test set: Average loss: 0.9319 \n",
      "Accuracy: 7490/10000 (74.90%)\n",
      "\n",
      "Round  28, Average loss 0.932 Test accuracy 74.900\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0008200154453516007\n",
      "conv1.bias 0.01730296015739441\n",
      "conv2.weight 0.0005835960805416107\n",
      "conv2.bias 0.0031237946823239326\n",
      "fc1.weight 0.0005404562689363956\n",
      "fc1.bias 0.0035665296018123627\n",
      "\n",
      "Test set: Average loss: 1.0539 \n",
      "Accuracy: 6910/10000 (69.10%)\n",
      "\n",
      "Round  29, Average loss 1.054 Test accuracy 69.100\n",
      "N,K,T,Z_array: 6 6 7 [-0.8668 -0.538  -0.087   0.087   0.538   0.8668]\n",
      "z_array: [-0.8668 -0.538  -0.087   0.087   0.538   0.8668]\n",
      "0.49586111464393723\n",
      "0.48895617904704053\n",
      "0.4897194008225054\n",
      "0.48971940082250653\n",
      "0.48895617904703803\n",
      "0.49586111464393606\n",
      "@BACC_Enc: N,K,T, m_i= 6 6 7 10000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 6 6 7 10000 \n",
      "\n",
      "(T, sigma)= 7 1 )  0 -th Trial!!\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.013469529151916505\n",
      "conv1.bias 0.0149314533919096\n",
      "conv2.weight 0.00041715171188116073\n",
      "conv2.bias 0.0003659414651338011\n",
      "fc1.weight 0.00032454777974635365\n",
      "fc1.bias 0.0004103052895516157\n",
      "\n",
      "Test set: Average loss: 2.3019 \n",
      "Accuracy: 1757/10000 (17.57%)\n",
      "\n",
      "Round   0, Average loss 2.302 Test accuracy 17.570\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.00046234101057052613\n",
      "conv1.bias 0.006174454465508461\n",
      "conv2.weight 0.001107952520251274\n",
      "conv2.bias 0.002544411923736334\n",
      "fc1.weight 0.00027260095812380313\n",
      "fc1.bias 0.002872314862906933\n",
      "\n",
      "Test set: Average loss: 2.3030 \n",
      "Accuracy: 1010/10000 (10.10%)\n",
      "\n",
      "Round   1, Average loss 2.303 Test accuracy 10.100\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.00020301062613725662\n",
      "conv1.bias 0.010146934539079666\n",
      "conv2.weight 0.00032873407006263735\n",
      "conv2.bias 0.0038458877243101597\n",
      "fc1.weight 0.00029143723659217355\n",
      "fc1.bias 0.0014956958591938018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.2542 \n",
      "Accuracy: 3906/10000 (39.06%)\n",
      "\n",
      "Round   2, Average loss 2.254 Test accuracy 39.060\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0005264716967940331\n",
      "conv1.bias 0.009623908437788486\n",
      "conv2.weight 0.00019939392805099488\n",
      "conv2.bias 0.0036022141575813293\n",
      "fc1.weight 0.0008169012144207954\n",
      "fc1.bias 0.0019335070624947547\n",
      "\n",
      "Test set: Average loss: 2.2244 \n",
      "Accuracy: 3270/10000 (32.70%)\n",
      "\n",
      "Round   3, Average loss 2.224 Test accuracy 32.700\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0010278338193893432\n",
      "conv1.bias 0.0050738295540213585\n",
      "conv2.weight 0.0002585973404347897\n",
      "conv2.bias 0.0030195568688213825\n",
      "fc1.weight 0.0007052958942949771\n",
      "fc1.bias 0.004011470079421997\n",
      "\n",
      "Test set: Average loss: 2.3011 \n",
      "Accuracy: 1093/10000 (10.93%)\n",
      "\n",
      "Round   4, Average loss 2.301 Test accuracy 10.930\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.000511225163936615\n",
      "conv1.bias 0.005010703578591347\n",
      "conv2.weight 0.0010056502372026443\n",
      "conv2.bias 0.0031796088442206383\n",
      "fc1.weight 0.0006341878790408373\n",
      "fc1.bias 0.0015689807012677192\n",
      "\n",
      "Test set: Average loss: 2.1241 \n",
      "Accuracy: 5050/10000 (50.50%)\n",
      "\n",
      "Round   5, Average loss 2.124 Test accuracy 50.500\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0005195547640323639\n",
      "conv1.bias 0.010325158014893532\n",
      "conv2.weight 0.00038978517055511476\n",
      "conv2.bias 0.0031593958847224712\n",
      "fc1.weight 0.00030349306762218476\n",
      "fc1.bias 0.0017779357731342316\n",
      "\n",
      "Test set: Average loss: 1.7818 \n",
      "Accuracy: 4698/10000 (46.98%)\n",
      "\n",
      "Round   6, Average loss 1.782 Test accuracy 46.980\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0018387392163276672\n",
      "conv1.bias 0.006711804773658514\n",
      "conv2.weight 0.00041956741362810136\n",
      "conv2.bias 0.003172588534653187\n",
      "fc1.weight 0.0009252417832612992\n",
      "fc1.bias 0.0025019533932209015\n",
      "\n",
      "Test set: Average loss: 1.3460 \n",
      "Accuracy: 5988/10000 (59.88%)\n",
      "\n",
      "Round   7, Average loss 1.346 Test accuracy 59.880\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0016192808747291566\n",
      "conv1.bias 0.009257379919290543\n",
      "conv2.weight 0.00029541956260800364\n",
      "conv2.bias 0.003196545410901308\n",
      "fc1.weight 0.0005574116483330726\n",
      "fc1.bias 0.005842241272330284\n",
      "\n",
      "Test set: Average loss: 1.4051 \n",
      "Accuracy: 7218/10000 (72.18%)\n",
      "\n",
      "Round   8, Average loss 1.405 Test accuracy 72.180\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0006856967508792877\n",
      "conv1.bias 0.009919320233166218\n",
      "conv2.weight 0.000732436552643776\n",
      "conv2.bias 0.0036895964294672012\n",
      "fc1.weight 0.0006202797405421734\n",
      "fc1.bias 0.00247730016708374\n",
      "\n",
      "Test set: Average loss: 1.0671 \n",
      "Accuracy: 7184/10000 (71.84%)\n",
      "\n",
      "Round   9, Average loss 1.067 Test accuracy 71.840\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0013093617558479309\n",
      "conv1.bias 0.006272548343986273\n",
      "conv2.weight 0.0012782177329063416\n",
      "conv2.bias 0.0038750693202018738\n",
      "fc1.weight 0.0005292223300784826\n",
      "fc1.bias 0.00825655236840248\n",
      "\n",
      "Test set: Average loss: 1.0154 \n",
      "Accuracy: 7487/10000 (74.87%)\n",
      "\n",
      "Round  10, Average loss 1.015 Test accuracy 74.870\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0012606561183929443\n",
      "conv1.bias 0.008238337934017181\n",
      "conv2.weight 0.0006614623218774796\n",
      "conv2.bias 0.002999981865286827\n",
      "fc1.weight 0.0005315333604812622\n",
      "fc1.bias 0.007184573262929916\n",
      "\n",
      "Test set: Average loss: 1.0831 \n",
      "Accuracy: 7295/10000 (72.95%)\n",
      "\n",
      "Round  11, Average loss 1.083 Test accuracy 72.950\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0019837431609630585\n",
      "conv1.bias 0.014698827639222145\n",
      "conv2.weight 0.00022426074370741845\n",
      "conv2.bias 0.0027525206096470356\n",
      "fc1.weight 0.0005779481492936611\n",
      "fc1.bias 0.005917113646864891\n",
      "\n",
      "Test set: Average loss: 1.1602 \n",
      "Accuracy: 6854/10000 (68.54%)\n",
      "\n",
      "Round  12, Average loss 1.160 Test accuracy 68.540\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0014205259084701539\n",
      "conv1.bias 0.009883577935397625\n",
      "conv2.weight 0.000275553148239851\n",
      "conv2.bias 0.002907064277678728\n",
      "fc1.weight 0.0009425676427781582\n",
      "fc1.bias 0.0065849989652633665\n",
      "\n",
      "Test set: Average loss: 1.6599 \n",
      "Accuracy: 6069/10000 (60.69%)\n",
      "\n",
      "Round  13, Average loss 1.660 Test accuracy 60.690\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.00065235935151577\n",
      "conv1.bias 0.005523586645722389\n",
      "conv2.weight 0.0009962944686412812\n",
      "conv2.bias 0.003260541707277298\n",
      "fc1.weight 0.0006040717009454966\n",
      "fc1.bias 0.005006932094693184\n",
      "\n",
      "Test set: Average loss: 1.1395 \n",
      "Accuracy: 6540/10000 (65.40%)\n",
      "\n",
      "Round  14, Average loss 1.139 Test accuracy 65.400\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0018173858523368835\n",
      "conv1.bias 0.007810968440026045\n",
      "conv2.weight 0.0002946451865136623\n",
      "conv2.bias 0.0033306805416941643\n",
      "fc1.weight 0.0003498875070363283\n",
      "fc1.bias 0.005857297405600548\n",
      "\n",
      "Test set: Average loss: 1.2427 \n",
      "Accuracy: 6305/10000 (63.05%)\n",
      "\n",
      "Round  15, Average loss 1.243 Test accuracy 63.050\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0014465251564979554\n",
      "conv1.bias 0.010510841384530067\n",
      "conv2.weight 0.0003179268166422844\n",
      "conv2.bias 0.003336711088195443\n",
      "fc1.weight 0.0009025375358760357\n",
      "fc1.bias 0.0035155396908521652\n",
      "\n",
      "Test set: Average loss: 1.2715 \n",
      "Accuracy: 6555/10000 (65.55%)\n",
      "\n",
      "Round  16, Average loss 1.271 Test accuracy 65.550\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.001188858449459076\n",
      "conv1.bias 0.008197184652090073\n",
      "conv2.weight 0.001267131268978119\n",
      "conv2.bias 0.003459068713709712\n",
      "fc1.weight 0.0006303670350462198\n",
      "fc1.bias 0.005230359733104706\n",
      "\n",
      "Test set: Average loss: 1.1115 \n",
      "Accuracy: 6563/10000 (65.63%)\n",
      "\n",
      "Round  17, Average loss 1.111 Test accuracy 65.630\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0016619861125946045\n",
      "conv1.bias 0.00677830446511507\n",
      "conv2.weight 0.0004389750584959984\n",
      "conv2.bias 0.002928046975284815\n",
      "fc1.weight 0.00039306879043579104\n",
      "fc1.bias 0.0036836039274930955\n",
      "\n",
      "Test set: Average loss: 1.2746 \n",
      "Accuracy: 5858/10000 (58.58%)\n",
      "\n",
      "Round  18, Average loss 1.275 Test accuracy 58.580\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.001436595320701599\n",
      "conv1.bias 0.0038918363861739635\n",
      "conv2.weight 0.0008134467154741287\n",
      "conv2.bias 0.003294191788882017\n",
      "fc1.weight 0.0007730665151029825\n",
      "fc1.bias 0.005147218704223633\n",
      "\n",
      "Test set: Average loss: 0.8556 \n",
      "Accuracy: 8059/10000 (80.59%)\n",
      "\n",
      "Round  19, Average loss 0.856 Test accuracy 80.590\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.001326214373111725\n",
      "conv1.bias 0.006020063534379005\n",
      "conv2.weight 0.0010163834691047668\n",
      "conv2.bias 0.003870696295052767\n",
      "fc1.weight 0.0007058452814817428\n",
      "fc1.bias 0.004445013031363488\n",
      "\n",
      "Test set: Average loss: 0.8732 \n",
      "Accuracy: 7680/10000 (76.80%)\n",
      "\n",
      "Round  20, Average loss 0.873 Test accuracy 76.800\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0013221472501754761\n",
      "conv1.bias 0.01050051674246788\n",
      "conv2.weight 0.0003740574792027473\n",
      "conv2.bias 0.003206095192581415\n",
      "fc1.weight 0.0006999227683991194\n",
      "fc1.bias 0.0031037181615829468\n",
      "\n",
      "Test set: Average loss: 1.0677 \n",
      "Accuracy: 7484/10000 (74.84%)\n",
      "\n",
      "Round  21, Average loss 1.068 Test accuracy 74.840\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0005746987089514733\n",
      "conv1.bias 0.006658197846263647\n",
      "conv2.weight 0.0009263806045055389\n",
      "conv2.bias 0.0036109534557908773\n",
      "fc1.weight 0.0007862289436161518\n",
      "fc1.bias 0.003609184920787811\n",
      "\n",
      "Test set: Average loss: 0.8715 \n",
      "Accuracy: 8054/10000 (80.54%)\n",
      "\n",
      "Round  22, Average loss 0.871 Test accuracy 80.540\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0014917124807834626\n",
      "conv1.bias 0.009627468883991241\n",
      "conv2.weight 0.00028806453570723534\n",
      "conv2.bias 0.003241516649723053\n",
      "fc1.weight 0.0004971361719071865\n",
      "fc1.bias 0.004941920191049576\n",
      "\n",
      "Test set: Average loss: 1.0803 \n",
      "Accuracy: 6884/10000 (68.84%)\n",
      "\n",
      "Round  23, Average loss 1.080 Test accuracy 68.840\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0012552183866500854\n",
      "conv1.bias 0.009676393121480942\n",
      "conv2.weight 0.00041809704154729846\n",
      "conv2.bias 0.0033345958217978477\n",
      "fc1.weight 0.0012739612720906734\n",
      "fc1.bias 0.004056210443377495\n",
      "\n",
      "Test set: Average loss: 1.1375 \n",
      "Accuracy: 6941/10000 (69.41%)\n",
      "\n",
      "Round  24, Average loss 1.138 Test accuracy 69.410\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0009455291926860809\n",
      "conv1.bias 0.012257315218448639\n",
      "conv2.weight 0.0007494521141052246\n",
      "conv2.bias 0.004003801383078098\n",
      "fc1.weight 0.000543123297393322\n",
      "fc1.bias 0.0031256318092346192\n",
      "\n",
      "Test set: Average loss: 0.8737 \n",
      "Accuracy: 7854/10000 (78.54%)\n",
      "\n",
      "Round  25, Average loss 0.874 Test accuracy 78.540\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0015353024005889892\n",
      "conv1.bias 0.013055656105279922\n",
      "conv2.weight 0.00024537205696105956\n",
      "conv2.bias 0.0034005874767899513\n",
      "fc1.weight 0.0005845299456268549\n",
      "fc1.bias 0.005511158332228661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.7661 \n",
      "Accuracy: 7924/10000 (79.24%)\n",
      "\n",
      "Round  26, Average loss 0.766 Test accuracy 79.240\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0011794890463352203\n",
      "conv1.bias 0.012738607823848724\n",
      "conv2.weight 0.0003234077244997025\n",
      "conv2.bias 0.0031217820942401886\n",
      "fc1.weight 0.0009755592793226242\n",
      "fc1.bias 0.00347428135573864\n",
      "\n",
      "Test set: Average loss: 0.8372 \n",
      "Accuracy: 7739/10000 (77.39%)\n",
      "\n",
      "Round  27, Average loss 0.837 Test accuracy 77.390\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0009107916057109833\n",
      "conv1.bias 0.011614661663770676\n",
      "conv2.weight 0.0010219763219356537\n",
      "conv2.bias 0.003340698778629303\n",
      "fc1.weight 0.0006213836837559939\n",
      "fc1.bias 0.0027653969824314117\n",
      "\n",
      "Test set: Average loss: 0.7333 \n",
      "Accuracy: 7938/10000 (79.38%)\n",
      "\n",
      "Round  28, Average loss 0.733 Test accuracy 79.380\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0014115622639656066\n",
      "conv1.bias 0.013729124329984188\n",
      "conv2.weight 0.00037873949855566025\n",
      "conv2.bias 0.0029597708489745855\n",
      "fc1.weight 0.0004875179380178452\n",
      "fc1.bias 0.003305131569504738\n",
      "\n",
      "Test set: Average loss: 1.0693 \n",
      "Accuracy: 7169/10000 (71.69%)\n",
      "\n",
      "Round  29, Average loss 1.069 Test accuracy 71.690\n",
      "N,K,T,Z_array: 6 6 7 [-0.538 -0.383 -0.087  0.087  0.383  0.538]\n",
      "z_array: [-0.538 -0.383 -0.087  0.087  0.383  0.538]\n",
      "0.48895617904704053\n",
      "0.4960643470070548\n",
      "0.4897194008225054\n",
      "0.48971940082250653\n",
      "0.496064347007058\n",
      "0.48895617904703803\n",
      "@BACC_Enc: N,K,T, m_i= 6 6 7 10000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 6 6 7 10000 \n",
      "\n",
      "(T, sigma)= 7 1 )  0 -th Trial!!\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.014165557622909546\n",
      "conv1.bias 0.01435232162475586\n",
      "conv2.weight 0.00041886206716299054\n",
      "conv2.bias 0.0004333530960138887\n",
      "fc1.weight 0.00032306262291967867\n",
      "fc1.bias 0.00041475226171314714\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1012/10000 (10.12%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 10.120\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0008052185922861099\n",
      "conv1.bias 0.00670655956491828\n",
      "conv2.weight 0.0011511018872261047\n",
      "conv2.bias 0.0024551688693463802\n",
      "fc1.weight 0.0007748115807771683\n",
      "fc1.bias 0.002524222806096077\n",
      "\n",
      "Test set: Average loss: 2.2418 \n",
      "Accuracy: 3478/10000 (34.78%)\n",
      "\n",
      "Round   1, Average loss 2.242 Test accuracy 34.780\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.00030974920839071273\n",
      "conv1.bias 0.011230630800127983\n",
      "conv2.weight 0.0005058739706873894\n",
      "conv2.bias 0.0032257926650345325\n",
      "fc1.weight 0.0007382379844784737\n",
      "fc1.bias 0.0008000757545232773\n",
      "\n",
      "Test set: Average loss: 1.7039 \n",
      "Accuracy: 4765/10000 (47.65%)\n",
      "\n",
      "Round   2, Average loss 1.704 Test accuracy 47.650\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0006605756282806397\n",
      "conv1.bias 0.017105696722865105\n",
      "conv2.weight 0.000318981297314167\n",
      "conv2.bias 0.0033401271793991327\n",
      "fc1.weight 0.0007178487721830606\n",
      "fc1.bias 0.0018920062109827995\n",
      "\n",
      "Test set: Average loss: 1.3453 \n",
      "Accuracy: 6385/10000 (63.85%)\n",
      "\n",
      "Round   3, Average loss 1.345 Test accuracy 63.850\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0007670402526855468\n",
      "conv1.bias 0.016914865002036095\n",
      "conv2.weight 0.000337451808154583\n",
      "conv2.bias 0.0030892828945070505\n",
      "fc1.weight 0.0011706644669175147\n",
      "fc1.bias 0.0031724810600280763\n",
      "\n",
      "Test set: Average loss: 1.0205 \n",
      "Accuracy: 7161/10000 (71.61%)\n",
      "\n",
      "Round   4, Average loss 1.020 Test accuracy 71.610\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0006454130262136459\n",
      "conv1.bias 0.013788977637887001\n",
      "conv2.weight 0.000668695867061615\n",
      "conv2.bias 0.003160949097946286\n",
      "fc1.weight 0.0008200230076909065\n",
      "fc1.bias 0.0029058804735541343\n",
      "\n",
      "Test set: Average loss: 0.9716 \n",
      "Accuracy: 6831/10000 (68.31%)\n",
      "\n",
      "Round   5, Average loss 0.972 Test accuracy 68.310\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0007639741897583008\n",
      "conv1.bias 0.014470979571342468\n",
      "conv2.weight 0.0006381019949913025\n",
      "conv2.bias 0.003073485102504492\n",
      "fc1.weight 0.0011861100792884827\n",
      "fc1.bias 0.004636617377400398\n",
      "\n",
      "Test set: Average loss: 2.2458 \n",
      "Accuracy: 3435/10000 (34.35%)\n",
      "\n",
      "Round   6, Average loss 2.246 Test accuracy 34.350\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0004911474883556366\n",
      "conv1.bias 0.007101746741682291\n",
      "conv2.weight 0.0009729157388210296\n",
      "conv2.bias 0.0032331955153495073\n",
      "fc1.weight 0.0008641934022307396\n",
      "fc1.bias 0.002424809895455837\n",
      "\n",
      "Test set: Average loss: 2.1343 \n",
      "Accuracy: 3707/10000 (37.07%)\n",
      "\n",
      "Round   7, Average loss 2.134 Test accuracy 37.070\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0006685888767242431\n",
      "conv1.bias 0.01038119662553072\n",
      "conv2.weight 0.0005511445552110672\n",
      "conv2.bias 0.003684312803670764\n",
      "fc1.weight 0.000688265822827816\n",
      "fc1.bias 0.0020931554958224297\n",
      "\n",
      "Test set: Average loss: 2.2680 \n",
      "Accuracy: 1789/10000 (17.89%)\n",
      "\n",
      "Round   8, Average loss 2.268 Test accuracy 17.890\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0007814673334360123\n",
      "conv1.bias 0.01825760491192341\n",
      "conv2.weight 0.0002248649299144745\n",
      "conv2.bias 0.0031053517013788223\n",
      "fc1.weight 0.0007983357645571232\n",
      "fc1.bias 0.0011733978986740111\n",
      "\n",
      "Test set: Average loss: 1.8496 \n",
      "Accuracy: 6223/10000 (62.23%)\n",
      "\n",
      "Round   9, Average loss 1.850 Test accuracy 62.230\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0005662205442786217\n",
      "conv1.bias 0.014464669860899448\n",
      "conv2.weight 0.0003804522752761841\n",
      "conv2.bias 0.003334618406370282\n",
      "fc1.weight 0.0007174302823841572\n",
      "fc1.bias 0.0009557034820318222\n",
      "\n",
      "Test set: Average loss: 1.2637 \n",
      "Accuracy: 6369/10000 (63.69%)\n",
      "\n",
      "Round  10, Average loss 1.264 Test accuracy 63.690\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0006990960240364075\n",
      "conv1.bias 0.016259171068668365\n",
      "conv2.weight 0.0004812173172831535\n",
      "conv2.bias 0.003032563254237175\n",
      "fc1.weight 0.0011299844831228257\n",
      "fc1.bias 0.0011215553618967532\n",
      "\n",
      "Test set: Average loss: 0.5514 \n",
      "Accuracy: 8838/10000 (88.38%)\n",
      "\n",
      "Round  11, Average loss 0.551 Test accuracy 88.380\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0005949275195598603\n",
      "conv1.bias 0.014416041783988476\n",
      "conv2.weight 0.0006818216294050217\n",
      "conv2.bias 0.003504703752696514\n",
      "fc1.weight 0.0011518362909555436\n",
      "fc1.bias 0.00506659671664238\n",
      "\n",
      "Test set: Average loss: 0.7264 \n",
      "Accuracy: 8035/10000 (80.35%)\n",
      "\n",
      "Round  12, Average loss 0.726 Test accuracy 80.350\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0007281482219696044\n",
      "conv1.bias 0.010951079428195953\n",
      "conv2.weight 0.0007619476318359375\n",
      "conv2.bias 0.0036431734915822744\n",
      "fc1.weight 0.0010006111115217209\n",
      "fc1.bias 0.002688545174896717\n",
      "\n",
      "Test set: Average loss: 0.8587 \n",
      "Accuracy: 7532/10000 (75.32%)\n",
      "\n",
      "Round  13, Average loss 0.859 Test accuracy 75.320\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0008161592483520507\n",
      "conv1.bias 0.010777181014418602\n",
      "conv2.weight 0.0012597312033176423\n",
      "conv2.bias 0.0037177451886236668\n",
      "fc1.weight 0.0014556284993886947\n",
      "fc1.bias 0.005035489425063133\n",
      "\n",
      "Test set: Average loss: 0.5522 \n",
      "Accuracy: 8546/10000 (85.46%)\n",
      "\n",
      "Round  14, Average loss 0.552 Test accuracy 85.460\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0007320384681224823\n",
      "conv1.bias 0.013574031181633472\n",
      "conv2.weight 0.0006450995057821273\n",
      "conv2.bias 0.002853456884622574\n",
      "fc1.weight 0.00089043490588665\n",
      "fc1.bias 0.004437221959233284\n",
      "\n",
      "Test set: Average loss: 0.6768 \n",
      "Accuracy: 8247/10000 (82.47%)\n",
      "\n",
      "Round  15, Average loss 0.677 Test accuracy 82.470\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0009040874242782593\n",
      "conv1.bias 0.012325610965490341\n",
      "conv2.weight 0.0008781560510396957\n",
      "conv2.bias 0.003264566883444786\n",
      "fc1.weight 0.0008997508324682712\n",
      "fc1.bias 0.00294045340269804\n",
      "\n",
      "Test set: Average loss: 0.3897 \n",
      "Accuracy: 9091/10000 (90.91%)\n",
      "\n",
      "Round  16, Average loss 0.390 Test accuracy 90.910\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0007405327260494232\n",
      "conv1.bias 0.012224657461047173\n",
      "conv2.weight 0.0008871079236268997\n",
      "conv2.bias 0.0031333230435848236\n",
      "fc1.weight 0.0012528907507658004\n",
      "fc1.bias 0.007617410272359848\n",
      "\n",
      "Test set: Average loss: 0.5104 \n",
      "Accuracy: 8832/10000 (88.32%)\n",
      "\n",
      "Round  17, Average loss 0.510 Test accuracy 88.320\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0009048791974782944\n",
      "conv1.bias 0.01627340540289879\n",
      "conv2.weight 0.0006230796873569488\n",
      "conv2.bias 0.002834058366715908\n",
      "fc1.weight 0.001096654310822487\n",
      "fc1.bias 0.004497639089822769\n",
      "\n",
      "Test set: Average loss: 0.6059 \n",
      "Accuracy: 8327/10000 (83.27%)\n",
      "\n",
      "Round  18, Average loss 0.606 Test accuracy 83.270\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0010854688286781311\n",
      "conv1.bias 0.012301522307097912\n",
      "conv2.weight 0.000609191507101059\n",
      "conv2.bias 0.0029831542633473873\n",
      "fc1.weight 0.0011604804545640945\n",
      "fc1.bias 0.002213368937373161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.4754 \n",
      "Accuracy: 9078/10000 (90.78%)\n",
      "\n",
      "Round  19, Average loss 0.475 Test accuracy 90.780\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0006679847091436386\n",
      "conv1.bias 0.011817343533039093\n",
      "conv2.weight 0.0008055561780929565\n",
      "conv2.bias 0.0034610109869390726\n",
      "fc1.weight 0.0012484933249652386\n",
      "fc1.bias 0.005711516737937928\n",
      "\n",
      "Test set: Average loss: 0.5161 \n",
      "Accuracy: 8811/10000 (88.11%)\n",
      "\n",
      "Round  20, Average loss 0.516 Test accuracy 88.110\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.000621626153588295\n",
      "conv1.bias 0.010343078523874283\n",
      "conv2.weight 0.001171463131904602\n",
      "conv2.bias 0.0037603299133479595\n",
      "fc1.weight 0.0011429271660745143\n",
      "fc1.bias 0.004682868346571922\n",
      "\n",
      "Test set: Average loss: 0.4796 \n",
      "Accuracy: 8768/10000 (87.68%)\n",
      "\n",
      "Round  21, Average loss 0.480 Test accuracy 87.680\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0006615418195724487\n",
      "conv1.bias 0.011251470074057579\n",
      "conv2.weight 0.001078421324491501\n",
      "conv2.bias 0.0030486981850117445\n",
      "fc1.weight 0.0013513293117284775\n",
      "fc1.bias 0.002211352251470089\n",
      "\n",
      "Test set: Average loss: 0.7497 \n",
      "Accuracy: 7802/10000 (78.02%)\n",
      "\n",
      "Round  22, Average loss 0.750 Test accuracy 78.020\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0008116376399993897\n",
      "conv1.bias 0.011724657379090786\n",
      "conv2.weight 0.001039464995265007\n",
      "conv2.bias 0.003180434927344322\n",
      "fc1.weight 0.0011520508676767349\n",
      "fc1.bias 0.002587158605456352\n",
      "\n",
      "Test set: Average loss: 0.6631 \n",
      "Accuracy: 8177/10000 (81.77%)\n",
      "\n",
      "Round  23, Average loss 0.663 Test accuracy 81.770\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0006257253885269165\n",
      "conv1.bias 0.013315586373209953\n",
      "conv2.weight 0.0008905142545700073\n",
      "conv2.bias 0.003193930257111788\n",
      "fc1.weight 0.001066430937498808\n",
      "fc1.bias 0.0035021960735321044\n",
      "\n",
      "Test set: Average loss: 0.7260 \n",
      "Accuracy: 8343/10000 (83.43%)\n",
      "\n",
      "Round  24, Average loss 0.726 Test accuracy 83.430\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0006832364201545716\n",
      "conv1.bias 0.01041349582374096\n",
      "conv2.weight 0.001026773601770401\n",
      "conv2.bias 0.003387256059795618\n",
      "fc1.weight 0.001092793233692646\n",
      "fc1.bias 0.00462133064866066\n",
      "\n",
      "Test set: Average loss: 0.7050 \n",
      "Accuracy: 8131/10000 (81.31%)\n",
      "\n",
      "Round  25, Average loss 0.705 Test accuracy 81.310\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0006050533801317215\n",
      "conv1.bias 0.009195826016366482\n",
      "conv2.weight 0.0011742235720157623\n",
      "conv2.bias 0.0032039773650467396\n",
      "fc1.weight 0.0011446557007730007\n",
      "fc1.bias 0.005124860629439354\n",
      "\n",
      "Test set: Average loss: 0.7516 \n",
      "Accuracy: 8196/10000 (81.96%)\n",
      "\n",
      "Round  26, Average loss 0.752 Test accuracy 81.960\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0007240183651447297\n",
      "conv1.bias 0.011108234524726868\n",
      "conv2.weight 0.00104787677526474\n",
      "conv2.bias 0.003205046057701111\n",
      "fc1.weight 0.0009868294931948186\n",
      "fc1.bias 0.004868993163108825\n",
      "\n",
      "Test set: Average loss: 0.6079 \n",
      "Accuracy: 8271/10000 (82.71%)\n",
      "\n",
      "Round  27, Average loss 0.608 Test accuracy 82.710\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.001235283762216568\n",
      "conv1.bias 0.014636009931564331\n",
      "conv2.weight 0.0006549316644668579\n",
      "conv2.bias 0.0031075472943484783\n",
      "fc1.weight 0.0011135631240904332\n",
      "fc1.bias 0.0027544355019927026\n",
      "\n",
      "Test set: Average loss: 0.6843 \n",
      "Accuracy: 8017/10000 (80.17%)\n",
      "\n",
      "Round  28, Average loss 0.684 Test accuracy 80.170\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.000997542440891266\n",
      "conv1.bias 0.01690547913312912\n",
      "conv2.weight 0.000820094645023346\n",
      "conv2.bias 0.0032422607764601707\n",
      "fc1.weight 0.0013303937390446662\n",
      "fc1.bias 0.004497962445020676\n",
      "\n",
      "Test set: Average loss: 0.5721 \n",
      "Accuracy: 8670/10000 (86.70%)\n",
      "\n",
      "Round  29, Average loss 0.572 Test accuracy 86.700\n",
      "N,K,T,Z_array: 12 6 7 [-0.9702 -0.8668 -0.765  -0.538  -0.383  -0.087   0.087   0.383   0.538\n",
      "  0.765   0.8668  0.9702]\n",
      "z_array: [-0.9702 -0.8668 -0.765  -0.538  -0.383  -0.087   0.087   0.383   0.538\n",
      "  0.765   0.8668  0.9702]\n",
      "0.49114422108359246\n",
      "0.49586111464393723\n",
      "0.497927255191399\n",
      "0.48895617904704053\n",
      "0.4960643470070548\n",
      "0.4897194008225054\n",
      "0.48971940082250653\n",
      "0.496064347007058\n",
      "0.48895617904703803\n",
      "0.4979272551914008\n",
      "0.49586111464393606\n",
      "0.49114422108359496\n",
      "@BACC_Enc: N,K,T, m_i= 12 6 7 10000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 12 6 7 10000 \n",
      "\n",
      "(T, sigma)= 7 1 )  0 -th Trial!!\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "conv1.weight 0.01251039981842041\n",
      "conv1.bias 0.012632114812731743\n",
      "conv2.weight 0.0004173775017261505\n",
      "conv2.bias 0.0003821580612566322\n",
      "fc1.weight 0.0003211740870028734\n",
      "fc1.bias 0.00022585049737244844\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "conv1.weight 0.000405401885509491\n",
      "conv1.bias 0.0018697710474953055\n",
      "conv2.weight 0.00024046344682574272\n",
      "conv2.bias 0.0004863387730438262\n",
      "fc1.weight 7.474506273865699e-05\n",
      "fc1.bias 0.00030089966021478175\n",
      "\n",
      "Test set: Average loss: 2.2962 \n",
      "Accuracy: 2733/10000 (27.33%)\n",
      "\n",
      "Round   1, Average loss 2.296 Test accuracy 27.330\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "conv1.weight 0.00019209541380405426\n",
      "conv1.bias 0.0010441744234412909\n",
      "conv2.weight 5.949490703642368e-05\n",
      "conv2.bias 0.0008979631820693612\n",
      "fc1.weight 5.6011462584137915e-05\n",
      "fc1.bias 0.0004106824286282063\n",
      "\n",
      "Test set: Average loss: 2.2891 \n",
      "Accuracy: 2036/10000 (20.36%)\n",
      "\n",
      "Round   2, Average loss 2.289 Test accuracy 20.360\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "conv1.weight 0.00022283263504505158\n",
      "conv1.bias 0.0009109302773140371\n",
      "conv2.weight 0.0001548810303211212\n",
      "conv2.bias 0.000818010070361197\n",
      "fc1.weight 0.0002719780895859003\n",
      "fc1.bias 0.00032495877239853145\n",
      "\n",
      "Test set: Average loss: 2.3010 \n",
      "Accuracy: 1020/10000 (10.20%)\n",
      "\n",
      "Round   3, Average loss 2.301 Test accuracy 10.200\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "conv1.weight 0.000328170470893383\n",
      "conv1.bias 0.0009521239553578198\n",
      "conv2.weight 0.00023483091965317726\n",
      "conv2.bias 0.0010041656205430627\n",
      "fc1.weight 0.0001592958113178611\n",
      "fc1.bias 0.0005113704130053521\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   4, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "conv1.weight 0.0001940828934311867\n",
      "conv1.bias 0.001402691355906427\n",
      "conv2.weight 0.00016552014276385307\n",
      "conv2.bias 0.0008605140610598028\n",
      "fc1.weight 0.00015196288004517555\n",
      "fc1.bias 0.00030340005178004504\n",
      "\n",
      "Test set: Average loss: 2.3025 \n",
      "Accuracy: 1028/10000 (10.28%)\n",
      "\n",
      "Round   5, Average loss 2.302 Test accuracy 10.280\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "conv1.weight 0.00012820681557059287\n",
      "conv1.bias 0.002104824176058173\n",
      "conv2.weight 0.00011585927568376064\n",
      "conv2.bias 0.0011949421605095267\n",
      "fc1.weight 0.00010260632261633873\n",
      "fc1.bias 0.0006577488034963608\n",
      "\n",
      "Test set: Average loss: 2.3017 \n",
      "Accuracy: 1135/10000 (11.35%)\n",
      "\n",
      "Round   6, Average loss 2.302 Test accuracy 11.350\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "conv1.weight 0.0005174294859170914\n",
      "conv1.bias 0.000607282854616642\n",
      "conv2.weight 0.00017086414620280267\n",
      "conv2.bias 0.0009748511365614831\n",
      "fc1.weight 0.00013970875879749657\n",
      "fc1.bias 0.0007922676391899586\n",
      "\n",
      "Test set: Average loss: 2.3022 \n",
      "Accuracy: 1135/10000 (11.35%)\n",
      "\n",
      "Round   7, Average loss 2.302 Test accuracy 11.350\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "conv1.weight 0.0002845980599522591\n",
      "conv1.bias 0.0009275469928979874\n",
      "conv2.weight 8.889003656804561e-05\n",
      "conv2.bias 0.0009535654680803418\n",
      "fc1.weight 0.00010839031310752034\n",
      "fc1.bias 0.0005140514113008976\n",
      "\n",
      "Test set: Average loss: 2.3017 \n",
      "Accuracy: 1135/10000 (11.35%)\n",
      "\n",
      "Round   8, Average loss 2.302 Test accuracy 11.350\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "conv1.weight 0.00040438272058963777\n",
      "conv1.bias 0.0009512832039035857\n",
      "conv2.weight 7.596455514431e-05\n",
      "conv2.bias 0.0009826206369325519\n",
      "fc1.weight 0.0001260830438695848\n",
      "fc1.bias 0.0004894719459116459\n",
      "\n",
      "Test set: Average loss: 2.3003 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   9, Average loss 2.300 Test accuracy 9.800\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "conv1.weight 0.00014165688306093217\n",
      "conv1.bias 0.001124118105508387\n",
      "conv2.weight 5.9896917082369325e-05\n",
      "conv2.bias 0.0008496459340676665\n",
      "fc1.weight 0.00024171159602701665\n",
      "fc1.bias 0.0007887653075158596\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  10, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "conv1.weight 0.0003322368860244751\n",
      "conv1.bias 0.0012677181512117386\n",
      "conv2.weight 0.00010834206826984883\n",
      "conv2.bias 0.0008641069289296865\n",
      "fc1.weight 0.00014390633441507816\n",
      "fc1.bias 0.00044927173294126985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.3023 \n",
      "Accuracy: 1135/10000 (11.35%)\n",
      "\n",
      "Round  11, Average loss 2.302 Test accuracy 11.350\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "conv1.weight 0.00034498006105422975\n",
      "conv1.bias 0.000627011526376009\n",
      "conv2.weight 6.133947987109423e-05\n",
      "conv2.bias 0.0011785192182287574\n",
      "fc1.weight 0.0001260786084458232\n",
      "fc1.bias 0.0002795935608446598\n",
      "\n",
      "Test set: Average loss: 2.2994 \n",
      "Accuracy: 2570/10000 (25.70%)\n",
      "\n",
      "Round  12, Average loss 2.299 Test accuracy 25.700\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "conv1.weight 0.0005576020479202271\n",
      "conv1.bias 0.0005718072061426938\n",
      "conv2.weight 8.364129811525345e-05\n",
      "conv2.bias 0.0009702988900244236\n",
      "fc1.weight 0.0001566711813211441\n",
      "fc1.bias 0.0004165122751146555\n",
      "\n",
      "Test set: Average loss: 2.3022 \n",
      "Accuracy: 1135/10000 (11.35%)\n",
      "\n",
      "Round  13, Average loss 2.302 Test accuracy 11.350\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "conv1.weight 0.000324828140437603\n",
      "conv1.bias 0.0015163584612309933\n",
      "conv2.weight 3.566080471500754e-05\n",
      "conv2.bias 0.0008866433054208755\n",
      "fc1.weight 0.00017541827401146293\n",
      "fc1.bias 0.0004007428884506226\n",
      "\n",
      "Test set: Average loss: 2.2047 \n",
      "Accuracy: 6672/10000 (66.72%)\n",
      "\n",
      "Round  14, Average loss 2.205 Test accuracy 66.720\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "conv1.weight 0.0004122703522443771\n",
      "conv1.bias 0.0003615149180404842\n",
      "conv2.weight 5.9623620472848415e-05\n",
      "conv2.bias 0.0010572721948847175\n",
      "fc1.weight 7.747560157440603e-05\n",
      "fc1.bias 0.0007434314582496881\n",
      "\n",
      "Test set: Average loss: 2.3025 \n",
      "Accuracy: 1066/10000 (10.66%)\n",
      "\n",
      "Round  15, Average loss 2.302 Test accuracy 10.660\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "conv1.weight 0.00011463920585811138\n",
      "conv1.bias 0.0011893094051629305\n",
      "conv2.weight 0.00020878734067082406\n",
      "conv2.bias 0.0009166016825474799\n",
      "fc1.weight 0.0005090801045298576\n",
      "fc1.bias 0.00027778982184827327\n",
      "\n",
      "Test set: Average loss: 2.3025 \n",
      "Accuracy: 1027/10000 (10.27%)\n",
      "\n",
      "Round  16, Average loss 2.302 Test accuracy 10.270\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "conv1.weight 0.00017185909673571586\n",
      "conv1.bias 0.000787898141425103\n",
      "conv2.weight 4.6785334125161173e-05\n",
      "conv2.bias 0.0007593936752527952\n",
      "fc1.weight 0.00015072831884026526\n",
      "fc1.bias 0.0004620131570845842\n",
      "\n",
      "Test set: Average loss: 2.3004 \n",
      "Accuracy: 1334/10000 (13.34%)\n",
      "\n",
      "Round  17, Average loss 2.300 Test accuracy 13.340\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "conv1.weight 0.0002879267930984497\n",
      "conv1.bias 0.0010536470217630267\n",
      "conv2.weight 0.00010624319314956665\n",
      "conv2.bias 0.0009373141801916063\n",
      "fc1.weight 0.00021820180118083953\n",
      "fc1.bias 0.0003926930017769337\n",
      "\n",
      "Test set: Average loss: 2.2860 \n",
      "Accuracy: 2373/10000 (23.73%)\n",
      "\n",
      "Round  18, Average loss 2.286 Test accuracy 23.730\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "conv1.weight 0.00031941261142492294\n",
      "conv1.bias 0.0009462356101721525\n",
      "conv2.weight 0.00024647079408168793\n",
      "conv2.bias 0.0007849513203836977\n",
      "fc1.weight 0.0002450407249853015\n",
      "fc1.bias 0.00038050792645663025\n",
      "\n",
      "Test set: Average loss: 2.2575 \n",
      "Accuracy: 4504/10000 (45.04%)\n",
      "\n",
      "Round  19, Average loss 2.258 Test accuracy 45.040\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "conv1.weight 0.00020441003143787385\n",
      "conv1.bias 0.0010798489674925804\n",
      "conv2.weight 0.00013350334018468856\n",
      "conv2.bias 0.0006426874897442758\n",
      "fc1.weight 0.00022683585993945598\n",
      "fc1.bias 0.0003391130594536662\n",
      "\n",
      "Test set: Average loss: 2.2334 \n",
      "Accuracy: 5270/10000 (52.70%)\n",
      "\n",
      "Round  20, Average loss 2.233 Test accuracy 52.700\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "conv1.weight 0.0002643463388085365\n",
      "conv1.bias 0.0009750656317919493\n",
      "conv2.weight 0.00010772847570478917\n",
      "conv2.bias 0.0006553760613314807\n",
      "fc1.weight 0.0002544509246945381\n",
      "fc1.bias 0.0007755721919238568\n",
      "\n",
      "Test set: Average loss: 2.1969 \n",
      "Accuracy: 5823/10000 (58.23%)\n",
      "\n",
      "Round  21, Average loss 2.197 Test accuracy 58.230\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "conv1.weight 0.0002679568529129028\n",
      "conv1.bias 0.0006393577205017209\n",
      "conv2.weight 0.00013657353818416596\n",
      "conv2.bias 0.0007435096777044237\n",
      "fc1.weight 0.0002258996246382594\n",
      "fc1.bias 0.0008892339654266834\n",
      "\n",
      "Test set: Average loss: 2.2668 \n",
      "Accuracy: 4179/10000 (41.79%)\n",
      "\n",
      "Round  22, Average loss 2.267 Test accuracy 41.790\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "conv1.weight 0.0001976550929248333\n",
      "conv1.bias 0.001611440791748464\n",
      "conv2.weight 0.00012429405003786087\n",
      "conv2.bias 0.0007286769687198102\n",
      "fc1.weight 0.00025641899555921553\n",
      "fc1.bias 0.00029169523622840643\n",
      "\n",
      "Test set: Average loss: 2.2523 \n",
      "Accuracy: 5896/10000 (58.96%)\n",
      "\n",
      "Round  23, Average loss 2.252 Test accuracy 58.960\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "conv1.weight 0.00010059984400868415\n",
      "conv1.bias 0.0024977647699415684\n",
      "conv2.weight 6.244965363293886e-05\n",
      "conv2.bias 0.0008992231450974941\n",
      "fc1.weight 0.00011913909111171961\n",
      "fc1.bias 0.00025012148544192316\n",
      "\n",
      "Test set: Average loss: 2.2790 \n",
      "Accuracy: 4131/10000 (41.31%)\n",
      "\n",
      "Round  24, Average loss 2.279 Test accuracy 41.310\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "conv1.weight 0.0002006508782505989\n",
      "conv1.bias 0.0018189011607319117\n",
      "conv2.weight 5.722605623304844e-05\n",
      "conv2.bias 0.0007859932957217097\n",
      "fc1.weight 0.00013943578815087677\n",
      "fc1.bias 0.0004294278100132942\n",
      "\n",
      "Test set: Average loss: 2.3018 \n",
      "Accuracy: 1936/10000 (19.36%)\n",
      "\n",
      "Round  25, Average loss 2.302 Test accuracy 19.360\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "conv1.weight 0.00031076889485120773\n",
      "conv1.bias 0.0026802392676472664\n",
      "conv2.weight 0.00017131153494119643\n",
      "conv2.bias 0.0010325789917260408\n",
      "fc1.weight 0.000353826186619699\n",
      "fc1.bias 0.0005570895969867707\n",
      "\n",
      "Test set: Average loss: 2.3012 \n",
      "Accuracy: 1220/10000 (12.20%)\n",
      "\n",
      "Round  26, Average loss 2.301 Test accuracy 12.200\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "conv1.weight 0.00011982789263129234\n",
      "conv1.bias 0.0012232520384714007\n",
      "conv2.weight 5.807762034237385e-05\n",
      "conv2.bias 0.0010705300373956561\n",
      "fc1.weight 0.00010316147236153484\n",
      "fc1.bias 0.00042818193323910234\n",
      "\n",
      "Test set: Average loss: 2.2947 \n",
      "Accuracy: 2257/10000 (22.57%)\n",
      "\n",
      "Round  27, Average loss 2.295 Test accuracy 22.570\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "conv1.weight 0.0005398965254426002\n",
      "conv1.bias 0.0004345526685938239\n",
      "conv2.weight 8.35077278316021e-05\n",
      "conv2.bias 0.0008040535030886531\n",
      "fc1.weight 0.0002829700009897351\n",
      "fc1.bias 0.0005567004438489676\n",
      "\n",
      "Test set: Average loss: 2.3004 \n",
      "Accuracy: 1467/10000 (14.67%)\n",
      "\n",
      "Round  28, Average loss 2.300 Test accuracy 14.670\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "conv1.weight 0.0001581025868654251\n",
      "conv1.bias 0.0006441636942327023\n",
      "conv2.weight 0.00020975621417164802\n",
      "conv2.bias 0.0007422055932693183\n",
      "fc1.weight 0.00032869691494852304\n",
      "fc1.bias 0.0005526702851057052\n",
      "\n",
      "Test set: Average loss: 2.3020 \n",
      "Accuracy: 1135/10000 (11.35%)\n",
      "\n",
      "Round  29, Average loss 2.302 Test accuracy 11.350\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "K = 6\n",
    "T = 7\n",
    "sigma = 1\n",
    "Noise_Alloc = [0,2,4,6,8,10,12]\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "\n",
    "N_array = [6,6,6,6,6,12]\n",
    "B_array = [0.5]\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 30\n",
    "\n",
    "\n",
    "\n",
    "loss_test_arr_K6_G1_N12 = np.zeros((len(N_array),len(B_array),N_trials,N_epochs))\n",
    "acc_test_arr_K6_G1_N12  = np.zeros((len(N_array),len(B_array),N_trials,N_epochs))\n",
    "\n",
    "for N_idx in range(len(N_array)):\n",
    "    \n",
    "    N = N_array[N_idx]\n",
    "    \n",
    "    \n",
    "           \n",
    "        \n",
    "    # print(\"alpha_array: \",alpha_array,'\\n')\n",
    "    \n",
    "    \n",
    "    for B_idx in range(len(B_array)):\n",
    "        \n",
    "        B = B_array[B_idx]\n",
    "        z_array = []\n",
    "#         while(len(z_array)<N):\n",
    "#             z_tmp = np.random.uniform(-1,1,1)\n",
    "#             MIS_tmp = MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_tmp], 1,sigma)\n",
    "#             if MIS_tmp < B and MIS_tmp > 0.1:\n",
    "#                 z_array.append(z_tmp[0])\n",
    "#         \n",
    "#         z_array = np.sort(z_array)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if N_idx==0:\n",
    "            z_array = np.array([-0.9702,  -0.765, -0.383, 0.383, 0.765, 0.9702])\n",
    "        elif N_idx ==1:\n",
    "            z_array = np.array([-0.9702,  -0.765, -0.087, 0.087, 0.765, 0.9702])\n",
    "        elif N_idx ==2:\n",
    "            z_array = np.array([-0.9702, -0.8668, -0.383, 0.383, 0.8668, 0.9702])\n",
    "        elif N_idx ==3:\n",
    "            z_array = np.array([-0.8668,  -0.538, -0.087, 0.087, 0.538, 0.8668])\n",
    "        elif N_idx ==4:\n",
    "            z_array = np.array([-0.538, -0.383, -0.087, 0.087,0.383, 0.538])\n",
    "        else:\n",
    "            z_array = np.array([-0.9702, -0.8668, -0.765, -0.538, -0.383, -0.087, 0.087,0.383, 0.538, 0.765,0.8668, 0.9702])\n",
    "        \n",
    "        print('N,K,T,Z_array:',N,K,T,z_array)\n",
    "            \n",
    "        print('z_array:',z_array)\n",
    "        for j in range(len(z_array)):\n",
    "            print(MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_array[j]], 1,sigma))\n",
    "        \n",
    "        \n",
    "        _Noise_label = np.ones((15000*T,10)) * 0.1\n",
    "\n",
    "        X_tilde,a,b = BACC_Enc_Data_v3(encoding_input_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde,a,b = BACC_Enc_Data_v3(encoding_label_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "\n",
    "        m = N # m is the number of received result @ master\n",
    "    #     print('number of results:',m)\n",
    "\n",
    "        for trial_idx in range(N_trials):\n",
    "            print('(T, sigma)=',T,sigma,') ',trial_idx,'-th Trial!!')\n",
    "\n",
    "            net_glob = CNNMnist2(args=args)\n",
    "            net_glob.cuda()\n",
    "            net_glob.train()\n",
    "\n",
    "            # copy weights\n",
    "            w_glob = net_glob.state_dict()\n",
    "\n",
    "            for iter in range(N_epochs): #args.epochs\n",
    "                w_locals, loss_locals = [], []\n",
    "                idxs_users = np.random.choice(range(N), m, replace=False)\n",
    "                idxs_users = np.sort(idxs_users)\n",
    "                print('selected users:',idxs_users)\n",
    "                \n",
    "                coded_net = BACC_Enc_Model_withNoise_v4(net_glob.cuda(), N, K, T, 1, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "                dec_z_array = []\n",
    "                for idx in idxs_users: #for idx in range(N):\n",
    "            #         print(idx)\n",
    "                    local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[idx,:,:], label=y_tilde[idx,:,:])\n",
    "                    w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "#                     w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                    w_locals.append(copy.deepcopy(w))\n",
    "                    loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                    dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "                # update global weights\n",
    "                #w_glob = FedAvg(w_locals)\n",
    "                w_glob = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "\n",
    "                # copy weight to net_glob\n",
    "                net_glob.load_state_dict(w_glob)\n",
    "\n",
    "                # print loss\n",
    "            #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "            #     loss_train_arr.append(loss_train)\n",
    "\n",
    "                acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "                acc_test_arr_K6_G1_N12[N_idx][B_idx][trial_idx][iter] = acc_test\n",
    "                loss_test_arr_K6_G1_N12[N_idx][B_idx][trial_idx][iter] = loss_test\n",
    "                if iter % 1 ==0:\n",
    "                    print('Round {:3d}, Average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_test, acc_test))\n",
    "                #print(loss_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. K=6, G=3, N=6 => K_i =2, N_i = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2 3\n",
      "##########################################\n",
      "###### 0 -th Trial!! ###########\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 10000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 10000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 10000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 10000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 10000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 10000 \n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.01285266399383545\n",
      "conv1.bias 0.010384839028120041\n",
      "conv2.weight 0.00041566651314496996\n",
      "conv2.bias 0.00041502033127471805\n",
      "fc1.weight 0.0003276859177276492\n",
      "fc1.bias 0.0003061801660805941\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.01285266399383545\n",
      "conv1.bias 0.010384839028120041\n",
      "conv2.weight 0.00041566651314496996\n",
      "conv2.bias 0.00041502033127471805\n",
      "fc1.weight 0.0003276859177276492\n",
      "fc1.bias 0.0003061801660805941\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.01285266399383545\n",
      "conv1.bias 0.010384839028120041\n",
      "conv2.weight 0.00041566651314496996\n",
      "conv2.bias 0.00041502033127471805\n",
      "fc1.weight 0.0003276859177276492\n",
      "fc1.bias 0.0003061801660805941\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002501591444015503\n",
      "conv1.bias 0.00737779401242733\n",
      "conv2.weight 0.0010624516010284424\n",
      "conv2.bias 0.0013945577666163445\n",
      "fc1.weight 0.00019922517240047454\n",
      "fc1.bias 0.001916678436100483\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002501591444015503\n",
      "conv1.bias 0.00737779401242733\n",
      "conv2.weight 0.0010624516010284424\n",
      "conv2.bias 0.0013945577666163445\n",
      "fc1.weight 0.00019922517240047454\n",
      "fc1.bias 0.001916678436100483\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002501591444015503\n",
      "conv1.bias 0.00737779401242733\n",
      "conv2.weight 0.0010624516010284424\n",
      "conv2.bias 0.0013945577666163445\n",
      "fc1.weight 0.00019922517240047454\n",
      "fc1.bias 0.001916678436100483\n",
      "\n",
      "Test set: Average loss: 2.3029 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016424622386693955\n",
      "conv1.bias 0.01206882018595934\n",
      "conv2.weight 0.0001338068675249815\n",
      "conv2.bias 0.0029839060734957457\n",
      "fc1.weight 0.0002531418344005942\n",
      "fc1.bias 0.001523888297379017\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016424622386693955\n",
      "conv1.bias 0.01206882018595934\n",
      "conv2.weight 0.0001338068675249815\n",
      "conv2.bias 0.0029839060734957457\n",
      "fc1.weight 0.0002531418344005942\n",
      "fc1.bias 0.001523888297379017\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00016424622386693955\n",
      "conv1.bias 0.01206882018595934\n",
      "conv2.weight 0.0001338068675249815\n",
      "conv2.bias 0.0029839060734957457\n",
      "fc1.weight 0.0002531418344005942\n",
      "fc1.bias 0.001523888297379017\n",
      "\n",
      "Test set: Average loss: 1.7288 \n",
      "Accuracy: 7381/10000 (73.81%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005956480279564857\n",
      "conv1.bias 0.012191306799650192\n",
      "conv2.weight 0.00024962060153484346\n",
      "conv2.bias 0.004667235072702169\n",
      "fc1.weight 0.0004279281012713909\n",
      "fc1.bias 0.00048350761644542215\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005956480279564857\n",
      "conv1.bias 0.012191306799650192\n",
      "conv2.weight 0.00024962060153484346\n",
      "conv2.bias 0.004667235072702169\n",
      "fc1.weight 0.0004279281012713909\n",
      "fc1.bias 0.00048350761644542215\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0005956480279564857\n",
      "conv1.bias 0.012191306799650192\n",
      "conv2.weight 0.00024962060153484346\n",
      "conv2.bias 0.004667235072702169\n",
      "fc1.weight 0.0004279281012713909\n",
      "fc1.bias 0.00048350761644542215\n",
      "\n",
      "Test set: Average loss: 0.5404 \n",
      "Accuracy: 8587/10000 (85.87%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0014038640260696411\n",
      "conv1.bias 0.013819335028529167\n",
      "conv2.weight 0.0005417778342962265\n",
      "conv2.bias 0.0049176812171936035\n",
      "fc1.weight 0.0010722918435931207\n",
      "fc1.bias 0.002045811153948307\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0014038640260696411\n",
      "conv1.bias 0.013819335028529167\n",
      "conv2.weight 0.0005417778342962265\n",
      "conv2.bias 0.0049176812171936035\n",
      "fc1.weight 0.0010722918435931207\n",
      "fc1.bias 0.002045811153948307\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0014038640260696411\n",
      "conv1.bias 0.013819335028529167\n",
      "conv2.weight 0.0005417778342962265\n",
      "conv2.bias 0.0049176812171936035\n",
      "fc1.weight 0.0010722918435931207\n",
      "fc1.bias 0.002045811153948307\n",
      "\n",
      "Test set: Average loss: 0.3157 \n",
      "Accuracy: 9439/10000 (94.39%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0013862991333007813\n",
      "conv1.bias 0.01666581630706787\n",
      "conv2.weight 0.0008632155507802963\n",
      "conv2.bias 0.004534028470516205\n",
      "fc1.weight 0.0009812665171921252\n",
      "fc1.bias 0.0029678259044885634\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0013862991333007813\n",
      "conv1.bias 0.01666581630706787\n",
      "conv2.weight 0.0008632155507802963\n",
      "conv2.bias 0.004534028470516205\n",
      "fc1.weight 0.0009812665171921252\n",
      "fc1.bias 0.0029678259044885634\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0013862991333007813\n",
      "conv1.bias 0.01666581630706787\n",
      "conv2.weight 0.0008632155507802963\n",
      "conv2.bias 0.004534028470516205\n",
      "fc1.weight 0.0009812665171921252\n",
      "fc1.bias 0.0029678259044885634\n",
      "\n",
      "Test set: Average loss: 0.4820 \n",
      "Accuracy: 9242/10000 (92.42%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0013952529430389404\n",
      "conv1.bias 0.01882854849100113\n",
      "conv2.weight 0.0008162281662225723\n",
      "conv2.bias 0.00401743408292532\n",
      "fc1.weight 0.0008249006234109402\n",
      "fc1.bias 0.0036578044295310975\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0013952529430389404\n",
      "conv1.bias 0.01882854849100113\n",
      "conv2.weight 0.0008162281662225723\n",
      "conv2.bias 0.00401743408292532\n",
      "fc1.weight 0.0008249006234109402\n",
      "fc1.bias 0.0036578044295310975\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0013952529430389404\n",
      "conv1.bias 0.01882854849100113\n",
      "conv2.weight 0.0008162281662225723\n",
      "conv2.bias 0.00401743408292532\n",
      "fc1.weight 0.0008249006234109402\n",
      "fc1.bias 0.0036578044295310975\n",
      "\n",
      "Test set: Average loss: 0.5667 \n",
      "Accuracy: 9240/10000 (92.40%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0015164914727211\n",
      "conv1.bias 0.019236093387007713\n",
      "conv2.weight 0.0009680045396089553\n",
      "conv2.bias 0.003795814700424671\n",
      "fc1.weight 0.0008591511286795139\n",
      "fc1.bias 0.0033277463167905807\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0015164914727211\n",
      "conv1.bias 0.019236093387007713\n",
      "conv2.weight 0.0009680045396089553\n",
      "conv2.bias 0.003795814700424671\n",
      "fc1.weight 0.0008591511286795139\n",
      "fc1.bias 0.0033277463167905807\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0015164914727211\n",
      "conv1.bias 0.019236093387007713\n",
      "conv2.weight 0.0009680045396089553\n",
      "conv2.bias 0.003795814700424671\n",
      "fc1.weight 0.0008591511286795139\n",
      "fc1.bias 0.0033277463167905807\n",
      "\n",
      "Test set: Average loss: 0.2619 \n",
      "Accuracy: 9507/10000 (95.07%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019445206224918367\n",
      "conv1.bias 0.019031217321753502\n",
      "conv2.weight 0.0006902510672807693\n",
      "conv2.bias 0.0034222975373268127\n",
      "fc1.weight 0.0009416894987225533\n",
      "fc1.bias 0.004099056497216225\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019445206224918367\n",
      "conv1.bias 0.019031217321753502\n",
      "conv2.weight 0.0006902510672807693\n",
      "conv2.bias 0.0034222975373268127\n",
      "fc1.weight 0.0009416894987225533\n",
      "fc1.bias 0.004099056497216225\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019445206224918367\n",
      "conv1.bias 0.019031217321753502\n",
      "conv2.weight 0.0006902510672807693\n",
      "conv2.bias 0.0034222975373268127\n",
      "fc1.weight 0.0009416894987225533\n",
      "fc1.bias 0.004099056497216225\n",
      "\n",
      "Test set: Average loss: 0.3682 \n",
      "Accuracy: 9391/10000 (93.91%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0014174695312976838\n",
      "conv1.bias 0.01819140464067459\n",
      "conv2.weight 0.001059780791401863\n",
      "conv2.bias 0.0038993575144559145\n",
      "fc1.weight 0.0010285092517733573\n",
      "fc1.bias 0.004299085214734077\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0014174695312976838\n",
      "conv1.bias 0.01819140464067459\n",
      "conv2.weight 0.001059780791401863\n",
      "conv2.bias 0.0038993575144559145\n",
      "fc1.weight 0.0010285092517733573\n",
      "fc1.bias 0.004299085214734077\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0014174695312976838\n",
      "conv1.bias 0.01819140464067459\n",
      "conv2.weight 0.001059780791401863\n",
      "conv2.bias 0.0038993575144559145\n",
      "fc1.weight 0.0010285092517733573\n",
      "fc1.bias 0.004299085214734077\n",
      "\n",
      "Test set: Average loss: 0.2809 \n",
      "Accuracy: 9520/10000 (95.20%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018970972299575805\n",
      "conv1.bias 0.021312158554792404\n",
      "conv2.weight 0.0008813648670911788\n",
      "conv2.bias 0.0032410763669759035\n",
      "fc1.weight 0.0012607304379343987\n",
      "fc1.bias 0.004236129298806191\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018970972299575805\n",
      "conv1.bias 0.021312158554792404\n",
      "conv2.weight 0.0008813648670911788\n",
      "conv2.bias 0.0032410763669759035\n",
      "fc1.weight 0.0012607304379343987\n",
      "fc1.bias 0.004236129298806191\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018970972299575805\n",
      "conv1.bias 0.021312158554792404\n",
      "conv2.weight 0.0008813648670911788\n",
      "conv2.bias 0.0032410763669759035\n",
      "fc1.weight 0.0012607304379343987\n",
      "fc1.bias 0.004236129298806191\n",
      "\n",
      "Test set: Average loss: 0.3671 \n",
      "Accuracy: 9381/10000 (93.81%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0017313927412033081\n",
      "conv1.bias 0.021426308900117874\n",
      "conv2.weight 0.0008709091693162918\n",
      "conv2.bias 0.0030209980905056\n",
      "fc1.weight 0.001715448684990406\n",
      "fc1.bias 0.0032424092292785644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected users: [0 1]\n",
      "conv1.weight 0.0017313927412033081\n",
      "conv1.bias 0.021426308900117874\n",
      "conv2.weight 0.0008709091693162918\n",
      "conv2.bias 0.0030209980905056\n",
      "fc1.weight 0.001715448684990406\n",
      "fc1.bias 0.0032424092292785644\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0017313927412033081\n",
      "conv1.bias 0.021426308900117874\n",
      "conv2.weight 0.0008709091693162918\n",
      "conv2.bias 0.0030209980905056\n",
      "fc1.weight 0.001715448684990406\n",
      "fc1.bias 0.0032424092292785644\n",
      "\n",
      "Test set: Average loss: 0.3088 \n",
      "Accuracy: 9441/10000 (94.41%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001754792183637619\n",
      "conv1.bias 0.022321099415421486\n",
      "conv2.weight 0.001002439633011818\n",
      "conv2.bias 0.0029399958439171314\n",
      "fc1.weight 0.0017398661002516747\n",
      "fc1.bias 0.0030089385807514192\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001754792183637619\n",
      "conv1.bias 0.022321099415421486\n",
      "conv2.weight 0.001002439633011818\n",
      "conv2.bias 0.0029399958439171314\n",
      "fc1.weight 0.0017398661002516747\n",
      "fc1.bias 0.0030089385807514192\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001754792183637619\n",
      "conv1.bias 0.022321099415421486\n",
      "conv2.weight 0.001002439633011818\n",
      "conv2.bias 0.0029399958439171314\n",
      "fc1.weight 0.0017398661002516747\n",
      "fc1.bias 0.0030089385807514192\n",
      "\n",
      "Test set: Average loss: 0.4574 \n",
      "Accuracy: 9231/10000 (92.31%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0014694091677665711\n",
      "conv1.bias 0.018547888845205307\n",
      "conv2.weight 0.0010163275897502899\n",
      "conv2.bias 0.0027508882340043783\n",
      "fc1.weight 0.0019073018804192543\n",
      "fc1.bias 0.004028716310858727\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0014694091677665711\n",
      "conv1.bias 0.018547888845205307\n",
      "conv2.weight 0.0010163275897502899\n",
      "conv2.bias 0.0027508882340043783\n",
      "fc1.weight 0.0019073018804192543\n",
      "fc1.bias 0.004028716310858727\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0014694091677665711\n",
      "conv1.bias 0.018547888845205307\n",
      "conv2.weight 0.0010163275897502899\n",
      "conv2.bias 0.0027508882340043783\n",
      "fc1.weight 0.0019073018804192543\n",
      "fc1.bias 0.004028716310858727\n",
      "\n",
      "Test set: Average loss: 0.3995 \n",
      "Accuracy: 9287/10000 (92.87%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0015844698250293732\n",
      "conv1.bias 0.01632661558687687\n",
      "conv2.weight 0.001063040941953659\n",
      "conv2.bias 0.0026854565367102623\n",
      "fc1.weight 0.0019297851249575615\n",
      "fc1.bias 0.004011884331703186\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0015844698250293732\n",
      "conv1.bias 0.01632661558687687\n",
      "conv2.weight 0.001063040941953659\n",
      "conv2.bias 0.0026854565367102623\n",
      "fc1.weight 0.0019297851249575615\n",
      "fc1.bias 0.004011884331703186\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0015844698250293732\n",
      "conv1.bias 0.01632661558687687\n",
      "conv2.weight 0.001063040941953659\n",
      "conv2.bias 0.0026854565367102623\n",
      "fc1.weight 0.0019297851249575615\n",
      "fc1.bias 0.004011884331703186\n",
      "\n",
      "Test set: Average loss: 0.3244 \n",
      "Accuracy: 9395/10000 (93.95%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0017478604614734649\n",
      "conv1.bias 0.015483805909752846\n",
      "conv2.weight 0.0013818298280239105\n",
      "conv2.bias 0.0026880744844675064\n",
      "fc1.weight 0.0019181139767169952\n",
      "fc1.bias 0.0037548378109931945\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0017478604614734649\n",
      "conv1.bias 0.015483805909752846\n",
      "conv2.weight 0.0013818298280239105\n",
      "conv2.bias 0.0026880744844675064\n",
      "fc1.weight 0.0019181139767169952\n",
      "fc1.bias 0.0037548378109931945\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0017478604614734649\n",
      "conv1.bias 0.015483805909752846\n",
      "conv2.weight 0.0013818298280239105\n",
      "conv2.bias 0.0026880744844675064\n",
      "fc1.weight 0.0019181139767169952\n",
      "fc1.bias 0.0037548378109931945\n",
      "\n",
      "Test set: Average loss: 0.4620 \n",
      "Accuracy: 9171/10000 (91.71%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019013530015945434\n",
      "conv1.bias 0.015513135120272636\n",
      "conv2.weight 0.0011906307935714722\n",
      "conv2.bias 0.002700841287150979\n",
      "fc1.weight 0.001965196989476681\n",
      "fc1.bias 0.003449604660272598\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019013530015945434\n",
      "conv1.bias 0.015513135120272636\n",
      "conv2.weight 0.0011906307935714722\n",
      "conv2.bias 0.002700841287150979\n",
      "fc1.weight 0.001965196989476681\n",
      "fc1.bias 0.003449604660272598\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019013530015945434\n",
      "conv1.bias 0.015513135120272636\n",
      "conv2.weight 0.0011906307935714722\n",
      "conv2.bias 0.002700841287150979\n",
      "fc1.weight 0.001965196989476681\n",
      "fc1.bias 0.003449604660272598\n",
      "\n",
      "Test set: Average loss: 0.3196 \n",
      "Accuracy: 9388/10000 (93.88%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001947581022977829\n",
      "conv1.bias 0.012707119807600975\n",
      "conv2.weight 0.0013577501475811006\n",
      "conv2.bias 0.0027076625265181065\n",
      "fc1.weight 0.0019504651427268982\n",
      "fc1.bias 0.004608828201889991\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001947581022977829\n",
      "conv1.bias 0.012707119807600975\n",
      "conv2.weight 0.0013577501475811006\n",
      "conv2.bias 0.0027076625265181065\n",
      "fc1.weight 0.0019504651427268982\n",
      "fc1.bias 0.004608828201889991\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001947581022977829\n",
      "conv1.bias 0.012707119807600975\n",
      "conv2.weight 0.0013577501475811006\n",
      "conv2.bias 0.0027076625265181065\n",
      "fc1.weight 0.0019504651427268982\n",
      "fc1.bias 0.004608828201889991\n",
      "\n",
      "Test set: Average loss: 0.3393 \n",
      "Accuracy: 9307/10000 (93.07%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0017695187032222748\n",
      "conv1.bias 0.013210458680987358\n",
      "conv2.weight 0.0014827620983123779\n",
      "conv2.bias 0.0028007826767861843\n",
      "fc1.weight 0.002064858190715313\n",
      "fc1.bias 0.004449153691530228\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0017695187032222748\n",
      "conv1.bias 0.013210458680987358\n",
      "conv2.weight 0.0014827620983123779\n",
      "conv2.bias 0.0028007826767861843\n",
      "fc1.weight 0.002064858190715313\n",
      "fc1.bias 0.004449153691530228\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0017695187032222748\n",
      "conv1.bias 0.013210458680987358\n",
      "conv2.weight 0.0014827620983123779\n",
      "conv2.bias 0.0028007826767861843\n",
      "fc1.weight 0.002064858190715313\n",
      "fc1.bias 0.004449153691530228\n",
      "\n",
      "Test set: Average loss: 0.3639 \n",
      "Accuracy: 9372/10000 (93.72%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0017108145356178285\n",
      "conv1.bias 0.012980500236153603\n",
      "conv2.weight 0.00139291450381279\n",
      "conv2.bias 0.002952677197754383\n",
      "fc1.weight 0.0015126371756196022\n",
      "fc1.bias 0.004842709004878998\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0017108145356178285\n",
      "conv1.bias 0.012980500236153603\n",
      "conv2.weight 0.00139291450381279\n",
      "conv2.bias 0.002952677197754383\n",
      "fc1.weight 0.0015126371756196022\n",
      "fc1.bias 0.004842709004878998\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0017108145356178285\n",
      "conv1.bias 0.012980500236153603\n",
      "conv2.weight 0.00139291450381279\n",
      "conv2.bias 0.002952677197754383\n",
      "fc1.weight 0.0015126371756196022\n",
      "fc1.bias 0.004842709004878998\n",
      "\n",
      "Test set: Average loss: 0.4876 \n",
      "Accuracy: 8938/10000 (89.38%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0017906405031681062\n",
      "conv1.bias 0.01194693148136139\n",
      "conv2.weight 0.0017029155790805816\n",
      "conv2.bias 0.002719439100474119\n",
      "fc1.weight 0.0017545552924275397\n",
      "fc1.bias 0.005128423124551773\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0017906405031681062\n",
      "conv1.bias 0.01194693148136139\n",
      "conv2.weight 0.0017029155790805816\n",
      "conv2.bias 0.002719439100474119\n",
      "fc1.weight 0.0017545552924275397\n",
      "fc1.bias 0.005128423124551773\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0017906405031681062\n",
      "conv1.bias 0.01194693148136139\n",
      "conv2.weight 0.0017029155790805816\n",
      "conv2.bias 0.002719439100474119\n",
      "fc1.weight 0.0017545552924275397\n",
      "fc1.bias 0.005128423124551773\n",
      "\n",
      "Test set: Average loss: 0.3831 \n",
      "Accuracy: 9358/10000 (93.58%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001687314659357071\n",
      "conv1.bias 0.009717954322695732\n",
      "conv2.weight 0.0017297683656215667\n",
      "conv2.bias 0.0026275338605046272\n",
      "fc1.weight 0.001923261396586895\n",
      "fc1.bias 0.007033146917819977\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001687314659357071\n",
      "conv1.bias 0.009717954322695732\n",
      "conv2.weight 0.0017297683656215667\n",
      "conv2.bias 0.0026275338605046272\n",
      "fc1.weight 0.001923261396586895\n",
      "fc1.bias 0.007033146917819977\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001687314659357071\n",
      "conv1.bias 0.009717954322695732\n",
      "conv2.weight 0.0017297683656215667\n",
      "conv2.bias 0.0026275338605046272\n",
      "fc1.weight 0.001923261396586895\n",
      "fc1.bias 0.007033146917819977\n",
      "\n",
      "Test set: Average loss: 0.3577 \n",
      "Accuracy: 9383/10000 (93.83%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001560683697462082\n",
      "conv1.bias 0.010566426441073418\n",
      "conv2.weight 0.0015118178725242615\n",
      "conv2.bias 0.0025534096639603376\n",
      "fc1.weight 0.0018148975446820258\n",
      "fc1.bias 0.0066042192280292514\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001560683697462082\n",
      "conv1.bias 0.010566426441073418\n",
      "conv2.weight 0.0015118178725242615\n",
      "conv2.bias 0.0025534096639603376\n",
      "fc1.weight 0.0018148975446820258\n",
      "fc1.bias 0.0066042192280292514\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001560683697462082\n",
      "conv1.bias 0.010566426441073418\n",
      "conv2.weight 0.0015118178725242615\n",
      "conv2.bias 0.0025534096639603376\n",
      "fc1.weight 0.0018148975446820258\n",
      "fc1.bias 0.0066042192280292514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.3270 \n",
      "Accuracy: 9427/10000 (94.27%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019084058701992034\n",
      "conv1.bias 0.010240098461508751\n",
      "conv2.weight 0.0015879200398921966\n",
      "conv2.bias 0.0026123467832803726\n",
      "fc1.weight 0.0019426412880420684\n",
      "fc1.bias 0.007629890739917755\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019084058701992034\n",
      "conv1.bias 0.010240098461508751\n",
      "conv2.weight 0.0015879200398921966\n",
      "conv2.bias 0.0026123467832803726\n",
      "fc1.weight 0.0019426412880420684\n",
      "fc1.bias 0.007629890739917755\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019084058701992034\n",
      "conv1.bias 0.010240098461508751\n",
      "conv2.weight 0.0015879200398921966\n",
      "conv2.bias 0.0026123467832803726\n",
      "fc1.weight 0.0019426412880420684\n",
      "fc1.bias 0.007629890739917755\n",
      "\n",
      "Test set: Average loss: 0.3243 \n",
      "Accuracy: 9453/10000 (94.53%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019351357221603393\n",
      "conv1.bias 0.009360967203974724\n",
      "conv2.weight 0.0015006344020366668\n",
      "conv2.bias 0.0025399071164429188\n",
      "fc1.weight 0.0018148988485336304\n",
      "fc1.bias 0.00817214474081993\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019351357221603393\n",
      "conv1.bias 0.009360967203974724\n",
      "conv2.weight 0.0015006344020366668\n",
      "conv2.bias 0.0025399071164429188\n",
      "fc1.weight 0.0018148988485336304\n",
      "fc1.bias 0.00817214474081993\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019351357221603393\n",
      "conv1.bias 0.009360967203974724\n",
      "conv2.weight 0.0015006344020366668\n",
      "conv2.bias 0.0025399071164429188\n",
      "fc1.weight 0.0018148988485336304\n",
      "fc1.bias 0.00817214474081993\n",
      "\n",
      "Test set: Average loss: 0.3002 \n",
      "Accuracy: 9451/10000 (94.51%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019364765286445618\n",
      "conv1.bias 0.012811065651476383\n",
      "conv2.weight 0.00136318638920784\n",
      "conv2.bias 0.0024727806448936462\n",
      "fc1.weight 0.0021996635943651198\n",
      "fc1.bias 0.007655734568834305\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019364765286445618\n",
      "conv1.bias 0.012811065651476383\n",
      "conv2.weight 0.00136318638920784\n",
      "conv2.bias 0.0024727806448936462\n",
      "fc1.weight 0.0021996635943651198\n",
      "fc1.bias 0.007655734568834305\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019364765286445618\n",
      "conv1.bias 0.012811065651476383\n",
      "conv2.weight 0.00136318638920784\n",
      "conv2.bias 0.0024727806448936462\n",
      "fc1.weight 0.0021996635943651198\n",
      "fc1.bias 0.007655734568834305\n",
      "\n",
      "Test set: Average loss: 0.3269 \n",
      "Accuracy: 9441/10000 (94.41%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00196434885263443\n",
      "conv1.bias 0.011345649138092995\n",
      "conv2.weight 0.001508626937866211\n",
      "conv2.bias 0.0028265477158129215\n",
      "fc1.weight 0.0019830673933029174\n",
      "fc1.bias 0.00726996511220932\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00196434885263443\n",
      "conv1.bias 0.011345649138092995\n",
      "conv2.weight 0.001508626937866211\n",
      "conv2.bias 0.0028265477158129215\n",
      "fc1.weight 0.0019830673933029174\n",
      "fc1.bias 0.00726996511220932\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00196434885263443\n",
      "conv1.bias 0.011345649138092995\n",
      "conv2.weight 0.001508626937866211\n",
      "conv2.bias 0.0028265477158129215\n",
      "fc1.weight 0.0019830673933029174\n",
      "fc1.bias 0.00726996511220932\n",
      "\n",
      "Test set: Average loss: 0.3302 \n",
      "Accuracy: 9261/10000 (92.61%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019670820236206057\n",
      "conv1.bias 0.013485253788530827\n",
      "conv2.weight 0.0010545069724321365\n",
      "conv2.bias 0.0035136318765580654\n",
      "fc1.weight 0.0011715518310666084\n",
      "fc1.bias 0.0053487751632928845\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019670820236206057\n",
      "conv1.bias 0.013485253788530827\n",
      "conv2.weight 0.0010545069724321365\n",
      "conv2.bias 0.0035136318765580654\n",
      "fc1.weight 0.0011715518310666084\n",
      "fc1.bias 0.0053487751632928845\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019670820236206057\n",
      "conv1.bias 0.013485253788530827\n",
      "conv2.weight 0.0010545069724321365\n",
      "conv2.bias 0.0035136318765580654\n",
      "fc1.weight 0.0011715518310666084\n",
      "fc1.bias 0.0053487751632928845\n",
      "\n",
      "Test set: Average loss: 0.2877 \n",
      "Accuracy: 9490/10000 (94.90%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019136971235275268\n",
      "conv1.bias 0.017581172287464142\n",
      "conv2.weight 0.000780337080359459\n",
      "conv2.bias 0.0032722316682338715\n",
      "fc1.weight 0.0011622031219303609\n",
      "fc1.bias 0.00622045025229454\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019136971235275268\n",
      "conv1.bias 0.017581172287464142\n",
      "conv2.weight 0.000780337080359459\n",
      "conv2.bias 0.0032722316682338715\n",
      "fc1.weight 0.0011622031219303609\n",
      "fc1.bias 0.00622045025229454\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019136971235275268\n",
      "conv1.bias 0.017581172287464142\n",
      "conv2.weight 0.000780337080359459\n",
      "conv2.bias 0.0032722316682338715\n",
      "fc1.weight 0.0011622031219303609\n",
      "fc1.bias 0.00622045025229454\n",
      "\n",
      "Test set: Average loss: 0.2730 \n",
      "Accuracy: 9516/10000 (95.16%)\n",
      "\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021762040257453917\n",
      "conv1.bias 0.017481844872236252\n",
      "conv2.weight 0.0005521894246339798\n",
      "conv2.bias 0.003299532923847437\n",
      "fc1.weight 0.0007207732181996107\n",
      "fc1.bias 0.004980864748358726\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021762040257453917\n",
      "conv1.bias 0.017481844872236252\n",
      "conv2.weight 0.0005521894246339798\n",
      "conv2.bias 0.003299532923847437\n",
      "fc1.weight 0.0007207732181996107\n",
      "fc1.bias 0.004980864748358726\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021762040257453917\n",
      "conv1.bias 0.017481844872236252\n",
      "conv2.weight 0.0005521894246339798\n",
      "conv2.bias 0.003299532923847437\n",
      "fc1.weight 0.0007207732181996107\n",
      "fc1.bias 0.004980864748358726\n",
      "\n",
      "Test set: Average loss: 0.2860 \n",
      "Accuracy: 9523/10000 (95.23%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "N = 6\n",
    "K = 6\n",
    "G = 3\n",
    "\n",
    "size_per_group = int(60000/G)\n",
    "\n",
    "X_group = np.reshape(encoding_input_array_np, (G,size_per_group,28*28))\n",
    "y_group = np.reshape(encoding_label_array_np, (G,size_per_group,args.num_classes)) \n",
    "\n",
    "\n",
    "N_i = int(N/G) # = 6\n",
    "K_i = int(K/G) # = 4\n",
    "T = K_i + 1\n",
    "sigma = 1\n",
    "Noise_Alloc = [0,2,4]\n",
    "m = N_i # number of selected workers (if there is no straggler, m=N_i)\n",
    "\n",
    "print(N_i,K_i,T)\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K_i+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K_i+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K_i+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "z_array = np.array([-0.81, 0.81])\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 30\n",
    "\n",
    "loss_test_arr_K6_G3_N6 = np.zeros((N_trials,N_epochs))\n",
    "acc_test_arr_K6_G3_N6  = np.zeros((N_trials,N_epochs))\n",
    "\n",
    "for trial_idx in range(N_trials):\n",
    "    \n",
    "    print('##########################################')\n",
    "    print('######',trial_idx,'-th Trial!! ###########')\n",
    "    \n",
    "    net_glob = CNNMnist2(args=args)\n",
    "    net_glob.cuda()\n",
    "    net_glob.train()\n",
    "    \n",
    "    # copy weights\n",
    "    w_glob = net_glob.state_dict()\n",
    "    \n",
    "    X_tilde = np.empty((N,Size_submatrices,28*28))\n",
    "    y_tilde = np.empty((N,Size_submatrices,10))\n",
    "    \n",
    "    for G_idx in range(G):\n",
    "        \n",
    "        _Noise_label = np.ones((size_per_group*T,10)) * 0.1\n",
    "        \n",
    "        X_tilde_tmp,a,b = BACC_Enc_Data_v3(X_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde_tmp,a,b = BACC_Enc_Data_v3(y_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "        \n",
    "        stt_pos = G_idx * N_i\n",
    "        end_pos = (G_idx+1) * N_i\n",
    "        \n",
    "        X_tilde[stt_pos:end_pos,:,:] = X_tilde_tmp\n",
    "        y_tilde[stt_pos:end_pos,:,:] = y_tilde_tmp\n",
    "        \n",
    "   \n",
    "\n",
    "    for iter in range(N_epochs): #args.epochs\n",
    "        \n",
    "        w_group_array = []\n",
    "        for G_idx in range(G):\n",
    "            w_locals, loss_locals = [], []\n",
    "            idxs_users = np.random.choice(range(N_i), m, replace=False)\n",
    "            idxs_users = np.sort(idxs_users)\n",
    "            print('selected users:',idxs_users)\n",
    "\n",
    "            coded_net = BACC_Enc_Model_withNoise_v4(net_glob.cuda(), N_i, K_i, T, 1, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "            dec_z_array = []\n",
    "            for idx in idxs_users: #for idx in range(N):\n",
    "        #         print(idx)\n",
    "                local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[G_idx*N_i+idx,:,:], label=y_tilde[G_idx*N_i+idx,:,:])\n",
    "                w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "#                 w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                w_locals.append(copy.deepcopy(w))\n",
    "                loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "            # update global weights\n",
    "            #w_glob = FedAvg(w_locals)\n",
    "            w_group = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "            \n",
    "            w_group_array.append(copy.deepcopy(w_group))\n",
    "        \n",
    "        w_glob = copy.deepcopy(w_group_array[0])\n",
    "        for k in w_glob.keys():\n",
    "            for G_idx in range(1,G):\n",
    "                w_glob[k] += w_group_array[G_idx][k]\n",
    "            w_glob[k] = torch.div(w_glob[k], len(w_group_array))\n",
    "        \n",
    "        # copy weight to net_glob\n",
    "        net_glob.load_state_dict(w_glob)\n",
    "\n",
    "        # print loss\n",
    "    #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "    #     loss_train_arr.append(loss_train)\n",
    "\n",
    "        acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "        acc_test_arr_K6_G3_N6[trial_idx][iter] = acc_test\n",
    "        loss_test_arr_K6_G3_N6[trial_idx][iter] = loss_test\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2. K=6, G=2, N=12 => K_i =3, N_i = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375 127 93 68 337\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.9969063956393398 0.015869569238180458\n",
      "-0.9961416754051027 0.014892130787111721\n",
      "-0.9926851674365291 0.010784286346841575\n",
      "-0.9914032438759146 0.009398276579128426\n",
      "-0.9888154896697541 0.006846381442283329\n",
      "-0.987876295586551 0.006005994512303329\n",
      "-0.983205407253134 0.0025685986948604128\n",
      "-0.9831510552533835 0.002536325610539046\n",
      "-0.9804722783365587 0.0011837125623086076\n",
      "-0.9801849720343394 0.0010672252087882612\n",
      "-0.9795494731437013 0.000829976621831478\n",
      "-0.9791880718765633 0.0007077626036794876\n",
      "-0.9764263126787542 8.998126532036522e-05\n",
      "-0.9749813192461096 1.1596415842975012e-07\n",
      "-0.9738791291661815 4.521272836513751e-05\n",
      "-0.9736219755573623 7.028234480248774e-05\n",
      "-0.9729443323458871 0.00016324343660581808\n",
      "-0.9727816392135311 0.00019143056204626123\n",
      "-0.9727185897453103 0.00020297046010390017\n",
      "-0.9711114777244578 0.0006154681710042074\n",
      "-0.9699200521878515 0.0010724509639323757\n",
      "-0.966436593388476 0.0031932497187404044\n",
      "-0.9645987330763623 0.004813597868895938\n",
      "-0.9628023946383752 0.006755095933982157\n",
      "-0.9616562799252688 0.008186999023353491\n",
      "-0.9597583983823983 0.010904068229696562\n",
      "-0.9596930462675732 0.011005531222662797\n",
      "-0.9589148169866455 0.012255227338631854\n",
      "-0.9581642290356469 0.013534034316100087\n",
      "-0.9563914271868068 0.016849553880689327\n",
      "-0.9556547128428321 0.018352918543723673\n",
      "-0.9547980863231773 0.02019645559783536\n",
      "-0.9546733798815354 0.020473525166017532\n",
      "-0.9539414915973041 0.022144804598606363\n",
      "-0.9515771740917012 0.028086417948327023\n",
      "-0.9490673035524622 0.03534549673488714\n",
      "-0.9465032338769672 0.04383905289463291\n",
      "-0.9463616127883097 0.04434132071638756\n",
      "-0.9453946666413897 0.04786620600815436\n",
      "-0.9450177219849267 0.04928601931845558\n",
      "-0.9447059587529041 0.0504799534083787\n",
      "-0.944596534356247 0.05090324912417986\n",
      "-0.9433166116243665 0.0560204248938181\n",
      "-0.9419159940012802 0.06197843815574678\n",
      "-0.9328613537182444 0.11041104301370093\n",
      "-0.9295972681525988 0.1325426223123159\n",
      "-0.9283870331975028 0.14144170277155188\n",
      "-0.927886422280394 0.14523631594478623\n",
      "-0.9277268928793192 0.1464596841544301\n",
      "-0.9264322002452257 0.156644399341278\n",
      "-0.92508416713289 0.1677428653487035\n",
      "-0.9207275881399222 0.20722313325137276\n",
      "-0.9167661363889557 0.24821988820620575\n",
      "-0.9137603125827325 0.2827998597138707\n",
      "-0.9109021899779297 0.3186244006377831\n",
      "-0.9035626040192535 0.42476260305867675\n",
      "-0.9024758188089783 0.4423131070563268\n",
      "-0.9014258406309701 0.45973999973318014\n",
      "-0.9001012786214688 0.4823951348885541\n",
      "-0.8959174286480722 0.559021038245258\n",
      "-0.894531291905428 0.5861573450149058\n",
      "-0.8939494038616564 0.5978153466803945\n",
      "-0.893384610116388 0.6092831353802648\n",
      "-0.8882369304697 0.7208963517197076\n",
      "-0.8864372010190367 0.7630246580693644\n",
      "-0.8854175754451306 0.7876282294245713\n",
      "-0.8837190204646088 0.8298163938730823\n",
      "-0.8823276362052934 0.8655113085139415\n",
      "-0.8797202541807081 0.935213851354635\n",
      "-0.875644118792869 1.0517405380542373\n",
      "-0.8733338604786776 1.1220103193445243\n",
      "-0.8693902517855971 1.2492841907619978\n",
      "-0.8669337421673962 1.3333808201098125\n",
      "-0.8628387581407839 1.4821167183156567\n",
      "-0.8612049216662083 1.544539619190803\n",
      "-0.8569297487210339 1.716548139074599\n",
      "-0.8561992290406626 1.7472332487561721\n",
      "-0.8551563375506088 1.7917125037172354\n",
      "-0.8547756106027224 1.8081498874690831\n",
      "-0.8506788454572736 1.9919703569896112\n",
      "-0.8495171171772578 2.0464843379089475\n",
      "-0.8487320791637141 2.0839408906145187\n",
      "-0.8441368549485249 2.3136330460655405\n",
      "-0.8424994967375814 2.399998841679682\n",
      "-0.8411465577537116 2.4732579058860886\n",
      "-0.839517690622871 2.5638190335812374\n",
      "-0.8289300350283069 3.2238257577945335\n",
      "-0.8287648864504014 3.2352322728249105\n",
      "-0.8274627604725442 3.3265355641060887\n",
      "-0.8246720374138241 3.530857588065559\n",
      "-0.8240005910779862 3.581891140933584\n",
      "-0.8235759786032206 3.6145597985244926\n",
      "-0.8234361521782456 3.625386028697543\n",
      "-0.8215995507249652 3.770843214268818\n",
      "-0.8210112851807667 3.818763467410264\n",
      "-0.820445451747027 3.8654940404597555\n",
      "-0.820392042928229 3.869937839947081\n",
      "-0.8198854830837874 3.9123728598178515\n",
      "-0.8173127408529339 4.1363559415381435\n",
      "-0.8139373349349976 4.454269706118172\n",
      "-0.812188171342112 4.631420238546829\n",
      "-0.810276976279098 4.836149682761198\n",
      "-0.8091197685201852 4.966412269958058\n",
      "-0.8089063945364428 4.990990361809947\n",
      "-0.8062955617456817 5.307285684938468\n",
      "-0.8060702881272095 5.336034181608642\n",
      "-0.8036826392553809 5.657121705337673\n",
      "-0.7936889093160024 7.5134616110161065\n",
      "-0.7924508565354542 7.843142247857205\n",
      "-0.7910344974493833 8.269328102494796\n",
      "-0.7906564096306361 8.393879754419416\n",
      "-0.7901306005128892 8.576008360186043\n",
      "-0.7899108882388368 8.655462854727906\n",
      "-0.7860552365407403 10.562572715692955\n",
      "-0.7823803676931396 16.484261931005985\n",
      "-0.7810854248971297 15.610542624521175\n",
      "-0.7809953976440642 15.28264010829476\n",
      "-0.7809798689885632 15.229682971383294\n",
      "-0.7707058877236828 7.907016358847951\n",
      "-0.7705057959868831 7.8573519990635745\n",
      "-0.7695301999315753 7.627530599523211\n",
      "-0.7688279256939066 7.473504741752423\n",
      "-0.768064233927376 7.315525459279849\n",
      "-0.7680167279863968 7.3060015012495345\n",
      "-0.7636027425004135 6.544213159847511\n",
      "-0.7631329105284685 6.47481013746789\n",
      "-0.7608084207645975 6.156494168498172\n",
      "-0.7538106593288236 5.3855699420578125\n",
      "-0.749829917780106 5.034364505950835\n",
      "-0.7490374855313913 4.9700836492296645\n",
      "-0.7485168988114217 4.92876052066286\n",
      "-0.7447161066409396 4.6467294036088775\n",
      "-0.7436000952162756 4.569835006813716\n",
      "-0.7428759331632935 4.521231751319725\n",
      "-0.7420885441717064 4.46948632520281\n",
      "-0.7407130770105019 4.381712515250277\n",
      "-0.738579342294672 4.25168380490716\n",
      "-0.7371633804075177 4.169194601561348\n",
      "-0.7369458260452595 4.156773501679026\n",
      "-0.7344265508059329 4.017542051459862\n",
      "-0.733321309423052 3.9589858330479824\n",
      "-0.7325499367756447 3.9189740082839726\n",
      "-0.7319469210227074 3.888167986306199\n",
      "-0.7296376206609358 3.773840806549326\n",
      "-0.7280640927580455 3.699057510313398\n",
      "-0.7240899192171109 3.520337017875778\n",
      "-0.722459036676212 3.4508425159746414\n",
      "-0.7196228709209589 3.3348023785962324\n",
      "-0.7136963843273805 3.1098705704164105\n",
      "-0.7121704681017567 3.0553891045766037\n",
      "-0.7090915165126228 2.949290412173055\n",
      "-0.706362812843683 2.8592449447178487\n",
      "-0.7013527398334098 2.702713147443864\n",
      "-0.7007144635580735 2.6835285781188842\n",
      "-0.6993633586997907 2.643449061826875\n",
      "-0.6973483752695746 2.5849694189672605\n",
      "-0.6961719173975811 2.5515147622398473\n",
      "-0.6932055429057582 2.4693042995884937\n",
      "-0.6915634470045902 2.425055177062968\n",
      "-0.6910568599913385 2.4115783785092155\n",
      "-0.6910119091973765 2.410386444569734\n",
      "-0.6878263053117686 2.327488542132691\n",
      "-0.6852122593594658 2.2616830809065\n",
      "-0.6837849741015707 2.2265538620126115\n",
      "-0.6836930045515401 2.224309091570485\n",
      "-0.6824482875466558 2.1941484171588357\n",
      "-0.6816945172924187 2.176080269194669\n",
      "-0.6748455513976692 2.0183127376948646\n",
      "-0.6731605144655086 1.9811641161151547\n",
      "-0.671270330796901 1.94022692439737\n",
      "-0.670659272246493 1.9271547229836514\n",
      "-0.6672317053248296 1.855247740211487\n",
      "-0.664871489445739 1.8070832230847904\n",
      "-0.6632030050782256 1.7736730389237783\n",
      "-0.661422223546672 1.7385800983416089\n",
      "-0.6560115383249869 1.6353859020385937\n",
      "-0.655066810556939 1.6178770979678017\n",
      "-0.6541178492994337 1.600437230009512\n",
      "-0.6535952040625572 1.590894547382534\n",
      "-0.6484621430346857 1.4994632588623125\n",
      "-0.64677412093743 1.470277091947183\n",
      "-0.6422484786741138 1.394086785699821\n",
      "-0.6416532117214118 1.3842829276767474\n",
      "-0.6406421697453313 1.3677448044349672\n",
      "-0.6355916135540427 1.2872208577164919\n",
      "-0.6322734849718172 1.236157755413713\n",
      "-0.6280709378762641 1.173500041994481\n",
      "-0.6254867251987737 1.1360580812763998\n",
      "-0.6215048199062823 1.0799409638563213\n",
      "-0.6210269777345172 1.0733329093967843\n",
      "-0.6204143856800965 1.0649004812140628\n",
      "-0.6184730028749885 1.0384650414184833\n",
      "-0.6171251837834189 1.0203672795059848\n",
      "-0.6149346682682857 0.9913951060191363\n",
      "-0.6131520689188894 0.9682163675232708\n",
      "-0.6130382095423226 0.9667479179612027\n",
      "-0.6128566548864522 0.9644093766616056\n",
      "-0.6107964619613728 0.9381279237582547\n",
      "-0.6078853916668716 0.9017834365101325\n",
      "-0.6066036386982543 0.8860712213433721\n",
      "-0.6064603753321791 0.884325985153866\n",
      "-0.6020521714773228 0.8316890305457476\n",
      "-0.6000608098837863 0.8085787190956244\n",
      "-0.5994726404794644 0.801831401763327\n",
      "-0.5929749788178662 0.7296399930732536\n",
      "-0.5928093912915919 0.7278558601589032\n",
      "-0.5927189088598723 0.7268821067388946\n",
      "-0.5848496977091 0.6452786370550446\n",
      "-0.5838888032912575 0.6357261817241033\n",
      "-0.5805435873779143 0.6031573282924079\n",
      "-0.5799019682535123 0.5970315929638981\n",
      "-0.5774660491678925 0.5741264553876521\n",
      "-0.5774494844175817 0.5739725908986196\n",
      "-0.577405903260019 0.5735679017624981\n",
      "-0.5772313751485705 0.5719490243172128\n",
      "-0.5701359574858704 0.5085045632684396\n",
      "-0.5678749351151946 0.4892459901599197\n",
      "-0.5672191304368526 0.4837454694529802\n",
      "-0.5669181257205695 0.481233605052158\n",
      "-0.5640723720209806 0.45788149697773756\n",
      "-0.5615070336235555 0.4374382650783816\n",
      "-0.5606269704496778 0.4305565155402903\n",
      "-0.5560915971727374 0.39614366697581765\n",
      "-0.5539559557933629 0.38054217773419297\n",
      "-0.5518427537961146 0.3654793602418482\n",
      "-0.5513950018826026 0.3623352997957586\n",
      "-0.5444551480463005 0.3156952291826545\n",
      "-0.5431244416854966 0.3071943776323955\n",
      "-0.5427461750205143 0.3048035251310313\n",
      "-0.54126597052197 0.29555616245839883\n",
      "-0.5409825016388838 0.2938048481698087\n",
      "-0.5403567837919145 0.2899612959263002\n",
      "-0.538388076695606 0.278066826765832\n",
      "-0.5375903565195714 0.27333246747013457\n",
      "-0.5368687085685802 0.2690916942954375\n",
      "-0.5343786026850121 0.2547634886222865\n",
      "-0.5334743424099901 0.24967648438698145\n",
      "-0.5320234698523236 0.24164263072426792\n",
      "-0.5316417923772863 0.23955528835877174\n",
      "-0.530520056300317 0.23348326028971508\n",
      "-0.5305028952257138 0.23339108968559025\n",
      "-0.5301157949742257 0.2313177840448672\n",
      "-0.5292468586656638 0.22670395179859865\n",
      "-0.5264998848532161 0.2124811619764197\n",
      "-0.5251004519978955 0.2054454408619452\n",
      "-0.5250546605819775 0.20521759810453447\n",
      "-0.5236893001429472 0.19849281974818248\n",
      "-0.5215617415808658 0.1882775357382746\n",
      "-0.5167252627924479 0.16623058848168026\n",
      "-0.5105638494424352 0.14044644742358284\n",
      "-0.5074769777086008 0.12847099429934655\n",
      "-0.5038725051204358 0.11526260732770167\n",
      "-0.5012285071382832 0.10609435378849534\n",
      "-0.4989625389848693 0.09858116525525139\n",
      "-0.49638119615759835 0.09040323082283712\n",
      "-0.49240408630560895 0.07858244168493032\n",
      "-0.4917013713256 0.0765905102332319\n",
      "-0.49123312181224943 0.0752791391693457\n",
      "-0.4898984659763481 0.07161089522825557\n",
      "-0.4871912454966332 0.06448347267133836\n",
      "-0.48487562984733446 0.05871541765016633\n",
      "-0.48310189232868495 0.05449890280844244\n",
      "-0.48186941873508604 0.051670938768130566\n",
      "-0.47723544651308125 0.04177311621064077\n",
      "-0.47710824339009084 0.04151758211938749\n",
      "-0.4760646643206643 0.039453366912325566\n",
      "-0.4710077078977981 0.030252710859016475\n",
      "-0.4673249732974072 0.02437174041559359\n",
      "-0.46705982198083684 0.023974461174268217\n",
      "-0.4662605171411691 0.022797889029854287\n",
      "-0.465366303900814 0.02151889902085436\n",
      "-0.4650068292213594 0.021015792391011073\n",
      "-0.45707252375238006 0.011494264905805988\n",
      "-0.44997097768616867 0.005462305270108157\n",
      "-0.4495100994624839 0.0051496769456548415\n",
      "-0.447363784385004 0.0038180214141034317\n",
      "-0.44118417501379437 0.0011083138607948012\n",
      "-0.43912183247572023 0.0005686707017990586\n",
      "-0.4384101128945159 0.00042415072160104626\n",
      "-0.4374483300771155 0.0002626482571246614\n",
      "-0.4356044873418634 6.1028319429112846e-05\n",
      "-0.4340974726129896 9.393518504999595e-07\n",
      "-0.4228725033840215 0.002452660978360544\n",
      "-0.4215506197841288 0.0030713155814212035\n",
      "-0.4186335178095344 0.004677714712846291\n",
      "-0.4177974105580313 0.0051989889045184244\n",
      "-0.41711622866798215 0.005643614888346794\n",
      "-0.41678373618652076 0.0058671314423832934\n",
      "-0.4164249739756585 0.006113075894491419\n",
      "-0.4157337771924665 0.006600849620152575\n",
      "-0.41470026410171346 0.007364343721271256\n",
      "-0.4136823176860671 0.008156236025964578\n",
      "-0.41198204479412626 0.00956691285085703\n",
      "-0.40972652959231914 0.011607337931125037\n",
      "-0.40318090834375675 0.018611022685360433\n",
      "-0.4023379493361925 0.019629155304930757\n",
      "-0.4018330267742576 0.020251635513447896\n",
      "-0.40033105950465475 0.022159146352857758\n",
      "-0.3974515494022892 0.02604932088291072\n",
      "-0.39724895051119136 0.026334544083774142\n",
      "-0.397112986065395 0.026526805527364077\n",
      "-0.39603417407151276 0.02807642625555503\n",
      "-0.3941394059777996 0.030901672253422618\n",
      "-0.3929660458367581 0.03271732942713225\n",
      "-0.39238714403851493 0.033631722951030174\n",
      "-0.38695734039849494 0.04280558415413689\n",
      "-0.38694156959896664 0.04283380029453064\n",
      "-0.383579803590115 0.04905598599168629\n",
      "-0.38216328597289273 0.05180147986403107\n",
      "-0.3810558136807145 0.05399908527671327\n",
      "-0.37903064632495687 0.058133757379007146\n",
      "-0.37749259421243475 0.0613742360323578\n",
      "-0.377218712212672 0.061960361715609966\n",
      "-0.3771820327902806 0.06203906689517858\n",
      "-0.3761037199584849 0.06437490953921365\n",
      "-0.3757894751336448 0.06506365346462901\n",
      "-0.372319979988915 0.07290904204551911\n",
      "-0.3694920310827685 0.07963167367083263\n",
      "-0.3615330592765973 0.10014362597927967\n",
      "-0.3594034588307866 0.10603350725713313\n",
      "-0.3586539925609895 0.1081469468181481\n",
      "-0.356388726883351 0.1146639256026614\n",
      "-0.35608006246106805 0.11556698702498898\n",
      "-0.353325033407633 0.1237881269215033\n",
      "-0.35278913065789763 0.12542098052771886\n",
      "-0.3418760974153301 0.16109305346257313\n",
      "-0.34169071826244823 0.16173937981237632\n",
      "-0.34158482314589556 0.16210919717354244\n",
      "-0.3410614503773346 0.16394352213020158\n",
      "-0.3386661906736099 0.1724780369564568\n",
      "-0.33589674797434466 0.18263340005991188\n",
      "-0.3320319396373903 0.19732702466288973\n",
      "-0.32685256282601216 0.21798697449328902\n",
      "-0.31986577754451195 0.24765503095119604\n",
      "-0.3159961692737756 0.2649974054780056\n",
      "-0.31536537489224203 0.26788710211270206\n",
      "-0.3135204073525275 0.27644086526575345\n",
      "-0.31226088437144117 0.28236810925477146\n",
      "-0.31209566535528666 0.2831509324450934\n",
      "-0.31187337577783714 0.28420611044635385\n",
      "-0.3091323034714861 0.29740237551862725\n",
      "-0.3042448375027438 0.3217915351952891\n",
      "-0.3004114929798465 0.34170668690332834\n",
      "-0.29854744089772933 0.35164537139182434\n",
      "-0.2962848930489914 0.36393568958586714\n",
      "-0.29282719730431017 0.383205604107231\n",
      "-0.2926639379435494 0.38413018474002436\n",
      "-0.29064320639836194 0.39568522445618437\n",
      "-0.28938866387029805 0.40296311928562323\n",
      "-0.2893278050563597 0.40331821660052897\n",
      "-0.2847531095796938 0.4305572617207924\n",
      "-0.28447926407642243 0.43222236871515574\n",
      "-0.2831362724733355 0.4404455549382234\n",
      "-0.28201058855517624 0.44741172070754986\n",
      "-0.2802127109472221 0.45867796774668274\n",
      "-0.2768465387431369 0.4802414520358105\n",
      "-0.27625343747718634 0.484104981066575\n",
      "-0.27491860011070823 0.4928713100652993\n",
      "-0.2731278946636946 0.5047871313513553\n",
      "-0.27262861282285145 0.5081415083285816\n",
      "-0.27161346188016333 0.5150050472589697\n",
      "-0.2650636797239265 0.560708507161253\n",
      "-0.2624834845069175 0.5794016439157246\n",
      "-0.259465541235679 0.6017725660012776\n",
      "-0.25865798140399066 0.6078525260718495\n",
      "-0.253554054909499 0.6472122182322153\n",
      "-0.25224191235398874 0.6575953291747646\n",
      "-0.24834120288353123 0.6891143432650481\n",
      "-0.24799481871027163 0.6919609906708198\n",
      "-0.2478750974738504 0.6929467092884938\n",
      "-0.24736554859711357 0.697152578972785\n",
      "-0.24411989618367191 0.7243459266004453\n",
      "-0.24367731396863568 0.7281085656386104\n",
      "-0.24334455205924654 0.7309462510928371\n",
      "-0.24289542752495707 0.7347880984403241\n",
      "-0.24228287683453975 0.7400499345341709\n",
      "-0.24218432228555886 0.7408989020387817\n",
      "-0.24144929846000251 0.7472514064965808\n",
      "-0.2406834475550761 0.7539096253791946\n",
      "-0.23893201113520401 0.7692881722159683\n",
      "-0.23811013522923563 0.7765780110034329\n",
      "-0.2363332958377371 0.7924997647795926\n",
      "-0.23606672384631966 0.794907626647437\n",
      "-0.2327576103742317 0.825219506388277\n",
      "-0.22981765713669122 0.852814096815699\n",
      "-0.229755281086665 0.853406429063555\n",
      "-0.2290472959858849 0.8601497252690684\n",
      "-0.2268362245045259 0.8814495930697678\n",
      "-0.22434876898270573 0.9058523076509762\n",
      "-0.2242212789339535 0.9071157243199083\n",
      "-0.22414704713335198 0.9078519297300013\n",
      "-0.2224186585897956 0.9251134355999779\n",
      "-0.22212501440131893 0.9280690439020888\n",
      "-0.2220976436917974 0.9283448790016551\n",
      "-0.22062169718617253 0.9433056910405003\n",
      "-0.21936466282079392 0.9561824575891394\n",
      "-0.21767282444452718 0.973711245953056\n",
      "-0.21764452590715178 0.9740063865313093\n",
      "-0.2135255226955124 1.0176578197703159\n",
      "-0.21336217409574254 1.0194175522774394\n",
      "-0.21142302743577446 1.0404771054570727\n",
      "-0.20888350210579532 1.0685347737161182\n",
      "-0.2083438136965461 1.0745681404370089\n",
      "-0.2064740096948261 1.0956652883479607\n",
      "-0.20129465850633688 1.1557060607902676\n",
      "-0.20124569083700217 1.15628514384156\n",
      "-0.1982688176045182 1.1918998614834204\n",
      "-0.19704667199495174 1.2067579538961246\n",
      "-0.1933043017713847 1.2531298504020845\n",
      "-0.19306909039532494 1.2560890097396806\n",
      "-0.19019780943850328 1.292645654602716\n",
      "-0.19010881080446995 1.2937916846509867\n",
      "-0.18892565012825768 1.309101794123113\n",
      "-0.1888948608974086 1.3095020666617638\n",
      "-0.18238589531278038 1.3962852258906786\n",
      "-0.18183566358397152 1.403822834236024\n",
      "-0.1815402265879864 1.407883247876016\n",
      "-0.1782850002129186 1.4532417246023566\n",
      "-0.17431313998643727 1.5101588274314868\n",
      "-0.1707928085591892 1.5620983840172986\n",
      "-0.1701921599080698 1.571104085717864\n",
      "-0.16938311084240154 1.583301603887935\n",
      "-0.16578015378235733 1.6385729379193164\n",
      "-0.16432551298009335 1.6613368730033031\n",
      "-0.16403987472690051 1.6658377443680337\n",
      "-0.16389019483135758 1.6682003582200637\n",
      "-0.15951899299813244 1.7384522457705758\n",
      "-0.1585814939844845 1.7538412343399565\n",
      "-0.1511093665952996 1.8807463991636266\n",
      "-0.15057694649216735 1.8900866079262257\n",
      "-0.15018671049169074 1.896958421633021\n",
      "-0.14983068350308826 1.9032470622717916\n",
      "-0.14846868901408405 1.927475167125936\n",
      "-0.1457167484642896 1.977268569113019\n",
      "-0.1437816076430798 2.012971713281596\n",
      "-0.14146686057229796 2.0564465432223393\n",
      "-0.1412404861434029 2.0607439533733634\n",
      "-0.14117594002493794 2.061970781693522\n",
      "-0.1410894118292696 2.063616474982391\n",
      "-0.13866024836815538 2.1103139337634897\n",
      "-0.13820688371233536 2.1191368257845173\n",
      "-0.1358611203116442 2.1653398844614267\n",
      "-0.13566263977621928 2.169292256545692\n",
      "-0.13517538288060305 2.1790239122276507\n",
      "-0.1327298477713319 2.2284944313703945\n",
      "-0.1257987164219876 2.3746462506355313\n",
      "-0.12403829877053996 2.413239061186148\n",
      "-0.12340702422931482 2.427229938323761\n",
      "-0.11910955926404032 2.5246738848171146\n",
      "-0.11804353852959903 2.549457412376479\n",
      "-0.1168127754920878 2.578383691728735\n",
      "-0.11558918470227542 2.607479745253969\n",
      "-0.11429748582619648 2.638568667977649\n",
      "-0.11329227277840093 2.663032682152689\n",
      "-0.11245145787051292 2.6836805190001596\n",
      "-0.11186589534991875 2.698160905721727\n",
      "-0.10877188709416807 2.7760809740911174\n",
      "-0.10681845035205284 2.8265356253315916\n",
      "-0.10468241444928439 2.8828687944973113\n",
      "-0.09741892795554707 3.0841542313162704\n",
      "-0.09279877250229474 3.220731432711004\n",
      "-0.0873718426500627 3.3906728307963583\n",
      "-0.08523476943250019 3.460666567167486\n",
      "-0.08428528363091692 3.4923568771730893\n",
      "-0.08317857042413457 3.529769805993557\n",
      "-0.0821109943993894 3.5663559374894405\n",
      "-0.08120784873288156 3.5976967414821996\n",
      "-0.08058993436548056 3.619349555922972\n",
      "-0.07733263863256834 3.736423210475791\n",
      "-0.07295551576000081 3.902063912403449\n",
      "-0.0708088092933492 3.9870925187575956\n",
      "-0.0697109646338423 4.031609100060749\n",
      "-0.06908584663174344 4.05728048320853\n",
      "-0.06896816687860463 4.062139879006381\n",
      "-0.06590082234121453 4.191896939234129\n",
      "-0.06399533224294052 4.2756611367861606\n",
      "-0.06387977859622307 4.280822628984604\n",
      "-0.06079488108750608 4.4222640585806925\n",
      "-0.060592570306722715 4.431794549580567\n",
      "-0.054867348253214265 4.715852810074806\n",
      "-0.051634766107599095 4.889869750852227\n",
      "-0.050947439878267886 4.928293249091435\n",
      "-0.04979080010149328 4.994154235066772\n",
      "-0.04885580273172119 5.048537288199378\n",
      "-0.04672090777773019 5.176765971640995\n",
      "-0.04351723494972459 5.38075280009182\n",
      "-0.043505324941774726 5.381539113671728\n",
      "-0.04234704504178266 5.459066978724119\n",
      "-0.04142310062947541 5.522459677199389\n",
      "-0.03586067038724661 5.937100824856294\n",
      "-0.03358609838169668 6.125659018340172\n",
      "-0.03189155683242895 6.274676043340866\n",
      "-0.0315472468840472 6.305925718161405\n",
      "-0.02879412257425118 6.568865771916048\n",
      "-0.028381366735463986 6.61044998961194\n",
      "-0.026983368612214065 6.7559471645668285\n",
      "-0.026941103923860066 6.760462810826858\n",
      "-0.026428376596754122 6.8158169090797385\n",
      "-0.025565583940345027 6.911442129571525\n",
      "-0.025239539959889923 6.9484235830562024\n",
      "-0.025081501490586744 6.966521812741033\n",
      "-0.024185428463401104 7.071351266983455\n",
      "-0.021994768096748052 7.344982623471017\n",
      "-0.021484143548774037 7.412687914948412\n",
      "-0.018626914519763504 7.824089643165366\n",
      "-0.014592156625125474 8.528055492492228\n",
      "-0.012085212693140335 9.071744554514344\n",
      "-0.012041817944528299 9.082120554056829\n",
      "-0.011428810999623007 9.232830917365105\n",
      "-0.004813260241861794 11.727670462069693\n",
      "-0.0042578412038083435 12.08143919304347\n",
      "-0.004117418455520605 12.178199632009756\n",
      "-0.0027151964435947473 13.379536524346209\n",
      "0.001973845027309462 14.299621317810004\n",
      "0.0029020457830517543 13.18751242279731\n",
      "0.0054092263203000535 11.390873949922266\n",
      "0.007642708705370538 10.393642084454102\n",
      "0.008023272930047565 10.253447190899745\n",
      "0.00995761885482338 9.630336918584776\n",
      "0.0152575097709855 8.399465597809392\n",
      "0.015475345420145503 8.358582614156909\n",
      "0.015815634088570008 8.295856788293435\n",
      "0.01800615655441029 7.921814226159998\n",
      "0.019805730135887822 7.647175553607812\n",
      "0.020628542641753178 7.529833607710772\n",
      "0.021474962785895224 7.413919924681246\n",
      "0.023656457301791667 7.135078208626906\n",
      "0.026531593072651294 6.804587533482442\n",
      "0.02660298261225935 6.79684636053994\n",
      "0.029437125016750665 6.50526224172718\n",
      "0.030919060717103264 6.363833239587936\n",
      "0.034254607355488575 6.068939858241661\n",
      "0.03790884593848154 5.777335115229389\n",
      "0.043323501483012006 5.393570378754135\n",
      "0.046527766786004454 5.188657617472731\n",
      "0.050681202522041335 4.943318038129645\n",
      "0.05246175447059365 4.844320307909796\n",
      "0.05550591343105826 4.682711455226062\n",
      "0.06117928203626244 4.404244477972515\n",
      "0.061744314498212605 4.377966156263997\n",
      "0.062214761225137805 4.3562734788629065\n",
      "0.06241249764034307 4.347205668352045\n",
      "0.06301025667679494 4.319971069912383\n",
      "0.0689157470463404 4.064307216102355\n",
      "0.07195120281387113 3.9415179076309257\n",
      "0.07222344023702787 3.930767325821001\n",
      "0.0741357852651996 3.856407924313392\n",
      "0.07989389350445997 3.643948106359816\n",
      "0.08055991655380135 3.620405833971096\n",
      "0.08573724516505532 3.4440451290762963\n",
      "0.08907156514732884 3.3362760522188486\n",
      "0.0897077259652499 3.3161974759013346\n",
      "0.09028927547931676 3.29797353421067\n",
      "0.09105533561742374 3.2741558917758335\n",
      "0.0930715435313112 3.2124691403708794\n",
      "0.0938691820128763 3.1884551113461637\n",
      "0.09495017647948445 3.156253924826312\n",
      "0.09514810934869522 3.1504000488259636\n",
      "0.09638523216578454 3.1141034121122657\n",
      "0.09995523740239842 3.0120870331245704\n",
      "0.10435876608115002 2.8915129819087606\n",
      "0.10829777527547835 2.7882352719367547\n",
      "0.10883618896731195 2.7744369942365834\n",
      "0.11033439079370111 2.736431088671406\n",
      "0.1106789476941823 2.727770445351999\n",
      "0.11210688626971743 2.6921913535202275\n",
      "0.11347852474969056 2.6584817811926253\n",
      "0.11466445989114504 2.629696810248502\n",
      "0.12288892132833529 2.4387732988191675\n",
      "0.12311917198554045 2.4336365161812674\n",
      "0.1233370065328876 2.4287867276888018\n",
      "0.12387172368491317 2.416922996737506\n",
      "0.12667509943086497 2.3556623619050256\n",
      "0.12703312728147487 2.347950004535194\n",
      "0.12793107663472525 2.328715996405598\n",
      "0.12831974902887056 2.3204385885930714\n",
      "0.13258522756236868 2.231453156376283\n",
      "0.13265304507529274 2.2300652389543836\n",
      "0.13486429212437367 2.1852586467280903\n",
      "0.1379980725606158 2.12321200919775\n",
      "0.13914265147531668 2.1009633797426757\n",
      "0.1403115676444895 2.0784647053405605\n",
      "0.14035832384480873 2.077569409052502\n",
      "0.14319062605939803 2.0239910071958627\n",
      "0.14321091609282588 2.023611777621038\n",
      "0.14774720071482372 1.9404201647410237\n",
      "0.1519660909055991 1.8658020901185302\n",
      "0.1522053397032106 1.8616473898047858\n",
      "0.15327829364861523 1.843114222646741\n",
      "0.15338504602333725 1.841279124166484\n",
      "0.1572771627353633 1.7754452464586077\n",
      "0.15905922893737423 1.7459847998467812\n",
      "0.16044066324955275 1.7234352385766378\n",
      "0.16089064425331734 1.7161436619372512\n",
      "0.1619705156486655 1.6987516022942462\n",
      "0.1645222828172479 1.6582422416610059\n",
      "0.16504602565004833 1.6500287044288529\n",
      "0.1656614788712163 1.6404203103941044\n",
      "0.16569014762725653 1.639973874837299\n",
      "0.17069614007916 1.563544903917472\n",
      "0.17074556312831524 1.562805214544791\n",
      "0.17096184417652394 1.5595716119895884\n",
      "0.17263604422854417 1.5347249618266134\n",
      "0.17446743512118634 1.5079147977791496\n",
      "0.17479948987892557 1.5030946110127916\n",
      "0.1762629733420349 1.4819980804641544\n",
      "0.1788158744300692 1.4457662995054388\n",
      "0.1790735225407203 1.4421493309751905\n",
      "0.17929421335554085 1.4390569160263371\n",
      "0.1838862422354075 1.3758938484844625\n",
      "0.18391464253040213 1.3755101272016117\n",
      "0.187390821475502 1.3291709082159555\n",
      "0.18953371815546727 1.3012160203947891\n",
      "0.190332530358031 1.2909123499127333\n",
      "0.19072547958459518 1.2858669307547634\n",
      "0.19247766527611998 1.2635532552424453\n",
      "0.19248174656761186 1.263501630126879\n",
      "0.19775408078621592 1.1981407749364337\n",
      "0.19962605715108395 1.1755614502501244\n",
      "0.20116091485323206 1.1572882029168405\n",
      "0.2034838395017855 1.1300376564860386\n",
      "0.20598053882492007 1.101283725457463\n",
      "0.20700260120592384 1.089670486145544\n",
      "0.2092831463505076 1.0640830595042947\n",
      "0.2109001671648023 1.0462093029809387\n",
      "0.21209125061691836 1.0331846511441074\n",
      "0.21269450988099536 1.0266331901741392\n",
      "0.21474707110993174 1.0045679694881484\n",
      "0.21601132697162484 0.9911491903865347\n",
      "0.21601663147886696 0.9910931626729278\n",
      "0.21758806450652934 0.9745954447154183\n",
      "0.22029646205530562 0.9466253704423949\n",
      "0.2271911361798964 0.878005968139523\n",
      "0.22959696357244708 0.8549111259021449\n",
      "0.23185221063189343 0.8336504196865041\n",
      "0.236500758619562 0.7909896946756246\n",
      "0.23662513230785742 0.7898694552770003\n",
      "0.24409878336887236 0.7245251192144754\n",
      "0.2441950095389689 0.7237086533324244\n",
      "0.24444781024042816 0.721566635774686\n",
      "0.24575344002055588 0.7105719020776767\n",
      "0.2470058495756331 0.700131857331659\n",
      "0.2520637443713065 0.6590136414582239\n",
      "0.25351564268069615 0.6475146252696478\n",
      "0.25739343575740126 0.6174534464288921\n",
      "0.26094301251422913 0.5907518154220555\n",
      "0.26212231568037825 0.5820499262219226\n",
      "0.26386861957886687 0.5693173746170883\n",
      "0.26603387639637743 0.553781441543409\n",
      "0.26812176189943315 0.5390609178940392\n",
      "0.26958662836925784 0.5288837388077581\n",
      "0.2696292973823444 0.5285891455263845\n",
      "0.26996742038139865 0.5262583833372232\n",
      "0.2772173247776579 0.47783593192004936\n",
      "0.2787757967083204 0.4678073784673902\n",
      "0.2882906824701674 0.40939875331242376\n",
      "0.2896002339073691 0.4017301287603793\n",
      "0.29253903049388885 0.38483847140766253\n",
      "0.29300836691634724 0.3821811570725008\n",
      "0.29876796112648574 0.3504608395559484\n",
      "0.302565301133509 0.33043107252206916\n",
      "0.3040061814823112 0.3230110530765405\n",
      "0.304180015036263 0.3221225084732136\n",
      "0.3057113162678293 0.31435670232134727\n",
      "0.30937081577526415 0.29624047219795224\n",
      "0.3099474484302487 0.29344220554112477\n",
      "0.3113998371792708 0.2864613996111116\n",
      "0.3130175200106118 0.2787988515793305\n",
      "0.31353386681087514 0.27637791152253\n",
      "0.31893568844564046 0.2517633856998198\n",
      "0.3192253417793194 0.2504798888111971\n",
      "0.32210147505463804 0.237933457258461\n",
      "0.324769552218374 0.22661361406202504\n",
      "0.32673528431542853 0.21846779525702487\n",
      "0.32679965765927843 0.21820380427368397\n",
      "0.32907471968195345 0.20898587503645447\n",
      "0.329600026884161 0.20688828877145815\n",
      "0.332976909061915 0.1936777827347287\n",
      "0.33316625787284204 0.19295098290295032\n",
      "0.33363013887273363 0.19117663494540982\n",
      "0.3368949613704979 0.17893727764389844\n",
      "0.3401162020083548 0.1672841091990837\n",
      "0.34103441310770566 0.16403857901742944\n",
      "0.3429343604916333 0.15742951925987608\n",
      "0.3432708516714813 0.1562739354640909\n",
      "0.3460697250605369 0.1468349294336982\n",
      "0.34832864120303 0.13944067597994583\n",
      "0.3513305110730587 0.12992110785274377\n",
      "0.35289342135071755 0.1251023524670719\n",
      "0.3555757719986332 0.11705018424338985\n",
      "0.3571948517366046 0.11232249656274944\n",
      "0.3585872035191311 0.10833631572975688\n",
      "0.36157327958445507 0.10003402727913888\n",
      "0.362663311024231 0.0970868257349114\n",
      "0.36465383529916195 0.09181956725432207\n",
      "0.3649943739419521 0.09093325696477712\n",
      "0.3656600573921023 0.0892131757844985\n",
      "0.3761645749040594 0.06424194993060256\n",
      "0.3787055020365506 0.05881157224736044\n",
      "0.37987457778703826 0.056392508547610624\n",
      "0.38411073690238906 0.04804583012638957\n",
      "0.3872400391126325 0.04230133943070729\n",
      "0.38749297530042814 0.04185265821240736\n",
      "0.38951078096333136 0.038357068954872106\n",
      "0.38957023791368206 0.03825632630931405\n",
      "0.3945997131631238 0.030203191163239884\n",
      "0.394604781233608 0.030195544050765546\n",
      "0.39481920300447415 0.02987287141282599\n",
      "0.39593203865207816 0.028225353752456054\n",
      "0.39618139667902286 0.02786243049593389\n",
      "0.39695550342687214 0.026750345751605776\n",
      "0.3972501920147584 0.026332791663841153\n",
      "0.3992007441112795 0.02364969227820215\n",
      "0.4027014121484258 0.019186925942733225\n",
      "0.4044039593579847 0.01718077996017709\n",
      "0.40463898627116546 0.016912317141908457\n",
      "0.4049262482471032 0.016586984058791412\n",
      "0.4098130688878361 0.0115255063566604\n",
      "0.41450125942608795 0.0075160457460519085\n",
      "0.4165867567189292 0.006001555602020055\n",
      "0.41668513489715187 0.005934233198777291\n",
      "0.4177923282531515 0.005202240029364501\n",
      "0.4210062724164261 0.003345943825510271\n",
      "0.4213754565737695 0.003158424552452229\n",
      "0.42147769238389454 0.0031074368670720408\n",
      "0.4322909484826596 5.202393752708305e-05\n",
      "0.43255195854764983 3.638504874154799e-05\n",
      "0.44591748551138943 0.003035200960653148\n",
      "0.4459448138565365 0.0030491435393588478\n",
      "0.446052576993194 0.003104440946584843\n",
      "0.44609068843439137 0.0031241188732810572\n",
      "0.44662707077551045 0.003407808729012443\n",
      "0.4469531353286116 0.00358642509169298\n",
      "0.4492783551305992 0.004996054344840423\n",
      "0.44951462215463533 0.005152698816458322\n",
      "0.45570762264438813 0.01015569871550792\n",
      "0.45633431215730647 0.010759574128814713\n",
      "0.45800940001793977 0.012463213029226768\n",
      "0.4601392865855152 0.01481926138117413\n",
      "0.4622952409928305 0.017423239418985118\n",
      "0.46346065201386377 0.018923639330348237\n",
      "0.46636997609267117 0.022957148856918668\n",
      "0.4668840743070679 0.023713054698091526\n",
      "0.47329134687247887 0.03424427077335079\n",
      "0.475632042144303 0.038614404029409966\n",
      "0.47696777756679687 0.04123639773284139\n",
      "0.4785826679017964 0.04453213441530852\n",
      "0.48350663757630685 0.05544577190274635\n",
      "0.48399894427605417 0.05660964557949953\n",
      "0.48660291642478515 0.06298949800756358\n",
      "0.49769516088848054 0.09451568194460153\n",
      "0.499626760914347 0.10075088504902074\n",
      "0.5048116459927137 0.11862456812079376\n",
      "0.5052315998636934 0.12014596375027617\n",
      "0.5070688340520728 0.12693375967201026\n",
      "0.5076819818842018 0.12924716528534316\n",
      "0.5121640415446518 0.14689961024512976\n",
      "0.5179492477913314 0.1716575952765421\n",
      "0.5179713796624648 0.17175666970899472\n",
      "0.5185427037647459 0.17432588249316822\n",
      "0.5193318440448231 0.17791159960666403\n",
      "0.525447912502359 0.2071791836952769\n",
      "0.5255466042752537 0.20767321058824656\n",
      "0.5284707591357543 0.2226298850416373\n",
      "0.5400741501169997 0.28823519269169956\n",
      "0.5406004550918138 0.2914544449252098\n",
      "0.5411758211682147 0.29499852397190346\n",
      "0.5418192021701633 0.29899224557143583\n",
      "0.550922933214155 0.35903838728830867\n",
      "0.5541863844856603 0.3822071347287046\n",
      "0.5576970239970489 0.4081248175512572\n",
      "0.5599863836970047 0.4255893544529032\n",
      "0.5617488605988148 0.4393409804069057\n",
      "0.5669119016914539 0.48118175076291764\n",
      "0.5746124512489643 0.5479950411884008\n",
      "0.5751282651851444 0.5526629119620464\n",
      "0.5754419697213264 0.555513755790236\n",
      "0.5814299018287641 0.6116830869514873\n",
      "0.5832563094140202 0.6294866612987927\n",
      "0.5847249043693756 0.6440330350524778\n",
      "0.5865913206081843 0.662818889592997\n",
      "0.5885495120254367 0.6828912248573697\n",
      "0.5903358879946963 0.701529155774961\n",
      "0.5914801656847426 0.7136328979012967\n",
      "0.5939629902420385 0.7403423146598344\n",
      "0.5977091983898 0.7818148165183193\n",
      "0.6036080216250448 0.8500328997133039\n",
      "0.6052709041626361 0.8699203765252304\n",
      "0.6061367043418304 0.880391093995351\n",
      "0.6088578293811635 0.9138219496798551\n",
      "0.6115123725653493 0.9472076387171641\n",
      "0.6138499108521407 0.9772479754314114\n",
      "0.614484774479515 0.9855117108462247\n",
      "0.6147102992744351 0.9884581270741022\n",
      "0.6150632545245638 0.993080842945697\n",
      "0.615753543977948 1.0021622029570285\n",
      "0.616037509055493 1.0059136216788693\n",
      "0.6172158653046815 1.0215783839834198\n",
      "0.6186451865790381 1.040792035840544\n",
      "0.6188420134148338 1.0434562533253542\n",
      "0.6199333936239921 1.0583102049128852\n",
      "0.6215851819753051 1.0810549180419307\n",
      "0.6227883948143487 1.0978244570327873\n",
      "0.6229937231130755 1.1007032765673574\n",
      "0.6235396773849304 1.1083822289092315\n",
      "0.6252467766176788 1.1326227570034209\n",
      "0.6267653735079481 1.1544821123833728\n",
      "0.6284820913585223 1.179532696161683\n",
      "0.6306105755932243 1.211099170833984\n",
      "0.6322129237936056 1.2352389790951375\n",
      "0.6341713614938076 1.2651894156696908\n",
      "0.6369892279859368 1.3091605422543375\n",
      "0.6401806328644011 1.3602423390816047\n",
      "0.6405547720088975 1.3663218607442698\n",
      "0.641671088705682 1.384576632505292\n",
      "0.6421089941744411 1.3917850576147235\n",
      "0.6430164146145512 1.4068081665376058\n",
      "0.6471798405081837 1.4772532422360352\n",
      "0.6502514196477127 1.5308698706840924\n",
      "0.6517725913547197 1.5579584361631211\n",
      "0.6525780577029654 1.5724487706678842\n",
      "0.6551681538037786 1.6197482666369694\n",
      "0.6566535867911791 1.6473696168747267\n",
      "0.6567099060325798 1.648424092368551\n",
      "0.6625494816351327 1.760727318902034\n",
      "0.6693057950750119 1.8984757336218427\n",
      "0.6731617248062103 1.9811905756042758\n",
      "0.6747590325637334 2.0163899869913586\n",
      "0.6764844290866276 2.0550544340200467\n",
      "0.677101675672491 2.0690523285078237\n",
      "0.6775708536050615 2.079751883883091\n",
      "0.67781563741074 2.085354746337413\n",
      "0.6788354103538541 2.1088499957087024\n",
      "0.6788975182350643 2.1102890160332795\n",
      "0.679492339449133 2.1241183057309874\n",
      "0.6812501941845848 2.165497948655743\n",
      "0.6812699113301453 2.165966481357336\n",
      "0.6827561405499949 2.201570107476479\n",
      "0.6839173331659605 2.2297884174029257\n",
      "0.6846157821784453 2.2469349659573274\n",
      "0.6931725789345919 2.4684074049251534\n",
      "0.6941194822766941 2.4943146103458638\n",
      "0.6945193530610727 2.50534531020756\n",
      "0.6965723991213608 2.562847459921403\n",
      "0.6985250064641308 2.6189334696821454\n",
      "0.699520094699579 2.64806215807297\n",
      "0.6999372089549443 2.660384957156884\n",
      "0.7017575647189447 2.7149659924547773\n",
      "0.7020079616028034 2.722578198203557\n",
      "0.7048258554636224 2.810061604184094\n",
      "0.7048448478112459 2.810662882364524\n",
      "0.7068783388958657 2.8759846083052256\n",
      "0.7107195001842879 3.0047701734630983\n",
      "0.7169148355448491 3.229256372846232\n",
      "0.7175504383999645 3.253593329263058\n",
      "0.7180752594410962 3.273886504792662\n",
      "0.7181930935619683 3.2784677482222584\n",
      "0.7186648136850962 3.2969004767311336\n",
      "0.7201184174911466 3.3546568341696488\n",
      "0.7211433947190207 3.3962798031501977\n",
      "0.7223547367375185 3.4464691093121527\n",
      "0.7225202942076752 3.45341501237651\n",
      "0.7233142520038272 3.487021437030371\n",
      "0.7258193504657193 3.5964080476769076\n",
      "0.7284153875153387 3.7155441736619985\n",
      "0.7388720726292954 4.269104168028156\n",
      "0.740410824829296 4.362852637032777\n",
      "0.7424099259625534 4.490471091713753\n",
      "0.7438044302505171 4.583730238316063\n",
      "0.7472722592746841 4.832727249897717\n",
      "0.7480365153530406 4.891242838119317\n",
      "0.7487174874024796 4.944599707094711\n",
      "0.7503246531246035 5.075373378901995\n",
      "0.7596287669654116 6.008916035480236\n",
      "0.7609050429103341 6.168968550083016\n",
      "0.765797744349155 6.895507904209722\n",
      "0.7664565059454975 7.010848360887214\n",
      "0.7664729777499542 7.013798192582066\n",
      "0.7684850032143238 7.401404926905437\n",
      "0.7696960399154846 7.665235236779177\n",
      "0.7731126897720018 8.589012815587177\n",
      "0.7753640246216396 9.430717741794794\n",
      "0.7801199606109595 13.223508678611601\n",
      "0.7840517317399835 12.436632737239389\n",
      "0.7872774786914243 9.817946258780683\n",
      "0.7874406557234943 9.731253661522757\n",
      "0.7881917564267373 9.361705461184766\n",
      "0.7890048134114267 9.007070172838835\n",
      "0.7891321541186882 8.955118726886582\n",
      "0.7912059635901785 8.21447272333899\n",
      "0.7912923402190792 8.187206962537408\n",
      "0.7938166312098376 7.481361789817578\n",
      "0.7978907030525861 6.599235827645766\n",
      "0.7984903976234534 6.487891538009546\n",
      "0.8003488220215653 6.165522582325536\n",
      "0.8041190943919181 5.596048987948763\n",
      "0.8057267317874206 5.380361624687581\n",
      "0.8065278432922287 5.277900537408131\n",
      "0.8069302425442331 5.227599513772554\n",
      "0.8083272237935697 5.0586235767657515\n",
      "0.8089040188930356 4.991265019777908\n",
      "0.8093916390273164 4.935353718715657\n",
      "0.8118213165412609 4.669772383966615\n",
      "0.8124823583614371 4.600974353343899\n",
      "0.8177400637135885 4.0981290743036505\n",
      "0.820778131794279 3.837942217198007\n",
      "0.8273109858220709 3.3373390145574353\n",
      "0.8286565715174936 3.2427342819568246\n",
      "0.8287827285760343 3.233998098433403\n",
      "0.8292036169482657 3.2050140405574203\n",
      "0.8294966155989707 3.1849824895557313\n",
      "0.8295512412637478 3.1812609633371434\n",
      "0.8297342258071634 3.1688244051768786\n",
      "0.8330938732519944 2.948305384803057\n",
      "0.8332615124799394 2.937674509368364\n",
      "0.8334199987570314 2.9276553366655804\n",
      "0.8341993889565085 2.87882097856669\n",
      "0.8350228685915979 2.8279990558526276\n",
      "0.8351773661835697 2.8185512020426473\n",
      "0.8370719762130694 2.704867835674897\n",
      "0.8445035340355851 2.2946273680191305\n",
      "0.8447745380922018 2.280658301789086\n",
      "0.8463721520820433 2.199627461861482\n",
      "0.8470578837708049 2.1655268406650263\n",
      "0.8478184120557011 2.128174315262553\n",
      "0.8531345667743782 1.8802398538955907\n",
      "0.8562962663399512 1.7431349992918812\n",
      "0.857872884501421 1.677499116927232\n",
      "0.859407863501737 1.6152913653601177\n",
      "0.8616548245077322 1.5271714202276419\n",
      "0.8623406983928195 1.5009561379480867\n",
      "0.8631399371096475 1.4708044753837617\n",
      "0.8655089980931767 1.3838994164656775\n",
      "0.8659733370285807 1.3672924347330997\n",
      "0.8698696799247385 1.233308009796855\n",
      "0.869940978480064 1.2309441457037273\n",
      "0.8700088993819859 1.2286951604296825\n",
      "0.8716109356014046 1.176462641260764\n",
      "0.8764326789809429 1.0284631616459665\n",
      "0.8788312186358247 0.959833231860489\n",
      "0.8797882445544982 0.9333490613067361\n",
      "0.8809579774663556 0.9016643551019676\n",
      "0.8844286349490003 0.8120071696919477\n",
      "0.8905850944972389 0.6683759867732639\n",
      "0.8906135233141583 0.6677568274286206\n",
      "0.8920920648668371 0.6360975852586922\n",
      "0.895005149129537 0.576780548673169\n",
      "0.9007954174634532 0.4704286025007388\n",
      "0.9031443963961101 0.43145805079829286\n",
      "0.9052582307695125 0.39835179538368604\n",
      "0.908112615748927 0.3564941312943947\n",
      "0.9101340755329865 0.32875997800875767\n",
      "0.9140528699872796 0.27929727063880255\n",
      "0.9145284494512844 0.27366731204946726\n",
      "0.9153716480524643 0.26387793462662995\n",
      "0.9157718210131693 0.25931727762874024\n",
      "0.9176913601065833 0.23818992869399216\n",
      "0.9181512423030802 0.23330930901377883\n",
      "0.9188426391777289 0.22610096375911704\n",
      "0.9191913879414675 0.22252336913179369\n",
      "0.9217930699491621 0.19704224059443484\n",
      "0.9248017083024405 0.17013352080249078\n",
      "0.9256836201418961 0.16274441530244071\n",
      "0.928242649286688 0.1425292459208767\n",
      "0.9283746816960632 0.14153452053714144\n",
      "0.9309648800442321 0.12294372584886751\n",
      "0.9313473871845552 0.12034407097774942\n",
      "0.9364862722014398 0.0888503648297655\n",
      "0.9397272919580169 0.0720681365084531\n",
      "0.9400469660151496 0.07053366558369216\n",
      "0.9404620834266739 0.06857245078171695\n",
      "0.9444003100451741 0.05166786828248868\n",
      "0.9449367684542567 0.04959432534799217\n",
      "0.9460920069752459 0.04530732072630222\n",
      "0.9469826028646706 0.042165133856093356\n",
      "0.9507504250754668 0.030366346609037087\n",
      "0.9511020638186527 0.029383508949067935\n",
      "0.9514232763423809 0.028502711151865118\n",
      "0.9529481832313729 0.0245385643176615\n",
      "0.9549623557437243 0.01983487727775869\n",
      "0.9586134048538351 0.012759999673931575\n",
      "0.9591998876923815 0.011788534259067585\n",
      "0.9606636418329622 0.009553229964635054\n",
      "0.9643327554569161 0.005078324530706239\n",
      "0.9650592927428883 0.004373512606348865\n",
      "0.9659331230949235 0.003601382857828271\n",
      "0.967634929905054 0.0023272965663650824\n",
      "0.9727177582185809 0.00020312495929549943\n",
      "0.9740257942558264 3.340271021892646e-05\n",
      "0.9759036901607607 3.8357446216659805e-05\n",
      "0.9790080947805389 0.0006503783065496629\n",
      "0.9802177606242761 0.0010802307491066423\n",
      "0.980385436200913 0.0011479019664821252\n",
      "0.9847002830173763 0.0035282354183178463\n",
      "0.9852004172169939 0.0038796586735686976\n",
      "0.9883633870592745 0.006435947208733105\n",
      "0.9886049385574742 0.006653885315263024\n",
      "0.9957679993835327 0.014423234712202665\n",
      "0.9960617151067828 0.014791308689001302\n",
      "0.9970044096496471 0.015996556654007375\n"
     ]
    }
   ],
   "source": [
    "K = 3\n",
    "T = 4\n",
    "Noise_Alloc = [0,2,4,6]\n",
    "sigma = 1\n",
    "\n",
    "N = 6\n",
    "\n",
    "j_array = np.array(range(K+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K+T)))\n",
    "\n",
    "i_array = np.array(range(N))\n",
    "beta_array = np.cos(i_array*2*math.pi/(N-1)/2) # np.cos(i_array*2*math.pi/N/2)\n",
    "# print(\"z_array: \",z_array,'\\n')\n",
    "\n",
    "N = 1000\n",
    "z_array = np.random.uniform(-1,1,N) #np.cos(i_array[1:]*2*math.pi/(K+T)/2) # np.cos(i_array*2*math.pi/N/2)\n",
    "z_array = np.sort(z_array)\n",
    "MIS_array = np.zeros((N))\n",
    "MIS_LCC_array = np.zeros((N))\n",
    "# print(z_array)\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "B = [0.5, 1, 1.5, 2]\n",
    "\n",
    "z_array_0 = []\n",
    "z_array_1 = []\n",
    "z_array_2 = []\n",
    "z_array_3 = []\n",
    "z_array_4 = []\n",
    "\n",
    "for j in range(len(z_array)):\n",
    "    MIS_array[j] = MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_array[j]], 1,sigma)\n",
    "    MIS_LCC_array[j] = MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_array[j]], 1,sigma, _is_LCC=True)\n",
    "    \n",
    "    if MIS_array[j] < B[0]:\n",
    "        z_array_0.append(z_array[j])\n",
    "    elif MIS_array[j] < B[1]:\n",
    "        z_array_1.append(z_array[j])\n",
    "    elif MIS_array[j] < B[2]:\n",
    "        z_array_2.append(z_array[j])\n",
    "    elif MIS_array[j] < B[3]:\n",
    "        z_array_3.append(z_array[j])\n",
    "    else:\n",
    "        z_array_4.append(z_array[j])\n",
    "#     print('(beta index, MIS) = ',j,',',MIS_array[j])\n",
    "#     print()\n",
    "\n",
    "\n",
    "\n",
    "print(len(z_array_0),len(z_array_1),len(z_array_2),len(z_array_3),len(z_array_4))\n",
    "\n",
    "\n",
    "plt.plot(z_array, MIS_array, label='Mutual Information Security, BACC')\n",
    "plt.plot(z_array, MIS_LCC_array, label='Mutual Information Security, LCC')\n",
    "plt.plot(alpha_array[Signal_Alloc],0*np.ones(len(Signal_Alloc)),'g*',label='alpha_i, for X')\n",
    "plt.plot(alpha_array[Noise_Alloc],0*np.ones(len(Noise_Alloc)),'r*',label='alpha_i, for N')\n",
    "# plt.plot(beta_array,0*np.ones(len(beta_array)),'b.',label='beta_i')\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('MIS')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "    \n",
    "for i in range(len(z_array)):\n",
    "    print(z_array[i],MIS_array[i])\n",
    "    \n",
    "# print(alpha_array[Signal_Alloc])\n",
    "# print(alpha_array[Noise_Alloc])\n",
    "# print(alpha_array)\n",
    "\n",
    "# plt.plot((2*j_array[Signal_Alloc]+1)/(K+T),alpha_array[Signal_Alloc],'g*',label='alpha_i, for X')\n",
    "# plt.plot((2*j_array[Noise_Alloc]+1)/(K+T),alpha_array[Noise_Alloc],'r*',label='alpha_i, for N')\n",
    "# plt.plot(2*i_array[1:]/(K+T), z_array,'b.',label='beta_i')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.48415855 0.49029931 0.49299416 0.49299416 0.49029931 0.48415855]\n"
     ]
    }
   ],
   "source": [
    "z_array_ = np.array([-0.900, -0.568, -0.2749, 0.2749,0.568, 0.900])\n",
    "MIS_array_ = np.zeros(len(z_array_))\n",
    "for j in range(len(z_array_)):\n",
    "    MIS_array_[j] = MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_array_[j]], 1,sigma)\n",
    "\n",
    "print(MIS_array_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. G=2, K=6, N= 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 3 4\n",
      "##########################################\n",
      "###### 0 -th Trial!! ###########\n",
      "@BACC_Enc: N,K,T, m_i= 6 3 4 10000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 6 3 4 10000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 6 3 4 10000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 6 3 4 10000 \n",
      "\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.013702858686447144\n",
      "conv1.bias 0.015688138082623482\n",
      "conv2.weight 0.0004159551113843918\n",
      "conv2.bias 0.00042428955202922225\n",
      "fc1.weight 0.0003288311883807182\n",
      "fc1.bias 0.00020257248543202876\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.013702858686447144\n",
      "conv1.bias 0.015688138082623482\n",
      "conv2.weight 0.0004159551113843918\n",
      "conv2.bias 0.00042428955202922225\n",
      "fc1.weight 0.0003288311883807182\n",
      "fc1.bias 0.00020257248543202876\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0003865440934896469\n",
      "conv1.bias 0.0021470850333571434\n",
      "conv2.weight 0.00032876696437597277\n",
      "conv2.bias 0.0004986294079571962\n",
      "fc1.weight 5.331346183083952e-05\n",
      "fc1.bias 0.00035845807287842034\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0003865440934896469\n",
      "conv1.bias 0.0021470850333571434\n",
      "conv2.weight 0.00032876696437597277\n",
      "conv2.bias 0.0004986294079571962\n",
      "fc1.weight 5.331346183083952e-05\n",
      "fc1.bias 0.00035845807287842034\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.00025525519624352454\n",
      "conv1.bias 0.0008248986559920013\n",
      "conv2.weight 5.2622407674789426e-05\n",
      "conv2.bias 0.0007607538136653602\n",
      "fc1.weight 7.855532458052039e-05\n",
      "fc1.bias 0.0005470792297273874\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.00025525519624352454\n",
      "conv1.bias 0.0008248986559920013\n",
      "conv2.weight 5.2622407674789426e-05\n",
      "conv2.bias 0.0007607538136653602\n",
      "fc1.weight 7.855532458052039e-05\n",
      "fc1.bias 0.0005470792297273874\n",
      "\n",
      "Test set: Average loss: 2.3012 \n",
      "Accuracy: 2018/10000 (20.18%)\n",
      "\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0002726232632994652\n",
      "conv1.bias 0.000868917559273541\n",
      "conv2.weight 0.00010051039047539235\n",
      "conv2.bias 0.0012427102774381638\n",
      "fc1.weight 0.00016045812517404557\n",
      "fc1.bias 0.0004810551181435585\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0002726232632994652\n",
      "conv1.bias 0.000868917559273541\n",
      "conv2.weight 0.00010051039047539235\n",
      "conv2.bias 0.0012427102774381638\n",
      "fc1.weight 0.00016045812517404557\n",
      "fc1.bias 0.0004810551181435585\n",
      "\n",
      "Test set: Average loss: 2.2943 \n",
      "Accuracy: 4413/10000 (44.13%)\n",
      "\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0006540261209011077\n",
      "conv1.bias 8.601666195318103e-05\n",
      "conv2.weight 0.00014989078044891357\n",
      "conv2.bias 0.0010613332269713283\n",
      "fc1.weight 0.00014095740625634788\n",
      "fc1.bias 0.00039920923300087454\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0006540261209011077\n",
      "conv1.bias 8.601666195318103e-05\n",
      "conv2.weight 0.00014989078044891357\n",
      "conv2.bias 0.0010613332269713283\n",
      "fc1.weight 0.00014095740625634788\n",
      "fc1.bias 0.00039920923300087454\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1032/10000 (10.32%)\n",
      "\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.00028922542929649353\n",
      "conv1.bias 0.0010326617630198598\n",
      "conv2.weight 8.949238806962967e-05\n",
      "conv2.bias 0.000728875573258847\n",
      "fc1.weight 0.00019828672520816326\n",
      "fc1.bias 0.0004518591333180666\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.00028922542929649353\n",
      "conv1.bias 0.0010326617630198598\n",
      "conv2.weight 8.949238806962967e-05\n",
      "conv2.bias 0.000728875573258847\n",
      "fc1.weight 0.00019828672520816326\n",
      "fc1.bias 0.0004518591333180666\n",
      "\n",
      "Test set: Average loss: 2.3010 \n",
      "Accuracy: 1397/10000 (13.97%)\n",
      "\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.00017266727983951567\n",
      "conv1.bias 0.001236054697073996\n",
      "conv2.weight 7.600573357194662e-05\n",
      "conv2.bias 0.0011128082405775785\n",
      "fc1.weight 8.802223019301891e-05\n",
      "fc1.bias 0.00034987437538802624\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.00017266727983951567\n",
      "conv1.bias 0.001236054697073996\n",
      "conv2.weight 7.600573357194662e-05\n",
      "conv2.bias 0.0011128082405775785\n",
      "fc1.weight 8.802223019301891e-05\n",
      "fc1.bias 0.00034987437538802624\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1042/10000 (10.42%)\n",
      "\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.00037275679409503937\n",
      "conv1.bias 0.0009967811638489366\n",
      "conv2.weight 0.0001794668845832348\n",
      "conv2.bias 0.0007957317866384983\n",
      "fc1.weight 0.00019906649831682443\n",
      "fc1.bias 0.0003862578189000487\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.00037275679409503937\n",
      "conv1.bias 0.0009967811638489366\n",
      "conv2.weight 0.0001794668845832348\n",
      "conv2.bias 0.0007957317866384983\n",
      "fc1.weight 0.00019906649831682443\n",
      "fc1.bias 0.0003862578189000487\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1302/10000 (13.02%)\n",
      "\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.00013374228961765766\n",
      "conv1.bias 0.0018416307866573334\n",
      "conv2.weight 7.160339504480362e-05\n",
      "conv2.bias 0.0012907239142805338\n",
      "fc1.weight 5.985214957036078e-05\n",
      "fc1.bias 0.00035591882187873124\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.00013374228961765766\n",
      "conv1.bias 0.0018416307866573334\n",
      "conv2.weight 7.160339504480362e-05\n",
      "conv2.bias 0.0012907239142805338\n",
      "fc1.weight 5.985214957036078e-05\n",
      "fc1.bias 0.00035591882187873124\n",
      "\n",
      "Test set: Average loss: 2.3024 \n",
      "Accuracy: 1135/10000 (11.35%)\n",
      "\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0005205702781677246\n",
      "conv1.bias 0.0010813761036843061\n",
      "conv2.weight 7.373612839728594e-05\n",
      "conv2.bias 0.0010381131432950497\n",
      "fc1.weight 0.00020112800411880015\n",
      "fc1.bias 0.0004311752039939165\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0005205702781677246\n",
      "conv1.bias 0.0010813761036843061\n",
      "conv2.weight 7.373612839728594e-05\n",
      "conv2.bias 0.0010381131432950497\n",
      "fc1.weight 0.00020112800411880015\n",
      "fc1.bias 0.0004311752039939165\n",
      "\n",
      "Test set: Average loss: 2.3019 \n",
      "Accuracy: 1135/10000 (11.35%)\n",
      "\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.00045519091188907625\n",
      "conv1.bias 0.0008675274439156055\n",
      "conv2.weight 8.393483236432075e-05\n",
      "conv2.bias 0.0009224920067936182\n",
      "fc1.weight 0.00012069580843672156\n",
      "fc1.bias 0.0005350765772163868\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.00045519091188907625\n",
      "conv1.bias 0.0008675274439156055\n",
      "conv2.weight 8.393483236432075e-05\n",
      "conv2.bias 0.0009224920067936182\n",
      "fc1.weight 0.00012069580843672156\n",
      "fc1.bias 0.0005350765772163868\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.00024127228185534479\n",
      "conv1.bias 0.0009308055159635842\n",
      "conv2.weight 7.759309373795986e-05\n",
      "conv2.bias 0.0011375213507562876\n",
      "fc1.weight 5.2766030421480536e-05\n",
      "fc1.bias 0.0003485226538032293\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.00024127228185534479\n",
      "conv1.bias 0.0009308055159635842\n",
      "conv2.weight 7.759309373795986e-05\n",
      "conv2.bias 0.0011375213507562876\n",
      "fc1.weight 5.2766030421480536e-05\n",
      "fc1.bias 0.0003485226538032293\n",
      "\n",
      "Test set: Average loss: 2.2965 \n",
      "Accuracy: 2124/10000 (21.24%)\n",
      "\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0006051555275917054\n",
      "conv1.bias 0.0005941689596511424\n",
      "conv2.weight 0.00014239749871194362\n",
      "conv2.bias 0.0009845031891018152\n",
      "fc1.weight 0.000169451127294451\n",
      "fc1.bias 0.0006593025755137205\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0006051555275917054\n",
      "conv1.bias 0.0005941689596511424\n",
      "conv2.weight 0.00014239749871194362\n",
      "conv2.bias 0.0009845031891018152\n",
      "fc1.weight 0.000169451127294451\n",
      "fc1.bias 0.0006593025755137205\n",
      "\n",
      "Test set: Average loss: 2.2999 \n",
      "Accuracy: 1360/10000 (13.60%)\n",
      "\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 8.797302842140198e-05\n",
      "conv1.bias 0.0025116829201579094\n",
      "conv2.weight 0.0001156588364392519\n",
      "conv2.bias 0.0009635146707296371\n",
      "fc1.weight 0.00011489179451018572\n",
      "fc1.bias 0.0002983053447678685\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 8.797302842140198e-05\n",
      "conv1.bias 0.0025116829201579094\n",
      "conv2.weight 0.0001156588364392519\n",
      "conv2.bias 0.0009635146707296371\n",
      "fc1.weight 0.00011489179451018572\n",
      "fc1.bias 0.0002983053447678685\n",
      "\n",
      "Test set: Average loss: 2.2986 \n",
      "Accuracy: 1187/10000 (11.87%)\n",
      "\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0003120607882738113\n",
      "conv1.bias 0.0012925465125590563\n",
      "conv2.weight 0.00010282757692039013\n",
      "conv2.bias 0.0009184384834952652\n",
      "fc1.weight 0.0001613771077245474\n",
      "fc1.bias 0.0011475657112896443\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0003120607882738113\n",
      "conv1.bias 0.0012925465125590563\n",
      "conv2.weight 0.00010282757692039013\n",
      "conv2.bias 0.0009184384834952652\n",
      "fc1.weight 0.0001613771077245474\n",
      "fc1.bias 0.0011475657112896443\n",
      "\n",
      "Test set: Average loss: 2.3022 \n",
      "Accuracy: 1135/10000 (11.35%)\n",
      "\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.00038215160369873044\n",
      "conv1.bias 0.0019353986717760563\n",
      "conv2.weight 5.274959839880466e-05\n",
      "conv2.bias 0.0008199538569897413\n",
      "fc1.weight 0.00015148677630349994\n",
      "fc1.bias 0.0004361219238489866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.00038215160369873044\n",
      "conv1.bias 0.0019353986717760563\n",
      "conv2.weight 5.274959839880466e-05\n",
      "conv2.bias 0.0008199538569897413\n",
      "fc1.weight 0.00015148677630349994\n",
      "fc1.bias 0.0004361219238489866\n",
      "\n",
      "Test set: Average loss: 2.2984 \n",
      "Accuracy: 1580/10000 (15.80%)\n",
      "\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0002943524159491062\n",
      "conv1.bias 0.0011667776852846146\n",
      "conv2.weight 0.00012882144190371037\n",
      "conv2.bias 0.0012687313137575984\n",
      "fc1.weight 9.160954505205154e-05\n",
      "fc1.bias 0.0005032386165112257\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0002943524159491062\n",
      "conv1.bias 0.0011667776852846146\n",
      "conv2.weight 0.00012882144190371037\n",
      "conv2.bias 0.0012687313137575984\n",
      "fc1.weight 9.160954505205154e-05\n",
      "fc1.bias 0.0005032386165112257\n",
      "\n",
      "Test set: Average loss: 2.2736 \n",
      "Accuracy: 3605/10000 (36.05%)\n",
      "\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0003749251365661621\n",
      "conv1.bias 0.0012964544584974647\n",
      "conv2.weight 6.586589384824037e-05\n",
      "conv2.bias 0.0008589040953665972\n",
      "fc1.weight 0.00014859918737784029\n",
      "fc1.bias 0.0006243596784770489\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0003749251365661621\n",
      "conv1.bias 0.0012964544584974647\n",
      "conv2.weight 6.586589384824037e-05\n",
      "conv2.bias 0.0008589040953665972\n",
      "fc1.weight 0.00014859918737784029\n",
      "fc1.bias 0.0006243596784770489\n",
      "\n",
      "Test set: Average loss: 2.3021 \n",
      "Accuracy: 1135/10000 (11.35%)\n",
      "\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0003348345682024956\n",
      "conv1.bias 0.0011220978340134025\n",
      "conv2.weight 0.00029277050867676735\n",
      "conv2.bias 0.0008349241688847542\n",
      "fc1.weight 0.0003632277715951204\n",
      "fc1.bias 0.00032709697261452676\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0003348345682024956\n",
      "conv1.bias 0.0011220978340134025\n",
      "conv2.weight 0.00029277050867676735\n",
      "conv2.bias 0.0008349241688847542\n",
      "fc1.weight 0.0003632277715951204\n",
      "fc1.bias 0.00032709697261452676\n",
      "\n",
      "Test set: Average loss: 2.3016 \n",
      "Accuracy: 1135/10000 (11.35%)\n",
      "\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.00012839785777032375\n",
      "conv1.bias 0.0009367535240016878\n",
      "conv2.weight 4.4843130744993686e-05\n",
      "conv2.bias 0.0010838285088539124\n",
      "fc1.weight 4.4807844096794726e-05\n",
      "fc1.bias 0.0006256359629333019\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.00012839785777032375\n",
      "conv1.bias 0.0009367535240016878\n",
      "conv2.weight 4.4843130744993686e-05\n",
      "conv2.bias 0.0010838285088539124\n",
      "fc1.weight 4.4807844096794726e-05\n",
      "fc1.bias 0.0006256359629333019\n",
      "\n",
      "Test set: Average loss: 2.2924 \n",
      "Accuracy: 2534/10000 (25.34%)\n",
      "\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.00033407364040613177\n",
      "conv1.bias 0.000979502801783383\n",
      "conv2.weight 0.00011043226346373559\n",
      "conv2.bias 0.0008218548027798533\n",
      "fc1.weight 0.00036592725664377215\n",
      "fc1.bias 0.00036244357470422984\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.00033407364040613177\n",
      "conv1.bias 0.000979502801783383\n",
      "conv2.weight 0.00011043226346373559\n",
      "conv2.bias 0.0008218548027798533\n",
      "fc1.weight 0.00036592725664377215\n",
      "fc1.bias 0.00036244357470422984\n",
      "\n",
      "Test set: Average loss: 2.3023 \n",
      "Accuracy: 1029/10000 (10.29%)\n",
      "\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.00018077436834573746\n",
      "conv1.bias 0.0010287892073392868\n",
      "conv2.weight 2.7351733297109605e-05\n",
      "conv2.bias 0.000754068954847753\n",
      "fc1.weight 0.00016756086843088268\n",
      "fc1.bias 0.0004623968619853258\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.00018077436834573746\n",
      "conv1.bias 0.0010287892073392868\n",
      "conv2.weight 2.7351733297109605e-05\n",
      "conv2.bias 0.000754068954847753\n",
      "fc1.weight 0.00016756086843088268\n",
      "fc1.bias 0.0004623968619853258\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0003257349878549576\n",
      "conv1.bias 0.0010157674551010132\n",
      "conv2.weight 4.227674566209316e-05\n",
      "conv2.bias 0.0006290682940743864\n",
      "fc1.weight 0.00016241794219240546\n",
      "fc1.bias 0.0004276045598089695\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0003257349878549576\n",
      "conv1.bias 0.0010157674551010132\n",
      "conv2.weight 4.227674566209316e-05\n",
      "conv2.bias 0.0006290682940743864\n",
      "fc1.weight 0.00016241794219240546\n",
      "fc1.bias 0.0004276045598089695\n",
      "\n",
      "Test set: Average loss: 2.3019 \n",
      "Accuracy: 1910/10000 (19.10%)\n",
      "\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.00023826394230127334\n",
      "conv1.bias 0.0011727632954716682\n",
      "conv2.weight 4.8995711840689185e-05\n",
      "conv2.bias 0.0011106773745268583\n",
      "fc1.weight 9.42160259000957e-05\n",
      "fc1.bias 0.00047582508996129035\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.00023826394230127334\n",
      "conv1.bias 0.0011727632954716682\n",
      "conv2.weight 4.8995711840689185e-05\n",
      "conv2.bias 0.0011106773745268583\n",
      "fc1.weight 9.42160259000957e-05\n",
      "fc1.bias 0.00047582508996129035\n",
      "\n",
      "Test set: Average loss: 2.3013 \n",
      "Accuracy: 2022/10000 (20.22%)\n",
      "\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0005091584846377373\n",
      "conv1.bias 0.0009186775423586369\n",
      "conv2.weight 7.255923468619585e-05\n",
      "conv2.bias 0.0008297307067550719\n",
      "fc1.weight 0.00021919505670666694\n",
      "fc1.bias 0.0004004592541605234\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0005091584846377373\n",
      "conv1.bias 0.0009186775423586369\n",
      "conv2.weight 7.255923468619585e-05\n",
      "conv2.bias 0.0008297307067550719\n",
      "fc1.weight 0.00021919505670666694\n",
      "fc1.bias 0.0004004592541605234\n",
      "\n",
      "Test set: Average loss: 2.3011 \n",
      "Accuracy: 2098/10000 (20.98%)\n",
      "\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0004097219556570053\n",
      "conv1.bias 0.0008238328155130148\n",
      "conv2.weight 5.319021642208099e-05\n",
      "conv2.bias 0.0012333032209426165\n",
      "fc1.weight 8.923448622226715e-05\n",
      "fc1.bias 0.00032900245860219003\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0004097219556570053\n",
      "conv1.bias 0.0008238328155130148\n",
      "conv2.weight 5.319021642208099e-05\n",
      "conv2.bias 0.0012333032209426165\n",
      "fc1.weight 8.923448622226715e-05\n",
      "fc1.bias 0.00032900245860219003\n",
      "\n",
      "Test set: Average loss: 2.3019 \n",
      "Accuracy: 1695/10000 (16.95%)\n",
      "\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0002585984766483307\n",
      "conv1.bias 0.0014832934830337763\n",
      "conv2.weight 5.6059854105114935e-05\n",
      "conv2.bias 0.0009761025430634618\n",
      "fc1.weight 0.00022115509491413832\n",
      "fc1.bias 0.0005739185027778149\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0002585984766483307\n",
      "conv1.bias 0.0014832934830337763\n",
      "conv2.weight 5.6059854105114935e-05\n",
      "conv2.bias 0.0009761025430634618\n",
      "fc1.weight 0.00022115509491413832\n",
      "fc1.bias 0.0005739185027778149\n",
      "\n",
      "Test set: Average loss: 2.3003 \n",
      "Accuracy: 1567/10000 (15.67%)\n",
      "\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0001887281984090805\n",
      "conv1.bias 0.0015229020500555634\n",
      "conv2.weight 7.41090951487422e-05\n",
      "conv2.bias 0.0010008610552176833\n",
      "fc1.weight 0.00013794892001897096\n",
      "fc1.bias 0.0003199392929673195\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0001887281984090805\n",
      "conv1.bias 0.0015229020500555634\n",
      "conv2.weight 7.41090951487422e-05\n",
      "conv2.bias 0.0010008610552176833\n",
      "fc1.weight 0.00013794892001897096\n",
      "fc1.bias 0.0003199392929673195\n",
      "\n",
      "Test set: Average loss: 2.2832 \n",
      "Accuracy: 2304/10000 (23.04%)\n",
      "\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.00036465249955654146\n",
      "conv1.bias 0.0007406057557091117\n",
      "conv2.weight 0.00011007759720087051\n",
      "conv2.bias 0.0008930591866374016\n",
      "fc1.weight 0.00022229573223739863\n",
      "fc1.bias 0.0007007383275777102\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.00036465249955654146\n",
      "conv1.bias 0.0007406057557091117\n",
      "conv2.weight 0.00011007759720087051\n",
      "conv2.bias 0.0008930591866374016\n",
      "fc1.weight 0.00022229573223739863\n",
      "fc1.bias 0.0007007383275777102\n",
      "\n",
      "Test set: Average loss: 2.2852 \n",
      "Accuracy: 3514/10000 (35.14%)\n",
      "\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0001673714630305767\n",
      "conv1.bias 0.0010678668040782213\n",
      "conv2.weight 3.117684042081237e-05\n",
      "conv2.bias 0.0007929524872452021\n",
      "fc1.weight 9.003719314932823e-05\n",
      "fc1.bias 0.0003391007659956813\n",
      "selected users: [0 1 2 3 4 5]\n",
      "conv1.weight 0.0001673714630305767\n",
      "conv1.bias 0.0010678668040782213\n",
      "conv2.weight 3.117684042081237e-05\n",
      "conv2.bias 0.0007929524872452021\n",
      "fc1.weight 9.003719314932823e-05\n",
      "fc1.bias 0.0003391007659956813\n",
      "\n",
      "Test set: Average loss: 2.2971 \n",
      "Accuracy: 2671/10000 (26.71%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "N = 12\n",
    "K = 6\n",
    "G = 2\n",
    "\n",
    "size_per_group = int(60000/G)\n",
    "\n",
    "X_group = np.reshape(encoding_input_array_np, (G,size_per_group,28*28))\n",
    "y_group = np.reshape(encoding_label_array_np, (G,size_per_group,args.num_classes)) \n",
    "\n",
    "\n",
    "N_i = int(N/G) # = 6\n",
    "K_i = int(K/G) # = 4\n",
    "T = 4\n",
    "sigma = 1\n",
    "Noise_Alloc = [0,2,4,6]\n",
    "m = N_i # number of selected workers (if there is no straggler, m=N_i)\n",
    "\n",
    "print(N_i,K_i,T)\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K_i+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K_i+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K_i+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "z_array = np.array([-0.900, -0.568, -0.2749, 0.2749,0.568, 0.900])\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 30\n",
    "\n",
    "loss_test_arr_K6_G2_N12 = np.zeros((N_trials,N_epochs))\n",
    "acc_test_arr_K6_G2_N12  = np.zeros((N_trials,N_epochs))\n",
    "\n",
    "for trial_idx in range(N_trials):\n",
    "    \n",
    "    print('##########################################')\n",
    "    print('######',trial_idx,'-th Trial!! ###########')\n",
    "    \n",
    "    net_glob = CNNMnist2(args=args)\n",
    "    net_glob.cuda()\n",
    "    net_glob.train()\n",
    "    \n",
    "    # copy weights\n",
    "    w_glob = net_glob.state_dict()\n",
    "    \n",
    "    X_tilde = np.empty((N,Size_submatrices,28*28))\n",
    "    y_tilde = np.empty((N,Size_submatrices,10))\n",
    "    \n",
    "    for G_idx in range(G):\n",
    "        \n",
    "        _Noise_label = np.ones((size_per_group*T,10)) * 0.1\n",
    "        \n",
    "        X_tilde_tmp,a,b = BACC_Enc_Data_v3(X_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde_tmp,a,b = BACC_Enc_Data_v3(y_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "        \n",
    "        stt_pos = G_idx * N_i\n",
    "        end_pos = (G_idx+1) * N_i\n",
    "        \n",
    "        X_tilde[stt_pos:end_pos,:,:] = X_tilde_tmp\n",
    "        y_tilde[stt_pos:end_pos,:,:] = y_tilde_tmp\n",
    "        \n",
    "   \n",
    "\n",
    "    for iter in range(N_epochs): #args.epochs\n",
    "        \n",
    "        w_group_array = []\n",
    "        for G_idx in range(G):\n",
    "            w_locals, loss_locals = [], []\n",
    "            idxs_users = np.random.choice(range(N_i), m, replace=False)\n",
    "            idxs_users = np.sort(idxs_users)\n",
    "            print('selected users:',idxs_users)\n",
    "\n",
    "            coded_net = BACC_Enc_Model_withNoise_v4(net_glob.cuda(), N_i, K_i, T, 1, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "            dec_z_array = []\n",
    "            for idx in idxs_users: #for idx in range(N):\n",
    "        #         print(idx)\n",
    "                local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[G_idx*N_i+idx,:,:], label=y_tilde[G_idx*N_i+idx,:,:])\n",
    "                w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "#                 w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                w_locals.append(copy.deepcopy(w))\n",
    "                loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "            # update global weights\n",
    "            #w_glob = FedAvg(w_locals)\n",
    "            w_group = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "            \n",
    "            w_group_array.append(copy.deepcopy(w_group))\n",
    "        \n",
    "        w_glob = copy.deepcopy(w_group_array[0])\n",
    "        for k in w_glob.keys():\n",
    "            for G_idx in range(1,G):\n",
    "                w_glob[k] += w_group_array[G_idx][k]\n",
    "            w_glob[k] = torch.div(w_glob[k], len(w_group_array))\n",
    "        \n",
    "        # copy weight to net_glob\n",
    "        net_glob.load_state_dict(w_glob)\n",
    "\n",
    "        # print loss\n",
    "    #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "    #     loss_train_arr.append(loss_train)\n",
    "\n",
    "        acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "        acc_test_arr_K6_G2_N12[trial_idx][iter] = acc_test\n",
    "        loss_test_arr_K6_G2_N12[trial_idx][iter] = loss_test\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. G=3, K=6, N=12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2 3\n",
      "##########################################\n",
      "###### 0 -th Trial!! ###########\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 10000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 10000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 10000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 10000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 10000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 2 3 10000 \n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "N = 12\n",
    "K = 6\n",
    "G = 3\n",
    "\n",
    "size_per_group = int(60000/G)\n",
    "\n",
    "X_group = np.reshape(encoding_input_array_np, (G,size_per_group,28*28))\n",
    "y_group = np.reshape(encoding_label_array_np, (G,size_per_group,args.num_classes)) \n",
    "\n",
    "\n",
    "N_i = int(N/G) # = 6\n",
    "K_i = int(K/G) # = 4\n",
    "T = 3\n",
    "sigma = 1\n",
    "Noise_Alloc = [0,2,4]\n",
    "m = N_i # number of selected workers (if there is no straggler, m=N_i)\n",
    "\n",
    "print(N_i,K_i,T)\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K_i+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K_i+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K_i+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "z_array = np.array([-0.81, -0.22, 0.22, 0.81])\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 30\n",
    "\n",
    "loss_test_arr_K6_G3_N12 = np.zeros((N_trials,N_epochs))\n",
    "acc_test_arr_K6_G3_N12  = np.zeros((N_trials,N_epochs))\n",
    "\n",
    "for trial_idx in range(N_trials):\n",
    "    \n",
    "    print('##########################################')\n",
    "    print('######',trial_idx,'-th Trial!! ###########')\n",
    "    \n",
    "    net_glob = CNNMnist2(args=args)\n",
    "    net_glob.cuda()\n",
    "    net_glob.train()\n",
    "    \n",
    "    # copy weights\n",
    "    w_glob = net_glob.state_dict()\n",
    "    \n",
    "    X_tilde = np.empty((N,Size_submatrices,28*28))\n",
    "    y_tilde = np.empty((N,Size_submatrices,10))\n",
    "    \n",
    "    for G_idx in range(G):\n",
    "        \n",
    "        _Noise_label = np.ones((size_per_group*T,10)) * 0.1\n",
    "        \n",
    "        X_tilde_tmp,a,b = BACC_Enc_Data_v3(X_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde_tmp,a,b = BACC_Enc_Data_v3(y_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "        \n",
    "        stt_pos = G_idx * N_i\n",
    "        end_pos = (G_idx+1) * N_i\n",
    "        \n",
    "        X_tilde[stt_pos:end_pos,:,:] = X_tilde_tmp\n",
    "        y_tilde[stt_pos:end_pos,:,:] = y_tilde_tmp\n",
    "        \n",
    "   \n",
    "\n",
    "    for iter in range(N_epochs): #args.epochs\n",
    "        \n",
    "        w_group_array = []\n",
    "        for G_idx in range(G):\n",
    "            w_locals, loss_locals = [], []\n",
    "            idxs_users = np.random.choice(range(N_i), m, replace=False)\n",
    "            idxs_users = np.sort(idxs_users)\n",
    "            print('selected users:',idxs_users)\n",
    "\n",
    "            coded_net = BACC_Enc_Model_withNoise_v3(net_glob.cuda(), N_i, K_i, T, 1, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "            dec_z_array = []\n",
    "            for idx in idxs_users: #for idx in range(N):\n",
    "        #         print(idx)\n",
    "                local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[G_idx*N_i+idx,:,:], label=y_tilde[G_idx*N_i+idx,:,:])\n",
    "                w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "#                 w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                w_locals.append(copy.deepcopy(w))\n",
    "                loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "            # update global weights\n",
    "            #w_glob = FedAvg(w_locals)\n",
    "            w_group = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "            \n",
    "            w_group_array.append(copy.deepcopy(w_group))\n",
    "        \n",
    "        w_glob = copy.deepcopy(w_group_array[0])\n",
    "        for k in w_glob.keys():\n",
    "            for G_idx in range(1,G):\n",
    "                w_glob[k] += w_group_array[G_idx][k]\n",
    "            w_glob[k] = torch.div(w_glob[k], len(w_group_array))\n",
    "        \n",
    "        # copy weight to net_glob\n",
    "        net_glob.load_state_dict(w_glob)\n",
    "\n",
    "        # print loss\n",
    "    #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "    #     loss_train_arr.append(loss_train)\n",
    "\n",
    "        acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "        acc_test_arr_K6_G3_N12[trial_idx][iter] = acc_test\n",
    "        loss_test_arr_K6_G3_N12[trial_idx][iter] = loss_test\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEGCAYAAABsLkJ6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOydd3xUxfr/37Ob3kMSQiAVQg8htNAhCAKKilgAQUBEUBS5V71ysf2ufpWrKOrFCiJNQcEKIkUQCALSIXQIqSQhlIT0umV+f2yyJJCyKZss4bxfr33t7jln5jyzZZ6ZZ2Y+I6SUKCgoKCjceaga2wAFBQUFhcZBcQAKCgoKdyiKA1BQUFC4Q1EcgIKCgsIdiuIAFBQUFO5QrBrbAFPw9PSUgYGBtUqbl5eHo6Nj/RrUyDS1MinlsXyaWpmaWnmg4jIdOXIkTUrpVVma28IBBAYGcvjw4VqljYyMJCIion4NamSaWpmU8lg+Ta1MTa08UHGZhBCJVaVRQkAKCgoKdyiKA1BQUFC4Q1EcgIKCgsIdiuIAFBQUFO5QFAegoKCgcIeiOAAFBQWFOxTFASgoKCjcodwW6wAUzEtWURYuNi4IIeqcl5SS8xnnOZt+llCvUFq7tq6XfBUUbjeklBTqCsktziVXk0tucS45mhzyNHkU6YrQ6XXopA6tXotO6ip8r5VaHu/4OO527maxUXEAdzCpuaksOLyArYlb8bDzoFeLXoT7hBPeIhx/Z3+TK+6soiz2pe5jb8pe9qbs5VrBNeM5P2c/hvgNIcIvgm7Nu2GlUn5yCk0LrV7L73G/83vs72QVZ5FTnEOuJpe84jy0UlunvFVCxaigUYoDaMrsu7SPpJwkRrUehaO1+ZenF+mKWH5qOUtPLgVgcqfJpBemcyj1EFsStgDg7eBNeItwo0No6dTSmF4v9Zy7fo49KXvYk7KHE9dOoJM6nG2c6deyHwNaDSDEI4SjV4+yM2kn35/7nm/OfIOrrSuDWg0iwi+C/q36V1lWKSXphekk5ySTkptCck4ymUWZjAwaSVevrub9gBTMgpSSPE0e1wqukVaQRlaRobLM0+SRo8kp11Iu+5yvzUdKiRACFSpjw0QgEEKUe1YLNdZqa6xV1liprLBWlX9d+px1PQvnK850b9691j1UrV7LpvhNLD6+mIs5F2nt2hp/Z3+C3YJxtHbE2cYZJ2snw8PGCWcbZxytHXGydsLOyg61UGOlskIt1KhV6grfq4R5o/SKA2hksoqyeGnXS+QU5/DxkY95MPhBJnScgJ+zX73fS0rJjqQdfHDoA1JyUxgeMJyXer5krNyllCRkJ3Do8iEOpB5gT8oeNsRtAKCVUyt6+/RGq9eyN2Uv6YXpAHTy6MSTIU8y0HcgXTy7lGvhB7sHM7b9WPI1+fx96W92Ju1kV/IuNsRtwFplTbhPOHf53cXV/KuknDVU8sm5ycZKv0BbUM5+G5UNq86uonvz7kzrMo0BrQaY/Q+i0Ws4k36GqKtRNLNrxsigkVirrOucr5SS49eOk1mUSVv3trR0bFlvIbiGCrnppZ58Tb6htavJI1eTS05xDmkFaaQVpHEt/5qxsi993PydlsXeyh5na2ccbRxxtnbGycYJb0dvHKwcEEIgpUQibzyXeY0EPXr0Uo9Gp0EjNWh1WjR6DYXaQjR6DRq9Bq3ecOxa3jX+2vIXQa5BPNz2YR5o84DJrWydXsfmhM0sPr6YhOwEOjTrwMIhCxniN+S2C3eK22FLyJ49e8qmqgW08OhClp5cyrwB89idspttCdvQSR0RfhE83vFxerXodcuPqjZlisuKY/7B+fx96W+C3YKZGz6X3j69q0yjl3piMmOMDuHwlcOohIp+Pv0Y4DuAfi374WnvWSM7tHotUVejiEyKZGfSTi7mXDSes7eyx9fZF18nX1o5tcLX2Rc/Zz98nXxp6dQSvdTzy4VfWHlmJZfzLhPsFszUkKncE3gP1uq6V8oA+Zp8jl87ztGrRzl65Sgnrp2gUFdoPO/r5Mv00Onc3/r+Su9Z1fcjpWRPyh6WnFzCsavHjMedrJ1o696Wtm5taefejnbN2hHsFoyzjXOFeaQVpJGYnUhSThKJ2YlczLnIxeyLJOUk0d27OwsGL6iX3qSUkmWnlrHlzBZsnG3IK84zVvh5mjxD5VsJztbOeDp44mlveHjZe+Fl74Wng+G1m60bTjaGFrKjtWODhge37thKgX8BP0X/RNS1KKxV1gzzH8bD7R6mV4teFTYsdHodfyT8waITi4jPiqedezue7fosQ/yHmL0hYgqVaAEdkVL2rCyN4gAakav5Vxn1yyiGBgzlvYHvAXAl7wprz6/lp+ifyCjKoJ17OyZ2nMi9QfdiZ2UH1KxMucW5LDq+iNVnV2NvZc9z3Z5jbPuxtWrF6qUeoN5+7FJK4rPj2bV/Fw8MeoBmds1MakFp9Bq2xG9h+enlXMi4gLeDN5M7Tebhdg/XuNK7XnidY1ePcfSKocI/e/0sOqlDJVS0d29PD+8edPfuTrfm3Tiddpovj3/J6fTTtHRsybQu0xgTPOYWR1DR96PT69h+cTtfn/yas9fP0sKxBVM7T6WTRyeiM6KJzojmQsYFLmRcIEeTY0zX0rEl7dzb4evsy5X8K1zMvsjFnIvlWtJWwgpfZ1/8XfzxtPdkfcx6Ont25sthX+Ji41Kjz6MsWr2Wt/a9xbqYdbSyboW/pz9ONk7GMEbZyrv02dnGGQ97DzztPbG3sq/1vc1N2e/oQsYFfr7wMxtiN5BdnI2/sz8PtX2I0cGj8bT3RC/1bE3cyqKoRcRmxRLsFszMrjMZFjDMIir+UhQHUAGW7ADe3vc2v8T8wm8P/nZLyKdQW8jm+M18e/ZbLmRcwN3WnUfaPcK49uM4c/AMQ4YMqTJvvdSzIXYDHx/5mOuF1xnTdgyzu83Gw97DnEWqFbX9jkpb08tOLePwlcM42zgzvv14JnScgKe9J1JKsouzuZR7yfDIMzyn5KaQmpdKSm4KOcWGytZGZUMXry50b96dHt496OrVFScbp0rvuej4Ik6knaCFYwumhUzjobYPYaO2uaU8Gr2GTXGb+Prk1yRkJxDoEsiTIU9yX+v7KuxBSCm5nHfZ4BAyLxB93eAcUnJT8Hb0xt/ZnwCXAPyc/QhwCcDfxR8fR59yreftidv511//ItgtmMV3L6aZXbMaf7ZFuiLm7JrDjqQdzOw6k44ZHav9zd1OVPSbK9QWsi1xGz9f+JkjV45gJayI8IsgITuBmMwY2ri24ZmwZxgeMNyiKv5SFAdQAZbqABKzExm9bjRj24/l1d6vVnqdlJJDlw+x6uwqIpMib+lylxsIK/MaoFhfTKhnKK/0foUQzxCzlqcu1Md3dOLaCVacXsGfiX9irbLG38Wf1LxU8jR55a6zt7KnlVMrWjq1xMfRBz9nP0K9Quns0dlYgZuClJJ9l/bx5fEviboWRXOH5kwLmcbD7R5m3+599BnQh19jfmX5qeWk5qXS3r09T4U+xd3+d6NWqetUVlPYk7KHf+78J75OviwZvgQvh0ol4W8hT5PH7B2zOXj5IHPD5zKx40SL/R/VlurKE5cVx8/Rhl6Bm50bz4Q+w4jAEQ3y3dUWxQFUgKX+cF/e9TK7knex6aFNJsfRk3KS2JqwlfOx5wkIDLh1UKzM4BgS2jdrzz1B91hka6Us9fkdJWQlsOrsKq7kXaGlU0taOrWklVMrfJx8aOXYCldb13odqJNScuDyAb6M+pKjV4/iZe9FJ6tOnNKcIr0wna5eXZkROoOBrQY2+ADhocuHmLV9Fh72Hnw9/OtyM7kq43rhdZ7981nOXT/H2/3f5v429wOW+z+qLaaWpyEH1etKbRyAMguoETiTfoYtCVuYETqjRoOofs5+TOsyjcj0SCLCIsxn4G1MoGsgr/d5vcHuJ4Sgj08ferfozaHLh1h0YhG7Lu+ir09fpodOp6d3z0arQHq16MVXw79i5p8zmbJlCkuHL8Xfxb/S6y/nXWbGthlcyr3EwiELGew3uAGttUxul8q/tlh207CJ8snRT3CzdeOJzk80tikK9YQQgnCfcJaNWMb7fu/z1fCvKpzB1dB09erK0uFLKdIWMWXLFGIyYiq8Lj4rnkmbJ3Et/xqLhi1SKv87BMUBNDAHUw+y99JenuryVIVT/BRuf+xVljX7paNHR5aPXI5AMPWPqZxNP1vu/On000zZPIViXTHLRiyjZ4tKIwYKTQzFATQgUkr+d/R/eDt4M77D+MY2R+EOoo1bG1aMXIG9lT3T/phG1NUowDBOMO2Padhb2fPNPd/Q0aNjI1uq0JAoDqAB2XFxByfTTvJc2HPYqm0b2xyFOwx/F39WjlyJu507M7bNYNHxRTyz7RlaOLTgm3u+IcAloLFNVGhgFAfQQGj1Wj459glBrkHGmRUKCg2Nj5MPK0auoKVjSz6P+pz2zdqzYuQKvB29G9s0hUZAmQXUQGyI3UBcVhz/i/ifooip0Kh4OXixfORyNsVvYkzwGBysHRrbJIVGQqmJGoAiXRGfR31OF88u3OV/V2Obo6CAu507EztOrPd8NTo91urGCSwUanRYq1WoVU176mZ9ojiABmDNuTVcyb/Cfwf8t9GnBSoo1Bf5xVpOpWQTlZTB8aQsopIyScksoG1zJ3oGutMjoBk9A9wJ8HCo1e++SKsjPi2PhLR8MvKLyczXkFlQTFa+xvg6M19DVoGGjPxiCjV6bNQq/D0cCPRwJMjTgUBPR4JKHt7OdqhMdA5anZ6cQi05hVoKNDqKtXqKdfpyz5rS91o9RTo9Wp0elRCoVAK1EKgExtdqlUAIUJe81+olBRodRRodBRodBcV6CrU6Cop1FJU8F2r0FGh0zH84lBaudjX+/ExBcQBmJqc4hyUnl9CvZT/CfcIb2xyFmyjU6LCzttzl/ZaCTi85m5pNVFImx5MyiUrKJPpKDvoSIQFfd3vC/N0YHdaSM6nZ/H4ile8PJgHg6WRLzwD3EqfgTueWrthY3egl5BRqiL2Wx4UrOcRcyyX2ai4xV3O5eD3fmH8pNmoVbg7Whoe9DX7NHOhib3jvam9NTpGWhLQ84tPy+OvCNYq1emNaO2sVgR6OBHo4os8t4o/rJ8kp1JBTqCW75Ln0fX6xzuyf6c1YqwV21mrsrdU3nm3UaHT66hPXEsUBmJkVp1eQVZTFP7r/o7FNua0p1urZfCqVEZ1b1FuF/dvxS/xzzTFGh7Vi7j0d8HYxTyurIqSUvP37WeysVbw8on299QxTswpQC0HzeirLpcwC5m06y7bT+RRv3Q2Aq701Xf3cGN7JmzB/N0J93fB0Kj+rTa+XXLiay6GE6xxJzOBw4nW2nL4MGCriUF83bNQqYq7mcjn7hty2tVrQ2tOJzi1deSCsFcHNnWjt6YiHkw1u9jbYWatM/qz0eklqdiHx1/KIT88jIc3wiL6aQ8p1Lc7pl3G2s8bZzgoXO2tauNjhbGeFs501LiXHne2scLCxwsZKhbVaYGOlwtZKhY1aXe6YjZUKa5UKvZTopESvp+RZGo4Znw3OtLSyt7NWY2+jxs5KhVUjhM4UB2BG0grS+PbMt4wMHEknj06NbU6DodXpORh/nT6tPUzuclfH6gOJvLXhDGN7+vL+I3XfESwxPY9XfzmJXzMHNp5M5Y/Tl3luSDDTBgQ1SI/g850xLNsbDxgqilfuqfv8+1MpWUxYsp8irZ7pA1szM6INjra1+4trdXpW/J3AR9uikRIGtrLigX4hdPV1Mymko1IJ2rdwpn0LZx7vY5heeiW7kMMJBmdwNDGDQo2Ofm08CPZ2ItjLieDmTvg3c6i3ilClErRys6eVmz0D2paXXGlq2ka1RXEAZuSrE19RrCtmVrdZjW1Kg/LRtmi+iIzlvYe6MD68cu0ZU9HrJSv/TsDOWsUPh5PpGdCMsb1qv2NasVbP898fQ60SfDe9Dzqd5J2NZ/jgj/OsPZTEa6M6MryTt9nGa7aducKCrdGMDmuJs50Vi3fF4WZvw8yINrXO88ylbB5fegBnO2sGB7jz2c4YfjicxL9GtOeR7r41csRRSZm8+stJzqRmM7RDc94a3ZmY4weJCGtVa/sAvF3sGBXqw6hQnzrlo1B/KOsAzERqbio/Rv/IQ20fuqMW2BxKuM6iXbEIAV/viUd/cxC3FkRGXyUhPZ/5D4fSP9iDN9af4syl7Frn98Ef5ziRnMX8h0Np5WaPv4cDX03uyappvbGzVvH0t0eYvOwgF67kVJ9ZDYm+ksM/1xwj1NeV+Q+H8n8PhPBA15bM33KO1QcSa5Xn+cs5PL70AHZWar6b3ptPH+vGL8/2o6WbPXN+OsEDn+/hQFx6tflkF2p4Y90pxnyxl+t5xSx6vDtfT+mJr7syTbSpojgAM/FrzK/o9Dqe6vJUY5vSYOQUanhhbRS+7g6882AIMVdziYy+Wud8l+9NoIWLHfd28WHh+G64OVjz7OojZBdqapzXzvNXWbI7nsf7+DMypEW5cwPaerJp9kDevL8Tx5MyGblwN2/+dpqs/JrfpyIy84uZ/s1h7G2sWDypB3bWalQqwYdju3JXh+a8vu4UG45fqlGeMVdzmfj1fqxUgu9n9CHAw7AjWnd/d359th8Lx4dxPbeYcV/tZ+aqI1xMz78lDyklG45fYuiHu1h9IJEn+gXy50uDGRnio8xaa+IoDsAM6KWe32J/o7dPb5M02JsKb/52hkuZBXw8LoyxPf3wcbXjq7/i6pRnzNUcdl9IY1LfAKzVKjydbPlsQneSMgr4908nqMl+FlezC/nXD8fp0MKZ10dVPCZjpVbxRP8gIl8ewvhefqzcl8CQDyNZfSARXR16M1qdnlnfHSM1s5DFk3rg43pDMM5areLzCd3pFdCMF9ZGsfO8aU4zPi2PCUv2A4ZQVpBn+e0whRCMDmvF9pciePHudkSev8awj3bx7uaz5JQ4z4vp+Tyx/BDPf3+MFi52rH9uAP+5vzNOtRw7ULi9UByAGThy5QgpuSk8GPxgY5vSYGw6mcrPR5OZNSSYHgHuWKtVPNk/iP1x1zmZnFXrfFf8nYCNlYrxZWL+vQKbMXdkBzafusyyvQkm5aPXS174IYq8Yi2fTehW7UBvM0cb5o3pwu/PDyDYy4nXfjWERqJrGRb676Zz7IlJ450xIfQIcL/lvL2Nmq+f6En7Fs7MXHWEQwnXq8wvMT2Px77aj1Yv+W56b4Kb37p9Zdm8Zw9ty85/RXB/15Ys3hXHkAWRvL7uJHd/vIsjiRn85/5OrHuuP118XWtVPoXbE8UBmIF1Metwsna6Y1b9Xsku5NVfT9LV15Xnh7Y1Hh8X7oeTrRVLdteuF5CVr+HnIyk8GNYSj5umGT41MIjhnbx5d9NZjiRWXVkCfLkrlr0x6bx5f2eCm5suw925pStrn+7DwvFhJGcUcN8ne/giMgZtDeZm/3g4iWV745naP5CxPSsfvHaxs2blk+G0dLXnyRWHOH2pYseZdD2fCUsOUKjVsWpab9p5m1aeFq52fDi2K7/N6k+QpyOr9l/krg7N+fPFwUztH6SsoL0DURxAPZOnyWNb4jZGBI7A3sqydOHNgV4v+dePxynS6Pl4XFg5GQAXO2seC/dj48lUUjILapz3D4eTKNDoeKJf0C3nhBB88GhXWrrZ89zqY6TnFlWaz5HEDD7aFs2oUB/G1WL2UGkoZesLgxjasTnvbznPw4v2EXO1+t7AkcQMXvv1FP2DPXjt3uqneno62fLtU71xtrVi8tKDxF3LLXf+UmYBjy3ZT06hhlXTetOppUuNyxPq68YPT/fl6Bt38+XjPcy2ylTB8lEcQD2zNWErBdqCOyb8s3JfArsvpPHaqI609ro1DDG1fxACWL4nvkb56vSSlfsS6B3UrNJKztXemi8mdud6fjH/XBtVYYw+q0DD7O+P4eNqx7sPdanToKanky1fTOzOJ491IzE9j3s/2cPiXbGVjg1czirkmVVHaOFqx2ePdTd5fnsrN3u+fao3AJOWHuRSifO8nFXIY0v2k5Wv4dtpvQlpVftwjRCCZo42tU6v0DRQHEA9sz52PYEugXT1qvtiJUsn+koO720+x10dmjOxd8Xz/Vu62TMq1IfvD14kq8D02TR/nr1CckYBU/sHVnldSCtX/u+Bzuy+kManOy6UOyel5NVfTnIlu5BPH+uGi521yfevDCEED3RtydYXBhHRzot3N5/jkUV/E3tTS71Qo2PGt4fJL9Ly9ZSeuNewsm3j5cTKJ8PJLtAwaekBzl3OZsKS/aTlFLFyWjhd/dzqXBYFBcUB1CNJ2UkcuXKE0cGjm/z0uWKtnn+uicLJ1or5D4dWWd7pA1uTV6xjzcGLJue/fG88rdzsGdaxep36cb38eLi7Lwu3X+Cv6GvG498fTGLjyVReGt6ebv63DrzWhebOdiye1IOF48OIu5bHvQt3s+SvOHR6iZSSV345yYnkLD4eF2ZyjP5mQlq58vWUniRnFHDPwt1czi5kxZPhdK/nsijcuSgOoB5ZH7selVBxf+umv+HLR9uiOZOazXsPh+LlXPXuZiGtXOnXxoPlexPKiXNVxtnUbPbHXWdy3wCTwiZCCN55MIR2zZ35x5pjXMosIPpKDm9tOM3Atp48Pai1yeWqCaVjA9teGMTAtp7M23SWsYv38UO0hl+PpfDS3e0Y3rlF9RlVQe/WHix6vAdtmzuxdEovegU2qyfrFRQUB1BvlM797+vTt8nvrrQ/Lp3Ff8XyWLgfd3cyrazTB7bmcnYhG09Wv9Bpxd4E7K3VjO9luoyEvY2aLx7vTrFWz3PfHeX5747hbGfFh2O71pseUWU0d7FjyeSefDS2Kxeu5LA5XsOoLj7Muiu4XvIf0qE5W18YTN82HvWSn4JCKWZ1AEKIF4QQp4UQp4QQ3wsh7IQQQUKIA0KIC0KItUKIJjESdfDyQVLzUhkdPLqxTTEr2YUaXvrhOAHNHCpdTFURg9t50ba5E0v+iq9y8db1vGLWRaUwpnsrXB1qFrNv4+XE/EdCOXYxk/NXcvhobBjNnRtmhosQgoe6+7L1hcE81sGGDx6tOiymoGAJmM0BCCFaAbOBnlLKEEANjAfmAx9LKdsCGcA0c9nQkKyPWY+ztTND/IY0tilm5T/rT3M5u5CPxoXVSGlSpRJMH9iaM6nZ/B1buS7N9wcvUqTVM7VfYK3suy+0JW/c14m3HwxhUDuvWuVRF1q42jEi0BoHG2UlrYLlY+4QkBVgL4SwAhyAVOAu4KeS8yuB236+ZG5xLn8m/sk9QfdgZ9U051RLKfnhcBK/Hkth1pDgWg1Eju7WEk8n20rlITQ6Pav2JzIg2JO2tRw4BZg2IIhJfe4cAT4FhdoiaqKlUuPMhfgHMA8oALYC/wD2SymDS877AZtLegg3p50BzADw9vbusWbNmlrZkJubi5NT5cvk64N9Ofv47vp3vNTiJQJtA816L2iYMpXlQoaOny8Uc+66nmA3FXPD7bCqZVz9t9hifrmg4Z3+9vg6G9ofpeU5mKrli+NF/LO7LWHNb98WdEN/Pw1BUytTUysPVFymIUOGHJFS9qw0kZTSLA/AHdgBeAHWwDpgEhBT5ho/4GR1efXo0UPWlp07d9Y6ralM2jRJ3v/r/VKv15v9XlI2TJmklPJkcqacsuyADPj377LH29vksj1xsqBYW6c8r+cWyQ6vb5b/+iHKeKy0PA9/sVcOen+H1Oka5nM0Fw31/TQkTa1MTa08UlZcJuCwrKJuNWczaxgQL6W8BiCE+AXoB7gJIayklFrAF6iZ/q2FkZidyLGrx3ihxwtNZtAv+koOH2+LZvOpy7jaW/PvkR2Y0i+gXuLa7o42PNrTl+8PXuTlEe2NWxeeTM7icGIG/+++TmaftaOgoGDAnGMAF4E+QggHYagZhwJngJ3AIyXXTAHWm9EGs7M+xjD3/77W9zW2KXUmIS2Pf645xoj//cXuC2n8Y2hbdv97CDMj2tTroOa0AUFoS6QeSln+dzyONmoe6elbb/dRUFCoGrP1AKSUB4QQPwFHAS1wDPgK2AisEUK8U3JsqblsMDc6vY7fYn+jX8t+NHdo3tjm1JpLmQV8uuMCPxxOxlotmDGoNc8MalNj+QJTCfBwZGTnFqzaf5FnI4LJKpL8fjyVx8L96kWuQUFBwTTMOtImpfwP8J+bDscB4ea8b0Nx4PIBruRf4eVeLze2KbUiOSOfRbti+eFQMgCT+gTwbEQbY1jGnDw1sDWbT13mx8NJHE/SUKzTM6WWUz8VFBRqx+071cICWB+zHhcbFyL8IhrblBqRmJ7HFztj+floMkLAIz38eG5Imwbd+7VHgDs9AtxZujee7DwtEe29KlQTVVBQMB+KA6glOcU5bL+4nQeDH8RWXbUWjqUQey2Xz3fGsD7qEmqVYEJvf54Z3IaWbo2zb8H0gUE8s+ooYJCNVlBQaFgUB1BLtiRsoUhXdFvo/kdfyeGzHTH8fuISNlYqnugXyIxBrfFugFBPVdzdqQWBHg4UFRYwMNizUW1RULgTURxALVkfs55gt2A6e3RubFMq5fSlLD7bEcPmU5dxsFEzfVBrpg9sjaeTZfRY1CrBqqd6c2D/fmXqp4JCI6A4gFoQnxXP8WvHeanHSxY79//9Lef4IjIWZ1srZg0J5skBQRa5A5SvuwMx9ooorYJCY6A4gFqwPmY9aqHmvjaWOfc/6Xo+i/+KY1SoD/8d0wVXe2VqpYKCwq0oTa8aotPr2BC7gQGtBuBpb5lx6yW741AJeH1UR6XyV1BQqBTFAdSQ/an7uVpw1WJ1/6/lFLH2UBIPdfPFx7VxZvcoKCjcHigOoIZsjNuIq60rg30HN7YpFbJsbzzFOj1PDzbPNogKCgpNB8UB1JCknCQ6NOuAjdryBlSzCjSs2pfIvSE+yqIqBQWFalEcQA3JKs7C1ca1sc2okFX7E8kp0jIzok1jm6KgoHAboDiAGpJVlIWrreU5gIJiHcv2xDO4nRchrSzPPgUFBctDcQA1QEpJdlG2RTqAHw4nkZ5XzLNK619BQcFEFAdQA/I0eWilFjdbt8Y2pRwanZ6v/oqjZ4A74UHNGtscBQWF2wTFAdblUkYAACAASURBVNSArOIsAFxsXBrZkvKsj7pESmYBzw5pY7ErkxUUFCwPxQHUgKwigwOwpBCQXi/5MjKGDi2cGdL+9t2URkFBoeFRHEANsEQHsPXMZWKv5fHskGCl9a+goFAjqtQCEkL4AOOAgUBLoAA4hWFbx60lu87fMZSGgCxlGqiUki8iYwnwcODekBaNbY6CgsJtRqU9ACHEEmBVyTULganAi8Ae4EFgrxBiQEMYaSlkF2UDltMD2BuTzonkLJ4e1AYrtdKZU1BQqBlV9QA+k1Ier+B4FPCDEMIO8DePWZaJpYWAPt8ZQ3NnWx7u0aqxTVFQULgNqbTZWFHlL4QIEEJ0LDlfKKWMNqdxlkZmUSb2VvYWIQNx7GIG++LSmT6wNbZW6sY2R0FB4TbE5P0AhBD/BnoCeiFEgZTyCbNZZaFY0irgLyJjcbW35rHed1QnTEFBoR6pagxgphCi7PnuUspHpZTjgO7mN83ysBQdoOQcPdvOXOGJfoE42Sp7+igoKNSOqkYOC4AtQoh7St5vF0LsEELsBLab3zTLw1JkIDbGF+Ngo+aJfoGNbYqCgsJtTFVjACswzPbpI4T4FfgbGA08IqV8oWHMsywsIQSUdD2fA6k6Hgv3x90C9/htsiT+DUmHGtsKBYV6pbr4gR+wEigC3gEKgf+Y2yhLJas4q9FlIBb/FYsApg9UNnxpUP54FawdYerGxrZEQaHeqNQBCCGWAo6APXBGSjlVCNETWC6E2COlfLehjLQEpJSN3gOIuZrLD4eT6d/Kihaudo1mxx1J7jWwyW9sKxQU6pWqxgB6SinHSylHAyMBpJSHpZSjgDtq+idAgbYAjV7TaEqgf8em8dAXe3G2teL+1spG7w2KlJCfBvnpjW2JgkK9UpUD+LNk0HcPsLbsCSnlz+Y1y/JozEVgPxxOYvLSg3i72LHuuf54OSirfhuU4jzQFkLBddDrG9saBYV6o9IQkJTyJSFEM0AnpcxqQJssksbQAdLrJR9sPc+XkbEMbOvJ5xO742JnTWyDWaAAGFr/AFIPBRng6NG49igo1BNVrQMYD2RUVvkLIQKFEP3MZpmFUdoDcLFtmEHggmIdz313lC8jY5nQ259lT/TCxU4J/TQKeWVCP6XOQEGhCVDVLKBWwDEhxEHgCHANsAOCgQggG/i3uQ20FBoyBHQ1u5Dp3xzmREoWr4/qyLQBQYrUc2NSttJXxgEUmhBVhYA+FEIsBO4G+gPhGBaHnQWmSSnjG8ZEy6ChQkBnU7OZtuIQmQUavprUk7s7eZv1fgomkJdW8WsFhducKtcBSCm1Qoh9UsrNDWWQpdIQPYCd564y67ujONlZ8cPTfQlp1firjhW4qQegOACFpoMpQjJHSsJAy6WUW81tkKloNBqSk5MpLCys8jpXV1fOnj1b5/t10XVhYaeFxF8wT8cnt0iLLl/DF6O88XCyRZ19ibPZlyq8tr7KZClYfHnsw2HEj4AEtStUY2tDlcfOzg5fX1+srZWxIYXaYYoDaAuMAKYLIT4HvgdWSikbdTJKcnIyzs7OBAYGVhkfz8nJwdnZuc73S8lNIbc4l/bN2tc5r5u5mlOIJquQYDtr/Jo5oFZVHe+vrzJZChZfnoxEKMoBqQMHD3D1rfLyhiiPlJL09HSSk5MJCgoy670Umi7VTiiXUuqllJullI8C04FpQJQQYrsQItzsFlZCYWEhHh4eDTY4qtPrUKvMo7ufVaDBwcaKAI/qK3+FRkCvBZWV4aHXNrY1AAgh8PDwqLYHrKBQFdX2AIQQbsBEYDKQAbwA/Ar0wLBArNGaHw05M0YndaiFeRyARidxsVUrM30sFb0W1FagF6CzDAcADfv7V2iamLKk9BDQHBgrpRwppfxBSqmRUu4HllSVUAjhJoT4SQhxTghxVgjRVwjRTAixTQhxoeTZvT4KYm7M5QD0UqLV6bG2Ulb3WiwW2ANQUKgPTKl12ksp/yOlTLz5hJTyv9WkXQhskVJ2ALpimEI6F9gupWyLYV+BuTW0uVGobQjo3XffZfXq1ZWe1+oM0gLWakNrbsuWLYSHh9OhQwfCwsIYN24cFy9erJ3RNeDee+8lMzOz3vN1cnIyvt60aRNt27Y1uTwXL15k+PDhdOzYkU6dOpGQkFDl9W+++SYODg5cvXq1wvubwqeffkr79u3p3Lkzc+bMMRyswgFcuXKFCRMm0Lp1a3r06EHfvn3ZsGGDyfd77bXX8PPzq7GdCgr1gSkOYFNJGAgAIYS7EKJaTVwhhAswCFgKIKUsllJmYthTYGXJZSsx7Dlg8dS2B7B161aGDx9e6XmNTgJgrVZx6tQpnn/+eVauXMm5c+eIiopi4sSJFVZ8Wm39tkQ3bdqEm5v5hO62b9/O888/z5YtW/D3N20by8mTJ/Pyyy9z9uxZDh48SPPmzatN4+npyYcfflgrG3fu3Mn69es5ceIEp0+f5l//+pdB+0fqyzsAafjOpJQ8+OCDDBo0iLi4OI4cOcKaNWtISUkx+Z73338/Bw8erJW9Cgp1xZRZQC1KKm4ApJQZQoiWJqRrjWH18HIhRFcMq4n/AXhLKVNL8koVQlT4rxZCzABmAHh7exMZGVnuvKurKzk5OdUaodPpTLquKvRSj5QSneZGXv/73/+wtbVl5syZzJ07l1OnTvH7778TGRnJqlWr+Prrr8nOzqagoAA7OztOnz7Nc889R1paGp6ennzxxRf4+fmRW2yoTIoLC3jnnXd48cUX8fX1Nd5nyJAhgGFmyb333kvv3r3Zt28fo0aNYvTo0RXm+cwzzzBy5EgefNDgW318fEhNTWX37t3MmzePZs2aceHCBfr3789HH32ESqUiJCSEXbt2kZuby8MPP0zfvn05cOAAPj4+rFmzBnt7e44cOcKsWbNwcHCgb9++bNu2jQMHDlT7+f3xxx8888wz/PTTTzRv3vyW76Oi7+jcuXMUFRXRp08f47nqvsuioiImTpzId999x7PPPkuzZs2Mn50pfPrpp8yePZvi4mKKi4uxt7cnNycTJ6CwWAtSjx2SnJxsECoiIyNRq9VMnDjReI9mzZoxffp0k+/ZuXNn4+va/E4LCwtv+W+Yg9zc3Aa5T0PR1MoDtSuTKQ5AJ4TwlVImAwghTN2F3ArD3sHPSykPlKwqNjncI6X8CvgKoGfPnjIiIqLc+bNnzxqn2r214TRnLmVXbLxOh1pd85Z7p5Yu/Od+w59To9NAMdjb2eNsZ7jn3XffzYcffsicOXM4ceIERUVF2NnZcfToUe666y6cnZ3Ztm0bw4cPx9nZmblz5zJ16lSmTJnCsmXLePXVV1m3bh2FOUVQUICrixPR0dG88sorlU4hVKvV5Ofns2XLFpydnbn//vsrzNPa2hp7e/ty+Tg7O+Pg4MCRI0c4c+YMAQEBjBw5km3btvHII48ghDCGIWJjY1m7di1hYWGMHTuWrVu38vjjjzNr1iy++uor+vXrx9y5c1GpVNVOdywqKuKxxx4jMjKS0NBQ4/HVq1fzwQcfAKDX61GpDJ3R4OBgfvrpJ1JSUvDw8GDKlCnEx8czbNgw3nvvvSq/S1tbW5ycnJg2bRrLli3jrbfeMpYdYODAgRVWsgsWLGDYsGHGVvy8efOws7NjwYIF9OraGfLAzsHJMA20OB1nBzuwsiU+Pp5evXrd8hmUTgM9f/4848aNq9DWyMjIW3pctZk6amdnR7du3WqcrqZERkZy83+wQSjMgkNLod/zoK6/9Q6NVh4zUpsymeIA/h+wVwixo+T9EGCmCemSgWQpZWkT8ScMDuCKEMKnpPXvA1ytNAcLQSd1AFiJGx9Xjx49OHLkCDk5Odja2tK9e3cOHz7M7t27+eSTTwBDPH/q1KkA7Nu3j19++QWASZMmGePLGp0elRCob5rRkZ6eztChQ8nPz2fGjBmGcASUq1Aqy7MqwsPDad3asJvYY489xp49e3jkkUfKXRMUFERYWJixnAkJCWRmZpKTk0O/fgb9vwkTJvD7779Xez9ra2v69evH0qVLWbhwofH4xIkTmThxIlDxvHmtVsvu3bs5duwY/v7+jBs3jhUrVjBt2rRq7zl79mzCwsJ46aWXyh3fvXt3lem0Wi0ZGRns37+fQ4cOMXbsWOLORCHAEP6RJd+RXgvY3pL+ueeeY8+ePajVao4ePUr79u2Jioqq1l6FKoj6Hra/Bd6dod2IxramyVGtA5BSbiyZ798XEMC/pZTVVtpSystCiCQhRHsp5XlgKHCm5DEFeK/keX1dCgAYW+oVUR+LckodQNkxAGtrawIDA1m+fDn9+vUjNDSUnTt3EhsbS8eOHQE4ePAgX375ZYV5lk7h0+j0WKtVCCHo3LkzR48epWvXrnh4eBAVFcWCBQvIzc01pnN0dKzUztI8rays0Jfo1kspKS4uvuWayt6DoSVdilqtpqCgAFkS964pKpWKH374gWHDhvHf//6XV199Fai+B+Dr60u3bt2MzurBBx9k//79JjkANzc3JkyYwBdffFHueHU9AF9fXx566CGEEISHh6NSqUi7egUvaypwAIbwzc8/39ga4/PPPyctLY0ePXoA1LgHcNsTvxu2vg4TfgDnetKwiossyfsvxQGYAVPnHhYCF4ErQHANZKCfB1YLIU4AYcB/MVT8dwshLmAQmnuvZiY3PDp9iQO4aRbQoEGDWLBgAYMGDWLgwIEsWrSIsLAwhBCcPn2aDh06GEMW/fr1Y82aNYCh8hswYABgGAQunQE0Z84c5s2bV05GID+/8m0IK8szMDCQI0eOALB+/Xo0Go0xzcGDB4mPj0ev17N27Vpjmupwd3fH2dmZ/fv3AxjvC5CSksLQoUMrTevg4MDvv//O6tWrWbp0KWDoAURFRREVFcXevXuNr3/66ScAevXqRUZGBteuXQNgx44ddOrUCYBXXnmFX3/9tUp7X3zxRRYvXlxusHz37t3G+5R9DBs2DDA4mR07DB3d6OhoiouL8WzmSkrqVYaOvNfgBMDoAO666y4KCwvLOfmy31dpD6CiR5Or/K/HwQ+TIDUKzm+qnzx1WkjYY3gd/1f95KlQjmodgBDiSeBvYAcwv+S5uumfAEgpo6SUPaWUoVLKB6WUGVLKdCnlUCll25Ln63UqQQNQUQ8ADC3K1NRU+vbti7e3N3Z2dgwcOBCAzZs3M3LkSOO1n3zyCcuXLyc0NJRvv/3WGA4p7QEAdOnShYULFzJ58mQ6dOhA//79OXv2LBMmTKjQrsrynD59Ort27SI8PJwDBw6U6zX07duXuXPnEhISQlBQEGPGjDH5c1i6dCkzZsygb9++SClxdTWI1aWmpmJlVXVnslmzZmzZsoV33nmH9eur7/Sp1WoWLFjA0KFD6dKlC1JKpk+fDsDJkydp0aJFlek9PT0ZM2YMRUVFJpYOnnzySeLi4ggJCWH8+PGsXLkSIXWkXk3Dysr6FgcghGDdunXs2rWLoKAgwsPDmTJlinHswRTmzJmDr68v+fn5+Pr68uabb5qc1mIozIbvHzPMjnL0gtjt9ZPvpaNQnAMtusDlk5Bv8VXF7YeUssoHcBLDxvBRJe87A99Xl64+Hz169JA3c+bMmVuOVUR2drZJ11XFtfxr8tS1U1Kr05qcZtiwYfLSpUtVXqPX6+WJpAyZmllQI3tqW6adO3fKUaNG1SqtlFLm5OQYX7/77rty9uzZUkopP/30U7l+/fpa51vT8gwfPrzW96oxGYny03mvGMqn10uZckzKrOQqk9THb85UTP0f1JWdO3dWfEKnk3L1OCnfdJcydqeU656T8r9+Umo1db9p5Hwp/+Mq5blNUv7HRcrTtf+N3Uyl5bmNqahMwGFZRd1qyiBwoZSyQAiBEMJGSnlaCNHBbB7JAtHqtQghUAnTV+tu27at2ms0OonkxiIwS2fjxo28++67aLVaAgICWLFiBQCzZs1qUDv++OOPhruZTsuspyZBc8O4jrIa+CZ2vgPRm+GeD6B1hGHLzGPfQsoR8O9dt7zjdoFPKAQPA2tHQxio0wP1YbVCCaY4gNSShWAbgD+EENcxjAXcMeilHrWof60ejXEVcMPIQERERNRp6tu4ceMqHdRsspSuAi5FZWVRekCNyqmfYfeH0H0KhBvCcwQNBqEyhIHq4gCK8yDpAPR91jD9M6CvMg5gBkxRA31ASpkppXwDeAdYjWE17x2DuZRAtQ3sABRqQUUOQOkBwKVjsO458O8L9y6A0saRQzNo2R1i6jgOcHEf6DUGhwIQNAjSzkPOHdX2NDtV1jxCCLUQ4njpeynldinlL1JK00fWmgDmEoIrNspA3B4hoDuSCh2ArvHssQRyrsCaieDoCWO/BSub8ueDhxoGcOsyaBsXCWobg4MBgwMASKh6LYdCzajSAUgpdcAZIUSrBrLHItFKrVkcgHERmLIHgGUi9YbVv+oyDkB9h/cAtEWw9nFDrH/8d+Dkdes1bYYaPrv4XbW/T1wk+PUGGwfD+xahYOdatzwVbsGU2IMncFYI8YcQ4pfSh7kNsyTMFQIquwhMwQIpbenf3AOQOkMFd6chJfz+AiQfhDGLDAO0FdGqB9i61j4MlJdumPbZevCNYyo1BAxQxgHqGVMcwHvAGOB94PMyjzuG0kHg2lCVHHTZRWClKHLQN2hoOegff/yRzp07o1KpOHz4sLGlv23nbnr06EGXLl3oMWgEO/YcNKkX8NFHH9GhQwe6dOlC165defHFF8styquKv/76i+7du2NlZWVcHNfo7P8SolbD4LnQqYphQLUVtB4EsTuNyqk1orSV33pI+eNBgyAjATLN/3+4UzBlEHh7RY+GMM4S0Et9nRxAVXLQ2jKLwABFDvomGloOOiQkhF9++YVBg0rizSWVvKdXczZs2MDJkydZueQLJv3jjWrHARYtWsTWrVvZv38/J0+e5NChQzRv3pyCggKTbPH392fFihWVLgJsaNyvH4Otr0GH+2Dwv6tP0OYuyE6GtOia3ywuEmxdwCes/PHScYB4ZRygvjBlJXCOECK75JEvhCgSQlQsvdkEqUwG4v333zeKvr3wwgvcddddgKGie/zxxwHIzs6muLgYLy8vEhMTGTp0KKGhoQwdOpTExMRbegDz58/n1VdfNWoJATzwwAPGCikiIoJXX32Ve+65h4ULF96SZ2nL+oknnijXaixtBUdGRjJo0CDGjBlDp06deOaZZ4yaQYGBgaSlpZGQkEDHjh2ZPn06nTt3Zvjw4cZK69ChQ4SGhtK3b19efvllQkJCTPoMd+/ezfTp09m4cSNt2rQxKc2ZM2fQarXcfffdxjI4ODhUm+7JJ59k7dq1XL9e8wHIjh070r59+xsHShxAt+49aNnSoIDeOaQLhYXFFBXkVZnXvHnz+PLLL41O1cbGhrlz5+Li4mKSLYGBgYSGhho1khqVtBg6nfkAvDrCmMVgik1tSqRBYndUfV1FxO+CwIHlx17AsBbDwVMJA9UjpojBGZXUhBAq4CEMu3tZDpvnGmKGFWCv0976QzKFFl3gnvcqlYEYNGgQH374IbNnz+bw4cMUFRWh0WjYs2ePUQ7izz//NGrkzJo1i8mTJxulm5+fPZt3Pl9Zrgdg3ISkCjIzM9m8ebNRDrpsnrNnz2bdunVVpj948GA5OehffvnlFjXQCxcu8P3337NkyRLGjh3Lzz//zOOPP87UqVPLyUGbQlFREaNHjyYyMpIOHW6sH6xODC46Oho3Nzceeughk+WgweAonnzySRYuXHiLJEN1YnC3UDrfv8wYwM/rfqdbSHtsq9jCMycnh9zcXIKCKt8ue9y4cZw/f/6W4y+++CKTJ0+uNF2Do9PCj1OQQg2PfQ+2JobU3APAI9gwDtDHFPHgEjISDI8+z916TghDLyD+L0NoSRk7qzM1al5IKfVSyp8wiLjdEVTmAG6Wg+7bt69RDrrUAWzZsoV77rkHMEg3l3bnJ02axN979wKVrwFIT08nLCyMdu3asWDBAuPxm+Wgy+a5Z8+eastTKgetVquNctA3Y6octCmUlYMuS3VicKVy0AsWLODQoUPExcUZVx5Xx+zZs1m5ciXZ2eU7qtWJwd2CvrwDOH36NP9+7Q0Wz3+tyhCQlLLcwP4ff/xBWFgYgYGB/P333wCsXbu2QlssqvIHOLwUrpwiut2zhkq9JrS5yyDmpik0PU1cafx/cMXngwZCziVIj62ZLQoVUm3TWAhRdu21CuiJQRbacrinckHRgjrKQVcWAqqrHHRp66VsCEiRg25cOehb0GtBqEEIkpOTGTNmDN+sXEmbQMcqB4FdXFxwdHQkPj6eoKAgRowYwYgRI7jvvvuM38Vt0QPIvQY75kHrIaR59ql5+jZD4eBXkLTfIBNhCnGR4OwDnu0qPl+6MCzhL/AMrrlNCuUwJTbyaJnXWiCBO2glcGU9ALghB71s2TK6dOnCiy++SI8ePaqUg540aRKrV6+mdx9DS7psD2DOnDmMGTOGPn36GJ2IKXLQpXneLAc9duzYSuWgAwICWLt2LTNmzDDpcygrB92nT59b5KAnT57M9u0Vzw0olYMeOHAg3t7eTJs2rdoNYcrKQXt5ebFjxw569uwJGOSgw8PDq1QyffHFF+nVq9ctctA1Qm8IH2ZmZjJq1Cjeffdd+g8YAKknjA5g8uTJzJo1i/Dw8HJJX3nlFWbOnMmaNWtwc3NDSklh4Y2W8Nq1a2tmS2Ow/S3Q5ME978PpSzVPHzgAVNaGMFDriOqv15esHWg7vPLwTrPW4NLKEAbq+WTNbVIohymzgCaVeUyVUr4lpbzcEMZZAlU5gLrIQb/57geImxaBKXLQN2gMOehff/0VX19f457LIx6eDCorPvvsM2JiYnj77bcJCwsj7O6xXL1i+AucOHECHx+fW/KaOXMmw4YNo3fv3oSGhtK/f3+6detm8vaNhw4dwtfXlx9//JGnn3663N7BDULyEYOoW59nwauS1nh12DqBfx/TB4Kvnob89But/IowjgPsNjgMhbpRlVRoSbd/KeBW5r07sKS6dPX5aEw56Mu5l+XptNNSr9fXKF11ctCJaXnybGpWrWxS5KAbSA76yhkp02NvPX7tvJTXomVWVpZ85JFHbjl928tB63RSLo6Q8oN2UhYaylJr+eS/PjRIOWenVn/t3k8M12alVH3dsdWG6y6fqp1NUpGDLn2YMgjcXUppXCEkpcwAepjBF1kkpTpANV2tu23btgpbhqVobloDcDuwceNGwsLCCAkJYffu3bz++uuAYYbTAw80nExvg8lB36wDVEqJIJyLiws//vhjw9jSkEStMmj5DH8bbOu2nSptDNOjid1Z/bVxkeDZHlxaVn1doKGXrUwHrTum1EAqIYRr6RshhDtgbT6TLAtzCcFp9A3vACIiIkzayL0yxo0bR1RUFKdOnWLjxo14eVWgA9NUkLJaB9AkKciAP980iLB1ebTay6ulRahh7n51u4RpiyHx78pn/5TFzQ/cg5QFYfWAKYPA/wP2CSHWAhIYj0EW4o7AHDpAUsoKZSAULIiKdIBKKXUATXEu+s7/GpzAPe/XT9lUKkMvIHaHIWZf2SKy5EOgyTd9tlDQIDi9zvA9mUGn607BlEHg5Rgq/SwgBxgnpVxhZrssBnP0ALR6Q/ztdgsB3VHob10EZsS4N3ATk4W+fAoOfQ09p1Uu9FYb2twF+Wlw+UTl18TvMmwkE9DftDyDBkFRFqQer/5ahUoxRQqiFxAnpfyflPJjIEEI0dP8plkGOr0ZHICyEYzlY5IDaEJhIClh08tg5wZDXq3fvI3jAFXMBoqLNGwkY2+iHpUyDlAvmFIDfQWUnYyeByw2jzmWh07WfwhIo2wEY/mUVu4VyYg0RQdw6me4+DcM+49hV6/6xNkbvLtU7gAKsyH5sOnhn9I8vTooDqCOmDQILOUN8fOS13fEIHBdlUChYjnoYp2evTv/ZHD/fk1G9rkyIiIiDNLKQEJCAm3btjV5Fk9xcTEzZsygXbt2dOjQgZ9//rnK6yMjIxFCsGHDBuOx++67j8jISJPtjYyMJCwsjM7d+zD44aeq7QFotVpeffVV2rZta1gjEBZmXOFsCm+88QahoaGEhYUxfPhwLl2qxYKrulKUA1tfN6hvdptknnu0GQIX90NR7q3nEv827LFgygBwWYIGGfLUFld/rUKFmOIA4oUQM0u2h1QJIZ7DsBq4yVPVIjBTqUgO+uTJk7z3//7NypUrmpzsc2UkJyczYsQIPvzwQ0aMGGFSmnnz5tG8eXOio6M5c+YMgwdXX0H4+voyb968WtmYmZnJs88+y2+//cbpg5H8uPj9ah3A66+/zqVLlzh58iRRUVHs3r3bZM1/gJdffpkTJ04QFRXFfffdx//93//VyvY68dcHkJNq2NvXXAOqwUMNe/wmVKBXFRcJVvbgG37ruaoIGmRYqXzpaL2YeCdiigN4GhgKXCl5DAamm9MoS6EyHSCouRx0WT79+EOenv0SnTp1Mh6rSPZ58ODBFco+JyUlAZYn+1wZly9fZvjw4bzzzjs1Wi+wbNkyXnnlFcCgKeTp6Vltmq5du+Lq6sq2bdtqbOd3333HQw89ZNivQK+luZeXYWDyZkocQH5uNkuWLOHTTz/Fzs4OAGdnZ6PekSmUlYfOy8tr+N3h0i7Avi8g7HHw62W++/j1MVTyFU0HjYuEgL5gbVezPAP6A0IJA9UBU+SgrwCPVHddYzL/4HzOXT9X4TmdTlethHBFdGjWgVndZgGV6wDVRA66LOfPneXJmbOrvH9mZia7dhmUEW+WfZ4zZ0618/kbWva5KiZPnsw777zDo4/emFd+/vx5o7JpWTE4oFzI5o033iAyMpI2bdrw2Wef4e3tXe39Xn/9dV5//XXjXgKlvPDCC+zceeuCpPHjxzN37lyio6PRaDRERESQk5nOP6ZNYPLzFcyGUalAqIiJicXf379KsUFTBOheyMh22wAAIABJREFUe+01vvnmG1xdXSu0z2xICZv/Ddb2hti/ObG2M2gD3bxNZM4VuHYWuo6veZ4OzQyy7fF/weA59WPnHYYps4BshRBPCyE+EUJ8VfpoCOMam6p6ADWVgy6HlFiVaADVRvZ537591dre0LLPVTFs2DC+/fbbcsJ27du3r1AOOioqCjc3N7RaLcnJyfTv35+jR4/St2/favdKKKX0879Z/O3jjz+uUIK51MlptVqOHDnCxo0b+WPt17z98WKioyvZ0UplBbJ8eG758uWEhYXRsWNHYy/NFAnqefPmkZSUxMSJE/nss89MKmO9cH6ToUU+5FVwqn63tToTPBSuxxr0/ksxbv8YUbs8gwZB0gHQmLbTmkJ5TFkI9g0QB9wHzAMmAKfNaVRN+Xd45VvUVaQ0aSoZhRlAxT2A2spBSylp3a4DZ08dZ+TgPk1G9rkq5syZw6pVq3j00UdZv349VlZW1fYAPDw8cHBwMIrVPfroo7fsKVAVr732GvPmzSsnUlddD8DX1xdPT08cHR1xdHdlUL9wjh8/Trt2FYihqawIDvDl4sWLxt/Y1KlTmTp1Kp06dUKnMzQeaiJBPWHCBEaNGnXLRjZmQVMAW+ZC807Qq4EiumWng5YqecbtAnt3w4rh2hA0GPZ9BkkHaz6IrGDSGEA7KeUrQK6UcikwEqhbUPg2obpB4FI56EGDBjFw4EAWLVpEWFhYhXLQxjz1kieemc0nH77P2bNnjcdNkX0Gg45+3759gRuyz0Clss96vZ61a9capaKro6zsM3CL7HNFIS1T+Pjjj3FxcWHatGlIKavtAQghuP/++43hoO3btxvHTH799Vfj2EBlDB8+nIyMDI4fv7FQqLoewOjRo9m9ezdarZb83BwOHDludOZDhw4lJSXlxg1UVjjYWTNt2jRmzZpllHrW6XTlHHF1PYALFy4Yr/3tt9/K7ZpmVs5vMmyuPvyd2u2YVxs824GL740wkJSG+H/QINO2mayIgL6GPRuUcYBaYcqnXlqrZAohOgLOQA23Bro9KQ0BqSoaCKRmctClaHSSth07M3/Bh7WWfZ4/fz5gmbLPlSGEYOXKlaSmpjJnjmnx2vnz5/Pmm28ay1262XtsbKxJe+u+9tprJCcnm2xjx44dGTlyJKGhoYTf+xhPTZlASEgIer2emJgYmjUrMz9eZQV6HfPmzcPHx4eQkBC6devGwIEDmTBhgnEP4eoo/Y5CQ0PZunWrUdLb7CQfBiu7GxutNwRCQPBdhspapzXs6pWdXPvwDxjE6lp1VxxAbalKKrQkHPA0Bgno/9/eucdXVZ15//vkTgIJkISQC1cBCaBCsAIqlIr3Uqt1Wsexvo5tx3bEt7b6Op3ptKN23o7ttPVtp+1oW0urtRdt1VHR2loloChQgtxBrgIJgRAgJ/f7ev9Ye4cknPvZ55x9zlnfz+d8crLP3muvlZ2zn72ey299BDgCNAJ3BzrOyVe85KDrWurU7lO7wzrWlxy0p71bbT16RrV19oTdr0Bjcqvssy9CvUa33XabamhocLwfA/T1KFW3WamWE0oppbZv366+/OUvD92nqVapui1eD08IOegnrlLqieBltR2TT97xvJZyPvyuUht/pt837o+szb88rNRDYwakq4PByEHrVzBZQHbV72pgYnTMkDuJRAfIVxpiTwLIQLzyyis88sgj9Pb2MmnSpIG1eO+55574dszi6aefju4JhslAzJkzh0cffXToPmkZQH9iipH19WgNnYsDL6/pOFM+rFNrD7wJDbugYKJe5SuiNpfAW9+Dw+/CjKsD728YIEbOv8QkGkqgPX0KQciIogzE0qVLWbp0adjH33LLLUOykFKOPj86QDb2/0V/b+IZgBM7obcTKuKwrEfuWK35s+91nRFUeUPkqqMTFkB6ll4n2BiAkHDvY2gQqChkrAwmGkqgPX39ZKRL7At+DMHjTwjOJi1z6L5xIOz//zqdOEB5nNZ1mrZMV+92eiLz/9tkjtBGwMQBQiaYOoBzvgXetsWanJwcTp06FVUjEC0D4Gb3j4HgDICdORMnSWilFKdOnRqoQA6Juhq9SMvoOOVynDcok8ypIPTkxVC/DdpPO9NeihDMjXwjUBXEtphSUVFBbW0tJ0+e9LtfZ2dneF8S4HjbcUZkjKAl+9w87nA50dxJZnoaPaeywm4jkjG5EdeNp7MZOpugKcu7FARoP3pLA5zsg6yhNRuxGk9OTg4VFRWhH1hXo5/+4zULLZ8P2QV6ZS+nCtCmLIHq/4DD66DyY860mQL4NAAiMg4oBUaIyAWA/d+SD+TGoG9+yczMZMqUKQH3q66uZt68eSG339Pfw6d+9SlWzF3BFyq/EE4Xz0Epxc0P/olbL5nI15dXht1OuGNyK64bz2tfhZpfwL/W+96nowm+vRiu/iZcNDQ47rrxDKazGU6+D3Nujl8f0jPgo9/VBWBOUT4fMnO1G8gYgKDxNwP4KPAZoAL4MWcNQAvw9WBPICLpwCagTim1XESmAL8DxgKbgduVUq7Tc23uagagILsgwJ4htNnZS3t3H6UFLnraNZxLe6N2kfgjp0DHAdobY9Mnpzj2HqB07nw8ufBTzraXkQUVF+uKYEPQ+HRGK6V+oZRaDHxWKbVEKbXYel2vlPp9COe4F9g96PdvA/9PKTUdOAPEIRctMJ5uDwAFWc4ZgOMeXS063hgAd9PWCHmF/vcRgdxCvW8iUafXZqAszgYgGpRVWRlOXfHuScIQTDRynIjkA4jI4yKyUUSC0gMQkQr0TOIJ63cBrgBsDeMngRtD7nUMiMYM4JhHC1aZGYDLCWYGANoAJFrQsW4zjD3P+VW/3EB5lV5z4PiOePckYQgmCHyXUupHInI12h30j+hlIoPJIfs+8E9o+QiAQqBJqQEZxVqg3NuBInIXcBdASUlJSKs6Daa1tTWsY3e063+i/Tv207Mv+AU+/LHmqG7ng11baDkUfiZQuGNyK24bz8LTdTT1j2VPgD5d1J1OWv0B3hu2n9vGM4BSLDq4jjNjLgo4tuG4dkyDyO7sZhGwd83vOFbuP3EjEcYTKuGMKRgDYOdZXgf8QilVI+IrNeIsIrIcaLD2X2pv9tP+0I1K/RRtaLj44otVuIVN1dXVYRVFNR9ohpNwxaIrmJA/IaxzD2fz63uRXfu44eqlEaWChjsmt+Kq8SgFb7cyfupsxgfq08lpUL/1nL67ajyD8dTBmjOMr7qe8QuWhnSoa8c0GKVg+78wI7eVGQH6mhDjCZFwxhSMAdgqIq8CM4B/FZGR+LhpD+My4AYRuR7IQWcPfR8YLSIZ1iygAojDIqiB8XTpGEB+dmDRsWA57umgeGS2qQNwMz3tuko2L1gX0Kno98kpbP9/+cXx7Ue0ENFxALNEZNAEcye6E3gIuEQp1Y6+mQcM3Cql/kUpVaGUmgz8LfCmUuo2tKaQvTTVHcCLYfQ76ni6PKRJGqOywltLwBv1nk5KR49wrD1DFLCDusHEAPKKdL1AnzMuwqhTV6MlE8YnsZp7eZVOc+1yrnYnmQloAJRSfcBUtO8fYEQwx/nhK8B9IrIfHRMIfpWPGNLU1UR+Vr5PKehwOO7ppDTfBIBdjZ3WGewMABInEFxbo5dQzMgOvG+iUlYFKF0VbAhIML78H6GloD9tbWoDHg/lJEqpaqXUcuv9QaXUJUqpaUqpTyqlXJmz1dzV7GgGEGgDYFJAXU6b5dIJdgYAieEG6u/TNQDx0v+JFWVWAZ5xAwVFMDGAS5VSVSLyHoBS6rSIhK9jkCB4uj2O1gC0dPbQ0tVrUkDdzsAMIEAdAAyaASRALcDJPdDTlrz+f5uRxVAwQae7GgIS1IpgVtaPAhCRQqA/qr1yAZ4uj6MB4BPNpggsIQglBmDvkwjFYPFWAI0lZfPMDCBIfBqAQYqfPwaeA4pF5GHgbXQ1b1Lj6fI4WwTWpA1AaYEJArua9kYdKM0OIvifSC6g2k1avqLwvHj3JPqUV8GZDxInNhNH/LmANgJVSqmnRKQGuBKdx/9JpVTSl9o57QKyZSCMC8jltJ3ST/bBKGWOsKppE8EA1G2OrwJoLLFlLo5thmlXxrcvLsefARj4T1FK7QR2Rr877qCvv4+W7hZGZ492rM16ywCMy0/iDIxkoD0IHSCb9AzIGe1+F1B3m15+8fz7492T2FA2V/+se88YgAD4MwDFInKfrw+VUo/6+izRae7WOkCOFoE1d1A0MpvsjARbPjDVaAtSB8gmr8j9QeD6raD6UsP/D5ara7qJAwSBPwOQDozEu3xDUmNXATsZA6j3dBr3TyLQ3ghjA68zMUBukftdQKkUALYpr4KDa+LdC9fjzwDUK6W+EbOeuIhoSUFPGBv3dXQMgbBjAMGSW6gDjm6mdhOMnqhTJFOFsirY9gw0H4P8snj3xrX4SwNNuSd/GzMDSFF6u6C7JfgYAOh93e4Cqtuc/Pn/w7EXvDn2Xnz74XL8GYCgNP+TEacNQHt3L56OHlMD4HZCqQGwsV1AKhh9xDjQ2gCeI6nl/gEomQOSbgrCAuBvRbCUTaK1g8BOuYBMCmiCEIoOkE1eEfT3QqcnOn2KFNv/X5FiM4CsXBg3ywSCA2B0ib3g6fIgiGNKoHYK6Ph8UwTmasKaAdhyEC4NBNdu0k/C4y+Md09iT/k87QJy6+zMBRgD4AVPl4dRWaNIT3MmZbPezAASA/smHsoMwO1yEHU1UDJbPxGnGmVV0HEGzhyKd09cizEAXmjqanI0AHzcWgvYxABczsAMIMQgMLhzBtDff7YCOBWxA8EmDuATYwC84LQMRL2nk7F5WeRkmiIwV9PeqN0lOSFUgLtZEfT0AejypK4BGDcLMnJMJpAfjAHwgtNrARz3dDLeLATjftoa9Q09LYSvhZtdQLXWEpCpFgC2Sc/UC+CYGYBPjAHwgtNS0KYGIEFoPxWa/x+0bz0z150uoLoayBoJRTPi3ZP4UValpTD6++LdE1diDIAXHFcCbTYrgSUE9gwgVNwqB1G3SWvjO5TMkJCUV+mFcE6+H++euBJjAIbRr/pp7mpmtA8/8IrfbOa2J9ajgkwt6+zp43Rbt5kBJALtjaHPAAByx7rPBdTTCcd3pK7/32awNLThHIwBGEZLdwsK5XUGsL3Wwyvb6lm3/xRv7G4Iqj27CGy8WQjG/YSqBGrjRkXQ49uhvyd1/f82hdMga5QJBPvAGIBh+JOB+OGb+8jPyWBSYS7f+dP79PUHngXYNQBlZgbgbvp6oLMpzBmAC11AqagA6o20NL0+gAkEe8UYgGH4MgC7jjXz510n+MzlU3jgmvN5/0QLL26pC9je8WZTA5AQ2MsHhhUDKNQqom6ibhOMKjNKmKDjICd2QG93vHviOowBAF0qvvdP0Ns9IAWdnzU0C+hHq/cxMjuDOy+dwvVzSpldls+jr++lu7ffb9MDMhDGALib9jCKwGzyCnWgsafD2T5FQl3N2UKoVKe8Cvq6tREwDMEYAICDq+E3n4Lq//A6A9h7ooU/7jjO3186mYLcTNLShH+6dia1Zzr47cYjfps+7umkYEQmuVn+ll4wxJ22MITgbHJdtjh8+2k4fdD4/21MINgnxgAAHKzWP9f9F56GXcBQA/CjN/czIjOdz1x+dqWoJdOLWDh1LD98cx9tXb0+mzY1AAlCexhCcDZ5LisGs/3dqe7/txk9Uc/s6kwgeDjGAAAcWqsrBvOK8Wx/BjjrAjpwspVV245x+6JJjM3LGjhERM8CGlu7Wfm2b7Gp4x5TA5AQtIUhBGfjNjmIuhpAtO/bACJ6FmBmAOdgDEDHGV0peP5H4aPfo7njJKPSMslI0y6bH6/eT1ZGGv+weOo5h1ZNHMNVs0r46dqDnGnzHmAyM4A4sPtleP+PoR1j37xHjA39fANyEC5xAdVtguKZkO2MnHlSUF4FJ/dAd1u8e+IqjAH4YB2ofpiyBCqX0zR2MvndHdCwh8On2nhxyzFuWzCJopHZXg9/4Jrzae3u5bE1B875rLu3n8bWLrMOQCzp74OXvwSr7tNqmMHS1ggjxkB6GLEaNymCKqVnABXG/TOEsir9Pa/fGu+euApjAA6thYwRUPEhADzF0ylQAi/9bx57cy/pacLnl5z79G8zo2QUN80r55fvfEC9Z2gWyIlmax2A0WYGEDOObtBP8y3HQiv+aQ+zCAwgu0CriLrBBXTmA22IjP9/KLY7zNQDDMEYgENrYdIiyND+fU9fBwVjp0HtRvK2ruTWD01gXAAlzy9fOQOlFP/1xr4h281CMHFg9ypIz9I35D2rgj+uLQwhOJu0NKsWwAUGYKAAzGQADWFUCeSXm4rgYaS2AWhtgJO7tfvHormrmYLC83l/1ELuT3+GFfMCuwQmjM3ltgWTeHZTLQdOtg5st2cExgDECKX0TX/qUph8eWgGoD1MITibPJdUA9fV6BntuFnx7on7KJtnAsHDSG0DcGit/jnIAHi6PGRIHv9w+tOkpaczrvorQa0pes8V08jOSOPRP+8d2GZ0gGLMiR3QdBhmLofKj0HjXji5N/BxoJ/ew50BgDYebjAAH7ytpQ/CiWUkO+VVuj6i40y8e+IaUtwArNH+29K5gFYC9XR72F/fzzFVSPuH/03XCLz3q4BNFY3M5nOXT+GV7fVsr9XFZPWeTkZlZzAy23wZY8LuVSBpcP71+gWw5+XAx/X3Q8fp8GMA4A4X0NG/wvFtMOfm+PbDrQwUhBk3kE2KG4C12lVg6aW39bTRr/rZcbSHT1SVM3bx52HSZfCnr0FzfcDmPrdkKmNyM/nPP+0BTA1AzNmzCiYshJHFUFCuv/C7g3ADdTbpDJFIZgBuUATd8Jh+oLno1vj2w61EMxDc2w37/xKUt8BNpK4BOHNYZ0wMcv80dTUB0Neby4qPTNPBvRt+CH1d8Mr9AS9ufk4mdy+dxlv7GnnnQCP1ZiGY2HH6kHYBVS4/u61yufb5egKI9rVFUAVsk1sEHU3Q57sqPKp46mDn/0DV7ZA9Mj59cDsjRsPY86IzA1j/3/D0zbD99863HUVS1wB48f8fadI3ggUTy5lUmKc3Fp4HH/kqvP8K7HwhYLO3L5pEaUEO//na+9Q3dZgAcKywA74zP3p228yPWZ+94v9Y+8k9L4IgcG4hoOLnX/7rz/T5L7krPudPFMqrnJ8B9PXAxp/q968/mFDFZqltAPKKYVzlwKZna3TA8Jb55w/dd+EKPX189YGA1Z45mel86crpbDnaRENLF6UmABwbdq+CkgtgzOSz24pn6PVwA8UBnJgB5MVRDqK7HTb9Qge/x0yK/fkTibIqaDlGVtdp59rc/TI018FlX9L1J+t+4FzbUSZqBkBEJojIahHZLSI7ReRea/tYEXldRPZZP8dEqw8+UUobgClLtE4IcKatm7/s1Zo+s0pKh+6fngE3/Ej7il/754DN31xVwdRiPYMwM4AY0NqgC8AGu39sZi7X1d7tfr7w7REogdrEUxF02zP6f3Ph3bE/d6JhSWSPatkXYMcQWP8YjJkCyx7UAfh1P4Cmo861H0WiOQPoBe5XSlUCC4EVIjIL+GfgDaXUdOAN6/fY0rgPWo8Pcf+sXHeIbqVz+POz8889ZvwcWHw/bH9Wrx3gh4z0NB64Ws8iJhflOddvg3f2vAIofbMfTuVyUH2w9zXfx9uzukjrACD2mUBK6RtQ6UUwcWFsz52IjL8QJI1RLfudaa92E9RuhIX/qGOGVz4MCPzlQWfajzJRMwBKqXql1GbrfQuwGygHPg48ae32JHBjtPrgk0Nr9E/LAHg6evjlug+YUaqzgbwtBwloA1A8U88C+vv8nuK6C0p54/4Ps2BKGOJihtDYs0q7fkpmn/tZWZWuAPWXDdTeCNn5kOFd7yko4qUIeuBNaHxfP/1bs1mDH7JyobiS/GaHZgDrH9P/O3P/Tv8+egJc9kXY8RwcWe/MOaJITBLURWQyMA/YAJQopepBGwkRGefjmLuAuwBKSkqorq4O69ytra3nHDt7x3OMyi5m/dbDIEd4YV83LV29FGedoqE7h3Vr1/lsr7j4Y8ze9R22P/9dThUtCHj+aEwEvY0pkYlkPOm9bVx2oJq68uUcWLPG6z7TRs2ldN/rrHvjNfrTz3XJVR7aRb7ksiGCv6n09/Jh4NDOGlqLxsfs+lyw7ZuMzBrD+lOFqCieM5n+585PK6WwZT3Vq1dHZDSzOxtZsPMF/b/3bs3A9rT+KhZkFdL97N3UzP+urk2JAWFdI6VUVF/ASKAG+IT1e9Owz88EamP+/PkqXFavXj10Q1+fUo9MVOqFu5VSSjW1d6s5D76m7nrqr+qrb31VXf37q/032Nuj1KOzlVp5Xdh9ipRzxpTgRDSebb9X6sF8pQ6/63ufA9V6n10vef/8yRuU+tmy8Ptg88gEpV55IHbX5+RePa7qb0f9VEn1P7fxCf13q98WWTuvP6TUQ6OVOn3o3M+2PqPPsfnpyM4RAt6uEbBJ+bm3RtU0iUgm8Bzwa6XU89bmEyJSan1eCjREsw/ncGK7DphZ7p+Vbx+ipbOXLy6bjqfL49v9Y5OeAQu+AIfXGWVBN7D7ZcgbBxWX+N5n0mWQM9q3G6jtVGQZQDa5hbF1AW14XAvfzb8zdudMBmZ9nJ6MkfDqP4UmGT6Y7nao+YVOOx6ceWZzwSe1wvAbD0NXS0TdjSbRzAIS4OfAbqXUo4M+egm4w3p/B/BitPrglYO2/38xno4eVq47xDWzS5hdVoCny+M9ADycqv8FWaPg3R9Ht68G//R06urLmdfrAJwv0jPg/Otg7x91zvZw2hsjqwGwyS2KXRC44wxs+Q1c8Cld+WwInrwiDpx3Jxx5JyiZF69se0ZfA1+ZVyJw7beh9QS89aj3fVxANGcAlwG3A1eIyBbrdT3wLeAqEdkHXGX9HjsOrYXC6ZBfNvD0f++yGQB4uj0UZAWYAQDk5MP8O3RhmKc2yh02+OTQGuhuPVvw5Y+Zy6HTo8XSBqOUvmk7MQPIK/Kfbuokm5+CnnZY+IXYnC/JOD5+GUxeDK9/HVpOhHbwkMyrRb73q5gPF/6tflA880FE/Y0W0cwCelspJUqpC5VSc63Xq0qpU0qpZUqp6dbPGH1j0E9/h9+BqR/G097DyrcPce3s8cwq00/9QbmAbBZ8Xv/c8HiUOmsIyO6XdQbGoHRen5x3hZZJHi4R3dUM/T2R1QDYxMoF1NcLG36qb2DjL4j++ZIREVj+fT2LfO0roR0bSubVlQ9qrbE/fz38vkaR1KoErtsMPW0wZQk/X3eIli7t+wcdDG/uamZ09ujg2ho9EWZ9HGqedLWPL2np79Pr/k6/emAxH79k5cK0ZbpmYLDf14kqYBtbETTagmB7VkFzrc49N4RP0TRY8oCeyb/vp05kOOsfg5ElMPumwPvml8Hl98Hul86dfbqA1DIAVv5/c8lCfjHs6b+9t51e1Rv8DABg0T36CfK9p6PRW4M/jqzXT9veqn99UfkxaKkfuiiIXbnrxAwgrwj6e0jv6wi8bySsf0wHHmdcG93zpAKX3QvFlVrssas18P4n98L+1+FDnwu+buTSe6BgIvwxcP1QrEkxA7AWxl/AE5s9Q57+4awSaH5WEEFgm4r5Wn54/X+77sImPXtWQXo2TLsy+GNmXANpGdp1ZDMwA3AoCAxk9ngib8sXdTVwdL3ORLNkzA0RkJEFN/yX1vJZ/c3A+4eTeZU5Aq56WGcghht0jhKpYwB6OuDoBromLD7n6R+0/x/8VAH7YtEKaDoy9KZiiC5K6ZTOqUshe1Twx40Yc3apSNtN44QOkI1lRLK6myNvyxfrH9cZaHNvi945Uo0Jl8CHPqtv7nU1vvdrPw1bfxte5tXsm2DipfDGv+tkBJeQOgbg6Abo6+bV1um0dPVy75XTh3wctgGw84BNSmjsOL4NPEdCc//YzFwOp/bDyff1707GAKxU0qjNAJrrtb963qd1JprBOZb9m/brv3Sv91RhiCzzSgSufUS7HNd+J7K+OkjqGICDa1CSzrd2jeW6OeOpLB36BfJ0WwYgmDTQwaSl62yA2o1wdKNTvTX4Y/DSj6FirxdgS0S3n4LMXB0kjpQBF1CUZgCbfg79vbDAaP47Tk4BXP8d7abx9jDX1wsbfxZZ5lXZXJh3m57FnToQWX8dInUMwKG11I+cxYmuzCG+f5vmLv2lDXkGAHo6nlMA7/4o0l4agmHPKp1/HY7bJr8MyuefXSTGqRoAGOhPVAxATwdsWqmN3tipzrdv0EkCM5dD9bf04vGD2fOylXkVoeT2Ff8GGTnwu9ugYU9kbTlAahiATg/q2GZebp7u9ekfInABgV6Cb/6dOg7g0oKPpOHUAWjY5V36OVhmLtfLAnpqnasCBj2TyMiJjgto++/1bMUUfkWX67+jEwVW3Tc0ndfW/J9xTWTtjyqBW56CtpPw06U6jTyO6winhgE4/C6i+qnuqfT69A/aAIzIGEFWehA55d645C7tltjwkwg6agiIt6UfQ6Vy0FKRbY3OZACB9vPmFjk/A1BKuw1K5mgXhCF65Jfp4q2Dq2Hbs3pbbY2OITqVeXXeFfCP63Tw+eUvwh8+E7fAcEoYgM69b9KlMimaudjr0z9YMhDhPP3bFJTD7E/oQFFHU/jtGPyze5Ve1COSpQ+LpkPR+XrG1u6QEJxNXqGzWUC9XVquoGGnLvwymv/R5+LPanHBP/2LFgrcYGn+z3Mw82rUeLj9BR183vUiPL5YLy4TY1LCADTvfoNN/TNYcbWXBUMsmrqaQg8AD2fRCq1Ns/mpyNoxeKfluA62Vwah/ROIyuVaFqTluDMpoDa5hc65gI69Bz/5MLzzQ6i6Ay68xZl2Df5JS4OP/UA/lf/PF6zMq9tDSzkO6jzpepGpO/+oZ3krr4G3vx++Qmk4XYjZmeJEd1sT49r30zhuITPH+06da+5qjmwGADpbXM1cAAAKoElEQVTKP3mxdgP5SiUzhI8duI3E/28z01oqsr/HORcQOOMC6u2G1f8BP1umpctv+4MuVkrPdKaPhsCUzNKLvO/7M6j+6GZeTVwAX3hLuzX/8iD8+ma9znUMSHoDcPTAVgAuWnKD3/1CEoLzx6IVOltgV2xVrlOCPa/oDJhxlZG3VTYP8iv0eydnAHkRGoATO+GJZbDm21pT/u53YfpVzvXPEDxLHtBLwM6+ybvmv5OMGA2ffFIL1B1+Bx67TIvORZmYLAkZL5rau8k9uZWOzFwmz7nc776ebk9oMhC+mH4NFE7TKaFzbo6Pz7a1QWvltBx3vu0xk2DqR4ITYHOSxv1aysMpP7iIfuLa+BNnYwC5hWT0dWjffShrDPf1wjs/gNWP6JvBLb8Or9DN4ByZOfD5tSAxktwQgYvvhAkL4A93wq9u0rOQK74WtdlfUhuAn799iJtkJ30Vi/SiID5QSjk3A0hL07nCr9wHR96FSZdG3qY/lNI5y0fW6wUujqzXla7RJGe0vjnNuRkmL/H7t40IT532v+58Xpfop2fpp2KnuPAWHa8pPt+5Nm13UlujTgwIhpN7ta+5rgZm3Qgf/Z6zsxJD+IRixJ2iZBb8w2odhH7nhzD7Rj1jjQJJbQD6m44yNe04VH7R734dvR309PcELwUdiItuhTf/XVcUOm0A+noZ2XIA1u/WBubIer3qEGitm4mLdMBw4iIYOwVwcgaitKT2zudh54taBTW3SMtiz/mE1jrxtzJXMLQ2aPfZjuf0+EAvvHHlw9ZUPILsn+FUzIevHou8z4Oxb9y/v0NLhucV6215xYNe1u+ZuVp/5o1vaMGwv1mpjarBkJWrA9GL7tFZa1EiqQ3AAzNOwi4CLhjS3B1BFbA3snJ1Ktlb39MVf3nFMHLc2RvAyHF6Hdu8Il1BbLs0+vv0DbDlmNZ9aanXKoXN9We3NddxcU+73n/0RC2INnGRfhXNcPZm5o0ZV+vXcms5xh3PaYGsTT+HUaX6CXbOzVBxcdCumoyeZqj5Jex4Hj54Swfdxs2Cj3xNG5bC86I3Hqf/XhMv5cS4JZRINxzbomcCXT6ygtIytLTDjOv0l31UibN9MSQ+Ubz5Q5IbgH/ftZJNFeXI+q/h70m4u78bCFEKOhAL79arBjXu10+y7acBLxV/6VnaKCiln+TVMFnptAx9Yx1VqqeG069mlyeHWdd+NngXQzTIzNFuoMrl0N0Ge1/TN/BNK3Xe9Mjx2pcdCKW49NR+Pe6x58Hi/6Nv+k4EeuNBXiG7Z91PydKlZ7f1dmlD0HZy0E/rVXpR/GJFhpQnqQ1A6chySru6yB0d+Aly3rh5zC+Z79zJ8wrhlkELxfT16qKjtgb9xW+1bwIN+r2Ivsnnl8KoMl2RmF+mXSzDnlIbqquZFc+b/3Cy8vRNbM7NOnd6z6s6g6GvK6jDa0fMZuL1X9IFXsl4I8zI1sbaTdfMYCDJDcDnbvgl06qrWTr4aSxepGfoKX6yT/NzCmDurfoVJAerq5lYelEUO2UwGLyR9HUABoPBYPCOMQAGg8GQohgDYDAYDCmKMQAGg8GQohgDYDAYDCmKMQAGg8GQohgDYDAYDCmKMQAGg8GQooiK44LEwSIiJ4HDYR5eBDQ62B03kGxjMuNxP8k2pmQbD3gf0ySlVLGvAxLCAESCiGxSSl0c7344SbKNyYzH/STbmJJtPBDemIwLyGAwGFIUYwAMBoMhRUkFA/DTeHcgCiTbmMx43E+yjSnZxgNhjCnpYwAGg8Fg8E4qzAAMBoPB4AVjAAwGgyFFSWoDICLXisj7IrJfRP453v2JFBH5QES2i8gWEdkU7/6Eg4isFJEGEdkxaNtYEXldRPZZP8fEs4+h4GM8D4lInXWdtojI9fHsYyiIyAQRWS0iu0Vkp4jca21P5Gvka0wJeZ1EJEdENorIVms8D1vbp4jIBusaPSMiWQHbStYYgIikA3uBq4Ba4K/ArUqpXXHtWASIyAfAxUqphC1gEZElQCvwlFJqjrXtP4HTSqlvWYZ6jFLqK/HsZ7D4GM9DQKtS6rvx7Fs4iEgpUKqU2iwio4Aa4Ebg70nca+RrTJ8iAa+TiAiQp5RqFZFM4G3gXuA+4Hml1O9E5HFgq1LqMX9tJfMM4BJgv1LqoFKqG/gd8PE49ynlUUqtBU4P2/xx4Enr/ZPoL2dC4GM8CYtSql4ptdl63wLsBspJ7Gvka0wJidK0Wr9mWi8FXAH8wdoe1DVKZgNQDhwd9HstCXzRLRTwZxGpEZG74t0ZBylRStWD/rIC4+LcHye4R0S2WS6ihHGXDEZEJgPzgA0kyTUaNiZI0OskIukisgVoAF4HDgBNSqlea5eg7nfJbADEy7ZE93ddppSqAq4DVljuB4P7eAw4D5gL1APfi293QkdERgLPAV9SSjXHuz9O4GVMCXudlFJ9Sqm5QAXa21HpbbdA7SSzAagFJgz6vQI4Fqe+OIJS6pj1swF4AX3hk4ETlp/W9tc2xLk/EaGUOmF9QfuBn5Fg18nyKz8H/Fop9by1OaGvkbcxJfp1AlBKNQHVwEJgtIhkWB8Fdb9LZgPwV2C6FRnPAv4WeCnOfQobEcmzAliISB5wNbDD/1EJw0vAHdb7O4AX49iXiLFvlBY3kUDXyQow/hzYrZR6dNBHCXuNfI0pUa+TiBSLyGjr/QjgSnRcYzXwN9ZuQV2jpM0CArDSur4PpAMrlVLfjHOXwkZEpqKf+gEygN8k4nhE5LfAUrR07QngQeB/gGeBicAR4JNKqYQIrPoYz1K0W0EBHwCft/3nbkdELgfeArYD/dbmr6J95ol6jXyN6VYS8DqJyIXoIG86+iH+WaXUN6x7xO+AscB7wKeVUl1+20pmA2AwGAwG3ySzC8hgMBgMfjAGwGAwGFIUYwAMBoMhRTEGwGAwGFIUYwAMBoMhRTEGwGCIAiKyVERWxbsfBoM/jAEwGAyGFMUYAENKIyKftrTVt4jITyyRrVYR+Z6IbBaRN0Sk2Np3roist8TDXrDFw0Rkmoj8xdJn3ywi51nNjxSRP4jIHhH5tVWRioh8S0R2We0klBSxIbkwBsCQsohIJXALWmRvLtAH3AbkAZst4b016OpegKeAryilLkRXldrbfw38WCl1EXApWlgMtOrkl4BZwFTgMhEZi5YdmG2183+jO0qDwTfGABhSmWXAfOCvlrTuMvSNuh94xtrnaeByESkARiul1ljbnwSWWPpM5UqpFwCUUp1KqXZrn41KqVpLbGwLMBloBjqBJ0TkE4C9r8EQc4wBMKQyAjyplJprvc5XSj3kZT9/eineZMdtBuuw9AEZll77JWhlyhuB10Lss8HgGMYAGFKZN4C/EZFxMLDu7ST098JWVfw74G2llAc4IyKLre23A2ssXflaEbnRaiNbRHJ9ndDSpC9QSr2Kdg/NjcbADIZgyAi8i8GQnCildonI19CrrKUBPcAKoA2YLSI1gAcdJwAtsfu4dYM/CNxpbb8d+ImIfMNq45N+TjsKeFFEctCzhy87PCyDIWiMGqjBMAwRaVVKjYx3PwyGaGNcQAaDwZCimBmAwWAwpChmBmAwGAwpijEABoPBkKIYA2AwGAwpijEABoPBkKIYA2AwGAwpyv8HQQgvoUpNqXYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "B_sel = 0\n",
    "\n",
    "# plt.plot(acc_test_arr_v1[0,B_sel,0,0:30],label='N=2, B='+str(B_array[B_sel]))\n",
    "plt.plot(acc_test_arr_K6_G1_N12[0,0,0,0:30],label='w/o Grouping, K=6, N=6, G=1' )\n",
    "# plt.plot(acc_test_arr_K6_G1_N12[1,0,0,0:30],label='w/o Grouping, K=6, N=6, G=1' )\n",
    "# plt.plot(acc_test_arr_K6_G1_N12[2,0,0,0:30],label='w/o Grouping, K=6, N=6, G=1' )\n",
    "# plt.plot(acc_test_arr_K6_G1_N12[3,0,0,0:30],label='w/o Grouping, K=6, N=6, G=1' )\n",
    "# plt.plot(acc_test_arr_K6_G1_N12[4,0,0,0:30],label='w/o Grouping, K=6, N=6, G=1' )\n",
    "plt.plot(acc_test_arr_K6_G1_N12[5,0,0,0:30],label='w/o Grouping, K=6, N=12, G=1' )\n",
    "# plt.plot(acc_test_arr_v1[2,B_sel,0,0:30],label='N=6, B='+str(B_array[B_sel]))\n",
    "# plt.plot(acc_test_arr_v1[3,B_sel,0,0:30],label='N=8, B='+str(B_array[B_sel]))\n",
    "plt.plot(acc_test_arr_K6_G3_N6[0,0:30],label='w/ Grouping,   K=6, N=6, G=3')\n",
    "# plt.plot(acc_test_arr_K6_G3_N12[0,0:30],label='w/ Grouping,   K=6, N=12, G=3')\n",
    "# plt.plot(acc_test_arr_K4_G4_N8[0,0:30],label='w/ Grouping,   K=4, N=8, G=4')\n",
    "# plt.plot(acc_test_arr_G3[0,0:30],label='Grouping, N=6, G=3, B='+str(B_array[B_sel]))\n",
    "\n",
    "# plt.plot(acc_test_arr_v1[4,B_sel,0,0:30],label='N=7, B='+str(B_array[B_sel]))\n",
    "# plt.plot(acc_test_arr_v1[5,B_sel,0,0:30],label='N=8, B='+str(B_array[B_sel]))\n",
    "\n",
    "# plt.plot(plot_acc_v2[2,sigma_sel,:],label='T=10')\n",
    "# plt.plot(plot_acc_v2[3,sigma_sel,:],label='T=11')\n",
    "\n",
    "# plt.plot(acc_test_arr_uncoded,'r',label='uncoded')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "# plt.title(title_name)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Test accuracy(%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
