{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Import basic libraries\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Import PyTorch\n",
    "import torch # import main library\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn # import modules\n",
    "from torch.autograd import Function # import Function to create custom activations\n",
    "from torch.nn.parameter import Parameter # import Parameter to create custom activations with learnable parameters\n",
    "from torch import optim # import optimizers for demonstrations\n",
    "import torch.nn.functional as F # import torch functions\n",
    "from torchvision import datasets, transforms # import transformations to use for demo\n",
    "from torch.nn.parameter import Parameter # import Parameter to create custom activations with learnable parameters\n",
    "\n",
    "from models.activ_func import *\n",
    "from models.Nets import *\n",
    "from models.test import test_img\n",
    "from models.Update import *\n",
    "\n",
    "\n",
    "from utils.sampling import mnist_iid, mnist_noniid, cifar_iid\n",
    "from utils.options import args_parser\n",
    "from models.Update import LocalUpdate\n",
    "from models.Nets import MLP, CNNMnist, CNNCifar, LeNet, CNNMnist2, CNNMnist3\n",
    "from models.Fed import FedAvg\n",
    "from models.Fed import FedQAvg, Quantization, Quantization_Finite, my_score, my_score_Finite\n",
    "from models.test import test_img\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from utils.functions import *\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "class my_argument:    \n",
    "    epochs    = 16    #\"rounds of training\"\n",
    "    num_users = 4  # \"number of users: K\"\n",
    "    frac      = 0.5 #\"the fraction of clients: C\"\n",
    "    local_ep  = 1 #\"the number of local epochs: E\"\n",
    "    local_bs  = 50  #\"local batch size: B\"\n",
    "    bs        = 50 #\"test batch size\"\n",
    "    lr        = 0.001 #\"learning rate\"\n",
    "    momentum  = 0.5 # \"SGD momentum (default: 0.5)\"\n",
    "    weight_decay = 5e-4\n",
    "    split     = 'user' # \"train-test split type, user or sample\"\n",
    "    opt='ADAM'\n",
    "    loss='Custom' # 'Custom' or 'Default'\n",
    "\n",
    "    # model arguments\n",
    "    model = 'cnn'\n",
    "    kernel_num=9 #, help='number of each kind of kernel')\n",
    "    kernel_sizes='3,4,5' #  help='comma-separated kernel size to use for convolution')\n",
    "    norm='batch_norm' #, help=\"batch_norm, layer_norm, or None\")\n",
    "    num_filters=32 #, help=\"number of filters for conv nets\")\n",
    "    max_pool='True' #help=\"Whether use max pooling rather than strided convolutions\")\n",
    "\n",
    "    # other arguments\n",
    "    dataset='cifar' #, help=\"name of dataset\")\n",
    "    iid=1\n",
    "    num_classes=10#, help=\"number of classes\")\n",
    "    num_channels=1#, help=\"number of channels of imges\")\n",
    "    gpu=1#, help=\"GPU ID, -1 for CPU\")\n",
    "    stopping_rounds=10#, help='rounds of early stopping')\n",
    "    verbose='False'#, help='verbose print')\n",
    "    seed=1#, help='random seed (default: 1)')\n",
    "    \n",
    "args = my_argument()\n",
    "\n",
    "args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)\n",
    "args.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(args.device)\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Normalize the test set same as training set without augmentation\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# trainset = torchvision.datasets.CIFAR10(\n",
    "#     root=\"./data/cifar\", train=True, download=True, transform=transform_train)\n",
    "# trainloader = torch.utils.data.DataLoader(\n",
    "#     trainset, batch_size=args.bs, shuffle=True, num_workers=2)\n",
    "\n",
    "# testset = torchvision.datasets.CIFAR10(\n",
    "#     root=\"./data/cifar\", train=False, download=True, transform=transform_test)\n",
    "# testloader = torch.utils.data.DataLoader(\n",
    "#     testset, batch_size=args.bs, shuffle=False, num_workers=2)\n",
    "\n",
    "dataset_train = datasets.CIFAR10('../data/cifar', train=True, download=True, transform=transform_train)\n",
    "dataset_test = datasets.CIFAR10('../data/cifar', train=False, download=True, transform=transform_test)\n",
    "if args.iid:\n",
    "    dict_users = cifar_iid(dataset_train, args.num_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, testloader, criterion):\n",
    "    test_loss = 0\n",
    "    accuracy = 0\n",
    "    for images, labels in testloader:\n",
    "\n",
    "#         images.resize_(images.shape[0], 3*32*32)\n",
    "\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        output = model.forward(images)\n",
    "        test_loss += criterion(output, labels).item()\n",
    "\n",
    "        ps = torch.exp(output)\n",
    "        equality = (labels.data == ps.max(dim=1)[1])\n",
    "        accuracy += equality.type(torch.FloatTensor).mean()\n",
    "\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Train LeNet with uncoded Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.9165 \n",
      "Accuracy: 3086/10000 (30.86%)\n",
      "\n",
      "Round   0, Average loss 2.052\n",
      "\n",
      "Test set: Average loss: 1.7166 \n",
      "Accuracy: 3844/10000 (38.44%)\n",
      "\n",
      "Round   1, Average loss 1.866\n",
      "\n",
      "Test set: Average loss: 1.6110 \n",
      "Accuracy: 4128/10000 (41.28%)\n",
      "\n",
      "Round   2, Average loss 1.749\n",
      "\n",
      "Test set: Average loss: 1.5467 \n",
      "Accuracy: 4440/10000 (44.40%)\n",
      "\n",
      "Round   3, Average loss 1.687\n",
      "\n",
      "Test set: Average loss: 1.4918 \n",
      "Accuracy: 4617/10000 (46.17%)\n",
      "\n",
      "Round   4, Average loss 1.633\n",
      "\n",
      "Test set: Average loss: 1.4462 \n",
      "Accuracy: 4749/10000 (47.49%)\n",
      "\n",
      "Round   5, Average loss 1.590\n",
      "\n",
      "Test set: Average loss: 1.4069 \n",
      "Accuracy: 4902/10000 (49.02%)\n",
      "\n",
      "Round   6, Average loss 1.555\n",
      "\n",
      "Test set: Average loss: 1.3776 \n",
      "Accuracy: 5043/10000 (50.43%)\n",
      "\n",
      "Round   7, Average loss 1.525\n",
      "\n",
      "Test set: Average loss: 1.3544 \n",
      "Accuracy: 5160/10000 (51.60%)\n",
      "\n",
      "Round   8, Average loss 1.495\n",
      "\n",
      "Test set: Average loss: 1.3273 \n",
      "Accuracy: 5259/10000 (52.59%)\n",
      "\n",
      "Round   9, Average loss 1.472\n",
      "\n",
      "Test set: Average loss: 1.3180 \n",
      "Accuracy: 5321/10000 (53.21%)\n",
      "\n",
      "Round  10, Average loss 1.455\n",
      "\n",
      "Test set: Average loss: 1.2940 \n",
      "Accuracy: 5439/10000 (54.39%)\n",
      "\n",
      "Round  11, Average loss 1.430\n",
      "\n",
      "Test set: Average loss: 1.2773 \n",
      "Accuracy: 5452/10000 (54.52%)\n",
      "\n",
      "Round  12, Average loss 1.413\n",
      "\n",
      "Test set: Average loss: 1.2589 \n",
      "Accuracy: 5518/10000 (55.18%)\n",
      "\n",
      "Round  13, Average loss 1.397\n",
      "\n",
      "Test set: Average loss: 1.2478 \n",
      "Accuracy: 5560/10000 (55.60%)\n",
      "\n",
      "Round  14, Average loss 1.387\n",
      "\n",
      "Test set: Average loss: 1.2330 \n",
      "Accuracy: 5619/10000 (56.19%)\n",
      "\n",
      "Round  15, Average loss 1.368\n",
      "\n",
      "Test set: Average loss: 1.2163 \n",
      "Accuracy: 5702/10000 (57.02%)\n",
      "\n",
      "Round  16, Average loss 1.353\n",
      "\n",
      "Test set: Average loss: 1.2089 \n",
      "Accuracy: 5701/10000 (57.01%)\n",
      "\n",
      "Round  17, Average loss 1.344\n",
      "\n",
      "Test set: Average loss: 1.2011 \n",
      "Accuracy: 5741/10000 (57.41%)\n",
      "\n",
      "Round  18, Average loss 1.330\n",
      "\n",
      "Test set: Average loss: 1.1903 \n",
      "Accuracy: 5750/10000 (57.50%)\n",
      "\n",
      "Round  19, Average loss 1.319\n",
      "\n",
      "Test set: Average loss: 1.1778 \n",
      "Accuracy: 5833/10000 (58.33%)\n",
      "\n",
      "Round  20, Average loss 1.305\n",
      "\n",
      "Test set: Average loss: 1.1689 \n",
      "Accuracy: 5838/10000 (58.38%)\n",
      "\n",
      "Round  21, Average loss 1.297\n",
      "\n",
      "Test set: Average loss: 1.1585 \n",
      "Accuracy: 5853/10000 (58.53%)\n",
      "\n",
      "Round  22, Average loss 1.287\n",
      "\n",
      "Test set: Average loss: 1.1536 \n",
      "Accuracy: 5880/10000 (58.80%)\n",
      "\n",
      "Round  23, Average loss 1.280\n",
      "\n",
      "Test set: Average loss: 1.1400 \n",
      "Accuracy: 5945/10000 (59.45%)\n",
      "\n",
      "Round  24, Average loss 1.271\n",
      "\n",
      "Test set: Average loss: 1.1354 \n",
      "Accuracy: 5923/10000 (59.23%)\n",
      "\n",
      "Round  25, Average loss 1.263\n",
      "\n",
      "Test set: Average loss: 1.1304 \n",
      "Accuracy: 5957/10000 (59.57%)\n",
      "\n",
      "Round  26, Average loss 1.251\n",
      "\n",
      "Test set: Average loss: 1.1270 \n",
      "Accuracy: 6017/10000 (60.17%)\n",
      "\n",
      "Round  27, Average loss 1.247\n",
      "\n",
      "Test set: Average loss: 1.1228 \n",
      "Accuracy: 6029/10000 (60.29%)\n",
      "\n",
      "Round  28, Average loss 1.237\n",
      "\n",
      "Test set: Average loss: 1.1083 \n",
      "Accuracy: 6099/10000 (60.99%)\n",
      "\n",
      "Round  29, Average loss 1.236\n",
      "\n",
      "Test set: Average loss: 1.1074 \n",
      "Accuracy: 6048/10000 (60.48%)\n",
      "\n",
      "Round  30, Average loss 1.226\n",
      "\n",
      "Test set: Average loss: 1.0931 \n",
      "Accuracy: 6126/10000 (61.26%)\n",
      "\n",
      "Round  31, Average loss 1.210\n",
      "\n",
      "Test set: Average loss: 1.0838 \n",
      "Accuracy: 6156/10000 (61.56%)\n",
      "\n",
      "Round  32, Average loss 1.212\n",
      "\n",
      "Test set: Average loss: 1.0819 \n",
      "Accuracy: 6156/10000 (61.56%)\n",
      "\n",
      "Round  33, Average loss 1.201\n",
      "\n",
      "Test set: Average loss: 1.0765 \n",
      "Accuracy: 6205/10000 (62.05%)\n",
      "\n",
      "Round  34, Average loss 1.198\n",
      "\n",
      "Test set: Average loss: 1.0676 \n",
      "Accuracy: 6209/10000 (62.09%)\n",
      "\n",
      "Round  35, Average loss 1.194\n",
      "\n",
      "Test set: Average loss: 1.0663 \n",
      "Accuracy: 6203/10000 (62.03%)\n",
      "\n",
      "Round  36, Average loss 1.186\n",
      "\n",
      "Test set: Average loss: 1.0665 \n",
      "Accuracy: 6216/10000 (62.16%)\n",
      "\n",
      "Round  37, Average loss 1.180\n",
      "\n",
      "Test set: Average loss: 1.0561 \n",
      "Accuracy: 6285/10000 (62.85%)\n",
      "\n",
      "Round  38, Average loss 1.178\n",
      "\n",
      "Test set: Average loss: 1.0548 \n",
      "Accuracy: 6287/10000 (62.87%)\n",
      "\n",
      "Round  39, Average loss 1.168\n",
      "\n",
      "Test set: Average loss: 1.0538 \n",
      "Accuracy: 6279/10000 (62.79%)\n",
      "\n",
      "Round  40, Average loss 1.171\n",
      "\n",
      "Test set: Average loss: 1.0471 \n",
      "Accuracy: 6294/10000 (62.94%)\n",
      "\n",
      "Round  41, Average loss 1.162\n",
      "\n",
      "Test set: Average loss: 1.0353 \n",
      "Accuracy: 6329/10000 (63.29%)\n",
      "\n",
      "Round  42, Average loss 1.153\n",
      "\n",
      "Test set: Average loss: 1.0290 \n",
      "Accuracy: 6359/10000 (63.59%)\n",
      "\n",
      "Round  43, Average loss 1.152\n",
      "\n",
      "Test set: Average loss: 1.0290 \n",
      "Accuracy: 6351/10000 (63.51%)\n",
      "\n",
      "Round  44, Average loss 1.142\n",
      "\n",
      "Test set: Average loss: 1.0384 \n",
      "Accuracy: 6280/10000 (62.80%)\n",
      "\n",
      "Round  45, Average loss 1.143\n",
      "\n",
      "Test set: Average loss: 1.0224 \n",
      "Accuracy: 6340/10000 (63.40%)\n",
      "\n",
      "Round  46, Average loss 1.132\n",
      "\n",
      "Test set: Average loss: 1.0188 \n",
      "Accuracy: 6385/10000 (63.85%)\n",
      "\n",
      "Round  47, Average loss 1.132\n",
      "\n",
      "Test set: Average loss: 1.0203 \n",
      "Accuracy: 6371/10000 (63.71%)\n",
      "\n",
      "Round  48, Average loss 1.127\n",
      "\n",
      "Test set: Average loss: 1.0202 \n",
      "Accuracy: 6355/10000 (63.55%)\n",
      "\n",
      "Round  49, Average loss 1.119\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "N_epochs = 50\n",
    "\n",
    "net_glob = CNNCifar(args=args)\n",
    "net_glob.cuda()\n",
    "\n",
    "acc_test_FedAvg = np.empty(N_epochs)\n",
    "loss_test_FedAvg = np.empty(N_epochs)\n",
    "\n",
    "for iter in range(N_epochs):\n",
    "    w_locals, loss_locals = [], []\n",
    "\n",
    "    m = args.num_users\n",
    "    \n",
    "    idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "    \n",
    "    for idx in idxs_users:\n",
    "        local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])\n",
    "        w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))\n",
    "        w_locals.append(copy.deepcopy(w))\n",
    "        loss_locals.append(copy.deepcopy(loss))\n",
    "        \n",
    "    # update global weights\n",
    "    w_glob = FedAvg(w_locals)\n",
    "\n",
    "    # copy weight to net_glob\n",
    "    net_glob.load_state_dict(w_glob)\n",
    "    \n",
    "    acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "\n",
    "    # print loss\n",
    "    loss_avg = sum(loss_locals) / len(loss_locals)\n",
    "    print('Round {:3d}, Average loss {:.3f}'.format(iter, loss_avg))\n",
    "#     loss_train.append(loss_avg)\n",
    "    \n",
    "    acc_test_FedAvg[iter] = acc_test\n",
    "    loss_test_FedAvg[iter] = loss_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. BACC without grouping, K=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1. K=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "class my_argument:    \n",
    "    epochs    = 16    #\"rounds of training\"\n",
    "    num_users = 2  # \"number of users: K\"\n",
    "    frac      = 0.5 #\"the fraction of clients: C\"\n",
    "    local_ep  = 5 #\"the number of local epochs: E\"\n",
    "    local_bs  = 50  #\"local batch size: B\"\n",
    "    bs        = 50 #\"test batch size\"\n",
    "    lr        = 0.001 #\"learning rate\"\n",
    "    momentum  = 0.5 # \"SGD momentum (default: 0.5)\"\n",
    "    weight_decay = 5e-4\n",
    "    split     = 'user' # \"train-test split type, user or sample\"\n",
    "    opt='ADAM'\n",
    "    loss='Custom' # 'Custom' or 'Default'\n",
    "\n",
    "    # model arguments\n",
    "    model = 'cnn'\n",
    "    kernel_num=9 #, help='number of each kind of kernel')\n",
    "    kernel_sizes='3,4,5' #  help='comma-separated kernel size to use for convolution')\n",
    "    norm='batch_norm' #, help=\"batch_norm, layer_norm, or None\")\n",
    "    num_filters=32 #, help=\"number of filters for conv nets\")\n",
    "    max_pool='True' #help=\"Whether use max pooling rather than strided convolutions\")\n",
    "\n",
    "    # other arguments\n",
    "    dataset='cifar' #, help=\"name of dataset\")\n",
    "    iid=1\n",
    "    num_classes=10#, help=\"number of classes\")\n",
    "    num_channels=1#, help=\"number of channels of imges\")\n",
    "    gpu=1#, help=\"GPU ID, -1 for CPU\")\n",
    "    stopping_rounds=10#, help='rounds of early stopping')\n",
    "    verbose='False'#, help='verbose print')\n",
    "    seed=1#, help='random seed (default: 1)')\n",
    "    \n",
    "args = my_argument()\n",
    "\n",
    "args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)\n",
    "args.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(args.device)\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Normalize the test set same as training set without augmentation\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# trainset = torchvision.datasets.CIFAR10(\n",
    "#     root=\"./data/cifar\", train=True, download=True, transform=transform_train)\n",
    "# trainloader = torch.utils.data.DataLoader(\n",
    "#     trainset, batch_size=args.bs, shuffle=True, num_workers=2)\n",
    "\n",
    "# testset = torchvision.datasets.CIFAR10(\n",
    "#     root=\"./data/cifar\", train=False, download=True, transform=transform_test)\n",
    "# testloader = torch.utils.data.DataLoader(\n",
    "#     testset, batch_size=args.bs, shuffle=False, num_workers=2)\n",
    "\n",
    "dataset_train = datasets.CIFAR10('../data/cifar', train=True, download=True, transform=transform_train)\n",
    "dataset_test = datasets.CIFAR10('../data/cifar', train=False, download=True, transform=transform_test)\n",
    "if args.iid:\n",
    "    dict_users = cifar_iid(dataset_train, args.num_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of X: (50000, 3072)\n",
      "size of Y: (50000, 10)\n",
      "(50000, 10)\n"
     ]
    }
   ],
   "source": [
    "encoding_input_array_np = np.empty((len(dataset_train),32*32*3))\n",
    "encoding_label_array_np = np.empty((len(dataset_train),args.num_classes))\n",
    "print(\"size of X:\" ,encoding_input_array_np.shape)\n",
    "print(\"size of Y:\" ,encoding_label_array_np.shape)\n",
    "\n",
    "Size_submatrices = int(50000/args.num_users)\n",
    "\n",
    "\n",
    "for i in range(args.num_users):\n",
    "    \n",
    "    stt_pos = i*Size_submatrices\n",
    "    end_pos = (i+1)*Size_submatrices\n",
    "#     print(i,stt_pos,end_pos)\n",
    "    Temp_train = DataLoader(DatasetSplit(dataset_train, dict_users[i]), batch_size=Size_submatrices, shuffle=True)\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(Temp_train):\n",
    "        \n",
    "        images_np = images.detach().cpu().numpy()\n",
    "        \n",
    "#         print(np.size(images_np))\n",
    "        encoding_input_array_np[stt_pos:end_pos,:] = np.reshape(images_np, (Size_submatrices,32*32*3))\n",
    "#         print(encoding_input_array_np[stt_pos:end_pos,:].shape)\n",
    "\n",
    "        onehot_labels = torch.nn.functional.one_hot(labels,num_classes=args.num_classes)\n",
    "        labels_np = onehot_labels.detach().cpu().numpy()\n",
    "#         print(labels_np.shape)\n",
    "        encoding_label_array_np[stt_pos:end_pos,:] = labels_np\n",
    "    \n",
    "print(np.shape(encoding_label_array_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "sigma = 0.5\n",
      "\n",
      "\n",
      "\n",
      "Learning Rate = 0.001\n",
      "\n",
      "\n",
      "\n",
      "z_array: [-0.81  0.81]\n",
      "0.8455629617601813\n",
      "0.8455629617601821\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 25000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 25000 \n",
      "\n",
      "(T, sigma)= 3 0.5 )  0 -th Trial!!\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.004366987546284993\n",
      "conv1.bias 0.0055085718631744385\n",
      "conv2.weight 0.0022214933236440024\n",
      "conv2.bias 0.00238136388361454\n",
      "fc1.weight 0.0008314328193664551\n",
      "fc1.bias 0.0008386549229423205\n",
      "fc2.weight 0.0027809090084499784\n",
      "fc2.bias 0.002993911149955931\n",
      "fc3.weight 0.004093751169386364\n",
      "fc3.bias 0.003163816034793854\n",
      "\n",
      "Test set: Average loss: 2.0214 \n",
      "Accuracy: 3013/10000 (30.13%)\n",
      "\n",
      "Round   0, Average loss 2.021 Test accuracy 30.130\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002171035607655843\n",
      "conv1.bias 0.0038141335050264993\n",
      "conv2.weight 0.0008994497855504354\n",
      "conv2.bias 0.0015949492808431387\n",
      "fc1.weight 0.00017204397916793822\n",
      "fc1.bias 0.000586057702700297\n",
      "fc2.weight 0.0002595631376145378\n",
      "fc2.bias 0.001172883879570734\n",
      "fc3.weight 0.0009284389870507377\n",
      "fc3.bias 0.0006216892041265964\n",
      "\n",
      "Test set: Average loss: 1.7578 \n",
      "Accuracy: 3591/10000 (35.91%)\n",
      "\n",
      "Round   1, Average loss 1.758 Test accuracy 35.910\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0017685356405046251\n",
      "conv1.bias 0.0029181468610962233\n",
      "conv2.weight 0.0012435391545295715\n",
      "conv2.bias 0.0025743411388248205\n",
      "fc1.weight 0.0003501101732254028\n",
      "fc1.bias 0.0016224513451258342\n",
      "fc2.weight 0.0005216581007790944\n",
      "fc2.bias 0.002381761336610431\n",
      "fc3.weight 0.0010970078763507661\n",
      "fc3.bias 0.00100667392835021\n",
      "\n",
      "Test set: Average loss: 1.7366 \n",
      "Accuracy: 3732/10000 (37.32%)\n",
      "\n",
      "Round   2, Average loss 1.737 Test accuracy 37.320\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001594033506181505\n",
      "conv1.bias 0.0024834335781633854\n",
      "conv2.weight 0.00124765674273173\n",
      "conv2.bias 0.003524081315845251\n",
      "fc1.weight 0.00041303038597106934\n",
      "fc1.bias 0.0025332818428675332\n",
      "fc2.weight 0.000725695632752918\n",
      "fc2.bias 0.0035889645417531333\n",
      "fc3.weight 0.0014643703188214983\n",
      "fc3.bias 0.0017111267894506454\n",
      "\n",
      "Test set: Average loss: 1.7479 \n",
      "Accuracy: 3765/10000 (37.65%)\n",
      "\n",
      "Round   3, Average loss 1.748 Test accuracy 37.650\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0015272637208302816\n",
      "conv1.bias 0.0029167747125029564\n",
      "conv2.weight 0.0012622195482254028\n",
      "conv2.bias 0.00402133259922266\n",
      "fc1.weight 0.0004453725814819336\n",
      "fc1.bias 0.0026522144675254823\n",
      "fc2.weight 0.0008303327219826834\n",
      "fc2.bias 0.003988254637945266\n",
      "fc3.weight 0.0017284203143346877\n",
      "fc3.bias 0.001712079904973507\n",
      "\n",
      "Test set: Average loss: 1.7562 \n",
      "Accuracy: 3766/10000 (37.66%)\n",
      "\n",
      "Round   4, Average loss 1.756 Test accuracy 37.660\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001527905199262831\n",
      "conv1.bias 0.003383288780848185\n",
      "conv2.weight 0.0012908487518628437\n",
      "conv2.bias 0.004348775837570429\n",
      "fc1.weight 0.0004669817686080933\n",
      "fc1.bias 0.0026368752121925356\n",
      "fc2.weight 0.0009131251819550045\n",
      "fc2.bias 0.00420285158214115\n",
      "fc3.weight 0.001956632307597569\n",
      "fc3.bias 0.001593610644340515\n",
      "\n",
      "Test set: Average loss: 1.7550 \n",
      "Accuracy: 3744/10000 (37.44%)\n",
      "\n",
      "Round   5, Average loss 1.755 Test accuracy 37.440\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001576632923550076\n",
      "conv1.bias 0.003589491049448649\n",
      "conv2.weight 0.0013266056776046753\n",
      "conv2.bias 0.004619571380317211\n",
      "fc1.weight 0.0004857176939646403\n",
      "fc1.bias 0.0025888750950495403\n",
      "fc2.weight 0.0009688155991690499\n",
      "fc2.bias 0.004285294030393873\n",
      "fc3.weight 0.00211772308463142\n",
      "fc3.bias 0.0015294577926397324\n",
      "\n",
      "Test set: Average loss: 1.7509 \n",
      "Accuracy: 3774/10000 (37.74%)\n",
      "\n",
      "Round   6, Average loss 1.751 Test accuracy 37.740\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0016635664304097493\n",
      "conv1.bias 0.0036207549273967743\n",
      "conv2.weight 0.0013451849420865376\n",
      "conv2.bias 0.0049483273178339005\n",
      "fc1.weight 0.0004982635974884034\n",
      "fc1.bias 0.002562383562326431\n",
      "fc2.weight 0.001014216835536654\n",
      "fc2.bias 0.004417137730689276\n",
      "fc3.weight 0.0022398863519941057\n",
      "fc3.bias 0.0013869166374206542\n",
      "\n",
      "Test set: Average loss: 1.7525 \n",
      "Accuracy: 3748/10000 (37.48%)\n",
      "\n",
      "Round   7, Average loss 1.752 Test accuracy 37.480\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0017564630508422852\n",
      "conv1.bias 0.00359133817255497\n",
      "conv2.weight 0.0013476631045341492\n",
      "conv2.bias 0.005065908655524254\n",
      "fc1.weight 0.0005111651420593262\n",
      "fc1.bias 0.002555641531944275\n",
      "fc2.weight 0.0010503242886255658\n",
      "fc2.bias 0.0043891143231164845\n",
      "fc3.weight 0.0023365327290126255\n",
      "fc3.bias 0.0012420179322361947\n",
      "\n",
      "Test set: Average loss: 1.7527 \n",
      "Accuracy: 3756/10000 (37.56%)\n",
      "\n",
      "Round   8, Average loss 1.753 Test accuracy 37.560\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018472691377003987\n",
      "conv1.bias 0.0035816744590799012\n",
      "conv2.weight 0.001353462338447571\n",
      "conv2.bias 0.004988327622413635\n",
      "fc1.weight 0.0005241969426472982\n",
      "fc1.bias 0.002517274767160416\n",
      "fc2.weight 0.0010858031492384653\n",
      "fc2.bias 0.004333500351224627\n",
      "fc3.weight 0.0024205497332981654\n",
      "fc3.bias 0.0010994777083396911\n",
      "\n",
      "Test set: Average loss: 1.7480 \n",
      "Accuracy: 3764/10000 (37.64%)\n",
      "\n",
      "Round   9, Average loss 1.748 Test accuracy 37.640\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018895951906840007\n",
      "conv1.bias 0.0033449511975049973\n",
      "conv2.weight 0.0013531096776326498\n",
      "conv2.bias 0.004826438147574663\n",
      "fc1.weight 0.0005318542718887329\n",
      "fc1.bias 0.002483265846967697\n",
      "fc2.weight 0.0011141551865471735\n",
      "fc2.bias 0.0041938526999382745\n",
      "fc3.weight 0.002479601474035354\n",
      "fc3.bias 0.0009825129061937332\n",
      "\n",
      "Test set: Average loss: 1.7482 \n",
      "Accuracy: 3759/10000 (37.59%)\n",
      "\n",
      "Round  10, Average loss 1.748 Test accuracy 37.590\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019020495149824354\n",
      "conv1.bias 0.0030880567307273545\n",
      "conv2.weight 0.0013479071855545045\n",
      "conv2.bias 0.004585424903780222\n",
      "fc1.weight 0.0005359160502751668\n",
      "fc1.bias 0.0024660443266232806\n",
      "fc2.weight 0.0011414404899354965\n",
      "fc2.bias 0.004151744147141774\n",
      "fc3.weight 0.002542496295202346\n",
      "fc3.bias 0.0009287738241255284\n",
      "\n",
      "Test set: Average loss: 1.7452 \n",
      "Accuracy: 3779/10000 (37.79%)\n",
      "\n",
      "Round  11, Average loss 1.745 Test accuracy 37.790\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018948343065049913\n",
      "conv1.bias 0.002846946008503437\n",
      "conv2.weight 0.001346943179766337\n",
      "conv2.bias 0.004503726027905941\n",
      "fc1.weight 0.0005390793085098267\n",
      "fc1.bias 0.002452901005744934\n",
      "fc2.weight 0.0011625076097155375\n",
      "fc2.bias 0.004153708971682049\n",
      "fc3.weight 0.0026004132770356676\n",
      "fc3.bias 0.0009303413331508637\n",
      "\n",
      "Test set: Average loss: 1.7436 \n",
      "Accuracy: 3778/10000 (37.78%)\n",
      "\n",
      "Round  12, Average loss 1.744 Test accuracy 37.780\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018994055853949652\n",
      "conv1.bias 0.0026062075048685074\n",
      "conv2.weight 0.0013499504327774048\n",
      "conv2.bias 0.004464201629161835\n",
      "fc1.weight 0.0005403698682785034\n",
      "fc1.bias 0.002506739894549052\n",
      "fc2.weight 0.0011752654635717\n",
      "fc2.bias 0.0041258686355182105\n",
      "fc3.weight 0.0026405692100524903\n",
      "fc3.bias 0.0009354540146887302\n",
      "\n",
      "Test set: Average loss: 1.7476 \n",
      "Accuracy: 3762/10000 (37.62%)\n",
      "\n",
      "Round  13, Average loss 1.748 Test accuracy 37.620\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001904649469587538\n",
      "conv1.bias 0.0024990447175999484\n",
      "conv2.weight 0.0013557609915733338\n",
      "conv2.bias 0.004369774833321571\n",
      "fc1.weight 0.000542697827021281\n",
      "fc1.bias 0.0025140898923079174\n",
      "fc2.weight 0.0011858357323540581\n",
      "fc2.bias 0.004088027846245538\n",
      "fc3.weight 0.0026806609971182687\n",
      "fc3.bias 0.0009254463948309422\n",
      "\n",
      "Test set: Average loss: 1.7323 \n",
      "Accuracy: 3829/10000 (38.29%)\n",
      "\n",
      "Round  14, Average loss 1.732 Test accuracy 38.290\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001917697853512234\n",
      "conv1.bias 0.0024128242706259093\n",
      "conv2.weight 0.0013583213090896606\n",
      "conv2.bias 0.004219615366309881\n",
      "fc1.weight 0.0005448334217071533\n",
      "fc1.bias 0.0025277085602283477\n",
      "fc2.weight 0.0011914622215997606\n",
      "fc2.bias 0.0040549032744907195\n",
      "fc3.weight 0.0026984586602165584\n",
      "fc3.bias 0.0009454312734305858\n",
      "\n",
      "Test set: Average loss: 1.7344 \n",
      "Accuracy: 3813/10000 (38.13%)\n",
      "\n",
      "Round  15, Average loss 1.734 Test accuracy 38.130\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019298243522644044\n",
      "conv1.bias 0.0023353065674503646\n",
      "conv2.weight 0.0013573547204335532\n",
      "conv2.bias 0.004294297657907009\n",
      "fc1.weight 0.0005476015408833821\n",
      "fc1.bias 0.002528522163629532\n",
      "fc2.weight 0.0012024604138873873\n",
      "fc2.bias 0.004107508630979629\n",
      "fc3.weight 0.0027156534649076915\n",
      "fc3.bias 0.0009426474571228027\n",
      "\n",
      "Test set: Average loss: 1.7348 \n",
      "Accuracy: 3837/10000 (38.37%)\n",
      "\n",
      "Round  16, Average loss 1.735 Test accuracy 38.370\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019466345840030246\n",
      "conv1.bias 0.0022428100928664207\n",
      "conv2.weight 0.0013507805267969767\n",
      "conv2.bias 0.004170775879174471\n",
      "fc1.weight 0.0005499945481618246\n",
      "fc1.bias 0.002551749845345815\n",
      "fc2.weight 0.0012096155257452102\n",
      "fc2.bias 0.004069577370371137\n",
      "fc3.weight 0.0027420492399306525\n",
      "fc3.bias 0.0009678330272436142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.7339 \n",
      "Accuracy: 3834/10000 (38.34%)\n",
      "\n",
      "Round  17, Average loss 1.734 Test accuracy 38.340\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019563819302452936\n",
      "conv1.bias 0.0022291202719012895\n",
      "conv2.weight 0.0013531843821207683\n",
      "conv2.bias 0.00409004045650363\n",
      "fc1.weight 0.0005512899955113729\n",
      "fc1.bias 0.002546579639116923\n",
      "fc2.weight 0.0012165427207946778\n",
      "fc2.bias 0.00404514620701472\n",
      "fc3.weight 0.0027579852512904578\n",
      "fc3.bias 0.0009629731066524982\n",
      "\n",
      "Test set: Average loss: 1.7308 \n",
      "Accuracy: 3840/10000 (38.40%)\n",
      "\n",
      "Round  18, Average loss 1.731 Test accuracy 38.400\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019772879282633463\n",
      "conv1.bias 0.0021917585593958697\n",
      "conv2.weight 0.0013523183266321817\n",
      "conv2.bias 0.0040431939996778965\n",
      "fc1.weight 0.0005551058053970336\n",
      "fc1.bias 0.0025339700281620027\n",
      "fc2.weight 0.001222804898307437\n",
      "fc2.bias 0.004040524008728209\n",
      "fc3.weight 0.002768877006712414\n",
      "fc3.bias 0.000952504575252533\n",
      "\n",
      "Test set: Average loss: 1.7356 \n",
      "Accuracy: 3828/10000 (38.28%)\n",
      "\n",
      "Round  19, Average loss 1.736 Test accuracy 38.280\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001991302304797702\n",
      "conv1.bias 0.0021560578607022762\n",
      "conv2.weight 0.0013548133770624796\n",
      "conv2.bias 0.004073428921401501\n",
      "fc1.weight 0.0005563540458679199\n",
      "fc1.bias 0.002491195499897003\n",
      "fc2.weight 0.0012254103781684997\n",
      "fc2.bias 0.004014668010530018\n",
      "fc3.weight 0.002783272379920596\n",
      "fc3.bias 0.0009481883607804775\n",
      "\n",
      "Test set: Average loss: 1.7300 \n",
      "Accuracy: 3842/10000 (38.42%)\n",
      "\n",
      "Round  20, Average loss 1.730 Test accuracy 38.420\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020023402902815076\n",
      "conv1.bias 0.0021169311366975307\n",
      "conv2.weight 0.0013481201728185019\n",
      "conv2.bias 0.003979494329541922\n",
      "fc1.weight 0.0005569180647532145\n",
      "fc1.bias 0.002456600467363993\n",
      "fc2.weight 0.0012274305971841962\n",
      "fc2.bias 0.003985346782775153\n",
      "fc3.weight 0.0027912168275742305\n",
      "fc3.bias 0.0009428843855857849\n",
      "\n",
      "Test set: Average loss: 1.7241 \n",
      "Accuracy: 3859/10000 (38.59%)\n",
      "\n",
      "Round  21, Average loss 1.724 Test accuracy 38.590\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020137793487972682\n",
      "conv1.bias 0.0021213099050025144\n",
      "conv2.weight 0.0013564215103785196\n",
      "conv2.bias 0.004025953821837902\n",
      "fc1.weight 0.0005597476959228516\n",
      "fc1.bias 0.002425443629423777\n",
      "fc2.weight 0.001230998266310919\n",
      "fc2.bias 0.003970827729929061\n",
      "fc3.weight 0.0028059837364015124\n",
      "fc3.bias 0.0009369997307658196\n",
      "\n",
      "Test set: Average loss: 1.7267 \n",
      "Accuracy: 3856/10000 (38.56%)\n",
      "\n",
      "Round  22, Average loss 1.727 Test accuracy 38.560\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020070587264166937\n",
      "conv1.bias 0.0020823885376254716\n",
      "conv2.weight 0.0013562619686126709\n",
      "conv2.bias 0.004054057411849499\n",
      "fc1.weight 0.0005610093673070271\n",
      "fc1.bias 0.0024137074748675027\n",
      "fc2.weight 0.0012349720985170396\n",
      "fc2.bias 0.004004587729771932\n",
      "fc3.weight 0.0028240760167439777\n",
      "fc3.bias 0.0009453663602471352\n",
      "\n",
      "Test set: Average loss: 1.7234 \n",
      "Accuracy: 3873/10000 (38.73%)\n",
      "\n",
      "Round  23, Average loss 1.723 Test accuracy 38.730\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020172925790150962\n",
      "conv1.bias 0.0021705180406570435\n",
      "conv2.weight 0.0013583320379257201\n",
      "conv2.bias 0.004008143674582243\n",
      "fc1.weight 0.0005618411699930826\n",
      "fc1.bias 0.0023871625463167826\n",
      "fc2.weight 0.0012409061666518923\n",
      "fc2.bias 0.003980350281511035\n",
      "fc3.weight 0.00285193437621707\n",
      "fc3.bias 0.0009509073570370674\n",
      "\n",
      "Test set: Average loss: 1.7246 \n",
      "Accuracy: 3876/10000 (38.76%)\n",
      "\n",
      "Round  24, Average loss 1.725 Test accuracy 38.760\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020242529445224336\n",
      "conv1.bias 0.0021714152147372565\n",
      "conv2.weight 0.001354930599530538\n",
      "conv2.bias 0.003996709361672401\n",
      "fc1.weight 0.0005627068678538005\n",
      "fc1.bias 0.002385296920935313\n",
      "fc2.weight 0.0012458237390669566\n",
      "fc2.bias 0.003961046891553062\n",
      "fc3.weight 0.0028581082820892333\n",
      "fc3.bias 0.0009325717575848103\n",
      "\n",
      "Test set: Average loss: 1.7154 \n",
      "Accuracy: 3899/10000 (38.99%)\n",
      "\n",
      "Round  25, Average loss 1.715 Test accuracy 38.990\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020353032482994925\n",
      "conv1.bias 0.002159056564172109\n",
      "conv2.weight 0.0013581409056981405\n",
      "conv2.bias 0.004027188755571842\n",
      "fc1.weight 0.0005651954809824625\n",
      "fc1.bias 0.0023914193113644916\n",
      "fc2.weight 0.001250831853775751\n",
      "fc2.bias 0.003958430673394885\n",
      "fc3.weight 0.0028666462217058456\n",
      "fc3.bias 0.0009429739788174629\n",
      "\n",
      "Test set: Average loss: 1.7131 \n",
      "Accuracy: 3898/10000 (38.98%)\n",
      "\n",
      "Round  26, Average loss 1.713 Test accuracy 38.980\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020313609970940482\n",
      "conv1.bias 0.0021573486737906933\n",
      "conv2.weight 0.0013576289017995199\n",
      "conv2.bias 0.004044031724333763\n",
      "fc1.weight 0.0005665273666381836\n",
      "fc1.bias 0.0023788765072822573\n",
      "fc2.weight 0.0012534290079086545\n",
      "fc2.bias 0.00393014330239523\n",
      "fc3.weight 0.0028730006445021857\n",
      "fc3.bias 0.000932210311293602\n",
      "\n",
      "Test set: Average loss: 1.7194 \n",
      "Accuracy: 3863/10000 (38.63%)\n",
      "\n",
      "Round  27, Average loss 1.719 Test accuracy 38.630\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002034757004843818\n",
      "conv1.bias 0.002189002310236295\n",
      "conv2.weight 0.0013588632146517435\n",
      "conv2.bias 0.00401326222345233\n",
      "fc1.weight 0.0005654228528340657\n",
      "fc1.bias 0.002357464532057444\n",
      "fc2.weight 0.0012508997841486854\n",
      "fc2.bias 0.0038873663260823206\n",
      "fc3.weight 0.002871348176683698\n",
      "fc3.bias 0.0009069811552762985\n",
      "\n",
      "Test set: Average loss: 1.7225 \n",
      "Accuracy: 3838/10000 (38.38%)\n",
      "\n",
      "Round  28, Average loss 1.722 Test accuracy 38.380\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020436655150519478\n",
      "conv1.bias 0.002192782238125801\n",
      "conv2.weight 0.0013600865999857585\n",
      "conv2.bias 0.003985267132520676\n",
      "fc1.weight 0.0005674264430999755\n",
      "fc1.bias 0.002365963657697042\n",
      "fc2.weight 0.001251260344944303\n",
      "fc2.bias 0.003928084813413166\n",
      "fc3.weight 0.002881504808153425\n",
      "fc3.bias 0.0009024782106280327\n",
      "\n",
      "Test set: Average loss: 1.7146 \n",
      "Accuracy: 3891/10000 (38.91%)\n",
      "\n",
      "Round  29, Average loss 1.715 Test accuracy 38.910\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020484344164530435\n",
      "conv1.bias 0.0022046250912050405\n",
      "conv2.weight 0.001360315978527069\n",
      "conv2.bias 0.004002021625638008\n",
      "fc1.weight 0.0005668281316757203\n",
      "fc1.bias 0.002359501272439957\n",
      "fc2.weight 0.0012528177291627914\n",
      "fc2.bias 0.003911043206850688\n",
      "fc3.weight 0.0028779540743146623\n",
      "fc3.bias 0.0009072870016098022\n",
      "\n",
      "Test set: Average loss: 1.7103 \n",
      "Accuracy: 3901/10000 (39.01%)\n",
      "\n",
      "Round  30, Average loss 1.710 Test accuracy 39.010\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002052943574057685\n",
      "conv1.bias 0.00217862194404006\n",
      "conv2.weight 0.0013643517096837362\n",
      "conv2.bias 0.003942788578569889\n",
      "fc1.weight 0.0005667986869812012\n",
      "fc1.bias 0.0023447131117184956\n",
      "fc2.weight 0.0012547069125705295\n",
      "fc2.bias 0.00386973683323179\n",
      "fc3.weight 0.0028808729989188058\n",
      "fc3.bias 0.0009011857211589814\n",
      "\n",
      "Test set: Average loss: 1.7145 \n",
      "Accuracy: 3883/10000 (38.83%)\n",
      "\n",
      "Round  31, Average loss 1.715 Test accuracy 38.830\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020509684085845948\n",
      "conv1.bias 0.0021536385950942836\n",
      "conv2.weight 0.0013646260897318522\n",
      "conv2.bias 0.004092940129339695\n",
      "fc1.weight 0.0005695620377858479\n",
      "fc1.bias 0.002346128225326538\n",
      "fc2.weight 0.0012573762545509944\n",
      "fc2.bias 0.003896507124106089\n",
      "fc3.weight 0.0028797524315970283\n",
      "fc3.bias 0.0008857947774231434\n",
      "\n",
      "Test set: Average loss: 1.7112 \n",
      "Accuracy: 3891/10000 (38.91%)\n",
      "\n",
      "Round  32, Average loss 1.711 Test accuracy 38.910\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020440125465393064\n",
      "conv1.bias 0.0021794835726420083\n",
      "conv2.weight 0.0013636267185211182\n",
      "conv2.bias 0.004163993522524834\n",
      "fc1.weight 0.0005698350270589193\n",
      "fc1.bias 0.002316826085249583\n",
      "fc2.weight 0.0012567101016877189\n",
      "fc2.bias 0.00391212637935366\n",
      "fc3.weight 0.0028936496802738736\n",
      "fc3.bias 0.000885245855897665\n",
      "\n",
      "Test set: Average loss: 1.7154 \n",
      "Accuracy: 3879/10000 (38.79%)\n",
      "\n",
      "Round  33, Average loss 1.715 Test accuracy 38.790\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002049391931957669\n",
      "conv1.bias 0.002160190294186274\n",
      "conv2.weight 0.0013633131980895995\n",
      "conv2.bias 0.004089043941348791\n",
      "fc1.weight 0.0005692430337270101\n",
      "fc1.bias 0.002339323361714681\n",
      "fc2.weight 0.00125404331419203\n",
      "fc2.bias 0.003923336664835612\n",
      "fc3.weight 0.0029005507628122964\n",
      "fc3.bias 0.0008854862302541733\n",
      "\n",
      "Test set: Average loss: 1.7110 \n",
      "Accuracy: 3897/10000 (38.97%)\n",
      "\n",
      "Round  34, Average loss 1.711 Test accuracy 38.970\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002046557929780748\n",
      "conv1.bias 0.0022075308176378408\n",
      "conv2.weight 0.0013666456937789916\n",
      "conv2.bias 0.004056364763528109\n",
      "fc1.weight 0.0005692644913991293\n",
      "fc1.bias 0.0023647581537564596\n",
      "fc2.weight 0.0012582153554946656\n",
      "fc2.bias 0.003895975649356842\n",
      "fc3.weight 0.002908919822602045\n",
      "fc3.bias 0.0008805539458990097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.7059 \n",
      "Accuracy: 3912/10000 (39.12%)\n",
      "\n",
      "Round  35, Average loss 1.706 Test accuracy 39.120\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020510254965888128\n",
      "conv1.bias 0.002177413087338209\n",
      "conv2.weight 0.00137042502562205\n",
      "conv2.bias 0.003957522101700306\n",
      "fc1.weight 0.0005705748001734415\n",
      "fc1.bias 0.002366486191749573\n",
      "fc2.weight 0.0012594682829720632\n",
      "fc2.bias 0.003924635904175895\n",
      "fc3.weight 0.0029270586513337636\n",
      "fc3.bias 0.0008876624517142773\n",
      "\n",
      "Test set: Average loss: 1.7109 \n",
      "Accuracy: 3902/10000 (39.02%)\n",
      "\n",
      "Round  36, Average loss 1.711 Test accuracy 39.020\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020521638128492567\n",
      "conv1.bias 0.002205974111954371\n",
      "conv2.weight 0.0013747690121332804\n",
      "conv2.bias 0.004045363515615463\n",
      "fc1.weight 0.0005721688667933146\n",
      "fc1.bias 0.0023689185579617816\n",
      "fc2.weight 0.0012641146069481259\n",
      "fc2.bias 0.003909517256986527\n",
      "fc3.weight 0.00292369354338873\n",
      "fc3.bias 0.0008919654414057731\n",
      "\n",
      "Test set: Average loss: 1.7154 \n",
      "Accuracy: 3892/10000 (38.92%)\n",
      "\n",
      "Round  37, Average loss 1.715 Test accuracy 38.920\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002061009407043457\n",
      "conv1.bias 0.002209762887408336\n",
      "conv2.weight 0.0013831320405006408\n",
      "conv2.bias 0.004024123307317495\n",
      "fc1.weight 0.000573344628016154\n",
      "fc1.bias 0.002362622817357381\n",
      "fc2.weight 0.0012688678408425951\n",
      "fc2.bias 0.003895907884552365\n",
      "fc3.weight 0.002918395541963123\n",
      "fc3.bias 0.0008615128695964813\n",
      "\n",
      "Test set: Average loss: 1.7112 \n",
      "Accuracy: 3911/10000 (39.11%)\n",
      "\n",
      "Round  38, Average loss 1.711 Test accuracy 39.110\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020749775568644204\n",
      "conv1.bias 0.0021885366489489875\n",
      "conv2.weight 0.0013856456677118938\n",
      "conv2.bias 0.004027429968118668\n",
      "fc1.weight 0.0005739842255910238\n",
      "fc1.bias 0.0023514082034428916\n",
      "fc2.weight 0.0012667532951112777\n",
      "fc2.bias 0.0038559986721901666\n",
      "fc3.weight 0.0029353136108035134\n",
      "fc3.bias 0.0008693010546267033\n",
      "\n",
      "Test set: Average loss: 1.7099 \n",
      "Accuracy: 3912/10000 (39.12%)\n",
      "\n",
      "Round  39, Average loss 1.710 Test accuracy 39.120\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002076398531595866\n",
      "conv1.bias 0.002210088539868593\n",
      "conv2.weight 0.0013900832335154215\n",
      "conv2.bias 0.003998930100351572\n",
      "fc1.weight 0.0005757517019907634\n",
      "fc1.bias 0.0023269911607106527\n",
      "fc2.weight 0.001270313679225861\n",
      "fc2.bias 0.003885833280427115\n",
      "fc3.weight 0.0029391180901300338\n",
      "fc3.bias 0.0008814921602606773\n",
      "\n",
      "Test set: Average loss: 1.7074 \n",
      "Accuracy: 3912/10000 (39.12%)\n",
      "\n",
      "Round  40, Average loss 1.707 Test accuracy 39.120\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020829786194695365\n",
      "conv1.bias 0.002206757664680481\n",
      "conv2.weight 0.0013922256231307982\n",
      "conv2.bias 0.0038877385668456554\n",
      "fc1.weight 0.0005765734910964966\n",
      "fc1.bias 0.0023448420067628223\n",
      "fc2.weight 0.0012735616593133835\n",
      "fc2.bias 0.0038737644042287555\n",
      "fc3.weight 0.0029484743163699194\n",
      "fc3.bias 0.0008991945534944535\n",
      "\n",
      "Test set: Average loss: 1.7053 \n",
      "Accuracy: 3922/10000 (39.22%)\n",
      "\n",
      "Round  41, Average loss 1.705 Test accuracy 39.220\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002070865101284451\n",
      "conv1.bias 0.0022583267030616603\n",
      "conv2.weight 0.001401437520980835\n",
      "conv2.bias 0.003946157172322273\n",
      "fc1.weight 0.0005771884520848592\n",
      "fc1.bias 0.002347719669342041\n",
      "fc2.weight 0.0012680784104362366\n",
      "fc2.bias 0.0038510199104036602\n",
      "fc3.weight 0.0029477809156690325\n",
      "fc3.bias 0.0008865835145115853\n",
      "\n",
      "Test set: Average loss: 1.7049 \n",
      "Accuracy: 3914/10000 (39.14%)\n",
      "\n",
      "Round  42, Average loss 1.705 Test accuracy 39.140\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020883112483554415\n",
      "conv1.bias 0.0023101638071238995\n",
      "conv2.weight 0.0014083019892374674\n",
      "conv2.bias 0.003999039530754089\n",
      "fc1.weight 0.0005767582257588704\n",
      "fc1.bias 0.002336120108763377\n",
      "fc2.weight 0.001268499427371555\n",
      "fc2.bias 0.0037762093402090528\n",
      "fc3.weight 0.002937610944112142\n",
      "fc3.bias 0.0008921552449464798\n",
      "\n",
      "Test set: Average loss: 1.7030 \n",
      "Accuracy: 3905/10000 (39.05%)\n",
      "\n",
      "Round  43, Average loss 1.703 Test accuracy 39.050\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002083991898430718\n",
      "conv1.bias 0.002318998333066702\n",
      "conv2.weight 0.001413570741812388\n",
      "conv2.bias 0.003976520616561174\n",
      "fc1.weight 0.0005783461729685466\n",
      "fc1.bias 0.002312083045641581\n",
      "fc2.weight 0.0012718619808318123\n",
      "fc2.bias 0.0038223000509398325\n",
      "fc3.weight 0.002935219094866798\n",
      "fc3.bias 0.0008852176368236541\n",
      "\n",
      "Test set: Average loss: 1.7052 \n",
      "Accuracy: 3926/10000 (39.26%)\n",
      "\n",
      "Round  44, Average loss 1.705 Test accuracy 39.260\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002089660962422689\n",
      "conv1.bias 0.002348055597394705\n",
      "conv2.weight 0.0014187908172607422\n",
      "conv2.bias 0.004052141215652227\n",
      "fc1.weight 0.0005786917606989542\n",
      "fc1.bias 0.002310648063818614\n",
      "fc2.weight 0.0012734223925878131\n",
      "fc2.bias 0.003863806880655743\n",
      "fc3.weight 0.00294808858916873\n",
      "fc3.bias 0.0008749101310968399\n",
      "\n",
      "Test set: Average loss: 1.7124 \n",
      "Accuracy: 3889/10000 (38.89%)\n",
      "\n",
      "Round  45, Average loss 1.712 Test accuracy 38.890\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020903674761454266\n",
      "conv1.bias 0.0023664234516521296\n",
      "conv2.weight 0.0014197605848312379\n",
      "conv2.bias 0.004036283120512962\n",
      "fc1.weight 0.0005800606807072957\n",
      "fc1.bias 0.002297834058602651\n",
      "fc2.weight 0.0012760996818542481\n",
      "fc2.bias 0.003818295896053314\n",
      "fc3.weight 0.002945762588864281\n",
      "fc3.bias 0.0008756468072533607\n",
      "\n",
      "Test set: Average loss: 1.7133 \n",
      "Accuracy: 3877/10000 (38.77%)\n",
      "\n",
      "Round  46, Average loss 1.713 Test accuracy 38.770\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002087161143620809\n",
      "conv1.bias 0.0023588452798624835\n",
      "conv2.weight 0.0014186813433965048\n",
      "conv2.bias 0.003997095860540867\n",
      "fc1.weight 0.0005790803829828898\n",
      "fc1.bias 0.0023196324706077575\n",
      "fc2.weight 0.0012752204660385375\n",
      "fc2.bias 0.003825768118812924\n",
      "fc3.weight 0.002942109675634475\n",
      "fc3.bias 0.0008775312453508377\n",
      "\n",
      "Test set: Average loss: 1.7120 \n",
      "Accuracy: 3889/10000 (38.89%)\n",
      "\n",
      "Round  47, Average loss 1.712 Test accuracy 38.890\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020897407001919215\n",
      "conv1.bias 0.0023783741829295955\n",
      "conv2.weight 0.0014200787742932638\n",
      "conv2.bias 0.003950006328523159\n",
      "fc1.weight 0.0005799030462900797\n",
      "fc1.bias 0.002325936903556188\n",
      "fc2.weight 0.0012758360022590274\n",
      "fc2.bias 0.0038285337033725922\n",
      "fc3.weight 0.0029338700430733815\n",
      "fc3.bias 0.000893632136285305\n",
      "\n",
      "Test set: Average loss: 1.7076 \n",
      "Accuracy: 3917/10000 (39.17%)\n",
      "\n",
      "Round  48, Average loss 1.708 Test accuracy 39.170\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020803871419694687\n",
      "conv1.bias 0.0023575246644516787\n",
      "conv2.weight 0.0014219474792480469\n",
      "conv2.bias 0.0038883602246642113\n",
      "fc1.weight 0.000581140915552775\n",
      "fc1.bias 0.0023154777785142264\n",
      "fc2.weight 0.0012774643443879627\n",
      "fc2.bias 0.0038399987277530486\n",
      "fc3.weight 0.0029572730972653345\n",
      "fc3.bias 0.0008963553234934806\n",
      "\n",
      "Test set: Average loss: 1.7090 \n",
      "Accuracy: 3887/10000 (38.87%)\n",
      "\n",
      "Round  49, Average loss 1.709 Test accuracy 38.870\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020809594790140787\n",
      "conv1.bias 0.002378285862505436\n",
      "conv2.weight 0.0014228814840316773\n",
      "conv2.bias 0.0038477021735161543\n",
      "fc1.weight 0.0005797566175460815\n",
      "fc1.bias 0.002332294980684916\n",
      "fc2.weight 0.0012757520827036055\n",
      "fc2.bias 0.003863619551772163\n",
      "fc3.weight 0.002947429815928141\n",
      "fc3.bias 0.000891212746500969\n",
      "\n",
      "Test set: Average loss: 1.7067 \n",
      "Accuracy: 3910/10000 (39.10%)\n",
      "\n",
      "Round  50, Average loss 1.707 Test accuracy 39.100\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020844652917650012\n",
      "conv1.bias 0.002399550440410773\n",
      "conv2.weight 0.0014301128188769024\n",
      "conv2.bias 0.0037437104620039463\n",
      "fc1.weight 0.0005803372065226237\n",
      "fc1.bias 0.002327564110358556\n",
      "fc2.weight 0.0012748472274295868\n",
      "fc2.bias 0.0038340091705322266\n",
      "fc3.weight 0.002946788924080985\n",
      "fc3.bias 0.0008994199335575104\n",
      "\n",
      "Test set: Average loss: 1.7099 \n",
      "Accuracy: 3896/10000 (38.96%)\n",
      "\n",
      "Round  51, Average loss 1.710 Test accuracy 38.960\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020835300286610923\n",
      "conv1.bias 0.0024062382678190866\n",
      "conv2.weight 0.0014330242077509562\n",
      "conv2.bias 0.003733249381184578\n",
      "fc1.weight 0.0005790571769078573\n",
      "fc1.bias 0.0023158460855484007\n",
      "fc2.weight 0.0012728917220282175\n",
      "fc2.bias 0.003863775659175146\n",
      "fc3.weight 0.0029329271543593635\n",
      "fc3.bias 0.0008949391543865204\n",
      "\n",
      "Test set: Average loss: 1.7086 \n",
      "Accuracy: 3903/10000 (39.03%)\n",
      "\n",
      "Round  52, Average loss 1.709 Test accuracy 39.030\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002089784410264757\n",
      "conv1.bias 0.0024031056091189384\n",
      "conv2.weight 0.001434353490670522\n",
      "conv2.bias 0.0036654206924140453\n",
      "fc1.weight 0.0005812345345815022\n",
      "fc1.bias 0.0022999987006187437\n",
      "fc2.weight 0.001274689795478942\n",
      "fc2.bias 0.003819144907451811\n",
      "fc3.weight 0.0029310084524608792\n",
      "fc3.bias 0.0008952497504651546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.7081 \n",
      "Accuracy: 3889/10000 (38.89%)\n",
      "\n",
      "Round  53, Average loss 1.708 Test accuracy 38.890\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020929645167456733\n",
      "conv1.bias 0.0023919129744172096\n",
      "conv2.weight 0.0014375823736190797\n",
      "conv2.bias 0.003740691114217043\n",
      "fc1.weight 0.0005825003782908122\n",
      "fc1.bias 0.0022688493132591247\n",
      "fc2.weight 0.0012722070254976786\n",
      "fc2.bias 0.0037682829868225824\n",
      "fc3.weight 0.0029299350011916386\n",
      "fc3.bias 0.0008932134136557579\n",
      "\n",
      "Test set: Average loss: 1.7062 \n",
      "Accuracy: 3887/10000 (38.87%)\n",
      "\n",
      "Round  54, Average loss 1.706 Test accuracy 38.870\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002100014289220174\n",
      "conv1.bias 0.0024458440020680428\n",
      "conv2.weight 0.001437098781267802\n",
      "conv2.bias 0.003685747506096959\n",
      "fc1.weight 0.000580927054087321\n",
      "fc1.bias 0.002281726896762848\n",
      "fc2.weight 0.001270983616511027\n",
      "fc2.bias 0.003776544261546362\n",
      "fc3.weight 0.002927487804776146\n",
      "fc3.bias 0.0008818531408905983\n",
      "\n",
      "Test set: Average loss: 1.7047 \n",
      "Accuracy: 3918/10000 (39.18%)\n",
      "\n",
      "Round  55, Average loss 1.705 Test accuracy 39.180\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002101186381445991\n",
      "conv1.bias 0.002390485412130753\n",
      "conv2.weight 0.0014410630861918133\n",
      "conv2.bias 0.0036709841806441545\n",
      "fc1.weight 0.0005823938449223836\n",
      "fc1.bias 0.0022824888428052267\n",
      "fc2.weight 0.0012722242446172806\n",
      "fc2.bias 0.003792641418320792\n",
      "fc3.weight 0.002933317706698463\n",
      "fc3.bias 0.0008786330930888653\n",
      "\n",
      "Test set: Average loss: 1.7095 \n",
      "Accuracy: 3892/10000 (38.92%)\n",
      "\n",
      "Round  56, Average loss 1.709 Test accuracy 38.920\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021010088920593263\n",
      "conv1.bias 0.002448948100209236\n",
      "conv2.weight 0.0014438140392303467\n",
      "conv2.bias 0.0036229619290679693\n",
      "fc1.weight 0.0005833612283070882\n",
      "fc1.bias 0.0022884507973988852\n",
      "fc2.weight 0.001275243361790975\n",
      "fc2.bias 0.0038066300607862927\n",
      "fc3.weight 0.0029463912759508403\n",
      "fc3.bias 0.0008805292658507824\n",
      "\n",
      "Test set: Average loss: 1.7147 \n",
      "Accuracy: 3887/10000 (38.87%)\n",
      "\n",
      "Round  57, Average loss 1.715 Test accuracy 38.870\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021059147516886392\n",
      "conv1.bias 0.002440463441113631\n",
      "conv2.weight 0.0014474188288052877\n",
      "conv2.bias 0.00352771719917655\n",
      "fc1.weight 0.000582451343536377\n",
      "fc1.bias 0.0022865218420823415\n",
      "fc2.weight 0.0012750324748811268\n",
      "fc2.bias 0.00380593573763257\n",
      "fc3.weight 0.002935703027816046\n",
      "fc3.bias 0.0008869797922670842\n",
      "\n",
      "Test set: Average loss: 1.7147 \n",
      "Accuracy: 3887/10000 (38.87%)\n",
      "\n",
      "Round  58, Average loss 1.715 Test accuracy 38.870\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002105081081390381\n",
      "conv1.bias 0.002379797709484895\n",
      "conv2.weight 0.0014457595348358154\n",
      "conv2.bias 0.003568413434550166\n",
      "fc1.weight 0.0005836565891901652\n",
      "fc1.bias 0.002311304211616516\n",
      "fc2.weight 0.0012745208210415311\n",
      "fc2.bias 0.0038066325443131582\n",
      "fc3.weight 0.0029327892121814546\n",
      "fc3.bias 0.0008950089104473591\n",
      "\n",
      "Test set: Average loss: 1.7154 \n",
      "Accuracy: 3870/10000 (38.70%)\n",
      "\n",
      "Round  59, Average loss 1.715 Test accuracy 38.700\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002117818461524116\n",
      "conv1.bias 0.002420008803407351\n",
      "conv2.weight 0.0014514474074045817\n",
      "conv2.bias 0.0035187257453799248\n",
      "fc1.weight 0.0005823496182759603\n",
      "fc1.bias 0.0023232921957969666\n",
      "fc2.weight 0.0012758260681515648\n",
      "fc2.bias 0.0038187908274786814\n",
      "fc3.weight 0.0029335495971498037\n",
      "fc3.bias 0.0009148884564638138\n",
      "\n",
      "Test set: Average loss: 1.7185 \n",
      "Accuracy: 3870/10000 (38.70%)\n",
      "\n",
      "Round  60, Average loss 1.718 Test accuracy 38.700\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002121053801642524\n",
      "conv1.bias 0.002407119454195102\n",
      "conv2.weight 0.0014489748080571492\n",
      "conv2.bias 0.0035518230870366096\n",
      "fc1.weight 0.000583450198173523\n",
      "fc1.bias 0.0023037632306416827\n",
      "fc2.weight 0.0012768856116703578\n",
      "fc2.bias 0.003849283570335025\n",
      "fc3.weight 0.002928272599265689\n",
      "fc3.bias 0.0008900425396859646\n",
      "\n",
      "Test set: Average loss: 1.7203 \n",
      "Accuracy: 3870/10000 (38.70%)\n",
      "\n",
      "Round  61, Average loss 1.720 Test accuracy 38.700\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021163467566172284\n",
      "conv1.bias 0.002370233957966169\n",
      "conv2.weight 0.0014525320132573445\n",
      "conv2.bias 0.003593419212847948\n",
      "fc1.weight 0.0005839519500732422\n",
      "fc1.bias 0.0022951290011405945\n",
      "fc2.weight 0.0012785992925129239\n",
      "fc2.bias 0.003830642927260626\n",
      "fc3.weight 0.0029335694653647285\n",
      "fc3.bias 0.0008713954128324985\n",
      "\n",
      "Test set: Average loss: 1.7215 \n",
      "Accuracy: 3852/10000 (38.52%)\n",
      "\n",
      "Round  62, Average loss 1.721 Test accuracy 38.520\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002101732359992133\n",
      "conv1.bias 0.002416855345169703\n",
      "conv2.weight 0.001452497939268748\n",
      "conv2.bias 0.003585269208997488\n",
      "fc1.weight 0.0005848017533620198\n",
      "fc1.bias 0.002293377121289571\n",
      "fc2.weight 0.0012797346190800744\n",
      "fc2.bias 0.0038251972624233793\n",
      "fc3.weight 0.0029331019946507047\n",
      "fc3.bias 0.0008729378692805767\n",
      "\n",
      "Test set: Average loss: 1.7212 \n",
      "Accuracy: 3864/10000 (38.64%)\n",
      "\n",
      "Round  63, Average loss 1.721 Test accuracy 38.640\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002110092904832628\n",
      "conv1.bias 0.0024040169082581997\n",
      "conv2.weight 0.0014547181129455566\n",
      "conv2.bias 0.00356512563303113\n",
      "fc1.weight 0.0005843699375788371\n",
      "fc1.bias 0.002295362701018651\n",
      "fc2.weight 0.0012820192745753697\n",
      "fc2.bias 0.0037748018900553384\n",
      "fc3.weight 0.0029308455330984934\n",
      "fc3.bias 0.0008823367767035961\n",
      "\n",
      "Test set: Average loss: 1.7169 \n",
      "Accuracy: 3865/10000 (38.65%)\n",
      "\n",
      "Round  64, Average loss 1.717 Test accuracy 38.650\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002108974324332343\n",
      "conv1.bias 0.0024441005662083626\n",
      "conv2.weight 0.001453100045522054\n",
      "conv2.bias 0.0035313903354108334\n",
      "fc1.weight 0.0005835914611816407\n",
      "fc1.bias 0.0022929539283116657\n",
      "fc2.weight 0.0012846983614422027\n",
      "fc2.bias 0.00377687102272397\n",
      "fc3.weight 0.002933242775145031\n",
      "fc3.bias 0.0008753609843552112\n",
      "\n",
      "Test set: Average loss: 1.7218 \n",
      "Accuracy: 3852/10000 (38.52%)\n",
      "\n",
      "Round  65, Average loss 1.722 Test accuracy 38.520\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021125135156843397\n",
      "conv1.bias 0.0024693465481201806\n",
      "conv2.weight 0.0014486571153004965\n",
      "conv2.bias 0.0035027037374675274\n",
      "fc1.weight 0.0005836596488952636\n",
      "fc1.bias 0.002298542857170105\n",
      "fc2.weight 0.0012798958354526095\n",
      "fc2.bias 0.0037862885565984818\n",
      "fc3.weight 0.0029280355998447965\n",
      "fc3.bias 0.0008915397338569164\n",
      "\n",
      "Test set: Average loss: 1.7166 \n",
      "Accuracy: 3864/10000 (38.64%)\n",
      "\n",
      "Round  66, Average loss 1.717 Test accuracy 38.640\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002115175459120009\n",
      "conv1.bias 0.00243403110653162\n",
      "conv2.weight 0.0014509338140487672\n",
      "conv2.bias 0.0035162074491381645\n",
      "fc1.weight 0.0005826468070348104\n",
      "fc1.bias 0.0022929991285006207\n",
      "fc2.weight 0.0012778668176560174\n",
      "fc2.bias 0.003773256071976253\n",
      "fc3.weight 0.0029341147059486028\n",
      "fc3.bias 0.0008871006779372692\n",
      "\n",
      "Test set: Average loss: 1.7120 \n",
      "Accuracy: 3900/10000 (39.00%)\n",
      "\n",
      "Round  67, Average loss 1.712 Test accuracy 39.000\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00211583760049608\n",
      "conv1.bias 0.0024037136075397334\n",
      "conv2.weight 0.0014548354347546895\n",
      "conv2.bias 0.0035015293397009373\n",
      "fc1.weight 0.0005822231769561768\n",
      "fc1.bias 0.002295309801896413\n",
      "fc2.weight 0.0012817987373897007\n",
      "fc2.bias 0.0038143091258548553\n",
      "fc3.weight 0.0029282439322698686\n",
      "fc3.bias 0.000890869740396738\n",
      "\n",
      "Test set: Average loss: 1.7168 \n",
      "Accuracy: 3873/10000 (38.73%)\n",
      "\n",
      "Round  68, Average loss 1.717 Test accuracy 38.730\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002117837005191379\n",
      "conv1.bias 0.0023602774987618127\n",
      "conv2.weight 0.0014492714405059815\n",
      "conv2.bias 0.0035274806432425976\n",
      "fc1.weight 0.0005831451018651326\n",
      "fc1.bias 0.0023023076355457306\n",
      "fc2.weight 0.0012839177298167395\n",
      "fc2.bias 0.003840371611572447\n",
      "fc3.weight 0.0029401742276691256\n",
      "fc3.bias 0.0008824885822832585\n",
      "\n",
      "Test set: Average loss: 1.7224 \n",
      "Accuracy: 3844/10000 (38.44%)\n",
      "\n",
      "Round  69, Average loss 1.722 Test accuracy 38.440\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002122061782413059\n",
      "conv1.bias 0.0023951812957723937\n",
      "conv2.weight 0.0014494876066843668\n",
      "conv2.bias 0.003512860741466284\n",
      "fc1.weight 0.0005827909310658773\n",
      "fc1.bias 0.002286237974961599\n",
      "fc2.weight 0.001282059389447409\n",
      "fc2.bias 0.0038243443483398074\n",
      "fc3.weight 0.002949905963171096\n",
      "fc3.bias 0.0008963994681835174\n",
      "\n",
      "Test set: Average loss: 1.7194 \n",
      "Accuracy: 3856/10000 (38.56%)\n",
      "\n",
      "Round  70, Average loss 1.719 Test accuracy 38.560\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002122236490249634\n",
      "conv1.bias 0.0023813072281579175\n",
      "conv2.weight 0.0014536869525909424\n",
      "conv2.bias 0.00354828336276114\n",
      "fc1.weight 0.0005818386475245158\n",
      "fc1.bias 0.0022916816174983977\n",
      "fc2.weight 0.0012810162135532925\n",
      "fc2.bias 0.003793015011719295\n",
      "fc3.weight 0.002951121614092872\n",
      "fc3.bias 0.0008625492453575134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.7131 \n",
      "Accuracy: 3890/10000 (38.90%)\n",
      "\n",
      "Round  71, Average loss 1.713 Test accuracy 38.900\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002133821513917711\n",
      "conv1.bias 0.0023391408224900565\n",
      "conv2.weight 0.0014522600173950196\n",
      "conv2.bias 0.003613634966313839\n",
      "fc1.weight 0.0005837910175323487\n",
      "fc1.bias 0.002310140679279963\n",
      "fc2.weight 0.001284454550061907\n",
      "fc2.bias 0.003833309170745668\n",
      "fc3.weight 0.0029410558087485175\n",
      "fc3.bias 0.0008589668199419975\n",
      "\n",
      "Test set: Average loss: 1.7083 \n",
      "Accuracy: 3908/10000 (39.08%)\n",
      "\n",
      "Round  72, Average loss 1.708 Test accuracy 39.080\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002131794558631049\n",
      "conv1.bias 0.0023114304058253765\n",
      "conv2.weight 0.0014461288849512736\n",
      "conv2.bias 0.0035413396544754505\n",
      "fc1.weight 0.0005818713903427124\n",
      "fc1.bias 0.002320575962464015\n",
      "fc2.weight 0.0012829800446828206\n",
      "fc2.bias 0.003794146790390923\n",
      "fc3.weight 0.002930882147380284\n",
      "fc3.bias 0.0008741372264921665\n",
      "\n",
      "Test set: Average loss: 1.7205 \n",
      "Accuracy: 3850/10000 (38.50%)\n",
      "\n",
      "Round  73, Average loss 1.720 Test accuracy 38.500\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002131298515531752\n",
      "conv1.bias 0.0023490645301838717\n",
      "conv2.weight 0.0014498156309127807\n",
      "conv2.bias 0.003500021528452635\n",
      "fc1.weight 0.0005812620321909587\n",
      "fc1.bias 0.0023037463426589964\n",
      "fc2.weight 0.0012816567269582598\n",
      "fc2.bias 0.0038073091279892694\n",
      "fc3.weight 0.002932030246371315\n",
      "fc3.bias 0.0008742962032556534\n",
      "\n",
      "Test set: Average loss: 1.7121 \n",
      "Accuracy: 3901/10000 (39.01%)\n",
      "\n",
      "Round  74, Average loss 1.712 Test accuracy 39.010\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021352214283413356\n",
      "conv1.bias 0.0023227528048058352\n",
      "conv2.weight 0.0014454907178878785\n",
      "conv2.bias 0.0034976594615727663\n",
      "fc1.weight 0.0005818188190460205\n",
      "fc1.bias 0.002307756741841634\n",
      "fc2.weight 0.0012774271624428886\n",
      "fc2.bias 0.003823750075839815\n",
      "fc3.weight 0.0029316470736549015\n",
      "fc3.bias 0.0008993602357804775\n",
      "\n",
      "Test set: Average loss: 1.7144 \n",
      "Accuracy: 3883/10000 (38.83%)\n",
      "\n",
      "Round  75, Average loss 1.714 Test accuracy 38.830\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002131389511956109\n",
      "conv1.bias 0.0023263508143524327\n",
      "conv2.weight 0.0014464002847671509\n",
      "conv2.bias 0.0034257443621754646\n",
      "fc1.weight 0.00058174200852712\n",
      "fc1.bias 0.0023110081752141316\n",
      "fc2.weight 0.001279397900142367\n",
      "fc2.bias 0.003847060813790276\n",
      "fc3.weight 0.0029310706115904307\n",
      "fc3.bias 0.0009073865599930287\n",
      "\n",
      "Test set: Average loss: 1.7180 \n",
      "Accuracy: 3884/10000 (38.84%)\n",
      "\n",
      "Round  76, Average loss 1.718 Test accuracy 38.840\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021366079648335776\n",
      "conv1.bias 0.0022717962662378945\n",
      "conv2.weight 0.0014464408159255981\n",
      "conv2.bias 0.0034354603849351406\n",
      "fc1.weight 0.0005817798773447673\n",
      "fc1.bias 0.0023280158638954163\n",
      "fc2.weight 0.0012800103142147972\n",
      "fc2.bias 0.0038634205148333593\n",
      "fc3.weight 0.0029438802174159458\n",
      "fc3.bias 0.0009070341475307941\n",
      "\n",
      "Test set: Average loss: 1.7234 \n",
      "Accuracy: 3850/10000 (38.50%)\n",
      "\n",
      "Round  77, Average loss 1.723 Test accuracy 38.500\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021241441037919786\n",
      "conv1.bias 0.0023043183609843254\n",
      "conv2.weight 0.001449888249238332\n",
      "conv2.bias 0.0034209429286420345\n",
      "fc1.weight 0.0005807703733444213\n",
      "fc1.bias 0.002330054094394048\n",
      "fc2.weight 0.001284571677919418\n",
      "fc2.bias 0.0038592002931095307\n",
      "fc3.weight 0.0029498015131269183\n",
      "fc3.bias 0.0009255409240722656\n",
      "\n",
      "Test set: Average loss: 1.7110 \n",
      "Accuracy: 3905/10000 (39.05%)\n",
      "\n",
      "Round  78, Average loss 1.711 Test accuracy 39.050\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021230032708909777\n",
      "conv1.bias 0.002316249224046866\n",
      "conv2.weight 0.001451762318611145\n",
      "conv2.bias 0.0034018163569271564\n",
      "fc1.weight 0.000582489291826884\n",
      "fc1.bias 0.0023225612938404085\n",
      "fc2.weight 0.0012858998207818894\n",
      "fc2.bias 0.0038889210139002118\n",
      "fc3.weight 0.0029496760595412483\n",
      "fc3.bias 0.0009230093099176883\n",
      "\n",
      "Test set: Average loss: 1.7165 \n",
      "Accuracy: 3887/10000 (38.87%)\n",
      "\n",
      "Round  79, Average loss 1.717 Test accuracy 38.870\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00211842934290568\n",
      "conv1.bias 0.002314772456884384\n",
      "conv2.weight 0.0014522363742192586\n",
      "conv2.bias 0.00347082014195621\n",
      "fc1.weight 0.000580912709236145\n",
      "fc1.bias 0.002313006669282913\n",
      "fc2.weight 0.0012841731782943483\n",
      "fc2.bias 0.0038610572616259256\n",
      "fc3.weight 0.002940893740881057\n",
      "fc3.bias 0.0009138699620962143\n",
      "\n",
      "Test set: Average loss: 1.7159 \n",
      "Accuracy: 3881/10000 (38.81%)\n",
      "\n",
      "Round  80, Average loss 1.716 Test accuracy 38.810\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002125174601872762\n",
      "conv1.bias 0.002311969641596079\n",
      "conv2.weight 0.0014448132117589314\n",
      "conv2.bias 0.0034836677368730307\n",
      "fc1.weight 0.0005807963609695435\n",
      "fc1.bias 0.002289620041847229\n",
      "fc2.weight 0.00127889665346297\n",
      "fc2.bias 0.003859390460309528\n",
      "fc3.weight 0.0029395557585216703\n",
      "fc3.bias 0.0009016184136271477\n",
      "\n",
      "Test set: Average loss: 1.7110 \n",
      "Accuracy: 3914/10000 (39.14%)\n",
      "\n",
      "Round  81, Average loss 1.711 Test accuracy 39.140\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021308257844713\n",
      "conv1.bias 0.00231229808802406\n",
      "conv2.weight 0.0014421047767003378\n",
      "conv2.bias 0.003489807015284896\n",
      "fc1.weight 0.0005800047318140666\n",
      "fc1.bias 0.0023165817062060037\n",
      "fc2.weight 0.0012752285079350547\n",
      "fc2.bias 0.0038760570543152945\n",
      "fc3.weight 0.002949469430106027\n",
      "fc3.bias 0.0009069649502635002\n",
      "\n",
      "Test set: Average loss: 1.7082 \n",
      "Accuracy: 3910/10000 (39.10%)\n",
      "\n",
      "Round  82, Average loss 1.708 Test accuracy 39.100\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021489820215437145\n",
      "conv1.bias 0.00223516703893741\n",
      "conv2.weight 0.0014415967464447021\n",
      "conv2.bias 0.003445111447945237\n",
      "fc1.weight 0.0005804155667622884\n",
      "fc1.bias 0.0022903313239415485\n",
      "fc2.weight 0.001279490523868137\n",
      "fc2.bias 0.0038864726112002417\n",
      "fc3.weight 0.002949352775301252\n",
      "fc3.bias 0.0008947986178100109\n",
      "\n",
      "Test set: Average loss: 1.7086 \n",
      "Accuracy: 3920/10000 (39.20%)\n",
      "\n",
      "Round  83, Average loss 1.709 Test accuracy 39.200\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002153625620736016\n",
      "conv1.bias 0.0023014162046213946\n",
      "conv2.weight 0.001447293758392334\n",
      "conv2.bias 0.0034594216849654913\n",
      "fc1.weight 0.0005796817938486735\n",
      "fc1.bias 0.0023124488691488904\n",
      "fc2.weight 0.001273360328068809\n",
      "fc2.bias 0.0039057426509403045\n",
      "fc3.weight 0.0029439176831926616\n",
      "fc3.bias 0.0008955667726695538\n",
      "\n",
      "Test set: Average loss: 1.7089 \n",
      "Accuracy: 3910/10000 (39.10%)\n",
      "\n",
      "Round  84, Average loss 1.709 Test accuracy 39.100\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021588977177937826\n",
      "conv1.bias 0.002241794175157944\n",
      "conv2.weight 0.0014506311217943827\n",
      "conv2.bias 0.003408128395676613\n",
      "fc1.weight 0.0005807864268620809\n",
      "fc1.bias 0.0022987926999727885\n",
      "fc2.weight 0.00126998538062686\n",
      "fc2.bias 0.003910998148577554\n",
      "fc3.weight 0.002938531693958101\n",
      "fc3.bias 0.0009052210487425327\n",
      "\n",
      "Test set: Average loss: 1.7071 \n",
      "Accuracy: 3909/10000 (39.09%)\n",
      "\n",
      "Round  85, Average loss 1.707 Test accuracy 39.090\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021344123946295842\n",
      "conv1.bias 0.002233883055547873\n",
      "conv2.weight 0.001448531448841095\n",
      "conv2.bias 0.0034664988052099943\n",
      "fc1.weight 0.0005821224848429362\n",
      "fc1.bias 0.0022960451742013296\n",
      "fc2.weight 0.0012770810770610021\n",
      "fc2.bias 0.003900041537625449\n",
      "fc3.weight 0.0029376103764488584\n",
      "fc3.bias 0.0009023456834256649\n",
      "\n",
      "Test set: Average loss: 1.7104 \n",
      "Accuracy: 3899/10000 (38.99%)\n",
      "\n",
      "Round  86, Average loss 1.710 Test accuracy 38.990\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021398644977145727\n",
      "conv1.bias 0.0022731280575195947\n",
      "conv2.weight 0.0014527912934621176\n",
      "conv2.bias 0.003461424494162202\n",
      "fc1.weight 0.0005827295780181885\n",
      "fc1.bias 0.002296982705593109\n",
      "fc2.weight 0.0012749802498590378\n",
      "fc2.bias 0.003890958925088247\n",
      "fc3.weight 0.0029307288782937186\n",
      "fc3.bias 0.0008938849903643131\n",
      "\n",
      "Test set: Average loss: 1.7083 \n",
      "Accuracy: 3903/10000 (39.03%)\n",
      "\n",
      "Round  87, Average loss 1.708 Test accuracy 39.030\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021361471547020807\n",
      "conv1.bias 0.00221929078300794\n",
      "conv2.weight 0.0014526899655659994\n",
      "conv2.bias 0.003500821068882942\n",
      "fc1.weight 0.0005834407409032186\n",
      "fc1.bias 0.0022999197244644165\n",
      "fc2.weight 0.0012782023066566105\n",
      "fc2.bias 0.003886736219837552\n",
      "fc3.weight 0.0029320035661969867\n",
      "fc3.bias 0.0009131154045462608\n",
      "\n",
      "Test set: Average loss: 1.7097 \n",
      "Accuracy: 3892/10000 (38.92%)\n",
      "\n",
      "Round  88, Average loss 1.710 Test accuracy 38.920\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021365753809611003\n",
      "conv1.bias 0.0021996195428073406\n",
      "conv2.weight 0.0014524281024932862\n",
      "conv2.bias 0.0035272934474051\n",
      "fc1.weight 0.0005832734902699788\n",
      "fc1.bias 0.0023088976740837097\n",
      "fc2.weight 0.0012780615261622837\n",
      "fc2.bias 0.0038763933948108126\n",
      "fc3.weight 0.0029371641931079683\n",
      "fc3.bias 0.0009223303757607936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.7093 \n",
      "Accuracy: 3892/10000 (38.92%)\n",
      "\n",
      "Round  89, Average loss 1.709 Test accuracy 38.920\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021352238125271266\n",
      "conv1.bias 0.002238313046594461\n",
      "conv2.weight 0.0014557466904322307\n",
      "conv2.bias 0.003504600841552019\n",
      "fc1.weight 0.0005829542477925619\n",
      "fc1.bias 0.002313791712125142\n",
      "fc2.weight 0.0012761804792616102\n",
      "fc2.bias 0.003866598719642276\n",
      "fc3.weight 0.002933174655551002\n",
      "fc3.bias 0.0009151562117040157\n",
      "\n",
      "Test set: Average loss: 1.7027 \n",
      "Accuracy: 3931/10000 (39.31%)\n",
      "\n",
      "Round  90, Average loss 1.703 Test accuracy 39.310\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002132092581854926\n",
      "conv1.bias 0.002236731660862764\n",
      "conv2.weight 0.0014534964164098104\n",
      "conv2.bias 0.0034685225691646338\n",
      "fc1.weight 0.0005822425683339437\n",
      "fc1.bias 0.0023082623879114787\n",
      "fc2.weight 0.0012749350260174463\n",
      "fc2.bias 0.0038638008492333548\n",
      "fc3.weight 0.002918436981382824\n",
      "fc3.bias 0.0009331073611974716\n",
      "\n",
      "Test set: Average loss: 1.7045 \n",
      "Accuracy: 3916/10000 (39.16%)\n",
      "\n",
      "Round  91, Average loss 1.705 Test accuracy 39.160\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00214356263478597\n",
      "conv1.bias 0.00218182522803545\n",
      "conv2.weight 0.001455237865447998\n",
      "conv2.bias 0.0034821596927940845\n",
      "fc1.weight 0.0005831921497980754\n",
      "fc1.bias 0.0023181902865568797\n",
      "fc2.weight 0.0012763572117638967\n",
      "fc2.bias 0.0038824893888973052\n",
      "fc3.weight 0.0029138976619357154\n",
      "fc3.bias 0.0009248520247638226\n",
      "\n",
      "Test set: Average loss: 1.7046 \n",
      "Accuracy: 3918/10000 (39.18%)\n",
      "\n",
      "Round  92, Average loss 1.705 Test accuracy 39.180\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021330240037706165\n",
      "conv1.bias 0.002193779218941927\n",
      "conv2.weight 0.0014563598235448202\n",
      "conv2.bias 0.003504208754748106\n",
      "fc1.weight 0.0005828134218851725\n",
      "fc1.bias 0.0023109808564186094\n",
      "fc2.weight 0.0012786613570319283\n",
      "fc2.bias 0.0038695321196601505\n",
      "fc3.weight 0.0029302625429062615\n",
      "fc3.bias 0.0009225235320627689\n",
      "\n",
      "Test set: Average loss: 1.7095 \n",
      "Accuracy: 3910/10000 (39.10%)\n",
      "\n",
      "Round  93, Average loss 1.710 Test accuracy 39.100\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021349154578314887\n",
      "conv1.bias 0.0021718613182504973\n",
      "conv2.weight 0.0014581072330474853\n",
      "conv2.bias 0.0034581543877720833\n",
      "fc1.weight 0.0005823529163996379\n",
      "fc1.bias 0.0023314692080020905\n",
      "fc2.weight 0.001275869021340022\n",
      "fc2.bias 0.0038766353612854367\n",
      "fc3.weight 0.0029325780414399647\n",
      "fc3.bias 0.000920686312019825\n",
      "\n",
      "Test set: Average loss: 1.7059 \n",
      "Accuracy: 3927/10000 (39.27%)\n",
      "\n",
      "Round  94, Average loss 1.706 Test accuracy 39.270\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021386856502956814\n",
      "conv1.bias 0.0022029243409633636\n",
      "conv2.weight 0.0014593121409416199\n",
      "conv2.bias 0.003516937606036663\n",
      "fc1.weight 0.000582935651143392\n",
      "fc1.bias 0.002338043600320816\n",
      "fc2.weight 0.0012781695714072577\n",
      "fc2.bias 0.0038613269016856237\n",
      "fc3.weight 0.002923602717263358\n",
      "fc3.bias 0.0009150738827884197\n",
      "\n",
      "Test set: Average loss: 1.7057 \n",
      "Accuracy: 3923/10000 (39.23%)\n",
      "\n",
      "Round  95, Average loss 1.706 Test accuracy 39.230\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002138935989803738\n",
      "conv1.bias 0.0021698096146186194\n",
      "conv2.weight 0.0014584862192471822\n",
      "conv2.bias 0.0035199811682105064\n",
      "fc1.weight 0.0005821619033813477\n",
      "fc1.bias 0.0023268950482209522\n",
      "fc2.weight 0.0012765664902944414\n",
      "fc2.bias 0.0038674094137691314\n",
      "fc3.weight 0.00291958905401684\n",
      "fc3.bias 0.0008990755304694176\n",
      "\n",
      "Test set: Average loss: 1.7047 \n",
      "Accuracy: 3942/10000 (39.42%)\n",
      "\n",
      "Round  96, Average loss 1.705 Test accuracy 39.420\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021491644117567275\n",
      "conv1.bias 0.0021928343921899796\n",
      "conv2.weight 0.0014579576253890992\n",
      "conv2.bias 0.003524273633956909\n",
      "fc1.weight 0.0005824366013209026\n",
      "fc1.bias 0.002333266039689382\n",
      "fc2.weight 0.001278168341470143\n",
      "fc2.bias 0.003853529691696167\n",
      "fc3.weight 0.0029150173777625675\n",
      "fc3.bias 0.0009038503281772137\n",
      "\n",
      "Test set: Average loss: 1.7082 \n",
      "Accuracy: 3921/10000 (39.21%)\n",
      "\n",
      "Round  97, Average loss 1.708 Test accuracy 39.210\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002154785527123345\n",
      "conv1.bias 0.002198013166586558\n",
      "conv2.weight 0.0014592736959457397\n",
      "conv2.bias 0.003428530180826783\n",
      "fc1.weight 0.0005819265842437744\n",
      "fc1.bias 0.002334673951069514\n",
      "fc2.weight 0.0012708638395581926\n",
      "fc2.bias 0.0038430680121694294\n",
      "fc3.weight 0.002908561627070109\n",
      "fc3.bias 0.0009115532040596009\n",
      "\n",
      "Test set: Average loss: 1.7072 \n",
      "Accuracy: 3932/10000 (39.32%)\n",
      "\n",
      "Round  98, Average loss 1.707 Test accuracy 39.320\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021598870224422877\n",
      "conv1.bias 0.0020928517915308475\n",
      "conv2.weight 0.0014649397134780884\n",
      "conv2.bias 0.0034395819529891014\n",
      "fc1.weight 0.0005828368663787841\n",
      "fc1.bias 0.0023461724321047466\n",
      "fc2.weight 0.0012763614692385234\n",
      "fc2.bias 0.0038498097232409884\n",
      "fc3.weight 0.0029036950497400194\n",
      "fc3.bias 0.0009163189679384232\n",
      "\n",
      "Test set: Average loss: 1.7073 \n",
      "Accuracy: 3922/10000 (39.22%)\n",
      "\n",
      "Round  99, Average loss 1.707 Test accuracy 39.220\n",
      "\n",
      "\n",
      "\n",
      "sigma = 1\n",
      "\n",
      "\n",
      "\n",
      "Learning Rate = 0.001\n",
      "\n",
      "\n",
      "\n",
      "z_array: [-0.81  0.81]\n",
      "0.4838626198316927\n",
      "0.48386261983169315\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 25000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 25000 \n",
      "\n",
      "(T, sigma)= 3 1 )  0 -th Trial!!\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.004226364559597439\n",
      "conv1.bias 0.005275756120681763\n",
      "conv2.weight 0.002128365437189738\n",
      "conv2.bias 0.0020038359798491\n",
      "fc1.weight 0.0008349548975626628\n",
      "fc1.bias 0.0009029436856508255\n",
      "fc2.weight 0.0027843800802079457\n",
      "fc2.bias 0.0029873940206709363\n",
      "fc3.weight 0.004005933659417289\n",
      "fc3.bias 0.003887316957116127\n",
      "\n",
      "Test set: Average loss: 2.0057 \n",
      "Accuracy: 3019/10000 (30.19%)\n",
      "\n",
      "Round   0, Average loss 2.006 Test accuracy 30.190\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00224659177992079\n",
      "conv1.bias 0.003325102229913076\n",
      "conv2.weight 0.0009837069114049276\n",
      "conv2.bias 0.0015287944115698338\n",
      "fc1.weight 0.00018455318609873455\n",
      "fc1.bias 0.0005242458234230677\n",
      "fc2.weight 0.00025795537327963206\n",
      "fc2.bias 0.0009335749560878391\n",
      "fc3.weight 0.0009222654359681265\n",
      "fc3.bias 0.0007086679339408875\n",
      "\n",
      "Test set: Average loss: 1.8369 \n",
      "Accuracy: 3384/10000 (33.84%)\n",
      "\n",
      "Round   1, Average loss 1.837 Test accuracy 33.840\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019002736939324272\n",
      "conv1.bias 0.0032394727071126304\n",
      "conv2.weight 0.0011448524395624798\n",
      "conv2.bias 0.002356940880417824\n",
      "fc1.weight 0.00035265461603800456\n",
      "fc1.bias 0.0009324346979459127\n",
      "fc2.weight 0.0005327038348667205\n",
      "fc2.bias 0.0016291161023435138\n",
      "fc3.weight 0.0011031006063733782\n",
      "fc3.bias 0.0005636516027152539\n",
      "\n",
      "Test set: Average loss: 1.8138 \n",
      "Accuracy: 3458/10000 (34.58%)\n",
      "\n",
      "Round   2, Average loss 1.814 Test accuracy 34.580\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0016735004054175483\n",
      "conv1.bias 0.0028313851604859033\n",
      "conv2.weight 0.00114315927028656\n",
      "conv2.bias 0.0025747332256287336\n",
      "fc1.weight 0.0004005439281463623\n",
      "fc1.bias 0.0012592201431592305\n",
      "fc2.weight 0.0007085118028852675\n",
      "fc2.bias 0.0023701462362493786\n",
      "fc3.weight 0.0014064252376556397\n",
      "fc3.bias 0.0008806796744465828\n",
      "\n",
      "Test set: Average loss: 1.7886 \n",
      "Accuracy: 3499/10000 (34.99%)\n",
      "\n",
      "Round   3, Average loss 1.789 Test accuracy 34.990\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001644305255677965\n",
      "conv1.bias 0.002710651606321335\n",
      "conv2.weight 0.0011365760366121927\n",
      "conv2.bias 0.002559248125180602\n",
      "fc1.weight 0.00042418094476064046\n",
      "fc1.bias 0.0015457302331924438\n",
      "fc2.weight 0.0007933015861208477\n",
      "fc2.bias 0.0028232245572975706\n",
      "fc3.weight 0.001621693798473903\n",
      "fc3.bias 0.0010473178699612617\n",
      "\n",
      "Test set: Average loss: 1.7722 \n",
      "Accuracy: 3589/10000 (35.89%)\n",
      "\n",
      "Round   4, Average loss 1.772 Test accuracy 35.890\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001671446164449056\n",
      "conv1.bias 0.002650989219546318\n",
      "conv2.weight 0.0011242886384328207\n",
      "conv2.bias 0.0025526578538119793\n",
      "fc1.weight 0.00043804872035980226\n",
      "fc1.bias 0.0017598234117031097\n",
      "fc2.weight 0.000845642884572347\n",
      "fc2.bias 0.0031317107024646943\n",
      "fc3.weight 0.0017881227391106742\n",
      "fc3.bias 0.0011190499179065227\n",
      "\n",
      "Test set: Average loss: 1.7634 \n",
      "Accuracy: 3642/10000 (36.42%)\n",
      "\n",
      "Round   5, Average loss 1.763 Test accuracy 36.420\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0016986378033955892\n",
      "conv1.bias 0.002635079436004162\n",
      "conv2.weight 0.0011201183994611105\n",
      "conv2.bias 0.002533235587179661\n",
      "fc1.weight 0.0004491128524144491\n",
      "fc1.bias 0.0019143469631671905\n",
      "fc2.weight 0.00088343412157089\n",
      "fc2.bias 0.003416944117773147\n",
      "fc3.weight 0.0019167934145246234\n",
      "fc3.bias 0.0011628461070358753\n",
      "\n",
      "Test set: Average loss: 1.7540 \n",
      "Accuracy: 3695/10000 (36.95%)\n",
      "\n",
      "Round   6, Average loss 1.754 Test accuracy 36.950\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0017376832167307536\n",
      "conv1.bias 0.002616208977997303\n",
      "conv2.weight 0.0011294559637705485\n",
      "conv2.bias 0.0025196843780577183\n",
      "fc1.weight 0.00045800964037577313\n",
      "fc1.bias 0.00197160542011261\n",
      "fc2.weight 0.0009079086402105906\n",
      "fc2.bias 0.003599364487897782\n",
      "fc3.weight 0.0020140891983395533\n",
      "fc3.bias 0.0011634173803031444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.7441 \n",
      "Accuracy: 3711/10000 (37.11%)\n",
      "\n",
      "Round   7, Average loss 1.744 Test accuracy 37.110\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0017637541559007432\n",
      "conv1.bias 0.0025863153859972954\n",
      "conv2.weight 0.0011454413334528605\n",
      "conv2.bias 0.0024683354422450066\n",
      "fc1.weight 0.0004665757417678833\n",
      "fc1.bias 0.0019966435929139456\n",
      "fc2.weight 0.0009258602346692767\n",
      "fc2.bias 0.0037676415273121427\n",
      "fc3.weight 0.0020876311120532807\n",
      "fc3.bias 0.0011540954932570458\n",
      "\n",
      "Test set: Average loss: 1.7376 \n",
      "Accuracy: 3737/10000 (37.37%)\n",
      "\n",
      "Round   8, Average loss 1.738 Test accuracy 37.370\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001773740980360243\n",
      "conv1.bias 0.002559525271256765\n",
      "conv2.weight 0.0011582829554875691\n",
      "conv2.bias 0.0024569721426814795\n",
      "fc1.weight 0.0004705623388290405\n",
      "fc1.bias 0.0019995879381895067\n",
      "fc2.weight 0.0009441008643498496\n",
      "fc2.bias 0.0038206960473741803\n",
      "fc3.weight 0.002150210596266247\n",
      "fc3.bias 0.0011238781735301018\n",
      "\n",
      "Test set: Average loss: 1.7353 \n",
      "Accuracy: 3735/10000 (37.35%)\n",
      "\n",
      "Round   9, Average loss 1.735 Test accuracy 37.350\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0017985422081417508\n",
      "conv1.bias 0.002513216808438301\n",
      "conv2.weight 0.0011705239613850912\n",
      "conv2.bias 0.0025019655004143715\n",
      "fc1.weight 0.0004745002190272013\n",
      "fc1.bias 0.0019877498348553975\n",
      "fc2.weight 0.0009621272011408731\n",
      "fc2.bias 0.003838783573536646\n",
      "fc3.weight 0.0022080263921192716\n",
      "fc3.bias 0.0010905343107879162\n",
      "\n",
      "Test set: Average loss: 1.7343 \n",
      "Accuracy: 3749/10000 (37.49%)\n",
      "\n",
      "Round  10, Average loss 1.734 Test accuracy 37.490\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018209240171644422\n",
      "conv1.bias 0.002498619258403778\n",
      "conv2.weight 0.0011786675453186034\n",
      "conv2.bias 0.0024965032935142517\n",
      "fc1.weight 0.0004761651357014974\n",
      "fc1.bias 0.001995423932870229\n",
      "fc2.weight 0.0009778934811788891\n",
      "fc2.bias 0.003886354466279348\n",
      "fc3.weight 0.0022629868416559128\n",
      "fc3.bias 0.0010638076812028885\n",
      "\n",
      "Test set: Average loss: 1.7309 \n",
      "Accuracy: 3767/10000 (37.67%)\n",
      "\n",
      "Round  11, Average loss 1.731 Test accuracy 37.670\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018298647138807508\n",
      "conv1.bias 0.0024493920306364694\n",
      "conv2.weight 0.0011870165665944417\n",
      "conv2.bias 0.0025314479134976864\n",
      "fc1.weight 0.0004786703189214071\n",
      "fc1.bias 0.00200072539349397\n",
      "fc2.weight 0.0009874640003083245\n",
      "fc2.bias 0.003927276602813176\n",
      "fc3.weight 0.0022953331470489504\n",
      "fc3.bias 0.001038244366645813\n",
      "\n",
      "Test set: Average loss: 1.7296 \n",
      "Accuracy: 3765/10000 (37.65%)\n",
      "\n",
      "Round  12, Average loss 1.730 Test accuracy 37.650\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018495239151848687\n",
      "conv1.bias 0.0024120045515398183\n",
      "conv2.weight 0.00119470477104187\n",
      "conv2.bias 0.0025404999032616615\n",
      "fc1.weight 0.0004820582469304403\n",
      "fc1.bias 0.0019953630864620207\n",
      "fc2.weight 0.000996159277265034\n",
      "fc2.bias 0.0038840845227241516\n",
      "fc3.weight 0.0023236911921274096\n",
      "fc3.bias 0.0010121410712599754\n",
      "\n",
      "Test set: Average loss: 1.7289 \n",
      "Accuracy: 3766/10000 (37.66%)\n",
      "\n",
      "Round  13, Average loss 1.729 Test accuracy 37.660\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018594947126176623\n",
      "conv1.bias 0.0023638991018136344\n",
      "conv2.weight 0.001198817491531372\n",
      "conv2.bias 0.002512415638193488\n",
      "fc1.weight 0.00048489701747894286\n",
      "fc1.bias 0.001983115077018738\n",
      "fc2.weight 0.0010068331445966447\n",
      "fc2.bias 0.0038524798694111054\n",
      "fc3.weight 0.002344082650684175\n",
      "fc3.bias 0.0010217983275651933\n",
      "\n",
      "Test set: Average loss: 1.7343 \n",
      "Accuracy: 3742/10000 (37.42%)\n",
      "\n",
      "Round  14, Average loss 1.734 Test accuracy 37.420\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001873984071943495\n",
      "conv1.bias 0.002361282085378965\n",
      "conv2.weight 0.001200196146965027\n",
      "conv2.bias 0.002510122722014785\n",
      "fc1.weight 0.00048647157351175944\n",
      "fc1.bias 0.0019675145546595254\n",
      "fc2.weight 0.0010120092876373776\n",
      "fc2.bias 0.0038202451098532904\n",
      "fc3.weight 0.0023551830223628454\n",
      "fc3.bias 0.0009917968884110451\n",
      "\n",
      "Test set: Average loss: 1.7355 \n",
      "Accuracy: 3747/10000 (37.47%)\n",
      "\n",
      "Round  15, Average loss 1.736 Test accuracy 37.470\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018839783138699003\n",
      "conv1.bias 0.0022772389153639474\n",
      "conv2.weight 0.001202829678853353\n",
      "conv2.bias 0.0025709779001772404\n",
      "fc1.weight 0.0004895724455515544\n",
      "fc1.bias 0.001959144324064255\n",
      "fc2.weight 0.001020518560258169\n",
      "fc2.bias 0.0038098951890355066\n",
      "fc3.weight 0.002371915749141148\n",
      "fc3.bias 0.000980081409215927\n",
      "\n",
      "Test set: Average loss: 1.7267 \n",
      "Accuracy: 3767/10000 (37.67%)\n",
      "\n",
      "Round  16, Average loss 1.727 Test accuracy 37.670\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018835557831658257\n",
      "conv1.bias 0.00232706218957901\n",
      "conv2.weight 0.0012057505051294962\n",
      "conv2.bias 0.0025613775942474604\n",
      "fc1.weight 0.0004914460976918538\n",
      "fc1.bias 0.0019571825861930845\n",
      "fc2.weight 0.001023573534829276\n",
      "fc2.bias 0.0038065406538191297\n",
      "fc3.weight 0.0023831026894705638\n",
      "fc3.bias 0.000977059453725815\n",
      "\n",
      "Test set: Average loss: 1.7312 \n",
      "Accuracy: 3753/10000 (37.53%)\n",
      "\n",
      "Round  17, Average loss 1.731 Test accuracy 37.530\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018911328580644396\n",
      "conv1.bias 0.0022450704127550125\n",
      "conv2.weight 0.0012107877930005392\n",
      "conv2.bias 0.0025926693342626095\n",
      "fc1.weight 0.0004928088188171386\n",
      "fc1.bias 0.0019365298251310984\n",
      "fc2.weight 0.0010282674479106116\n",
      "fc2.bias 0.0038232519513084775\n",
      "fc3.weight 0.0023965858277820405\n",
      "fc3.bias 0.0009581443853676319\n",
      "\n",
      "Test set: Average loss: 1.7371 \n",
      "Accuracy: 3743/10000 (37.43%)\n",
      "\n",
      "Round  18, Average loss 1.737 Test accuracy 37.430\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001890678670671251\n",
      "conv1.bias 0.002352373984952768\n",
      "conv2.weight 0.0012133344014485677\n",
      "conv2.bias 0.0025889384560287\n",
      "fc1.weight 0.000494186004002889\n",
      "fc1.bias 0.0019079737365245819\n",
      "fc2.weight 0.0010305874877505833\n",
      "fc2.bias 0.0038153351772399176\n",
      "fc3.weight 0.0024002120608375187\n",
      "fc3.bias 0.0009260560385882854\n",
      "\n",
      "Test set: Average loss: 1.7294 \n",
      "Accuracy: 3789/10000 (37.89%)\n",
      "\n",
      "Round  19, Average loss 1.729 Test accuracy 37.890\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018904389275444878\n",
      "conv1.bias 0.002302844232569138\n",
      "conv2.weight 0.0012121212482452393\n",
      "conv2.bias 0.002575171645730734\n",
      "fc1.weight 0.0004937784671783447\n",
      "fc1.bias 0.0019002857307593027\n",
      "fc2.weight 0.0010346959507654583\n",
      "fc2.bias 0.0037487816242944626\n",
      "fc3.weight 0.0024081894329616\n",
      "fc3.bias 0.0009282730519771576\n",
      "\n",
      "Test set: Average loss: 1.7281 \n",
      "Accuracy: 3777/10000 (37.77%)\n",
      "\n",
      "Round  20, Average loss 1.728 Test accuracy 37.770\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018954580359988743\n",
      "conv1.bias 0.0023503207291165986\n",
      "conv2.weight 0.001213287115097046\n",
      "conv2.bias 0.002602183260023594\n",
      "fc1.weight 0.0004945623477300009\n",
      "fc1.bias 0.0019198844830195108\n",
      "fc2.weight 0.001036823552752298\n",
      "fc2.bias 0.00369240627402351\n",
      "fc3.weight 0.002412079061780657\n",
      "fc3.bias 0.0009045377373695374\n",
      "\n",
      "Test set: Average loss: 1.7273 \n",
      "Accuracy: 3786/10000 (37.86%)\n",
      "\n",
      "Round  21, Average loss 1.727 Test accuracy 37.860\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019062901867760553\n",
      "conv1.bias 0.00234885032599171\n",
      "conv2.weight 0.0012167636553446451\n",
      "conv2.bias 0.0025505227968096733\n",
      "fc1.weight 0.0004958702723185222\n",
      "fc1.bias 0.0019126292318105698\n",
      "fc2.weight 0.00103718430276901\n",
      "fc2.bias 0.003705779001826332\n",
      "fc3.weight 0.002419577609925043\n",
      "fc3.bias 0.0009065208956599236\n",
      "\n",
      "Test set: Average loss: 1.7321 \n",
      "Accuracy: 3768/10000 (37.68%)\n",
      "\n",
      "Round  22, Average loss 1.732 Test accuracy 37.680\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019138138824039035\n",
      "conv1.bias 0.00229725893586874\n",
      "conv2.weight 0.0012159105141957601\n",
      "conv2.bias 0.0025869032833725214\n",
      "fc1.weight 0.0004971740245819092\n",
      "fc1.bias 0.001906189074118932\n",
      "fc2.weight 0.0010396703841194274\n",
      "fc2.bias 0.0036817178839728946\n",
      "fc3.weight 0.0024189983095441545\n",
      "fc3.bias 0.0008979778736829758\n",
      "\n",
      "Test set: Average loss: 1.7305 \n",
      "Accuracy: 3772/10000 (37.72%)\n",
      "\n",
      "Round  23, Average loss 1.731 Test accuracy 37.720\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019104496637980144\n",
      "conv1.bias 0.002273347694426775\n",
      "conv2.weight 0.0012201709548632304\n",
      "conv2.bias 0.0025632157921791077\n",
      "fc1.weight 0.0004981945753097534\n",
      "fc1.bias 0.0019011445343494414\n",
      "fc2.weight 0.001046184982572283\n",
      "fc2.bias 0.003654644602820987\n",
      "fc3.weight 0.0024186017967405774\n",
      "fc3.bias 0.0009024173021316528\n",
      "\n",
      "Test set: Average loss: 1.7341 \n",
      "Accuracy: 3764/10000 (37.64%)\n",
      "\n",
      "Round  24, Average loss 1.734 Test accuracy 37.640\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019179189205169678\n",
      "conv1.bias 0.0022791274823248386\n",
      "conv2.weight 0.001219065288702647\n",
      "conv2.bias 0.002578380750492215\n",
      "fc1.weight 0.0004964343706766764\n",
      "fc1.bias 0.0018906693905591965\n",
      "fc2.weight 0.0010440487710256425\n",
      "fc2.bias 0.0036436617374420166\n",
      "fc3.weight 0.002417803378332229\n",
      "fc3.bias 0.0008931966498494148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.7315 \n",
      "Accuracy: 3769/10000 (37.69%)\n",
      "\n",
      "Round  25, Average loss 1.732 Test accuracy 37.690\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019166888131035699\n",
      "conv1.bias 0.002253794732193152\n",
      "conv2.weight 0.001219512422879537\n",
      "conv2.bias 0.0025735606905072927\n",
      "fc1.weight 0.0004964664777119955\n",
      "fc1.bias 0.0018931043644746144\n",
      "fc2.weight 0.001045765100963532\n",
      "fc2.bias 0.003612917803582691\n",
      "fc3.weight 0.002418942962374006\n",
      "fc3.bias 0.0008913131430745124\n",
      "\n",
      "Test set: Average loss: 1.7365 \n",
      "Accuracy: 3758/10000 (37.58%)\n",
      "\n",
      "Round  26, Average loss 1.737 Test accuracy 37.580\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019177504380544026\n",
      "conv1.bias 0.002227631707986196\n",
      "conv2.weight 0.0012187038858731587\n",
      "conv2.bias 0.0025613424368202686\n",
      "fc1.weight 0.0004975459178288777\n",
      "fc1.bias 0.0018717830379803976\n",
      "fc2.weight 0.001048900399889265\n",
      "fc2.bias 0.0035736699189458576\n",
      "fc3.weight 0.002420111781074887\n",
      "fc3.bias 0.0008939934894442558\n",
      "\n",
      "Test set: Average loss: 1.7356 \n",
      "Accuracy: 3741/10000 (37.41%)\n",
      "\n",
      "Round  27, Average loss 1.736 Test accuracy 37.410\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001926810344060262\n",
      "conv1.bias 0.002278669116397699\n",
      "conv2.weight 0.0012183977166811625\n",
      "conv2.bias 0.0025664709974080324\n",
      "fc1.weight 0.0004970348278681438\n",
      "fc1.bias 0.0018869927773873011\n",
      "fc2.weight 0.001047858170100621\n",
      "fc2.bias 0.0035305175752866837\n",
      "fc3.weight 0.0024071463516780306\n",
      "fc3.bias 0.000888464692980051\n",
      "\n",
      "Test set: Average loss: 1.7338 \n",
      "Accuracy: 3755/10000 (37.55%)\n",
      "\n",
      "Round  28, Average loss 1.734 Test accuracy 37.550\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019265945752461751\n",
      "conv1.bias 0.0022579478099942207\n",
      "conv2.weight 0.001219399869441986\n",
      "conv2.bias 0.002605134155601263\n",
      "fc1.weight 0.0004975576400756836\n",
      "fc1.bias 0.001888442039489746\n",
      "fc2.weight 0.0010491382508050827\n",
      "fc2.bias 0.003522273330461411\n",
      "fc3.weight 0.0024191104230426607\n",
      "fc3.bias 0.0008987602777779102\n",
      "\n",
      "Test set: Average loss: 1.7282 \n",
      "Accuracy: 3776/10000 (37.76%)\n",
      "\n",
      "Round  29, Average loss 1.728 Test accuracy 37.760\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019222907225290935\n",
      "conv1.bias 0.0022524269297719\n",
      "conv2.weight 0.001221199631690979\n",
      "conv2.bias 0.002547089708968997\n",
      "fc1.weight 0.0004982508420944214\n",
      "fc1.bias 0.0018847424536943436\n",
      "fc2.weight 0.001049857669406467\n",
      "fc2.bias 0.003540566989353725\n",
      "fc3.weight 0.0024161988780612037\n",
      "fc3.bias 0.0008930879645049572\n",
      "\n",
      "Test set: Average loss: 1.7384 \n",
      "Accuracy: 3739/10000 (37.39%)\n",
      "\n",
      "Round  30, Average loss 1.738 Test accuracy 37.390\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019173283047146268\n",
      "conv1.bias 0.0022142883390188217\n",
      "conv2.weight 0.00121779998143514\n",
      "conv2.bias 0.0025083241052925587\n",
      "fc1.weight 0.0004977204004923502\n",
      "fc1.bias 0.0018855815132459005\n",
      "fc2.weight 0.0010498753615788051\n",
      "fc2.bias 0.00349667774779456\n",
      "fc3.weight 0.002425133614313035\n",
      "fc3.bias 0.0008896940387785434\n",
      "\n",
      "Test set: Average loss: 1.7360 \n",
      "Accuracy: 3760/10000 (37.60%)\n",
      "\n",
      "Round  31, Average loss 1.736 Test accuracy 37.600\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019225949711269802\n",
      "conv1.bias 0.0022145862070222697\n",
      "conv2.weight 0.0012148513396581014\n",
      "conv2.bias 0.002519352361559868\n",
      "fc1.weight 0.0004976421594619751\n",
      "fc1.bias 0.0018915988504886628\n",
      "fc2.weight 0.001055103824252174\n",
      "fc2.bias 0.0034956825631005423\n",
      "fc3.weight 0.0024194933119274322\n",
      "fc3.bias 0.0008823911659419537\n",
      "\n",
      "Test set: Average loss: 1.7374 \n",
      "Accuracy: 3757/10000 (37.57%)\n",
      "\n",
      "Round  32, Average loss 1.737 Test accuracy 37.570\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019169918696085612\n",
      "conv1.bias 0.002273880566159884\n",
      "conv2.weight 0.0012189493576685588\n",
      "conv2.bias 0.0025309459306299686\n",
      "fc1.weight 0.0004987297455469767\n",
      "fc1.bias 0.0019003111869096757\n",
      "fc2.weight 0.0010602765613132053\n",
      "fc2.bias 0.0034750753215381075\n",
      "fc3.weight 0.002425428799220494\n",
      "fc3.bias 0.0008548827841877938\n",
      "\n",
      "Test set: Average loss: 1.7386 \n",
      "Accuracy: 3753/10000 (37.53%)\n",
      "\n",
      "Round  33, Average loss 1.739 Test accuracy 37.530\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019071790907118056\n",
      "conv1.bias 0.002256177676220735\n",
      "conv2.weight 0.001218823989232381\n",
      "conv2.bias 0.002459948183968663\n",
      "fc1.weight 0.0004990787506103516\n",
      "fc1.bias 0.0019149812559286752\n",
      "fc2.weight 0.001063823037677341\n",
      "fc2.bias 0.0034618711187725977\n",
      "fc3.weight 0.0024374680859701975\n",
      "fc3.bias 0.0008642159402370453\n",
      "\n",
      "Test set: Average loss: 1.7337 \n",
      "Accuracy: 3766/10000 (37.66%)\n",
      "\n",
      "Round  34, Average loss 1.734 Test accuracy 37.660\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019081298510233561\n",
      "conv1.bias 0.0021814688419302306\n",
      "conv2.weight 0.0012198557456334432\n",
      "conv2.bias 0.0025350828655064106\n",
      "fc1.weight 0.0004982209205627442\n",
      "fc1.bias 0.0019125292698542278\n",
      "fc2.weight 0.0010604063669840496\n",
      "fc2.bias 0.003506613274415334\n",
      "fc3.weight 0.00243396900949024\n",
      "fc3.bias 0.0008679551072418689\n",
      "\n",
      "Test set: Average loss: 1.7353 \n",
      "Accuracy: 3752/10000 (37.52%)\n",
      "\n",
      "Round  35, Average loss 1.735 Test accuracy 37.520\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019066248999701607\n",
      "conv1.bias 0.002163439057767391\n",
      "conv2.weight 0.0012172482411066691\n",
      "conv2.bias 0.0025644064880907536\n",
      "fc1.weight 0.000499423344930013\n",
      "fc1.bias 0.0019110182921091715\n",
      "fc2.weight 0.0010646297818138486\n",
      "fc2.bias 0.003520224420797257\n",
      "fc3.weight 0.00244136026927403\n",
      "fc3.bias 0.0008748962543904781\n",
      "\n",
      "Test set: Average loss: 1.7347 \n",
      "Accuracy: 3760/10000 (37.60%)\n",
      "\n",
      "Round  36, Average loss 1.735 Test accuracy 37.600\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019017267227172852\n",
      "conv1.bias 0.0022140604754288993\n",
      "conv2.weight 0.001219490667184194\n",
      "conv2.bias 0.0025822757743299007\n",
      "fc1.weight 0.000499280055363973\n",
      "fc1.bias 0.0018984025965134302\n",
      "fc2.weight 0.0010642167121645003\n",
      "fc2.bias 0.003487350330466316\n",
      "fc3.weight 0.0024436919462113155\n",
      "fc3.bias 0.0008516309782862664\n",
      "\n",
      "Test set: Average loss: 1.7346 \n",
      "Accuracy: 3760/10000 (37.60%)\n",
      "\n",
      "Round  37, Average loss 1.735 Test accuracy 37.600\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019034043947855632\n",
      "conv1.bias 0.002234351821243763\n",
      "conv2.weight 0.0012213873863220215\n",
      "conv2.bias 0.002512388862669468\n",
      "fc1.weight 0.0004983123938242594\n",
      "fc1.bias 0.0018829471121231715\n",
      "fc2.weight 0.0010593731251973954\n",
      "fc2.bias 0.003462345472403935\n",
      "fc3.weight 0.0024327124868120465\n",
      "fc3.bias 0.0008655773475766182\n",
      "\n",
      "Test set: Average loss: 1.7404 \n",
      "Accuracy: 3743/10000 (37.43%)\n",
      "\n",
      "Round  38, Average loss 1.740 Test accuracy 37.430\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001905611621008979\n",
      "conv1.bias 0.0022663604468107224\n",
      "conv2.weight 0.0012263017892837525\n",
      "conv2.bias 0.002514052204787731\n",
      "fc1.weight 0.000497663935025533\n",
      "fc1.bias 0.0018918617318073908\n",
      "fc2.weight 0.0010589499322194902\n",
      "fc2.bias 0.003460239086832319\n",
      "fc3.weight 0.002435043312254406\n",
      "fc3.bias 0.0008604809641838074\n",
      "\n",
      "Test set: Average loss: 1.7412 \n",
      "Accuracy: 3736/10000 (37.36%)\n",
      "\n",
      "Round  39, Average loss 1.741 Test accuracy 37.360\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018977896372477214\n",
      "conv1.bias 0.0022900615197916827\n",
      "conv2.weight 0.0012268913785616557\n",
      "conv2.bias 0.002501476090401411\n",
      "fc1.weight 0.0004978930950164795\n",
      "fc1.bias 0.0018876624604066213\n",
      "fc2.weight 0.0010600725809733072\n",
      "fc2.bias 0.003471747040748596\n",
      "fc3.weight 0.002435720534551711\n",
      "fc3.bias 0.0008584531024098396\n",
      "\n",
      "Test set: Average loss: 1.7367 \n",
      "Accuracy: 3752/10000 (37.52%)\n",
      "\n",
      "Round  40, Average loss 1.737 Test accuracy 37.520\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018963719738854302\n",
      "conv1.bias 0.002240616517762343\n",
      "conv2.weight 0.0012290703256924948\n",
      "conv2.bias 0.0025149479042738676\n",
      "fc1.weight 0.0004960723320643107\n",
      "fc1.bias 0.0018934191515048344\n",
      "fc2.weight 0.0010600481714521136\n",
      "fc2.bias 0.0034672717253367105\n",
      "fc3.weight 0.002432052862076532\n",
      "fc3.bias 0.0008628365583717823\n",
      "\n",
      "Test set: Average loss: 1.7282 \n",
      "Accuracy: 3788/10000 (37.88%)\n",
      "\n",
      "Round  41, Average loss 1.728 Test accuracy 37.880\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018993826707204182\n",
      "conv1.bias 0.0022410539289315543\n",
      "conv2.weight 0.0012322181463241576\n",
      "conv2.bias 0.0025242576375603676\n",
      "fc1.weight 0.0004978493054707845\n",
      "fc1.bias 0.0018889843175808588\n",
      "fc2.weight 0.0010603800652519105\n",
      "fc2.bias 0.003453612682365236\n",
      "fc3.weight 0.0024346377168382918\n",
      "fc3.bias 0.0008570202626287937\n",
      "\n",
      "Test set: Average loss: 1.7324 \n",
      "Accuracy: 3764/10000 (37.64%)\n",
      "\n",
      "Round  42, Average loss 1.732 Test accuracy 37.640\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018988189432356092\n",
      "conv1.bias 0.0022977363939086595\n",
      "conv2.weight 0.0012367760141690572\n",
      "conv2.bias 0.0024836400989443064\n",
      "fc1.weight 0.0004983596007029215\n",
      "fc1.bias 0.0019036045918862025\n",
      "fc2.weight 0.0010640837843455965\n",
      "fc2.bias 0.003443848164308639\n",
      "fc3.weight 0.002437608014969599\n",
      "fc3.bias 0.0008628901094198226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.7303 \n",
      "Accuracy: 3779/10000 (37.79%)\n",
      "\n",
      "Round  43, Average loss 1.730 Test accuracy 37.790\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019029343128204345\n",
      "conv1.bias 0.002221178418646256\n",
      "conv2.weight 0.0012318611145019532\n",
      "conv2.bias 0.0025248622987419367\n",
      "fc1.weight 0.0004973825613657633\n",
      "fc1.bias 0.001911917080481847\n",
      "fc2.weight 0.0010629761786687942\n",
      "fc2.bias 0.0034489833882876803\n",
      "fc3.weight 0.0024360009602137976\n",
      "fc3.bias 0.0008572235703468323\n",
      "\n",
      "Test set: Average loss: 1.7349 \n",
      "Accuracy: 3764/10000 (37.64%)\n",
      "\n",
      "Round  44, Average loss 1.735 Test accuracy 37.640\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019000854757097033\n",
      "conv1.bias 0.0022128584484259286\n",
      "conv2.weight 0.001237787405649821\n",
      "conv2.bias 0.0025425138883292675\n",
      "fc1.weight 0.0004977990786234538\n",
      "fc1.bias 0.0019080501049757003\n",
      "fc2.weight 0.0010650402023678735\n",
      "fc2.bias 0.003458356928257715\n",
      "fc3.weight 0.0024405601478758314\n",
      "fc3.bias 0.000846186000853777\n",
      "\n",
      "Test set: Average loss: 1.7333 \n",
      "Accuracy: 3770/10000 (37.70%)\n",
      "\n",
      "Round  45, Average loss 1.733 Test accuracy 37.700\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018960976600646974\n",
      "conv1.bias 0.0022618266132970652\n",
      "conv2.weight 0.0012395866711934407\n",
      "conv2.bias 0.0025713795330375433\n",
      "fc1.weight 0.0004991881847381591\n",
      "fc1.bias 0.0018945083022117615\n",
      "fc2.weight 0.0010681235601031592\n",
      "fc2.bias 0.0034183309901328314\n",
      "fc3.weight 0.0024426278613862537\n",
      "fc3.bias 0.0008426536805927753\n",
      "\n",
      "Test set: Average loss: 1.7293 \n",
      "Accuracy: 3798/10000 (37.98%)\n",
      "\n",
      "Round  46, Average loss 1.729 Test accuracy 37.980\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018939916292826334\n",
      "conv1.bias 0.0023155175149440765\n",
      "conv2.weight 0.001243852178255717\n",
      "conv2.bias 0.0026047001592814922\n",
      "fc1.weight 0.0004978477160135905\n",
      "fc1.bias 0.001894227663675944\n",
      "fc2.weight 0.0010671700750078475\n",
      "fc2.bias 0.0033997325670151483\n",
      "fc3.weight 0.0024385412534077963\n",
      "fc3.bias 0.0008506549522280693\n",
      "\n",
      "Test set: Average loss: 1.7350 \n",
      "Accuracy: 3767/10000 (37.67%)\n",
      "\n",
      "Round  47, Average loss 1.735 Test accuracy 37.670\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019042128986782498\n",
      "conv1.bias 0.0023110747958223024\n",
      "conv2.weight 0.0012443604071935017\n",
      "conv2.bias 0.002535820472985506\n",
      "fc1.weight 0.0004965991576512654\n",
      "fc1.bias 0.0018704110135634741\n",
      "fc2.weight 0.001066459265966264\n",
      "fc2.bias 0.0033773772773288543\n",
      "fc3.weight 0.002447581858862014\n",
      "fc3.bias 0.0008425017818808556\n",
      "\n",
      "Test set: Average loss: 1.7339 \n",
      "Accuracy: 3784/10000 (37.84%)\n",
      "\n",
      "Round  48, Average loss 1.734 Test accuracy 37.840\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001908748812145657\n",
      "conv1.bias 0.0023345823089281716\n",
      "conv2.weight 0.0012441227833429973\n",
      "conv2.bias 0.0025498121976852417\n",
      "fc1.weight 0.0004977138837178549\n",
      "fc1.bias 0.0018733948469161986\n",
      "fc2.weight 0.0010706612042018346\n",
      "fc2.bias 0.0034106969833374023\n",
      "fc3.weight 0.0024513446149371917\n",
      "fc3.bias 0.0008604200556874275\n",
      "\n",
      "Test set: Average loss: 1.7381 \n",
      "Accuracy: 3767/10000 (37.67%)\n",
      "\n",
      "Round  49, Average loss 1.738 Test accuracy 37.670\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001899700164794922\n",
      "conv1.bias 0.002265822763244311\n",
      "conv2.weight 0.0012509302298227946\n",
      "conv2.bias 0.0026092941407114267\n",
      "fc1.weight 0.000497829000155131\n",
      "fc1.bias 0.0018666932980219524\n",
      "fc2.weight 0.001071272956000434\n",
      "fc2.bias 0.003412292826743353\n",
      "fc3.weight 0.002459194546654111\n",
      "fc3.bias 0.0008679750375449657\n",
      "\n",
      "Test set: Average loss: 1.7387 \n",
      "Accuracy: 3754/10000 (37.54%)\n",
      "\n",
      "Round  50, Average loss 1.739 Test accuracy 37.540\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019047413931952582\n",
      "conv1.bias 0.0021872629101077714\n",
      "conv2.weight 0.0012515089909235636\n",
      "conv2.bias 0.0026067192666232586\n",
      "fc1.weight 0.0004991531769434611\n",
      "fc1.bias 0.0018623008082310358\n",
      "fc2.weight 0.0010734501339140392\n",
      "fc2.bias 0.003431466363725208\n",
      "fc3.weight 0.002456197000685192\n",
      "fc3.bias 0.0008744808845221996\n",
      "\n",
      "Test set: Average loss: 1.7352 \n",
      "Accuracy: 3774/10000 (37.74%)\n",
      "\n",
      "Round  51, Average loss 1.735 Test accuracy 37.740\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019013637966579862\n",
      "conv1.bias 0.0023174614955981574\n",
      "conv2.weight 0.0012525172034899394\n",
      "conv2.bias 0.002589292824268341\n",
      "fc1.weight 0.0004999749660491943\n",
      "fc1.bias 0.001849249005317688\n",
      "fc2.weight 0.0010776033477177696\n",
      "fc2.bias 0.0034299095471700034\n",
      "fc3.weight 0.0024570975984845845\n",
      "fc3.bias 0.0008617737330496311\n",
      "\n",
      "Test set: Average loss: 1.7415 \n",
      "Accuracy: 3751/10000 (37.51%)\n",
      "\n",
      "Round  52, Average loss 1.741 Test accuracy 37.510\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018949276871151393\n",
      "conv1.bias 0.002354276987413565\n",
      "conv2.weight 0.001247313419977824\n",
      "conv2.bias 0.0025512133724987507\n",
      "fc1.weight 0.0004993222554524739\n",
      "fc1.bias 0.0018502133587996164\n",
      "fc2.weight 0.0010766599859510148\n",
      "fc2.bias 0.003397387762864431\n",
      "fc3.weight 0.0024644176165262857\n",
      "fc3.bias 0.0008479387499392033\n",
      "\n",
      "Test set: Average loss: 1.7406 \n",
      "Accuracy: 3773/10000 (37.73%)\n",
      "\n",
      "Round  53, Average loss 1.741 Test accuracy 37.730\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018802375263637966\n",
      "conv1.bias 0.0023839998369415603\n",
      "conv2.weight 0.0012502447764078776\n",
      "conv2.bias 0.002610130701214075\n",
      "fc1.weight 0.0004984039465586344\n",
      "fc1.bias 0.0018438156694173813\n",
      "fc2.weight 0.001075068163493323\n",
      "fc2.bias 0.0034060652057329812\n",
      "fc3.weight 0.00246865295228504\n",
      "fc3.bias 0.000852045975625515\n",
      "\n",
      "Test set: Average loss: 1.7468 \n",
      "Accuracy: 3743/10000 (37.43%)\n",
      "\n",
      "Round  54, Average loss 1.747 Test accuracy 37.430\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018732154369354248\n",
      "conv1.bias 0.002402711194008589\n",
      "conv2.weight 0.0012517420450846354\n",
      "conv2.bias 0.0025600146036595106\n",
      "fc1.weight 0.0005007349252700806\n",
      "fc1.bias 0.0018430359661579133\n",
      "fc2.weight 0.0010791492840600391\n",
      "fc2.bias 0.0033999564392226084\n",
      "fc3.weight 0.0024753249826885405\n",
      "fc3.bias 0.0008427338674664497\n",
      "\n",
      "Test set: Average loss: 1.7393 \n",
      "Accuracy: 3767/10000 (37.67%)\n",
      "\n",
      "Round  55, Average loss 1.739 Test accuracy 37.670\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018762319617801242\n",
      "conv1.bias 0.002332381128023068\n",
      "conv2.weight 0.0012515192230542502\n",
      "conv2.bias 0.0025718177203089\n",
      "fc1.weight 0.0005002997318903605\n",
      "fc1.bias 0.0018390636891126634\n",
      "fc2.weight 0.0010785611848982554\n",
      "fc2.bias 0.0033869573048182894\n",
      "fc3.weight 0.0024763541562216624\n",
      "fc3.bias 0.0008485637605190277\n",
      "\n",
      "Test set: Average loss: 1.7446 \n",
      "Accuracy: 3740/10000 (37.40%)\n",
      "\n",
      "Round  56, Average loss 1.745 Test accuracy 37.400\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018835406833224825\n",
      "conv1.bias 0.0023649269714951515\n",
      "conv2.weight 0.0012526283661524454\n",
      "conv2.bias 0.0025474335998296738\n",
      "fc1.weight 0.0005000645319620769\n",
      "fc1.bias 0.0018420485158761343\n",
      "fc2.weight 0.0010787243880922832\n",
      "fc2.bias 0.0034106913067045667\n",
      "fc3.weight 0.0024826364857809882\n",
      "fc3.bias 0.0008493591099977494\n",
      "\n",
      "Test set: Average loss: 1.7422 \n",
      "Accuracy: 3759/10000 (37.59%)\n",
      "\n",
      "Round  57, Average loss 1.742 Test accuracy 37.590\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018806297249264188\n",
      "conv1.bias 0.002338311324516932\n",
      "conv2.weight 0.0012489850322405496\n",
      "conv2.bias 0.0026025748811662197\n",
      "fc1.weight 0.0004999666213989258\n",
      "fc1.bias 0.0018202332158883414\n",
      "fc2.weight 0.001078886834401933\n",
      "fc2.bias 0.0033825855879556564\n",
      "fc3.weight 0.0024716916538420177\n",
      "fc3.bias 0.0008455191738903523\n",
      "\n",
      "Test set: Average loss: 1.7443 \n",
      "Accuracy: 3766/10000 (37.66%)\n",
      "\n",
      "Round  58, Average loss 1.744 Test accuracy 37.660\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018898846043480767\n",
      "conv1.bias 0.0023205674563845\n",
      "conv2.weight 0.00125340739885966\n",
      "conv2.bias 0.0026121963746845722\n",
      "fc1.weight 0.0004999469916025798\n",
      "fc1.bias 0.0018266201019287109\n",
      "fc2.weight 0.0010773212190658328\n",
      "fc2.bias 0.0033649071341469174\n",
      "fc3.weight 0.0024675193287077403\n",
      "fc3.bias 0.0008498454466462135\n",
      "\n",
      "Test set: Average loss: 1.7423 \n",
      "Accuracy: 3769/10000 (37.69%)\n",
      "\n",
      "Round  59, Average loss 1.742 Test accuracy 37.690\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018851873609754773\n",
      "conv1.bias 0.0023076393020649752\n",
      "conv2.weight 0.001253406008084615\n",
      "conv2.bias 0.0025930185802280903\n",
      "fc1.weight 0.0005004223187764485\n",
      "fc1.bias 0.001813062404592832\n",
      "fc2.weight 0.0010790276148962595\n",
      "fc2.bias 0.003370746260597592\n",
      "fc3.weight 0.0024684491611662363\n",
      "fc3.bias 0.0008680105209350586\n",
      "\n",
      "Test set: Average loss: 1.7455 \n",
      "Accuracy: 3760/10000 (37.60%)\n",
      "\n",
      "Round  60, Average loss 1.745 Test accuracy 37.600\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001882014274597168\n",
      "conv1.bias 0.0022844605458279452\n",
      "conv2.weight 0.0012540040413538615\n",
      "conv2.bias 0.002594482619315386\n",
      "fc1.weight 0.0005013193289438884\n",
      "fc1.bias 0.0018283164749542872\n",
      "fc2.weight 0.0010804340952918644\n",
      "fc2.bias 0.003374602823030381\n",
      "fc3.weight 0.0024753105072748095\n",
      "fc3.bias 0.0008756684139370919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.7383 \n",
      "Accuracy: 3774/10000 (37.74%)\n",
      "\n",
      "Round  61, Average loss 1.738 Test accuracy 37.740\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00188961837026808\n",
      "conv1.bias 0.0022113475327690444\n",
      "conv2.weight 0.0012490848700205486\n",
      "conv2.bias 0.0025556471664458513\n",
      "fc1.weight 0.0005013098319371542\n",
      "fc1.bias 0.0018331552545229593\n",
      "fc2.weight 0.0010775897237989638\n",
      "fc2.bias 0.0033933775765555246\n",
      "fc3.weight 0.0024763374101547967\n",
      "fc3.bias 0.0008739341050386429\n",
      "\n",
      "Test set: Average loss: 1.7401 \n",
      "Accuracy: 3774/10000 (37.74%)\n",
      "\n",
      "Round  62, Average loss 1.740 Test accuracy 37.740\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001879794200261434\n",
      "conv1.bias 0.002302386642744144\n",
      "conv2.weight 0.0012512574593226114\n",
      "conv2.bias 0.0025468748062849045\n",
      "fc1.weight 0.0005034472147623697\n",
      "fc1.bias 0.001829693466424942\n",
      "fc2.weight 0.001078873872756958\n",
      "fc2.bias 0.003429016541867029\n",
      "fc3.weight 0.0024773103850228447\n",
      "fc3.bias 0.0008727339096367359\n",
      "\n",
      "Test set: Average loss: 1.7412 \n",
      "Accuracy: 3761/10000 (37.61%)\n",
      "\n",
      "Round  63, Average loss 1.741 Test accuracy 37.610\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018862042162153456\n",
      "conv1.bias 0.0022802259773015976\n",
      "conv2.weight 0.0012503653764724732\n",
      "conv2.bias 0.002581861102953553\n",
      "fc1.weight 0.0005030659437179566\n",
      "fc1.bias 0.0018484202524026235\n",
      "fc2.weight 0.0010853087145184714\n",
      "fc2.bias 0.003438070061660948\n",
      "fc3.weight 0.00248341475214277\n",
      "fc3.bias 0.0008702993392944336\n",
      "\n",
      "Test set: Average loss: 1.7407 \n",
      "Accuracy: 3740/10000 (37.40%)\n",
      "\n",
      "Round  64, Average loss 1.741 Test accuracy 37.400\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018829584121704101\n",
      "conv1.bias 0.0022921101190149784\n",
      "conv2.weight 0.001248187522093455\n",
      "conv2.bias 0.0025515519082546234\n",
      "fc1.weight 0.000503323515256246\n",
      "fc1.bias 0.0018416184931993485\n",
      "fc2.weight 0.0010807354298848955\n",
      "fc2.bias 0.0034458268256414505\n",
      "fc3.weight 0.002480007637114752\n",
      "fc3.bias 0.0008609749376773835\n",
      "\n",
      "Test set: Average loss: 1.7429 \n",
      "Accuracy: 3756/10000 (37.56%)\n",
      "\n",
      "Round  65, Average loss 1.743 Test accuracy 37.560\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001885754797193739\n",
      "conv1.bias 0.002312909966955582\n",
      "conv2.weight 0.0012520209948221842\n",
      "conv2.bias 0.002518939785659313\n",
      "fc1.weight 0.000503961722056071\n",
      "fc1.bias 0.001858241856098175\n",
      "fc2.weight 0.0010840334589519197\n",
      "fc2.bias 0.003432805694284893\n",
      "fc3.weight 0.002485549449920654\n",
      "fc3.bias 0.0008640220388770103\n",
      "\n",
      "Test set: Average loss: 1.7438 \n",
      "Accuracy: 3751/10000 (37.51%)\n",
      "\n",
      "Round  66, Average loss 1.744 Test accuracy 37.510\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018940999772813585\n",
      "conv1.bias 0.0022593376537164054\n",
      "conv2.weight 0.0012551498413085936\n",
      "conv2.bias 0.0025612139143049717\n",
      "fc1.weight 0.000504693587621053\n",
      "fc1.bias 0.0018653056273857753\n",
      "fc2.weight 0.0010849210951063368\n",
      "fc2.bias 0.0034358930729684375\n",
      "fc3.weight 0.0024800153005690804\n",
      "fc3.bias 0.0008737909607589245\n",
      "\n",
      "Test set: Average loss: 1.7440 \n",
      "Accuracy: 3754/10000 (37.54%)\n",
      "\n",
      "Round  67, Average loss 1.744 Test accuracy 37.540\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018845571411980522\n",
      "conv1.bias 0.0022371127270162106\n",
      "conv2.weight 0.0012539924184481303\n",
      "conv2.bias 0.002497437410056591\n",
      "fc1.weight 0.0005037058194478352\n",
      "fc1.bias 0.0018775155146916708\n",
      "fc2.weight 0.001083608279152522\n",
      "fc2.bias 0.003433498598280407\n",
      "fc3.weight 0.0024783980278741745\n",
      "fc3.bias 0.0008710172958672046\n",
      "\n",
      "Test set: Average loss: 1.7448 \n",
      "Accuracy: 3765/10000 (37.65%)\n",
      "\n",
      "Round  68, Average loss 1.745 Test accuracy 37.650\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001881346967485216\n",
      "conv1.bias 0.002298971482863029\n",
      "conv2.weight 0.0012557864189147948\n",
      "conv2.bias 0.002490198239684105\n",
      "fc1.weight 0.0005037229061126709\n",
      "fc1.bias 0.001882601281007131\n",
      "fc2.weight 0.0010833988114008829\n",
      "fc2.bias 0.003399929830006191\n",
      "fc3.weight 0.0024846900077093214\n",
      "fc3.bias 0.0008778668940067291\n",
      "\n",
      "Test set: Average loss: 1.7426 \n",
      "Accuracy: 3769/10000 (37.69%)\n",
      "\n",
      "Round  69, Average loss 1.743 Test accuracy 37.690\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001880346934000651\n",
      "conv1.bias 0.0022807838395237923\n",
      "conv2.weight 0.0012508628765741984\n",
      "conv2.bias 0.0024883910082280636\n",
      "fc1.weight 0.0005029953320821127\n",
      "fc1.bias 0.0018907820185025534\n",
      "fc2.weight 0.0010828355002024817\n",
      "fc2.bias 0.0034059506087076095\n",
      "fc3.weight 0.002488169783637637\n",
      "fc3.bias 0.0008528577163815498\n",
      "\n",
      "Test set: Average loss: 1.7457 \n",
      "Accuracy: 3744/10000 (37.44%)\n",
      "\n",
      "Round  70, Average loss 1.746 Test accuracy 37.440\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018846129046546087\n",
      "conv1.bias 0.002297590176264445\n",
      "conv2.weight 0.0012442286809285481\n",
      "conv2.bias 0.002480636117979884\n",
      "fc1.weight 0.0005027101834615071\n",
      "fc1.bias 0.0018760511030753455\n",
      "fc2.weight 0.0010826552671099467\n",
      "fc2.bias 0.003437302651859465\n",
      "fc3.weight 0.002491624014718192\n",
      "fc3.bias 0.0008686203509569168\n",
      "\n",
      "Test set: Average loss: 1.7432 \n",
      "Accuracy: 3749/10000 (37.49%)\n",
      "\n",
      "Round  71, Average loss 1.743 Test accuracy 37.490\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001887618965572781\n",
      "conv1.bias 0.002291982683042685\n",
      "conv2.weight 0.001249439517656962\n",
      "conv2.bias 0.0024947633501142263\n",
      "fc1.weight 0.0005038129488627116\n",
      "fc1.bias 0.0018802287677923838\n",
      "fc2.weight 0.001086603459857759\n",
      "fc2.bias 0.003434799611568451\n",
      "fc3.weight 0.0024904849983396985\n",
      "fc3.bias 0.0008705952204763889\n",
      "\n",
      "Test set: Average loss: 1.7453 \n",
      "Accuracy: 3755/10000 (37.55%)\n",
      "\n",
      "Round  72, Average loss 1.745 Test accuracy 37.550\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018850141101413303\n",
      "conv1.bias 0.002306688421716293\n",
      "conv2.weight 0.001251344680786133\n",
      "conv2.bias 0.002462726552039385\n",
      "fc1.weight 0.0005042379299799602\n",
      "fc1.bias 0.0018682443847258885\n",
      "fc2.weight 0.0010845897689698235\n",
      "fc2.bias 0.00342360387245814\n",
      "fc3.weight 0.0024839245137714206\n",
      "fc3.bias 0.0008626281283795834\n",
      "\n",
      "Test set: Average loss: 1.7492 \n",
      "Accuracy: 3732/10000 (37.32%)\n",
      "\n",
      "Round  73, Average loss 1.749 Test accuracy 37.320\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018863734934065077\n",
      "conv1.bias 0.0023491280153393745\n",
      "conv2.weight 0.0012491625547409058\n",
      "conv2.bias 0.002499273279681802\n",
      "fc1.weight 0.0005034519831339518\n",
      "fc1.bias 0.0018667805939912795\n",
      "fc2.weight 0.0010827354968540252\n",
      "fc2.bias 0.003404945844695682\n",
      "fc3.weight 0.002484379212061564\n",
      "fc3.bias 0.0008702506311237812\n",
      "\n",
      "Test set: Average loss: 1.7421 \n",
      "Accuracy: 3753/10000 (37.53%)\n",
      "\n",
      "Round  74, Average loss 1.742 Test accuracy 37.530\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018870686160193549\n",
      "conv1.bias 0.0023817364126443863\n",
      "conv2.weight 0.0012521910667419433\n",
      "conv2.bias 0.002435053698718548\n",
      "fc1.weight 0.0005027567148208618\n",
      "fc1.bias 0.0018742899099985757\n",
      "fc2.weight 0.0010865055379413423\n",
      "fc2.bias 0.0034333339759281705\n",
      "fc3.weight 0.002482295036315918\n",
      "fc3.bias 0.0008716154843568802\n",
      "\n",
      "Test set: Average loss: 1.7420 \n",
      "Accuracy: 3764/10000 (37.64%)\n",
      "\n",
      "Round  75, Average loss 1.742 Test accuracy 37.640\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018853423330518935\n",
      "conv1.bias 0.0022961956759293876\n",
      "conv2.weight 0.0012526126702626546\n",
      "conv2.bias 0.002447653328999877\n",
      "fc1.weight 0.000503821055094401\n",
      "fc1.bias 0.001869356632232666\n",
      "fc2.weight 0.0010863448892320905\n",
      "fc2.bias 0.0034195653029850553\n",
      "fc3.weight 0.0024878235090346566\n",
      "fc3.bias 0.000856898631900549\n",
      "\n",
      "Test set: Average loss: 1.7419 \n",
      "Accuracy: 3767/10000 (37.67%)\n",
      "\n",
      "Round  76, Average loss 1.742 Test accuracy 37.670\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018836851914723713\n",
      "conv1.bias 0.0023568949351708093\n",
      "conv2.weight 0.0012541809678077698\n",
      "conv2.bias 0.0024862196296453476\n",
      "fc1.weight 0.0005037879943847656\n",
      "fc1.bias 0.0018721461296081544\n",
      "fc2.weight 0.0010860331474788604\n",
      "fc2.bias 0.0034196252624193826\n",
      "fc3.weight 0.002498312507356916\n",
      "fc3.bias 0.0008653894066810608\n",
      "\n",
      "Test set: Average loss: 1.7418 \n",
      "Accuracy: 3775/10000 (37.75%)\n",
      "\n",
      "Round  77, Average loss 1.742 Test accuracy 37.750\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018895302878485785\n",
      "conv1.bias 0.0023782489200433097\n",
      "conv2.weight 0.0012574342886606852\n",
      "conv2.bias 0.002462198259308934\n",
      "fc1.weight 0.0005056798458099365\n",
      "fc1.bias 0.0018765809635321298\n",
      "fc2.weight 0.0010861970129467191\n",
      "fc2.bias 0.003405023898397173\n",
      "fc3.weight 0.0024987396739778065\n",
      "fc3.bias 0.0008661330677568913\n",
      "\n",
      "Test set: Average loss: 1.7430 \n",
      "Accuracy: 3768/10000 (37.68%)\n",
      "\n",
      "Round  78, Average loss 1.743 Test accuracy 37.680\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001888800859451294\n",
      "conv1.bias 0.0022969385609030724\n",
      "conv2.weight 0.0012527310848236084\n",
      "conv2.bias 0.0025234781205654144\n",
      "fc1.weight 0.0005042563279469808\n",
      "fc1.bias 0.001872079074382782\n",
      "fc2.weight 0.0010856830884539892\n",
      "fc2.bias 0.0033924427060853866\n",
      "fc3.weight 0.002500240859531221\n",
      "fc3.bias 0.0008607370778918267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.7453 \n",
      "Accuracy: 3747/10000 (37.47%)\n",
      "\n",
      "Round  79, Average loss 1.745 Test accuracy 37.470\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018830141756269667\n",
      "conv1.bias 0.00228082233419021\n",
      "conv2.weight 0.0012544357776641845\n",
      "conv2.bias 0.002488092752173543\n",
      "fc1.weight 0.000504269520441691\n",
      "fc1.bias 0.0018718916922807693\n",
      "fc2.weight 0.001084188809470525\n",
      "fc2.bias 0.0034002012440136503\n",
      "fc3.weight 0.0024908761183420817\n",
      "fc3.bias 0.0008647569455206394\n",
      "\n",
      "Test set: Average loss: 1.7481 \n",
      "Accuracy: 3735/10000 (37.35%)\n",
      "\n",
      "Round  80, Average loss 1.748 Test accuracy 37.350\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001889731354183621\n",
      "conv1.bias 0.0022744393597046533\n",
      "conv2.weight 0.001251882016658783\n",
      "conv2.bias 0.002516042673960328\n",
      "fc1.weight 0.000504268765449524\n",
      "fc1.bias 0.0018561482429504395\n",
      "fc2.weight 0.0010870963808089968\n",
      "fc2.bias 0.0033781095629646664\n",
      "fc3.weight 0.00249595528557187\n",
      "fc3.bias 0.0008469583466649055\n",
      "\n",
      "Test set: Average loss: 1.7418 \n",
      "Accuracy: 3765/10000 (37.65%)\n",
      "\n",
      "Round  81, Average loss 1.742 Test accuracy 37.650\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018874888949924045\n",
      "conv1.bias 0.0022298418916761875\n",
      "conv2.weight 0.0012534491221110025\n",
      "conv2.bias 0.0024922117590904236\n",
      "fc1.weight 0.0005045558214187622\n",
      "fc1.bias 0.001869905243317286\n",
      "fc2.weight 0.0010852980235266307\n",
      "fc2.bias 0.003388230289731707\n",
      "fc3.weight 0.0025022194499061223\n",
      "fc3.bias 0.0008543475531041622\n",
      "\n",
      "Test set: Average loss: 1.7419 \n",
      "Accuracy: 3759/10000 (37.59%)\n",
      "\n",
      "Round  82, Average loss 1.742 Test accuracy 37.590\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018820918930901422\n",
      "conv1.bias 0.0022858750695983567\n",
      "conv2.weight 0.0012553288539250691\n",
      "conv2.bias 0.0024207215756177902\n",
      "fc1.weight 0.0005057197411855062\n",
      "fc1.bias 0.001875516027212143\n",
      "fc2.weight 0.0010922821741255503\n",
      "fc2.bias 0.0033920258283615112\n",
      "fc3.weight 0.0025078503858475456\n",
      "fc3.bias 0.0008587004616856575\n",
      "\n",
      "Test set: Average loss: 1.7456 \n",
      "Accuracy: 3729/10000 (37.29%)\n",
      "\n",
      "Round  83, Average loss 1.746 Test accuracy 37.290\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018875434663560655\n",
      "conv1.bias 0.002266538329422474\n",
      "conv2.weight 0.0012565094232559204\n",
      "conv2.bias 0.0024724488612264395\n",
      "fc1.weight 0.0005044803619384766\n",
      "fc1.bias 0.0018710604558388392\n",
      "fc2.weight 0.0010900547580113486\n",
      "fc2.bias 0.0033826849290302823\n",
      "fc3.weight 0.0025047523634774346\n",
      "fc3.bias 0.0008525342680513858\n",
      "\n",
      "Test set: Average loss: 1.7443 \n",
      "Accuracy: 3747/10000 (37.47%)\n",
      "\n",
      "Round  84, Average loss 1.744 Test accuracy 37.470\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018872911400265163\n",
      "conv1.bias 0.0022602039389312267\n",
      "conv2.weight 0.0012564243872960408\n",
      "conv2.bias 0.002446340164169669\n",
      "fc1.weight 0.0005037684837977092\n",
      "fc1.bias 0.0018887750804424286\n",
      "fc2.weight 0.0010906206236945258\n",
      "fc2.bias 0.0034365905892281305\n",
      "fc3.weight 0.0025058638481866746\n",
      "fc3.bias 0.0008670276962220669\n",
      "\n",
      "Test set: Average loss: 1.7423 \n",
      "Accuracy: 3741/10000 (37.41%)\n",
      "\n",
      "Round  85, Average loss 1.742 Test accuracy 37.410\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018839401668972438\n",
      "conv1.bias 0.0023185573518276215\n",
      "conv2.weight 0.0012552266319592793\n",
      "conv2.bias 0.0024409524630755186\n",
      "fc1.weight 0.0005037447611490885\n",
      "fc1.bias 0.0018885998676220577\n",
      "fc2.weight 0.0010885490311516655\n",
      "fc2.bias 0.003420241531871614\n",
      "fc3.weight 0.002514936810448056\n",
      "fc3.bias 0.000853907223790884\n",
      "\n",
      "Test set: Average loss: 1.7431 \n",
      "Accuracy: 3757/10000 (37.57%)\n",
      "\n",
      "Round  86, Average loss 1.743 Test accuracy 37.570\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018915753894382054\n",
      "conv1.bias 0.0022810776717960835\n",
      "conv2.weight 0.0012517463167508443\n",
      "conv2.bias 0.0024488982744514942\n",
      "fc1.weight 0.0005055256287256877\n",
      "fc1.bias 0.0018879656990369162\n",
      "fc2.weight 0.0010927964770604693\n",
      "fc2.bias 0.00341448826449258\n",
      "fc3.weight 0.0025146700087047757\n",
      "fc3.bias 0.0008476939983665943\n",
      "\n",
      "Test set: Average loss: 1.7480 \n",
      "Accuracy: 3748/10000 (37.48%)\n",
      "\n",
      "Round  87, Average loss 1.748 Test accuracy 37.480\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001896514892578125\n",
      "conv1.bias 0.002276576434572538\n",
      "conv2.weight 0.0012503457069396972\n",
      "conv2.bias 0.002423668745905161\n",
      "fc1.weight 0.0005040011803309122\n",
      "fc1.bias 0.0018924683332443236\n",
      "fc2.weight 0.0010880826011536613\n",
      "fc2.bias 0.0033633637995946976\n",
      "fc3.weight 0.0025160868962605795\n",
      "fc3.bias 0.0008562400005757808\n",
      "\n",
      "Test set: Average loss: 1.7462 \n",
      "Accuracy: 3755/10000 (37.55%)\n",
      "\n",
      "Round  88, Average loss 1.746 Test accuracy 37.550\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018926348951127795\n",
      "conv1.bias 0.002276364558686813\n",
      "conv2.weight 0.0012466251850128175\n",
      "conv2.bias 0.0023705060593783855\n",
      "fc1.weight 0.0005028427441914876\n",
      "fc1.bias 0.0018886825690666834\n",
      "fc2.weight 0.001088159614139133\n",
      "fc2.bias 0.0033731676992915923\n",
      "fc3.weight 0.0025091176941281273\n",
      "fc3.bias 0.0008551761507987976\n",
      "\n",
      "Test set: Average loss: 1.7437 \n",
      "Accuracy: 3755/10000 (37.55%)\n",
      "\n",
      "Round  89, Average loss 1.744 Test accuracy 37.550\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018996281094021267\n",
      "conv1.bias 0.002291239177187284\n",
      "conv2.weight 0.001249257723490397\n",
      "conv2.bias 0.0023589555639773607\n",
      "fc1.weight 0.0005029017130533854\n",
      "fc1.bias 0.0018999035159746806\n",
      "fc2.weight 0.0010879562014625187\n",
      "fc2.bias 0.003398155172665914\n",
      "fc3.weight 0.002513820216769264\n",
      "fc3.bias 0.0008563035167753697\n",
      "\n",
      "Test set: Average loss: 1.7423 \n",
      "Accuracy: 3764/10000 (37.64%)\n",
      "\n",
      "Round  90, Average loss 1.742 Test accuracy 37.640\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019077503681182862\n",
      "conv1.bias 0.002266132893661658\n",
      "conv2.weight 0.0012507319450378418\n",
      "conv2.bias 0.002369436901062727\n",
      "fc1.weight 0.0005030899047851563\n",
      "fc1.bias 0.0018913554648558298\n",
      "fc2.weight 0.001087323162290785\n",
      "fc2.bias 0.0034238373239835105\n",
      "fc3.weight 0.0025101048605782644\n",
      "fc3.bias 0.0008558410219848156\n",
      "\n",
      "Test set: Average loss: 1.7464 \n",
      "Accuracy: 3743/10000 (37.43%)\n",
      "\n",
      "Round  91, Average loss 1.746 Test accuracy 37.430\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018994254536098905\n",
      "conv1.bias 0.0023108720779418945\n",
      "conv2.weight 0.0012512656052907308\n",
      "conv2.bias 0.002416343428194523\n",
      "fc1.weight 0.0005039558410644532\n",
      "fc1.bias 0.0018938937534888586\n",
      "fc2.weight 0.0010889223643711634\n",
      "fc2.bias 0.0034182284559522358\n",
      "fc3.weight 0.002519297032129197\n",
      "fc3.bias 0.0008633638732135296\n",
      "\n",
      "Test set: Average loss: 1.7423 \n",
      "Accuracy: 3752/10000 (37.52%)\n",
      "\n",
      "Round  92, Average loss 1.742 Test accuracy 37.520\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018987086084153917\n",
      "conv1.bias 0.0022838073782622814\n",
      "conv2.weight 0.001254194974899292\n",
      "conv2.bias 0.002379598096013069\n",
      "fc1.weight 0.0005033425490061442\n",
      "fc1.bias 0.0018976661066214243\n",
      "fc2.weight 0.0010865129175640289\n",
      "fc2.bias 0.003458515519187564\n",
      "fc3.weight 0.002519350392477853\n",
      "fc3.bias 0.0008699451573193073\n",
      "\n",
      "Test set: Average loss: 1.7435 \n",
      "Accuracy: 3749/10000 (37.49%)\n",
      "\n",
      "Round  93, Average loss 1.744 Test accuracy 37.490\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019001236226823595\n",
      "conv1.bias 0.002286131183306376\n",
      "conv2.weight 0.001253230373064677\n",
      "conv2.bias 0.002413379028439522\n",
      "fc1.weight 0.000502262274424235\n",
      "fc1.bias 0.0018994885186354319\n",
      "fc2.weight 0.0010822158011179121\n",
      "fc2.bias 0.0034358111165818713\n",
      "fc3.weight 0.00250428801491147\n",
      "fc3.bias 0.0008778690360486507\n",
      "\n",
      "Test set: Average loss: 1.7537 \n",
      "Accuracy: 3720/10000 (37.20%)\n",
      "\n",
      "Round  94, Average loss 1.754 Test accuracy 37.200\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018955785698360867\n",
      "conv1.bias 0.0023147969817121825\n",
      "conv2.weight 0.0012500267227490743\n",
      "conv2.bias 0.0024225604720413685\n",
      "fc1.weight 0.0005055349270502727\n",
      "fc1.bias 0.001879950612783432\n",
      "fc2.weight 0.001088481384610373\n",
      "fc2.bias 0.0034234427980014254\n",
      "fc3.weight 0.002513739608582996\n",
      "fc3.bias 0.0008450552821159362\n",
      "\n",
      "Test set: Average loss: 1.7458 \n",
      "Accuracy: 3751/10000 (37.51%)\n",
      "\n",
      "Round  95, Average loss 1.746 Test accuracy 37.510\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018967439068688287\n",
      "conv1.bias 0.0023459801450371742\n",
      "conv2.weight 0.0012587995330492657\n",
      "conv2.bias 0.002441170159727335\n",
      "fc1.weight 0.0005029373168945313\n",
      "fc1.bias 0.0018835189441839854\n",
      "fc2.weight 0.00108358415346297\n",
      "fc2.bias 0.0034485959580966403\n",
      "fc3.weight 0.0025082670506976898\n",
      "fc3.bias 0.0008366340771317482\n",
      "\n",
      "Test set: Average loss: 1.7456 \n",
      "Accuracy: 3746/10000 (37.46%)\n",
      "\n",
      "Round  96, Average loss 1.746 Test accuracy 37.460\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018898442056443955\n",
      "conv1.bias 0.0022979810213049254\n",
      "conv2.weight 0.0012557356556256612\n",
      "conv2.bias 0.002431246917694807\n",
      "fc1.weight 0.0005038448969523112\n",
      "fc1.bias 0.0018753744661808014\n",
      "fc2.weight 0.001085890569384136\n",
      "fc2.bias 0.0034421373690877643\n",
      "fc3.weight 0.0025090115410940986\n",
      "fc3.bias 0.0008400959894061088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.7486 \n",
      "Accuracy: 3713/10000 (37.13%)\n",
      "\n",
      "Round  97, Average loss 1.749 Test accuracy 37.130\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018885873423682318\n",
      "conv1.bias 0.0023551099002361298\n",
      "conv2.weight 0.0012576339642206829\n",
      "conv2.bias 0.0024314254987984896\n",
      "fc1.weight 0.0005040867328643799\n",
      "fc1.bias 0.0018664630750815073\n",
      "fc2.weight 0.0010849366112360879\n",
      "fc2.bias 0.0034407820729982284\n",
      "fc3.weight 0.002523747512272426\n",
      "fc3.bias 0.0008398231118917465\n",
      "\n",
      "Test set: Average loss: 1.7464 \n",
      "Accuracy: 3727/10000 (37.27%)\n",
      "\n",
      "Round  98, Average loss 1.746 Test accuracy 37.270\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018883309099409315\n",
      "conv1.bias 0.0023556696251034737\n",
      "conv2.weight 0.0012551220258076987\n",
      "conv2.bias 0.002435856033116579\n",
      "fc1.weight 0.000502463976542155\n",
      "fc1.bias 0.0018450329701105753\n",
      "fc2.weight 0.0010833583180866544\n",
      "fc2.bias 0.0034251071157909577\n",
      "fc3.weight 0.002528649568557739\n",
      "fc3.bias 0.0008549747988581657\n",
      "\n",
      "Test set: Average loss: 1.7444 \n",
      "Accuracy: 3738/10000 (37.38%)\n",
      "\n",
      "Round  99, Average loss 1.744 Test accuracy 37.380\n",
      "\n",
      "\n",
      "\n",
      "sigma = 1.5\n",
      "\n",
      "\n",
      "\n",
      "Learning Rate = 0.001\n",
      "\n",
      "\n",
      "\n",
      "z_array: [-0.81  0.81]\n",
      "0.3398845084624935\n",
      "0.3398845084624938\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 25000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 25000 \n",
      "\n",
      "(T, sigma)= 3 1.5 )  0 -th Trial!!\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.004484352535671658\n",
      "conv1.bias 0.0034941624229153\n",
      "conv2.weight 0.002255459229151408\n",
      "conv2.bias 0.0022501032799482346\n",
      "fc1.weight 0.0008372494379679362\n",
      "fc1.bias 0.0007484213759501775\n",
      "fc2.weight 0.0028220131283714658\n",
      "fc2.bias 0.0028398696865354267\n",
      "fc3.weight 0.003739445550101144\n",
      "fc3.bias 0.0025473210960626604\n",
      "\n",
      "Test set: Average loss: 2.0747 \n",
      "Accuracy: 2967/10000 (29.67%)\n",
      "\n",
      "Round   0, Average loss 2.075 Test accuracy 29.670\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0023515807257758247\n",
      "conv1.bias 0.0008918244081238905\n",
      "conv2.weight 0.0008929975827534994\n",
      "conv2.bias 0.0009356709197163582\n",
      "fc1.weight 0.00015009591976801554\n",
      "fc1.bias 0.00043835689624150597\n",
      "fc2.weight 0.000209008204558539\n",
      "fc2.bias 0.0011319236918574287\n",
      "fc3.weight 0.000786206481002626\n",
      "fc3.bias 0.001411175448447466\n",
      "\n",
      "Test set: Average loss: 1.8374 \n",
      "Accuracy: 3327/10000 (33.27%)\n",
      "\n",
      "Round   1, Average loss 1.837 Test accuracy 33.270\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018018531799316407\n",
      "conv1.bias 0.0011143852801372607\n",
      "conv2.weight 0.0009825933972994487\n",
      "conv2.bias 0.0011809926945716143\n",
      "fc1.weight 0.0003001265128453573\n",
      "fc1.bias 0.0008945501099030177\n",
      "fc2.weight 0.0004435031186966669\n",
      "fc2.bias 0.001707799555290313\n",
      "fc3.weight 0.0010058560541697911\n",
      "fc3.bias 0.0008372636511921883\n",
      "\n",
      "Test set: Average loss: 1.7925 \n",
      "Accuracy: 3509/10000 (35.09%)\n",
      "\n",
      "Round   2, Average loss 1.792 Test accuracy 35.090\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001569394138124254\n",
      "conv1.bias 0.0011192335126300652\n",
      "conv2.weight 0.0009739714860916138\n",
      "conv2.bias 0.0010823451448231936\n",
      "fc1.weight 0.00034334266185760497\n",
      "fc1.bias 0.0011395053317149481\n",
      "fc2.weight 0.0005709637252111283\n",
      "fc2.bias 0.0022530419131120047\n",
      "fc3.weight 0.001233926841190883\n",
      "fc3.bias 0.0008227776736021042\n",
      "\n",
      "Test set: Average loss: 1.7888 \n",
      "Accuracy: 3537/10000 (35.37%)\n",
      "\n",
      "Round   3, Average loss 1.789 Test accuracy 35.370\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0015550369686550564\n",
      "conv1.bias 0.0011878924754758675\n",
      "conv2.weight 0.0009649532039960225\n",
      "conv2.bias 0.0010561771923676133\n",
      "fc1.weight 0.00036585346857706704\n",
      "fc1.bias 0.0014009607334931691\n",
      "fc2.weight 0.0006408865016604226\n",
      "fc2.bias 0.00271041744521686\n",
      "fc3.weight 0.0014170201051802863\n",
      "fc3.bias 0.0009615479968488217\n",
      "\n",
      "Test set: Average loss: 1.7866 \n",
      "Accuracy: 3559/10000 (35.59%)\n",
      "\n",
      "Round   4, Average loss 1.787 Test accuracy 35.590\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001600778897603353\n",
      "conv1.bias 0.0013130886169771354\n",
      "conv2.weight 0.000961336890856425\n",
      "conv2.bias 0.001055383589118719\n",
      "fc1.weight 0.00038178964455922446\n",
      "fc1.bias 0.0016048916925986607\n",
      "fc2.weight 0.0006892246859414237\n",
      "fc2.bias 0.0030265606585003083\n",
      "fc3.weight 0.0015414457945596605\n",
      "fc3.bias 0.0011002558283507824\n",
      "\n",
      "Test set: Average loss: 1.7811 \n",
      "Accuracy: 3568/10000 (35.68%)\n",
      "\n",
      "Round   5, Average loss 1.781 Test accuracy 35.680\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0016585779190063476\n",
      "conv1.bias 0.0015031425282359123\n",
      "conv2.weight 0.0009733437498410543\n",
      "conv2.bias 0.001073390943929553\n",
      "fc1.weight 0.0003980901638666789\n",
      "fc1.bias 0.0017297364771366119\n",
      "fc2.weight 0.0007403907794801016\n",
      "fc2.bias 0.003162043790022532\n",
      "fc3.weight 0.001664325027238755\n",
      "fc3.bias 0.0011904981918632984\n",
      "\n",
      "Test set: Average loss: 1.7773 \n",
      "Accuracy: 3603/10000 (36.03%)\n",
      "\n",
      "Round   6, Average loss 1.777 Test accuracy 36.030\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0017315803633795844\n",
      "conv1.bias 0.0016082568715016048\n",
      "conv2.weight 0.0009849673509597777\n",
      "conv2.bias 0.0011170369107276201\n",
      "fc1.weight 0.00041226478417714436\n",
      "fc1.bias 0.0018036220222711562\n",
      "fc2.weight 0.0007733964730822851\n",
      "fc2.bias 0.00334052528653826\n",
      "fc3.weight 0.0017610220682053339\n",
      "fc3.bias 0.0012772868387401104\n",
      "\n",
      "Test set: Average loss: 1.7777 \n",
      "Accuracy: 3597/10000 (35.97%)\n",
      "\n",
      "Round   7, Average loss 1.778 Test accuracy 35.970\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018031213018629285\n",
      "conv1.bias 0.0017112498171627522\n",
      "conv2.weight 0.0010016314188639323\n",
      "conv2.bias 0.0011484152637422085\n",
      "fc1.weight 0.00042069260279337567\n",
      "fc1.bias 0.0018131886919339499\n",
      "fc2.weight 0.0008023841040475028\n",
      "fc2.bias 0.0034886073498498825\n",
      "fc3.weight 0.0018504988579522996\n",
      "fc3.bias 0.001307559758424759\n",
      "\n",
      "Test set: Average loss: 1.7664 \n",
      "Accuracy: 3640/10000 (36.40%)\n",
      "\n",
      "Round   8, Average loss 1.766 Test accuracy 36.400\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0018594911363389757\n",
      "conv1.bias 0.0018168364961942036\n",
      "conv2.weight 0.001022949715455373\n",
      "conv2.bias 0.0011861041421070695\n",
      "fc1.weight 0.0004318076769510905\n",
      "fc1.bias 0.0017888194570938746\n",
      "fc2.weight 0.0008271066915421258\n",
      "fc2.bias 0.0035806833988144284\n",
      "fc3.weight 0.0019146138713473366\n",
      "fc3.bias 0.0012923062779009343\n",
      "\n",
      "Test set: Average loss: 1.7584 \n",
      "Accuracy: 3688/10000 (36.88%)\n",
      "\n",
      "Round   9, Average loss 1.758 Test accuracy 36.880\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019169935915205214\n",
      "conv1.bias 0.0018631936982274055\n",
      "conv2.weight 0.001037872036298116\n",
      "conv2.bias 0.0011913543567061424\n",
      "fc1.weight 0.0004383172194163005\n",
      "fc1.bias 0.0017675422132015228\n",
      "fc2.weight 0.0008435266358511788\n",
      "fc2.bias 0.0036130331101871674\n",
      "fc3.weight 0.0019509491466340565\n",
      "fc3.bias 0.0012593531049788\n",
      "\n",
      "Test set: Average loss: 1.7512 \n",
      "Accuracy: 3721/10000 (37.21%)\n",
      "\n",
      "Round  10, Average loss 1.751 Test accuracy 37.210\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0019511535432603624\n",
      "conv1.bias 0.0019262169177333515\n",
      "conv2.weight 0.0010563325881958008\n",
      "conv2.bias 0.001209744717925787\n",
      "fc1.weight 0.0004452524185180664\n",
      "fc1.bias 0.0017321753005186717\n",
      "fc2.weight 0.0008582313855489095\n",
      "fc2.bias 0.0036690103865805127\n",
      "fc3.weight 0.0019850078083219983\n",
      "fc3.bias 0.0012274816632270812\n",
      "\n",
      "Test set: Average loss: 1.7453 \n",
      "Accuracy: 3740/10000 (37.40%)\n",
      "\n",
      "Round  11, Average loss 1.745 Test accuracy 37.400\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.001984022723303901\n",
      "conv1.bias 0.001914273016154766\n",
      "conv2.weight 0.0010641395052274068\n",
      "conv2.bias 0.0012033472303301096\n",
      "fc1.weight 0.0004498894214630127\n",
      "fc1.bias 0.001695455734928449\n",
      "fc2.weight 0.000876454890720428\n",
      "fc2.bias 0.0036401652864047457\n",
      "fc3.weight 0.0020210808231717063\n",
      "fc3.bias 0.0011864053085446358\n",
      "\n",
      "Test set: Average loss: 1.7401 \n",
      "Accuracy: 3772/10000 (37.72%)\n",
      "\n",
      "Round  12, Average loss 1.740 Test accuracy 37.720\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020090516408284506\n",
      "conv1.bias 0.0018912625188628833\n",
      "conv2.weight 0.0010704906781514485\n",
      "conv2.bias 0.0012050195364281535\n",
      "fc1.weight 0.0004540047248204549\n",
      "fc1.bias 0.0016796885679165523\n",
      "fc2.weight 0.0008828301278371659\n",
      "fc2.bias 0.0036577777493567694\n",
      "fc3.weight 0.0020367697590873354\n",
      "fc3.bias 0.001175309345126152\n",
      "\n",
      "Test set: Average loss: 1.7382 \n",
      "Accuracy: 3760/10000 (37.60%)\n",
      "\n",
      "Round  13, Average loss 1.738 Test accuracy 37.600\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00202862368689643\n",
      "conv1.bias 0.0018952636358638604\n",
      "conv2.weight 0.0010701358318328858\n",
      "conv2.bias 0.0011943456484004855\n",
      "fc1.weight 0.0004564372698465983\n",
      "fc1.bias 0.0016660980880260467\n",
      "fc2.weight 0.0008878039935278514\n",
      "fc2.bias 0.0036322946349779763\n",
      "fc3.weight 0.002058102261452448\n",
      "fc3.bias 0.0011528275907039642\n",
      "\n",
      "Test set: Average loss: 1.7393 \n",
      "Accuracy: 3771/10000 (37.71%)\n",
      "\n",
      "Round  14, Average loss 1.739 Test accuracy 37.710\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020463656054602727\n",
      "conv1.bias 0.0018720513520141442\n",
      "conv2.weight 0.001075680653254191\n",
      "conv2.bias 0.0011926264269277453\n",
      "fc1.weight 0.0004605277379353841\n",
      "fc1.bias 0.0016562383621931075\n",
      "fc2.weight 0.0008921213566310823\n",
      "fc2.bias 0.003605707770302182\n",
      "fc3.weight 0.0020757746128808882\n",
      "fc3.bias 0.0011562643572688102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.7379 \n",
      "Accuracy: 3790/10000 (37.90%)\n",
      "\n",
      "Round  15, Average loss 1.738 Test accuracy 37.900\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020549302630954317\n",
      "conv1.bias 0.0019278304340938728\n",
      "conv2.weight 0.0010803008079528808\n",
      "conv2.bias 0.0011954773217439651\n",
      "fc1.weight 0.00046063204606374103\n",
      "fc1.bias 0.001656473676363627\n",
      "fc2.weight 0.0008969520765637595\n",
      "fc2.bias 0.0035785862377711703\n",
      "fc3.weight 0.002091331425167265\n",
      "fc3.bias 0.0011519666761159897\n",
      "\n",
      "Test set: Average loss: 1.7365 \n",
      "Accuracy: 3802/10000 (38.02%)\n",
      "\n",
      "Round  16, Average loss 1.737 Test accuracy 38.020\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002079577843348185\n",
      "conv1.bias 0.001894262619316578\n",
      "conv2.weight 0.0010809688766797384\n",
      "conv2.bias 0.0011924391146749258\n",
      "fc1.weight 0.0004604579210281372\n",
      "fc1.bias 0.0016504645347595215\n",
      "fc2.weight 0.0008996696699233283\n",
      "fc2.bias 0.0035529381462505887\n",
      "fc3.weight 0.0021004909560793923\n",
      "fc3.bias 0.0011361107230186463\n",
      "\n",
      "Test set: Average loss: 1.7396 \n",
      "Accuracy: 3811/10000 (38.11%)\n",
      "\n",
      "Round  17, Average loss 1.740 Test accuracy 38.110\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021075571907891166\n",
      "conv1.bias 0.0018400492457052071\n",
      "conv2.weight 0.0010780256986618042\n",
      "conv2.bias 0.0011901311809197068\n",
      "fc1.weight 0.0004613089164098104\n",
      "fc1.bias 0.0016250789165496826\n",
      "fc2.weight 0.0009039240224020822\n",
      "fc2.bias 0.0035377691189448037\n",
      "fc3.weight 0.0021122861476171586\n",
      "fc3.bias 0.0011456371285021305\n",
      "\n",
      "Test set: Average loss: 1.7396 \n",
      "Accuracy: 3814/10000 (38.14%)\n",
      "\n",
      "Round  18, Average loss 1.740 Test accuracy 38.140\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021149849891662596\n",
      "conv1.bias 0.0017696479335427284\n",
      "conv2.weight 0.0010761916637420654\n",
      "conv2.bias 0.0012055689003318548\n",
      "fc1.weight 0.00046219587326049803\n",
      "fc1.bias 0.0016207824150721232\n",
      "fc2.weight 0.0009069009432716975\n",
      "fc2.bias 0.0035868024542218165\n",
      "fc3.weight 0.002122427594094049\n",
      "fc3.bias 0.0011203519068658351\n",
      "\n",
      "Test set: Average loss: 1.7395 \n",
      "Accuracy: 3827/10000 (38.27%)\n",
      "\n",
      "Round  19, Average loss 1.740 Test accuracy 38.270\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021252642737494575\n",
      "conv1.bias 0.0017797894154985745\n",
      "conv2.weight 0.001079573929309845\n",
      "conv2.bias 0.00118874735198915\n",
      "fc1.weight 0.00046338280042012534\n",
      "fc1.bias 0.0016074032833178838\n",
      "fc2.weight 0.0009137664522443499\n",
      "fc2.bias 0.0035623407789639066\n",
      "fc3.weight 0.002136186474845523\n",
      "fc3.bias 0.001126561313867569\n",
      "\n",
      "Test set: Average loss: 1.7418 \n",
      "Accuracy: 3792/10000 (37.92%)\n",
      "\n",
      "Round  20, Average loss 1.742 Test accuracy 37.920\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021402263641357422\n",
      "conv1.bias 0.0018022141108910243\n",
      "conv2.weight 0.001081471045811971\n",
      "conv2.bias 0.0011771798599511385\n",
      "fc1.weight 0.0004639891783396403\n",
      "fc1.bias 0.0016054083903630574\n",
      "fc2.weight 0.0009162837550753639\n",
      "fc2.bias 0.0035938705716814312\n",
      "fc3.weight 0.0021457376934233166\n",
      "fc3.bias 0.0011171317659318448\n",
      "\n",
      "Test set: Average loss: 1.7374 \n",
      "Accuracy: 3840/10000 (38.40%)\n",
      "\n",
      "Round  21, Average loss 1.737 Test accuracy 38.400\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021642427974277074\n",
      "conv1.bias 0.0017879786901175976\n",
      "conv2.weight 0.0010794731974601746\n",
      "conv2.bias 0.0011943918652832508\n",
      "fc1.weight 0.0004647053082784017\n",
      "fc1.bias 0.0016163203865289689\n",
      "fc2.weight 0.00091928726150876\n",
      "fc2.bias 0.0035946411745888846\n",
      "fc3.weight 0.002157606113524664\n",
      "fc3.bias 0.001114883366972208\n",
      "\n",
      "Test set: Average loss: 1.7376 \n",
      "Accuracy: 3818/10000 (38.18%)\n",
      "\n",
      "Round  22, Average loss 1.738 Test accuracy 38.180\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021596630414326985\n",
      "conv1.bias 0.001827770999322335\n",
      "conv2.weight 0.0010815135637919108\n",
      "conv2.bias 0.0011811668518930674\n",
      "fc1.weight 0.0004651947816212972\n",
      "fc1.bias 0.0016246361037095388\n",
      "fc2.weight 0.0009210457877507286\n",
      "fc2.bias 0.003624669143131801\n",
      "fc3.weight 0.002156598227364676\n",
      "fc3.bias 0.0011212654411792756\n",
      "\n",
      "Test set: Average loss: 1.7354 \n",
      "Accuracy: 3819/10000 (38.19%)\n",
      "\n",
      "Round  23, Average loss 1.735 Test accuracy 38.190\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002161205609639486\n",
      "conv1.bias 0.0018497193232178688\n",
      "conv2.weight 0.0010793618361155193\n",
      "conv2.bias 0.0011743416544049978\n",
      "fc1.weight 0.0004650583267211914\n",
      "fc1.bias 0.0016352643569310507\n",
      "fc2.weight 0.0009210520320468479\n",
      "fc2.bias 0.00362612981171835\n",
      "fc3.weight 0.0021563189370291573\n",
      "fc3.bias 0.0011039510369300841\n",
      "\n",
      "Test set: Average loss: 1.7366 \n",
      "Accuracy: 3832/10000 (38.32%)\n",
      "\n",
      "Round  24, Average loss 1.737 Test accuracy 38.320\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002173762983745999\n",
      "conv1.bias 0.0018275633143881957\n",
      "conv2.weight 0.0010783197482426962\n",
      "conv2.bias 0.0011803200468420982\n",
      "fc1.weight 0.00046562592188517253\n",
      "fc1.bias 0.0016368016600608825\n",
      "fc2.weight 0.0009214802393837581\n",
      "fc2.bias 0.003601422267300742\n",
      "fc3.weight 0.002161872386932373\n",
      "fc3.bias 0.0011115774512290954\n",
      "\n",
      "Test set: Average loss: 1.7370 \n",
      "Accuracy: 3802/10000 (38.02%)\n",
      "\n",
      "Round  25, Average loss 1.737 Test accuracy 38.020\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00217925681008233\n",
      "conv1.bias 0.001813630573451519\n",
      "conv2.weight 0.0010773142178853352\n",
      "conv2.bias 0.0011790955904871225\n",
      "fc1.weight 0.00046697052319844565\n",
      "fc1.bias 0.0016324061900377274\n",
      "fc2.weight 0.0009219320993574839\n",
      "fc2.bias 0.0035720052463667734\n",
      "fc3.weight 0.002165929476420085\n",
      "fc3.bias 0.0011092875152826308\n",
      "\n",
      "Test set: Average loss: 1.7391 \n",
      "Accuracy: 3803/10000 (38.03%)\n",
      "\n",
      "Round  26, Average loss 1.739 Test accuracy 38.030\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021700673633151583\n",
      "conv1.bias 0.0018597437689701717\n",
      "conv2.weight 0.0010817158222198487\n",
      "conv2.bias 0.0011765684466809034\n",
      "fc1.weight 0.00046725583076477053\n",
      "fc1.bias 0.0016405211140712102\n",
      "fc2.weight 0.0009204294000353132\n",
      "fc2.bias 0.0035766249611264185\n",
      "fc3.weight 0.002171447731199719\n",
      "fc3.bias 0.0011202029883861542\n",
      "\n",
      "Test set: Average loss: 1.7372 \n",
      "Accuracy: 3810/10000 (38.10%)\n",
      "\n",
      "Round  27, Average loss 1.737 Test accuracy 38.100\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002180515660179986\n",
      "conv1.bias 0.0017961577201882999\n",
      "conv2.weight 0.001084824502468109\n",
      "conv2.bias 0.0011991590727120638\n",
      "fc1.weight 0.00046746885776519776\n",
      "fc1.bias 0.0016403769453366597\n",
      "fc2.weight 0.0009257613666473874\n",
      "fc2.bias 0.003616088203021458\n",
      "fc3.weight 0.002182097662062872\n",
      "fc3.bias 0.0011356704868376254\n",
      "\n",
      "Test set: Average loss: 1.7345 \n",
      "Accuracy: 3832/10000 (38.32%)\n",
      "\n",
      "Round  28, Average loss 1.735 Test accuracy 38.320\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002182412147521973\n",
      "conv1.bias 0.0018285828021665413\n",
      "conv2.weight 0.0010849263270696005\n",
      "conv2.bias 0.0012092289980500937\n",
      "fc1.weight 0.0004690280755360921\n",
      "fc1.bias 0.0016435513893763225\n",
      "fc2.weight 0.0009275293539440821\n",
      "fc2.bias 0.0036348129312197366\n",
      "fc3.weight 0.0021853129069010417\n",
      "fc3.bias 0.0011385108344256877\n",
      "\n",
      "Test set: Average loss: 1.7339 \n",
      "Accuracy: 3829/10000 (38.29%)\n",
      "\n",
      "Round  29, Average loss 1.734 Test accuracy 38.290\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021890396542019316\n",
      "conv1.bias 0.001840719332297643\n",
      "conv2.weight 0.001083513895670573\n",
      "conv2.bias 0.0012347727315500379\n",
      "fc1.weight 0.00046713777383168537\n",
      "fc1.bias 0.0016395183900992075\n",
      "fc2.weight 0.0009225113997383723\n",
      "fc2.bias 0.0036343389323779513\n",
      "fc3.weight 0.0021849697544461203\n",
      "fc3.bias 0.001118442788720131\n",
      "\n",
      "Test set: Average loss: 1.7306 \n",
      "Accuracy: 3838/10000 (38.38%)\n",
      "\n",
      "Round  30, Average loss 1.731 Test accuracy 38.380\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0022013272179497614\n",
      "conv1.bias 0.0018199970945715904\n",
      "conv2.weight 0.00108513077100118\n",
      "conv2.bias 0.001233450835570693\n",
      "fc1.weight 0.0004672757387161255\n",
      "fc1.bias 0.0016318635394175847\n",
      "fc2.weight 0.0009202406519935245\n",
      "fc2.bias 0.003684664411204202\n",
      "fc3.weight 0.0021887652930759247\n",
      "fc3.bias 0.0011418050155043602\n",
      "\n",
      "Test set: Average loss: 1.7277 \n",
      "Accuracy: 3848/10000 (38.48%)\n",
      "\n",
      "Round  31, Average loss 1.728 Test accuracy 38.480\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002203629281785753\n",
      "conv1.bias 0.0018597251425186794\n",
      "conv2.weight 0.001085255742073059\n",
      "conv2.bias 0.0012425434542819858\n",
      "fc1.weight 0.00046854209899902346\n",
      "fc1.bias 0.0016328521072864533\n",
      "fc2.weight 0.0009204369688790942\n",
      "fc2.bias 0.0036397665029480344\n",
      "fc3.weight 0.00219251967611767\n",
      "fc3.bias 0.0011336766183376312\n",
      "\n",
      "Test set: Average loss: 1.7286 \n",
      "Accuracy: 3844/10000 (38.44%)\n",
      "\n",
      "Round  32, Average loss 1.729 Test accuracy 38.440\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0022180721494886612\n",
      "conv1.bias 0.0018703402020037174\n",
      "conv2.weight 0.0010835336645444234\n",
      "conv2.bias 0.0012236564652994275\n",
      "fc1.weight 0.0004661996364593506\n",
      "fc1.bias 0.0016237199306488037\n",
      "fc2.weight 0.0009161801565261115\n",
      "fc2.bias 0.0036055640805335272\n",
      "fc3.weight 0.0021953455039433072\n",
      "fc3.bias 0.0011331588961184026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.7310 \n",
      "Accuracy: 3837/10000 (38.37%)\n",
      "\n",
      "Round  33, Average loss 1.731 Test accuracy 38.370\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002219307025273641\n",
      "conv1.bias 0.001837986521422863\n",
      "conv2.weight 0.0010811958710352579\n",
      "conv2.bias 0.0012401589192450047\n",
      "fc1.weight 0.0004654637575149536\n",
      "fc1.bias 0.0016230656454960504\n",
      "fc2.weight 0.0009168819775657049\n",
      "fc2.bias 0.0036480912850016638\n",
      "fc3.weight 0.0021937736443110875\n",
      "fc3.bias 0.0011317544616758824\n",
      "\n",
      "Test set: Average loss: 1.7276 \n",
      "Accuracy: 3838/10000 (38.38%)\n",
      "\n",
      "Round  34, Average loss 1.728 Test accuracy 38.380\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002227208349439833\n",
      "conv1.bias 0.0018287294854720433\n",
      "conv2.weight 0.0010825615127881367\n",
      "conv2.bias 0.0012235069880262017\n",
      "fc1.weight 0.00046705734729766843\n",
      "fc1.bias 0.0016002729535102844\n",
      "fc2.weight 0.0009172347802964468\n",
      "fc2.bias 0.0036748849919864108\n",
      "fc3.weight 0.0021978582654680525\n",
      "fc3.bias 0.0011239253915846347\n",
      "\n",
      "Test set: Average loss: 1.7330 \n",
      "Accuracy: 3812/10000 (38.12%)\n",
      "\n",
      "Round  35, Average loss 1.733 Test accuracy 38.120\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002207868364122179\n",
      "conv1.bias 0.0018343948759138584\n",
      "conv2.weight 0.0010865765810012817\n",
      "conv2.bias 0.0012488311622291803\n",
      "fc1.weight 0.0004670495986938477\n",
      "fc1.bias 0.0016159902016321817\n",
      "fc2.weight 0.0009172144390287853\n",
      "fc2.bias 0.003700034249396551\n",
      "fc3.weight 0.0022060099102201914\n",
      "fc3.bias 0.001129683293402195\n",
      "\n",
      "Test set: Average loss: 1.7292 \n",
      "Accuracy: 3836/10000 (38.36%)\n",
      "\n",
      "Round  36, Average loss 1.729 Test accuracy 38.360\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0022233899434407553\n",
      "conv1.bias 0.001820929969350497\n",
      "conv2.weight 0.0010888053973515828\n",
      "conv2.bias 0.0012573665007948875\n",
      "fc1.weight 0.00046560728549957276\n",
      "fc1.bias 0.001623540868361791\n",
      "fc2.weight 0.000914048675506834\n",
      "fc2.bias 0.003733286900179727\n",
      "fc3.weight 0.002204393063272749\n",
      "fc3.bias 0.0011088652536273003\n",
      "\n",
      "Test set: Average loss: 1.7325 \n",
      "Accuracy: 3814/10000 (38.14%)\n",
      "\n",
      "Round  37, Average loss 1.733 Test accuracy 38.140\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0022215411398145888\n",
      "conv1.bias 0.0018083999554316204\n",
      "conv2.weight 0.0010859102010726928\n",
      "conv2.bias 0.0012682611122727394\n",
      "fc1.weight 0.000464956800142924\n",
      "fc1.bias 0.001634824648499489\n",
      "fc2.weight 0.0009176050859784323\n",
      "fc2.bias 0.00375392252490634\n",
      "fc3.weight 0.002207658120564052\n",
      "fc3.bias 0.0011022337712347507\n",
      "\n",
      "Test set: Average loss: 1.7326 \n",
      "Accuracy: 3811/10000 (38.11%)\n",
      "\n",
      "Round  38, Average loss 1.733 Test accuracy 38.110\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002211990223990546\n",
      "conv1.bias 0.0017640615502993267\n",
      "conv2.weight 0.001087467074394226\n",
      "conv2.bias 0.0012590308906510472\n",
      "fc1.weight 0.0004658714135487874\n",
      "fc1.bias 0.0016378464798132578\n",
      "fc2.weight 0.0009159667151314872\n",
      "fc2.bias 0.003761330885546548\n",
      "fc3.weight 0.0021985394614083427\n",
      "fc3.bias 0.0011334474198520183\n",
      "\n",
      "Test set: Average loss: 1.7311 \n",
      "Accuracy: 3837/10000 (38.37%)\n",
      "\n",
      "Round  39, Average loss 1.731 Test accuracy 38.370\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0022008785936567517\n",
      "conv1.bias 0.0017951860403021176\n",
      "conv2.weight 0.0010841854413350424\n",
      "conv2.bias 0.0012665223330259323\n",
      "fc1.weight 0.000465011199315389\n",
      "fc1.bias 0.0016506134221951168\n",
      "fc2.weight 0.0009160617041209388\n",
      "fc2.bias 0.0037471705249377658\n",
      "fc3.weight 0.0021981508958907356\n",
      "fc3.bias 0.0011270223185420036\n",
      "\n",
      "Test set: Average loss: 1.7337 \n",
      "Accuracy: 3831/10000 (38.31%)\n",
      "\n",
      "Round  40, Average loss 1.734 Test accuracy 38.310\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021887365976969403\n",
      "conv1.bias 0.001818030762175719\n",
      "conv2.weight 0.0010836629072825115\n",
      "conv2.bias 0.001267607556656003\n",
      "fc1.weight 0.0004650635719299316\n",
      "fc1.bias 0.0016642389198144277\n",
      "fc2.weight 0.0009155645256950742\n",
      "fc2.bias 0.0037482508591243197\n",
      "fc3.weight 0.0022031144017264958\n",
      "fc3.bias 0.0011322874575853347\n",
      "\n",
      "Test set: Average loss: 1.7300 \n",
      "Accuracy: 3853/10000 (38.53%)\n",
      "\n",
      "Round  41, Average loss 1.730 Test accuracy 38.530\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021948349475860596\n",
      "conv1.bias 0.0018694748481114705\n",
      "conv2.weight 0.0010851542154947916\n",
      "conv2.bias 0.0012832763604819775\n",
      "fc1.weight 0.0004641633828481038\n",
      "fc1.bias 0.0016677419344584147\n",
      "fc2.weight 0.0009142559672158862\n",
      "fc2.bias 0.003768684608595712\n",
      "fc3.weight 0.0022025491510118756\n",
      "fc3.bias 0.001106984820216894\n",
      "\n",
      "Test set: Average loss: 1.7309 \n",
      "Accuracy: 3839/10000 (38.39%)\n",
      "\n",
      "Round  42, Average loss 1.731 Test accuracy 38.390\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002176080544789632\n",
      "conv1.bias 0.0018235294458766778\n",
      "conv2.weight 0.0010883571704228719\n",
      "conv2.bias 0.0012867755722254515\n",
      "fc1.weight 0.00046416417757670083\n",
      "fc1.bias 0.0016658015549182892\n",
      "fc2.weight 0.000915992259979248\n",
      "fc2.bias 0.003743369664464678\n",
      "fc3.weight 0.002202439591998146\n",
      "fc3.bias 0.0011037645861506462\n",
      "\n",
      "Test set: Average loss: 1.7344 \n",
      "Accuracy: 3826/10000 (38.26%)\n",
      "\n",
      "Round  43, Average loss 1.734 Test accuracy 38.260\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021799133883582222\n",
      "conv1.bias 0.0018421993590891361\n",
      "conv2.weight 0.0010866667826970417\n",
      "conv2.bias 0.0012718273792415857\n",
      "fc1.weight 0.00046256824334462484\n",
      "fc1.bias 0.0016476248701413473\n",
      "fc2.weight 0.000911263246384878\n",
      "fc2.bias 0.003731328816640945\n",
      "fc3.weight 0.0022041430075963337\n",
      "fc3.bias 0.0011216290295124054\n",
      "\n",
      "Test set: Average loss: 1.7309 \n",
      "Accuracy: 3841/10000 (38.41%)\n",
      "\n",
      "Round  44, Average loss 1.731 Test accuracy 38.410\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021747684478759764\n",
      "conv1.bias 0.0018218383193016052\n",
      "conv2.weight 0.0010869660973548889\n",
      "conv2.bias 0.0012680233921855688\n",
      "fc1.weight 0.00046181106567382815\n",
      "fc1.bias 0.0016687904795010884\n",
      "fc2.weight 0.0009112332548413958\n",
      "fc2.bias 0.0037113353610038757\n",
      "fc3.weight 0.002196191180320013\n",
      "fc3.bias 0.0011024003848433494\n",
      "\n",
      "Test set: Average loss: 1.7332 \n",
      "Accuracy: 3833/10000 (38.33%)\n",
      "\n",
      "Round  45, Average loss 1.733 Test accuracy 38.330\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021719284852345783\n",
      "conv1.bias 0.0017601813500126202\n",
      "conv2.weight 0.0010877775152524313\n",
      "conv2.bias 0.0012570726685225964\n",
      "fc1.weight 0.0004613121747970581\n",
      "fc1.bias 0.0016693495213985442\n",
      "fc2.weight 0.0009133810088748024\n",
      "fc2.bias 0.0036948766736757187\n",
      "fc3.weight 0.002201349252746219\n",
      "fc3.bias 0.0011051533743739127\n",
      "\n",
      "Test set: Average loss: 1.7358 \n",
      "Accuracy: 3821/10000 (38.21%)\n",
      "\n",
      "Round  46, Average loss 1.736 Test accuracy 38.210\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002169799009958903\n",
      "conv1.bias 0.0017633051611483097\n",
      "conv2.weight 0.0010916403929392498\n",
      "conv2.bias 0.0012667359551414847\n",
      "fc1.weight 0.00046165788173675536\n",
      "fc1.bias 0.0016551510741313299\n",
      "fc2.weight 0.0009123592149643671\n",
      "fc2.bias 0.003713310119651613\n",
      "fc3.weight 0.002205683645747957\n",
      "fc3.bias 0.0010937721468508243\n",
      "\n",
      "Test set: Average loss: 1.7360 \n",
      "Accuracy: 3832/10000 (38.32%)\n",
      "\n",
      "Round  47, Average loss 1.736 Test accuracy 38.320\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002174120479159885\n",
      "conv1.bias 0.0017836006979147594\n",
      "conv2.weight 0.001098281443119049\n",
      "conv2.bias 0.001274262322112918\n",
      "fc1.weight 0.0004623223940531413\n",
      "fc1.bias 0.0016355807582537332\n",
      "fc2.weight 0.000915529519792587\n",
      "fc2.bias 0.003755576553798857\n",
      "fc3.weight 0.002204749413899013\n",
      "fc3.bias 0.0010829094797372818\n",
      "\n",
      "Test set: Average loss: 1.7385 \n",
      "Accuracy: 3805/10000 (38.05%)\n",
      "\n",
      "Round  48, Average loss 1.738 Test accuracy 38.050\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021900273693932428\n",
      "conv1.bias 0.0017612087540328503\n",
      "conv2.weight 0.0010997583468755087\n",
      "conv2.bias 0.0012832877691835165\n",
      "fc1.weight 0.00046134106318155925\n",
      "fc1.bias 0.0016343346486488977\n",
      "fc2.weight 0.0009119263717106411\n",
      "fc2.bias 0.0037310240524155752\n",
      "fc3.weight 0.0022069556372506277\n",
      "fc3.bias 0.001096002198755741\n",
      "\n",
      "Test set: Average loss: 1.7405 \n",
      "Accuracy: 3787/10000 (37.87%)\n",
      "\n",
      "Round  49, Average loss 1.741 Test accuracy 37.870\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021822592947218153\n",
      "conv1.bias 0.001824873344351848\n",
      "conv2.weight 0.001098311146100362\n",
      "conv2.bias 0.001255188719369471\n",
      "fc1.weight 0.00046220600605010985\n",
      "fc1.bias 0.0016296924402316411\n",
      "fc2.weight 0.000914567992800758\n",
      "fc2.bias 0.0037239729648544675\n",
      "fc3.weight 0.0022020453498477026\n",
      "fc3.bias 0.0010761025361716748\n",
      "\n",
      "Test set: Average loss: 1.7396 \n",
      "Accuracy: 3787/10000 (37.87%)\n",
      "\n",
      "Round  50, Average loss 1.740 Test accuracy 37.870\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00218029432826572\n",
      "conv1.bias 0.0018112785182893276\n",
      "conv2.weight 0.0010996466875076293\n",
      "conv2.bias 0.0012549124658107758\n",
      "fc1.weight 0.0004609740177790324\n",
      "fc1.bias 0.0016212426126003265\n",
      "fc2.weight 0.0009116691256326342\n",
      "fc2.bias 0.0037227123975753784\n",
      "fc3.weight 0.002198817900248936\n",
      "fc3.bias 0.0010839595459401607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.7346 \n",
      "Accuracy: 3822/10000 (38.22%)\n",
      "\n",
      "Round  51, Average loss 1.735 Test accuracy 38.220\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021841031975216338\n",
      "conv1.bias 0.0018201310498019059\n",
      "conv2.weight 0.0011062024037043253\n",
      "conv2.bias 0.0012405698653310537\n",
      "fc1.weight 0.0004601288636525472\n",
      "fc1.bias 0.00163738081852595\n",
      "fc2.weight 0.0009121942141699413\n",
      "fc2.bias 0.003725943466027578\n",
      "fc3.weight 0.002205135850679307\n",
      "fc3.bias 0.001082715019583702\n",
      "\n",
      "Test set: Average loss: 1.7342 \n",
      "Accuracy: 3831/10000 (38.31%)\n",
      "\n",
      "Round  52, Average loss 1.734 Test accuracy 38.310\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002183416287104289\n",
      "conv1.bias 0.0017512341340382893\n",
      "conv2.weight 0.0011097170909245809\n",
      "conv2.bias 0.001228222157806158\n",
      "fc1.weight 0.00045987602074941\n",
      "fc1.bias 0.0016183458268642426\n",
      "fc2.weight 0.0009107921804700578\n",
      "fc2.bias 0.003664627316452208\n",
      "fc3.weight 0.0022028048833211264\n",
      "fc3.bias 0.0010859815403819085\n",
      "\n",
      "Test set: Average loss: 1.7328 \n",
      "Accuracy: 3823/10000 (38.23%)\n",
      "\n",
      "Round  53, Average loss 1.733 Test accuracy 38.230\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002185717953575982\n",
      "conv1.bias 0.0018142876215279102\n",
      "conv2.weight 0.0011063859860102336\n",
      "conv2.bias 0.001226465916261077\n",
      "fc1.weight 0.0004588894446690877\n",
      "fc1.bias 0.001631598547101021\n",
      "fc2.weight 0.0009055883165389772\n",
      "fc2.bias 0.0036721680135953995\n",
      "fc3.weight 0.0021984352951958065\n",
      "fc3.bias 0.0010863741859793662\n",
      "\n",
      "Test set: Average loss: 1.7328 \n",
      "Accuracy: 3809/10000 (38.09%)\n",
      "\n",
      "Round  54, Average loss 1.733 Test accuracy 38.090\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021908068656921386\n",
      "conv1.bias 0.0018065320327877998\n",
      "conv2.weight 0.0011098961035410563\n",
      "conv2.bias 0.001224464038386941\n",
      "fc1.weight 0.0004597881635030111\n",
      "fc1.bias 0.0016406220694382986\n",
      "fc2.weight 0.0009088101841154552\n",
      "fc2.bias 0.0036840587854385376\n",
      "fc3.weight 0.002195646365483602\n",
      "fc3.bias 0.0011013324372470378\n",
      "\n",
      "Test set: Average loss: 1.7351 \n",
      "Accuracy: 3819/10000 (38.19%)\n",
      "\n",
      "Round  55, Average loss 1.735 Test accuracy 38.190\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021855174170600043\n",
      "conv1.bias 0.0018156711012125015\n",
      "conv2.weight 0.0011106322209040323\n",
      "conv2.bias 0.001234619878232479\n",
      "fc1.weight 0.00045886985460917156\n",
      "fc1.bias 0.0016466771562894185\n",
      "fc2.weight 0.000907189410830301\n",
      "fc2.bias 0.0036926216312817167\n",
      "fc3.weight 0.002202175060908\n",
      "fc3.bias 0.0011038411408662795\n",
      "\n",
      "Test set: Average loss: 1.7355 \n",
      "Accuracy: 3826/10000 (38.26%)\n",
      "\n",
      "Round  56, Average loss 1.736 Test accuracy 38.260\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021769955423143173\n",
      "conv1.bias 0.0018184945608178775\n",
      "conv2.weight 0.001111061970392863\n",
      "conv2.bias 0.0012477177660912275\n",
      "fc1.weight 0.00045889031887054444\n",
      "fc1.bias 0.0016419102748235067\n",
      "fc2.weight 0.0009096325389922612\n",
      "fc2.bias 0.003668241912410373\n",
      "fc3.weight 0.0022081474463144938\n",
      "fc3.bias 0.0011049356311559678\n",
      "\n",
      "Test set: Average loss: 1.7346 \n",
      "Accuracy: 3842/10000 (38.42%)\n",
      "\n",
      "Round  57, Average loss 1.735 Test accuracy 38.420\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002172200944688585\n",
      "conv1.bias 0.001818004995584488\n",
      "conv2.weight 0.0011113156874974568\n",
      "conv2.bias 0.0012279391521587968\n",
      "fc1.weight 0.0004589925209681193\n",
      "fc1.bias 0.001640636846423149\n",
      "fc2.weight 0.0009077558441767617\n",
      "fc2.bias 0.003695195274693625\n",
      "fc3.weight 0.0022064448822112313\n",
      "fc3.bias 0.0011327404528856278\n",
      "\n",
      "Test set: Average loss: 1.7376 \n",
      "Accuracy: 3809/10000 (38.09%)\n",
      "\n",
      "Round  58, Average loss 1.738 Test accuracy 38.090\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021819645828670925\n",
      "conv1.bias 0.001803017221391201\n",
      "conv2.weight 0.0011154093345006307\n",
      "conv2.bias 0.0012298854999244213\n",
      "fc1.weight 0.00046060001850128175\n",
      "fc1.bias 0.0016330731411774953\n",
      "fc2.weight 0.0009124120076497396\n",
      "fc2.bias 0.003647643540586744\n",
      "fc3.weight 0.0022082948968524026\n",
      "fc3.bias 0.0011150830425322056\n",
      "\n",
      "Test set: Average loss: 1.7339 \n",
      "Accuracy: 3834/10000 (38.34%)\n",
      "\n",
      "Round  59, Average loss 1.734 Test accuracy 38.340\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021798030535380044\n",
      "conv1.bias 0.0018231092641750972\n",
      "conv2.weight 0.001121042271455129\n",
      "conv2.bias 0.0012188886757940054\n",
      "fc1.weight 0.0004605873028437297\n",
      "fc1.bias 0.0016255223502715428\n",
      "fc2.weight 0.0009125187283470517\n",
      "fc2.bias 0.003648946327822549\n",
      "fc3.weight 0.002219364188966297\n",
      "fc3.bias 0.0010918520390987396\n",
      "\n",
      "Test set: Average loss: 1.7322 \n",
      "Accuracy: 3847/10000 (38.47%)\n",
      "\n",
      "Round  60, Average loss 1.732 Test accuracy 38.470\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002174887657165527\n",
      "conv1.bias 0.0018402618976930778\n",
      "conv2.weight 0.0011204354961713155\n",
      "conv2.bias 0.001203053747303784\n",
      "fc1.weight 0.000460481603940328\n",
      "fc1.bias 0.0016328345984220505\n",
      "fc2.weight 0.0009105860240875729\n",
      "fc2.bias 0.0036443479004360383\n",
      "fc3.weight 0.002219470909663609\n",
      "fc3.bias 0.0011095168069005013\n",
      "\n",
      "Test set: Average loss: 1.7285 \n",
      "Accuracy: 3856/10000 (38.56%)\n",
      "\n",
      "Round  61, Average loss 1.729 Test accuracy 38.560\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002178813351525201\n",
      "conv1.bias 0.0018399418331682682\n",
      "conv2.weight 0.0011233921845753988\n",
      "conv2.bias 0.0012241053627803922\n",
      "fc1.weight 0.0004607992172241211\n",
      "fc1.bias 0.0016242244591315588\n",
      "fc2.weight 0.0009081782802702889\n",
      "fc2.bias 0.0036826665912355694\n",
      "fc3.weight 0.002216480743317377\n",
      "fc3.bias 0.0011247978545725346\n",
      "\n",
      "Test set: Average loss: 1.7318 \n",
      "Accuracy: 3851/10000 (38.51%)\n",
      "\n",
      "Round  62, Average loss 1.732 Test accuracy 38.510\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002171178658803304\n",
      "conv1.bias 0.0018414872077604134\n",
      "conv2.weight 0.001125860313574473\n",
      "conv2.bias 0.001207282766699791\n",
      "fc1.weight 0.00046167771021525067\n",
      "fc1.bias 0.0016410270084937413\n",
      "fc2.weight 0.0009101277305966332\n",
      "fc2.bias 0.0036998235044025237\n",
      "fc3.weight 0.0022089948256810506\n",
      "fc3.bias 0.0011132059618830681\n",
      "\n",
      "Test set: Average loss: 1.7308 \n",
      "Accuracy: 3838/10000 (38.38%)\n",
      "\n",
      "Round  63, Average loss 1.731 Test accuracy 38.380\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021786385112338596\n",
      "conv1.bias 0.0018326231899360816\n",
      "conv2.weight 0.0011272784074147543\n",
      "conv2.bias 0.0012187572428956628\n",
      "fc1.weight 0.00046245054403940835\n",
      "fc1.bias 0.0016599869976441064\n",
      "fc2.weight 0.0009132471349504259\n",
      "fc2.bias 0.003719712651911236\n",
      "fc3.weight 0.0022193885984874906\n",
      "fc3.bias 0.0011337371543049812\n",
      "\n",
      "Test set: Average loss: 1.7289 \n",
      "Accuracy: 3856/10000 (38.56%)\n",
      "\n",
      "Round  64, Average loss 1.729 Test accuracy 38.560\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00216797047191196\n",
      "conv1.bias 0.0019105810982485612\n",
      "conv2.weight 0.0011245999733606974\n",
      "conv2.bias 0.0012188482796773314\n",
      "fc1.weight 0.00046117711067199707\n",
      "fc1.bias 0.00165909839173158\n",
      "fc2.weight 0.0009091280755542573\n",
      "fc2.bias 0.0037046660270009723\n",
      "fc3.weight 0.0022150156043824697\n",
      "fc3.bias 0.0011379095725715161\n",
      "\n",
      "Test set: Average loss: 1.7308 \n",
      "Accuracy: 3852/10000 (38.52%)\n",
      "\n",
      "Round  65, Average loss 1.731 Test accuracy 38.520\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.00218472507264879\n",
      "conv1.bias 0.0018886217537025611\n",
      "conv2.weight 0.0011267898480097453\n",
      "conv2.bias 0.0012261185329407454\n",
      "fc1.weight 0.00046271546681722007\n",
      "fc1.bias 0.0016538081069787343\n",
      "fc2.weight 0.0009104625573233952\n",
      "fc2.bias 0.0037071825492949713\n",
      "fc3.weight 0.0022059376750673564\n",
      "fc3.bias 0.0011322087608277797\n",
      "\n",
      "Test set: Average loss: 1.7270 \n",
      "Accuracy: 3870/10000 (38.70%)\n",
      "\n",
      "Round  66, Average loss 1.727 Test accuracy 38.700\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021638127168019613\n",
      "conv1.bias 0.0018373532220721245\n",
      "conv2.weight 0.0011282014846801759\n",
      "conv2.bias 0.0012112350668758154\n",
      "fc1.weight 0.00046317378679911294\n",
      "fc1.bias 0.0016508004317680994\n",
      "fc2.weight 0.0009148049922216506\n",
      "fc2.bias 0.0037018206147920517\n",
      "fc3.weight 0.002218495664142427\n",
      "fc3.bias 0.0011278271675109863\n",
      "\n",
      "Test set: Average loss: 1.7261 \n",
      "Accuracy: 3862/10000 (38.62%)\n",
      "\n",
      "Round  67, Average loss 1.726 Test accuracy 38.620\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021626080407036673\n",
      "conv1.bias 0.0018209184830387433\n",
      "conv2.weight 0.0011303871870040894\n",
      "conv2.bias 0.0012219150085002184\n",
      "fc1.weight 0.0004639248847961426\n",
      "fc1.bias 0.0016637736310561497\n",
      "fc2.weight 0.0009152895874447293\n",
      "fc2.bias 0.0036910339480354672\n",
      "fc3.weight 0.002220283377738226\n",
      "fc3.bias 0.001133390050381422\n",
      "\n",
      "Test set: Average loss: 1.7290 \n",
      "Accuracy: 3865/10000 (38.65%)\n",
      "\n",
      "Round  68, Average loss 1.729 Test accuracy 38.650\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021595083342658148\n",
      "conv1.bias 0.00184726823742191\n",
      "conv2.weight 0.001128274699052175\n",
      "conv2.bias 0.0012226940598338842\n",
      "fc1.weight 0.00046319584051767983\n",
      "fc1.bias 0.001670526961485545\n",
      "fc2.weight 0.0009127073817782932\n",
      "fc2.bias 0.0036835017658415296\n",
      "fc3.weight 0.002215327677272615\n",
      "fc3.bias 0.0011618253774940968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.7314 \n",
      "Accuracy: 3835/10000 (38.35%)\n",
      "\n",
      "Round  69, Average loss 1.731 Test accuracy 38.350\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021573567390441896\n",
      "conv1.bias 0.001840651345749696\n",
      "conv2.weight 0.0011320255200068156\n",
      "conv2.bias 0.0012140597682446241\n",
      "fc1.weight 0.0004608126481374105\n",
      "fc1.bias 0.0016719475388526917\n",
      "fc2.weight 0.0009140482970646449\n",
      "fc2.bias 0.0036786173780759177\n",
      "fc3.weight 0.002220997923896426\n",
      "fc3.bias 0.0011420561000704764\n",
      "\n",
      "Test set: Average loss: 1.7307 \n",
      "Accuracy: 3843/10000 (38.43%)\n",
      "\n",
      "Round  70, Average loss 1.731 Test accuracy 38.430\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021614713139004177\n",
      "conv1.bias 0.001848796537766854\n",
      "conv2.weight 0.0011348197857538858\n",
      "conv2.bias 0.0012113689444959164\n",
      "fc1.weight 0.00046208691596984864\n",
      "fc1.bias 0.0016626755396525065\n",
      "fc2.weight 0.0009146363962264288\n",
      "fc2.bias 0.003686096696626572\n",
      "fc3.weight 0.002216684250604539\n",
      "fc3.bias 0.0011274226941168307\n",
      "\n",
      "Test set: Average loss: 1.7302 \n",
      "Accuracy: 3823/10000 (38.23%)\n",
      "\n",
      "Round  71, Average loss 1.730 Test accuracy 38.230\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002158799436357286\n",
      "conv1.bias 0.0018186929325262706\n",
      "conv2.weight 0.0011364628871281942\n",
      "conv2.bias 0.0012084809131920338\n",
      "fc1.weight 0.00046221339702606204\n",
      "fc1.bias 0.0016777644554773967\n",
      "fc2.weight 0.0009183823116241939\n",
      "fc2.bias 0.003708834804239727\n",
      "fc3.weight 0.0022236127228963945\n",
      "fc3.bias 0.0011327161453664302\n",
      "\n",
      "Test set: Average loss: 1.7303 \n",
      "Accuracy: 3822/10000 (38.22%)\n",
      "\n",
      "Round  72, Average loss 1.730 Test accuracy 38.220\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021710704432593452\n",
      "conv1.bias 0.001779281844695409\n",
      "conv2.weight 0.0011377431948979696\n",
      "conv2.bias 0.001205512904562056\n",
      "fc1.weight 0.00046203370889027915\n",
      "fc1.bias 0.0016862913966178895\n",
      "fc2.weight 0.0009157451372297983\n",
      "fc2.bias 0.003672188591389429\n",
      "fc3.weight 0.00222980607123602\n",
      "fc3.bias 0.0011452212929725647\n",
      "\n",
      "Test set: Average loss: 1.7286 \n",
      "Accuracy: 3847/10000 (38.47%)\n",
      "\n",
      "Round  73, Average loss 1.729 Test accuracy 38.470\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021663541264004176\n",
      "conv1.bias 0.001790592602143685\n",
      "conv2.weight 0.0011417017380396526\n",
      "conv2.bias 0.0012206274550408125\n",
      "fc1.weight 0.00046185390154520673\n",
      "fc1.bias 0.001699000597000122\n",
      "fc2.weight 0.0009171448056660001\n",
      "fc2.bias 0.0036855116486549377\n",
      "fc3.weight 0.002224937223252796\n",
      "fc3.bias 0.00114276185631752\n",
      "\n",
      "Test set: Average loss: 1.7305 \n",
      "Accuracy: 3850/10000 (38.50%)\n",
      "\n",
      "Round  74, Average loss 1.730 Test accuracy 38.500\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002159312433666653\n",
      "conv1.bias 0.0018359608948230743\n",
      "conv2.weight 0.0011380707224210103\n",
      "conv2.bias 0.001200779457576573\n",
      "fc1.weight 0.0004628381729125977\n",
      "fc1.bias 0.0017291828989982605\n",
      "fc2.weight 0.000915117207027617\n",
      "fc2.bias 0.00366958479086558\n",
      "fc3.weight 0.0022279869942438035\n",
      "fc3.bias 0.0011432834900915623\n",
      "\n",
      "Test set: Average loss: 1.7291 \n",
      "Accuracy: 3828/10000 (38.28%)\n",
      "\n",
      "Round  75, Average loss 1.729 Test accuracy 38.280\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021543746524386935\n",
      "conv1.bias 0.0017725260307391484\n",
      "conv2.weight 0.0011367276310920715\n",
      "conv2.bias 0.0012049395591020584\n",
      "fc1.weight 0.00046122888724009194\n",
      "fc1.bias 0.0017360322177410125\n",
      "fc2.weight 0.0009186344487326486\n",
      "fc2.bias 0.003675412918840136\n",
      "fc3.weight 0.0022276543435596286\n",
      "fc3.bias 0.0011496949009597301\n",
      "\n",
      "Test set: Average loss: 1.7319 \n",
      "Accuracy: 3820/10000 (38.20%)\n",
      "\n",
      "Round  76, Average loss 1.732 Test accuracy 38.200\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021468708250257703\n",
      "conv1.bias 0.001845968731989463\n",
      "conv2.weight 0.0011384169260660808\n",
      "conv2.bias 0.0012051514349877834\n",
      "fc1.weight 0.0004595674673716227\n",
      "fc1.bias 0.0017552560816208522\n",
      "fc2.weight 0.0009211544952695332\n",
      "fc2.bias 0.0036985324252219428\n",
      "fc3.weight 0.0022286872069040933\n",
      "fc3.bias 0.001158587820827961\n",
      "\n",
      "Test set: Average loss: 1.7296 \n",
      "Accuracy: 3834/10000 (38.34%)\n",
      "\n",
      "Round  77, Average loss 1.730 Test accuracy 38.340\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021458784739176433\n",
      "conv1.bias 0.0018706792034208775\n",
      "conv2.weight 0.0011403552691141765\n",
      "conv2.bias 0.0011906943982467055\n",
      "fc1.weight 0.0004601755142211914\n",
      "fc1.bias 0.0017652707795302073\n",
      "fc2.weight 0.0009191080691322448\n",
      "fc2.bias 0.003685359443937029\n",
      "fc3.weight 0.002236498821349371\n",
      "fc3.bias 0.0011668907478451728\n",
      "\n",
      "Test set: Average loss: 1.7315 \n",
      "Accuracy: 3835/10000 (38.35%)\n",
      "\n",
      "Round  78, Average loss 1.731 Test accuracy 38.350\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021439242362976075\n",
      "conv1.bias 0.0018838743368784587\n",
      "conv2.weight 0.001139504313468933\n",
      "conv2.bias 0.0011898897355422378\n",
      "fc1.weight 0.0004586970806121826\n",
      "fc1.bias 0.0017692285279432933\n",
      "fc2.weight 0.0009236385897984581\n",
      "fc2.bias 0.003689828018347422\n",
      "fc3.weight 0.0022398863519941057\n",
      "fc3.bias 0.0011715949513018131\n",
      "\n",
      "Test set: Average loss: 1.7296 \n",
      "Accuracy: 3851/10000 (38.51%)\n",
      "\n",
      "Round  79, Average loss 1.730 Test accuracy 38.510\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021434373325771756\n",
      "conv1.bias 0.0018811534779767196\n",
      "conv2.weight 0.0011345186829566955\n",
      "conv2.bias 0.0012207955587655306\n",
      "fc1.weight 0.00045761771996816\n",
      "fc1.bias 0.0017632892976204553\n",
      "fc2.weight 0.0009239543051946731\n",
      "fc2.bias 0.0036889931985310148\n",
      "fc3.weight 0.0022309009517942156\n",
      "fc3.bias 0.001178933121263981\n",
      "\n",
      "Test set: Average loss: 1.7321 \n",
      "Accuracy: 3825/10000 (38.25%)\n",
      "\n",
      "Round  80, Average loss 1.732 Test accuracy 38.250\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021428412861294217\n",
      "conv1.bias 0.0019380912805596988\n",
      "conv2.weight 0.001130427916844686\n",
      "conv2.bias 0.0012121570762246847\n",
      "fc1.weight 0.0004568429390589396\n",
      "fc1.bias 0.0017809738715489705\n",
      "fc2.weight 0.0009234653578864203\n",
      "fc2.bias 0.003710448032333737\n",
      "fc3.weight 0.0022379693530854726\n",
      "fc3.bias 0.0011763477697968483\n",
      "\n",
      "Test set: Average loss: 1.7294 \n",
      "Accuracy: 3865/10000 (38.65%)\n",
      "\n",
      "Round  81, Average loss 1.729 Test accuracy 38.650\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002136703332265218\n",
      "conv1.bias 0.0019289689759413402\n",
      "conv2.weight 0.0011313244700431823\n",
      "conv2.bias 0.0012065076734870672\n",
      "fc1.weight 0.0004580243428548177\n",
      "fc1.bias 0.001776540031035741\n",
      "fc2.weight 0.0009269962235102577\n",
      "fc2.bias 0.0037256752451260886\n",
      "fc3.weight 0.0022498386246817452\n",
      "fc3.bias 0.0011610845103859901\n",
      "\n",
      "Test set: Average loss: 1.7295 \n",
      "Accuracy: 3829/10000 (38.29%)\n",
      "\n",
      "Round  82, Average loss 1.729 Test accuracy 38.290\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021329696973164877\n",
      "conv1.bias 0.0019045602530241013\n",
      "conv2.weight 0.0011293049653371176\n",
      "conv2.bias 0.0011983879376202822\n",
      "fc1.weight 0.0004571865797042847\n",
      "fc1.bias 0.001772097498178482\n",
      "fc2.weight 0.0009267863773164295\n",
      "fc2.bias 0.0037261112814857846\n",
      "fc3.weight 0.002244129918870472\n",
      "fc3.bias 0.0011794458143413067\n",
      "\n",
      "Test set: Average loss: 1.7314 \n",
      "Accuracy: 3843/10000 (38.43%)\n",
      "\n",
      "Round  83, Average loss 1.731 Test accuracy 38.430\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002129553821351793\n",
      "conv1.bias 0.0018166756878296535\n",
      "conv2.weight 0.0011293189724286398\n",
      "conv2.bias 0.0012325383722782135\n",
      "fc1.weight 0.0004583290417989095\n",
      "fc1.bias 0.0017637215554714204\n",
      "fc2.weight 0.0009261942099011134\n",
      "fc2.bias 0.0037215078870455423\n",
      "fc3.weight 0.0022400846083958943\n",
      "fc3.bias 0.0011679602786898613\n",
      "\n",
      "Test set: Average loss: 1.7318 \n",
      "Accuracy: 3837/10000 (38.37%)\n",
      "\n",
      "Round  84, Average loss 1.732 Test accuracy 38.370\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021278030342525905\n",
      "conv1.bias 0.0018909992650151253\n",
      "conv2.weight 0.001127307613690694\n",
      "conv2.bias 0.001235259580425918\n",
      "fc1.weight 0.00045588914553324383\n",
      "fc1.bias 0.001753118634223938\n",
      "fc2.weight 0.0009227801883031451\n",
      "fc2.bias 0.003716711841878437\n",
      "fc3.weight 0.0022372330938066755\n",
      "fc3.bias 0.0012057410553097726\n",
      "\n",
      "Test set: Average loss: 1.7336 \n",
      "Accuracy: 3823/10000 (38.23%)\n",
      "\n",
      "Round  85, Average loss 1.734 Test accuracy 38.230\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021159047550625273\n",
      "conv1.bias 0.0018571459998687108\n",
      "conv2.weight 0.0011238928635915121\n",
      "conv2.bias 0.0012492742389440536\n",
      "fc1.weight 0.0004546805222829183\n",
      "fc1.bias 0.0017515681684017181\n",
      "fc2.weight 0.0009209190096173968\n",
      "fc2.bias 0.003693427358354841\n",
      "fc3.weight 0.0022219635191417877\n",
      "fc3.bias 0.0011895282194018364\n",
      "\n",
      "Test set: Average loss: 1.7321 \n",
      "Accuracy: 3839/10000 (38.39%)\n",
      "\n",
      "Round  86, Average loss 1.732 Test accuracy 38.390\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002110283374786377\n",
      "conv1.bias 0.0018564816564321518\n",
      "conv2.weight 0.0011264739433924358\n",
      "conv2.bias 0.0012469366192817688\n",
      "fc1.weight 0.0004564567009607951\n",
      "fc1.bias 0.001747855544090271\n",
      "fc2.weight 0.000924703147676256\n",
      "fc2.bias 0.0036748782509849185\n",
      "fc3.weight 0.0022189331906182427\n",
      "fc3.bias 0.0011772625148296356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.7335 \n",
      "Accuracy: 3838/10000 (38.38%)\n",
      "\n",
      "Round  87, Average loss 1.733 Test accuracy 38.380\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0021007290151384144\n",
      "conv1.bias 0.00184233362476031\n",
      "conv2.weight 0.001125002404054006\n",
      "conv2.bias 0.001264091464690864\n",
      "fc1.weight 0.0004552643696467082\n",
      "fc1.bias 0.0017375111579895019\n",
      "fc2.weight 0.0009200255076090495\n",
      "fc2.bias 0.0036619018231119427\n",
      "fc3.weight 0.002221010696320307\n",
      "fc3.bias 0.00117996446788311\n",
      "\n",
      "Test set: Average loss: 1.7348 \n",
      "Accuracy: 3823/10000 (38.23%)\n",
      "\n",
      "Round  88, Average loss 1.735 Test accuracy 38.230\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002082831727133857\n",
      "conv1.bias 0.0017902182104686897\n",
      "conv2.weight 0.001126806338628133\n",
      "conv2.bias 0.0012801217380911112\n",
      "fc1.weight 0.00045518426100413006\n",
      "fc1.bias 0.0017206095159053802\n",
      "fc2.weight 0.0009176791660369389\n",
      "fc2.bias 0.003682582860901242\n",
      "fc3.weight 0.002221158572605678\n",
      "fc3.bias 0.0011801021173596382\n",
      "\n",
      "Test set: Average loss: 1.7355 \n",
      "Accuracy: 3827/10000 (38.27%)\n",
      "\n",
      "Round  89, Average loss 1.735 Test accuracy 38.270\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020816507604387072\n",
      "conv1.bias 0.00173659964154164\n",
      "conv2.weight 0.0011269139250119527\n",
      "conv2.bias 0.0013138093054294586\n",
      "fc1.weight 0.00045419275760650636\n",
      "fc1.bias 0.0017151899635791778\n",
      "fc2.weight 0.0009107971002185155\n",
      "fc2.bias 0.003691402929169791\n",
      "fc3.weight 0.0022171786853245326\n",
      "fc3.bias 0.0011983910575509072\n",
      "\n",
      "Test set: Average loss: 1.7381 \n",
      "Accuracy: 3791/10000 (37.91%)\n",
      "\n",
      "Round  90, Average loss 1.738 Test accuracy 37.910\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020651185512542726\n",
      "conv1.bias 0.0018118202375868957\n",
      "conv2.weight 0.0011242530743281046\n",
      "conv2.bias 0.001293811365030706\n",
      "fc1.weight 0.00045575292905171715\n",
      "fc1.bias 0.001702771211663882\n",
      "fc2.weight 0.000914700258345831\n",
      "fc2.bias 0.0036535926517986114\n",
      "fc3.weight 0.0022093406745365687\n",
      "fc3.bias 0.0011817919090390205\n",
      "\n",
      "Test set: Average loss: 1.7394 \n",
      "Accuracy: 3777/10000 (37.77%)\n",
      "\n",
      "Round  91, Average loss 1.739 Test accuracy 37.770\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020587628417544895\n",
      "conv1.bias 0.0018044356256723404\n",
      "conv2.weight 0.0011295151710510253\n",
      "conv2.bias 0.0013242524582892656\n",
      "fc1.weight 0.00045318408807118736\n",
      "fc1.bias 0.001704450324177742\n",
      "fc2.weight 0.000911847088072035\n",
      "fc2.bias 0.0036751258940923783\n",
      "fc3.weight 0.002202078700065613\n",
      "fc3.bias 0.0011732829734683037\n",
      "\n",
      "Test set: Average loss: 1.7403 \n",
      "Accuracy: 3787/10000 (37.87%)\n",
      "\n",
      "Round  92, Average loss 1.740 Test accuracy 37.870\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002046611573961046\n",
      "conv1.bias 0.0018239440396428108\n",
      "conv2.weight 0.0011297913392384847\n",
      "conv2.bias 0.0013489993289113045\n",
      "fc1.weight 0.0004519908428192139\n",
      "fc1.bias 0.0016838294764359792\n",
      "fc2.weight 0.0009090912720513722\n",
      "fc2.bias 0.0036566186518896195\n",
      "fc3.weight 0.002204709677469163\n",
      "fc3.bias 0.0011667142622172832\n",
      "\n",
      "Test set: Average loss: 1.7414 \n",
      "Accuracy: 3783/10000 (37.83%)\n",
      "\n",
      "Round  93, Average loss 1.741 Test accuracy 37.830\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002052360905541314\n",
      "conv1.bias 0.0017981994897127151\n",
      "conv2.weight 0.001132078468799591\n",
      "conv2.bias 0.001326030003838241\n",
      "fc1.weight 0.00045209519068400067\n",
      "fc1.bias 0.0016806542873382569\n",
      "fc2.weight 0.0009102256525130499\n",
      "fc2.bias 0.003650096200761341\n",
      "fc3.weight 0.0022076049021312167\n",
      "fc3.bias 0.0011936945840716362\n",
      "\n",
      "Test set: Average loss: 1.7415 \n",
      "Accuracy: 3792/10000 (37.92%)\n",
      "\n",
      "Round  94, Average loss 1.742 Test accuracy 37.920\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002055101924472385\n",
      "conv1.bias 0.0018235088015596073\n",
      "conv2.weight 0.0011300777395566305\n",
      "conv2.bias 0.0013257344253361225\n",
      "fc1.weight 0.00045053847630818685\n",
      "fc1.bias 0.0016579385846853256\n",
      "fc2.weight 0.0009078367361946712\n",
      "fc2.bias 0.0036750251338595434\n",
      "fc3.weight 0.002207616681144351\n",
      "fc3.bias 0.0011996394954621793\n",
      "\n",
      "Test set: Average loss: 1.7396 \n",
      "Accuracy: 3784/10000 (37.84%)\n",
      "\n",
      "Round  95, Average loss 1.740 Test accuracy 37.840\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020486842261420355\n",
      "conv1.bias 0.0018299186291793983\n",
      "conv2.weight 0.0011272210876146953\n",
      "conv2.bias 0.001336564077064395\n",
      "fc1.weight 0.00044969900449117026\n",
      "fc1.bias 0.0016586204369862874\n",
      "fc2.weight 0.0009052992813170902\n",
      "fc2.bias 0.0036840939096042086\n",
      "fc3.weight 0.0022006102970668247\n",
      "fc3.bias 0.0011990360915660858\n",
      "\n",
      "Test set: Average loss: 1.7413 \n",
      "Accuracy: 3780/10000 (37.80%)\n",
      "\n",
      "Round  96, Average loss 1.741 Test accuracy 37.800\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002061293257607354\n",
      "conv1.bias 0.0018406597276528676\n",
      "conv2.weight 0.0011353695392608643\n",
      "conv2.bias 0.001340636983513832\n",
      "fc1.weight 0.0004482003053029378\n",
      "fc1.bias 0.0016476597636938095\n",
      "fc2.weight 0.0009038019747961135\n",
      "fc2.bias 0.003715438502175467\n",
      "fc3.weight 0.0022071150087174915\n",
      "fc3.bias 0.0012097466737031938\n",
      "\n",
      "Test set: Average loss: 1.7426 \n",
      "Accuracy: 3776/10000 (37.76%)\n",
      "\n",
      "Round  97, Average loss 1.743 Test accuracy 37.760\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.0020701323615180123\n",
      "conv1.bias 0.0018548633282383282\n",
      "conv2.weight 0.0011362302303314208\n",
      "conv2.bias 0.0013437042944133282\n",
      "fc1.weight 0.00044816923141479493\n",
      "fc1.bias 0.0016242643197377524\n",
      "fc2.weight 0.0009045848770747109\n",
      "fc2.bias 0.0036754973587535674\n",
      "fc3.weight 0.0022169558774857295\n",
      "fc3.bias 0.0012068803422152995\n",
      "\n",
      "Test set: Average loss: 1.7413 \n",
      "Accuracy: 3802/10000 (38.02%)\n",
      "\n",
      "Round  98, Average loss 1.741 Test accuracy 38.020\n",
      "selected users: [0 1]\n",
      "conv1.weight 0.002062183221181234\n",
      "conv1.bias 0.0018612765707075596\n",
      "conv2.weight 0.0011298838257789612\n",
      "conv2.bias 0.0013474745210260153\n",
      "fc1.weight 0.00044686702887217206\n",
      "fc1.bias 0.0016017787158489228\n",
      "fc2.weight 0.0008985299912710039\n",
      "fc2.bias 0.003694798265184675\n",
      "fc3.weight 0.0022067595095861528\n",
      "fc3.bias 0.0012167326174676418\n",
      "\n",
      "Test set: Average loss: 1.7407 \n",
      "Accuracy: 3782/10000 (37.82%)\n",
      "\n",
      "Round  99, Average loss 1.741 Test accuracy 37.820\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "K = 2\n",
    "T = 3\n",
    "sigma = 1\n",
    "Noise_Alloc = [0,2,4]\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "\n",
    "N_array = [2]\n",
    "B_array = [0.5]\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 100\n",
    "\n",
    "lr_array = [0.001] # 0.001 is the bset\n",
    "\n",
    "sigma_array = [0.5,1,1.5]\n",
    "# lr_array = [0.1, 0.01, 0.005, 0.001,0.0005]\n",
    "\n",
    "\n",
    "loss_test_arr_K4_G1_v2 = np.zeros((len(sigma_array),len(lr_array),N_trials,N_epochs))\n",
    "acc_test_arr_K4_G1_v2  = np.zeros((len(sigma_array),len(lr_array),N_trials,N_epochs))\n",
    "\n",
    "N = 2\n",
    "for sigma_idx in range(len(sigma_array)):\n",
    "    \n",
    "    sigma = sigma_array[sigma_idx]\n",
    "    \n",
    "    print('\\n\\n')\n",
    "    print('sigma =',sigma)\n",
    "    print('\\n\\n')\n",
    "           \n",
    "        \n",
    "    # print(\"alpha_array: \",alpha_array,'\\n')\n",
    "    \n",
    "    \n",
    "    for lr_idx in range(len(lr_array)):\n",
    "        \n",
    "        args.lr = lr_array[lr_idx]\n",
    "        \n",
    "        print('Learning Rate =',args.lr)\n",
    "        print('\\n\\n')\n",
    "#         while(len(z_array)<N):\n",
    "#             z_tmp = np.random.uniform(-1,1,1)\n",
    "#             MIS_tmp = MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_tmp], 1,sigma)\n",
    "#             if MIS_tmp < B and MIS_tmp > 0.1:\n",
    "#                 z_array.append(z_tmp[0])\n",
    "#         \n",
    "#         z_array = np.sort(z_array)\n",
    "#         print(N_idx,'!!!')\n",
    "#         if N_idx==0:\n",
    "# #             z_array = np.array([-0.94,-0.534,0.534, 0.94])\n",
    "# #         elif N_idx==1:\n",
    "# #             z_array = np.array([-0.94, -0.73, 0.73, 0.94])\n",
    "#         elif N_idx==1:\n",
    "#             z_array = np.array([-0.94, -0.125, 0.125, 0.94])\n",
    "#         else:\n",
    "# #             z_array = np.array([-0.94, -0.73, -0.534, -0.125, 0.125, 0.534, 0.73, 0.94])\n",
    "# #             z_array = np.array([-0.9, -0.81, -0.22, -0.20, 0.20, 0.22, 0.81, 0.9])\n",
    "        \n",
    "        z_array = np.array([-0.81, 0.81])\n",
    "        \n",
    "        print('z_array:',z_array)\n",
    "        if sigma != 0:\n",
    "            for j in range(len(z_array)):\n",
    "                print(MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_array[j]], 1,sigma))\n",
    "\n",
    "        \n",
    "        _Noise_label = np.ones((25000*T,10)) * 0.1\n",
    "\n",
    "        X_tilde,a,b = BACC_Enc_Data_v3(encoding_input_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde,a,b = BACC_Enc_Data_v3(encoding_label_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "\n",
    "        m = N # m is the number of received result @ master\n",
    "    #     print('number of results:',m)\n",
    "\n",
    "        for trial_idx in range(N_trials):\n",
    "            print('(T, sigma)=',T,sigma,') ',trial_idx,'-th Trial!!')\n",
    "\n",
    "            net_glob = CNNCifar(args=args)\n",
    "            net_glob.cuda()\n",
    "            net_glob.train()\n",
    "\n",
    "            # copy weights\n",
    "            w_glob = net_glob.state_dict()\n",
    "\n",
    "            for iter in range(N_epochs): #args.epochs\n",
    "                w_locals, loss_locals = [], []\n",
    "                idxs_users = np.random.choice(range(N), m, replace=False)\n",
    "                idxs_users = np.sort(idxs_users)\n",
    "                print('selected users:',idxs_users)\n",
    "                \n",
    "                coded_net = BACC_Enc_Model_withNoise_v4(net_glob.cuda(), N, K, T, 1, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "                dec_z_array = []\n",
    "                for idx in idxs_users: #for idx in range(N):\n",
    "            #         print(idx)\n",
    "                    local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[idx,:,:], label=y_tilde[idx,:,:])\n",
    "                    w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "#                     w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                    w_locals.append(copy.deepcopy(w))\n",
    "                    loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                    dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "                # update global weights\n",
    "                #w_glob = FedAvg(w_locals)\n",
    "                w_glob = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "\n",
    "                # copy weight to net_glob\n",
    "                net_glob.load_state_dict(w_glob)\n",
    "\n",
    "                # print loss\n",
    "            #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "            #     loss_train_arr.append(loss_train)\n",
    "\n",
    "                acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "                acc_test_arr_K4_G1_v2[sigma_idx][lr_idx][trial_idx][iter] = acc_test\n",
    "                loss_test_arr_K4_G1_v2[sigma_idx][lr_idx][trial_idx][iter] = loss_test\n",
    "                if iter % 1 ==0:\n",
    "                    print('Round {:3d}, Average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_test, acc_test))\n",
    "                #print(loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z_array: [ 0.65710678 -0.75710678]\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 0 25000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 0 25000 \n",
      "\n",
      "(T, sigma)= 0 1 )  0 -th Trial!!\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5493 \n",
      "Accuracy: 4610/10000 (46.10%)\n",
      "\n",
      "Round   0, Average loss 1.549 Test accuracy 46.100\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.2163 \n",
      "Accuracy: 5754/10000 (57.54%)\n",
      "\n",
      "Round   1, Average loss 1.216 Test accuracy 57.540\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.5787 \n",
      "Accuracy: 2595/10000 (25.95%)\n",
      "\n",
      "Round   2, Average loss 2.579 Test accuracy 25.950\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 14.9973 \n",
      "Accuracy: 2696/10000 (26.96%)\n",
      "\n",
      "Round   3, Average loss 14.997 Test accuracy 26.960\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 19.2357 \n",
      "Accuracy: 2234/10000 (22.34%)\n",
      "\n",
      "Round   4, Average loss 19.236 Test accuracy 22.340\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 49.7180 \n",
      "Accuracy: 1844/10000 (18.44%)\n",
      "\n",
      "Round   5, Average loss 49.718 Test accuracy 18.440\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 26.2585 \n",
      "Accuracy: 1743/10000 (17.43%)\n",
      "\n",
      "Round   6, Average loss 26.259 Test accuracy 17.430\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 94.3647 \n",
      "Accuracy: 1591/10000 (15.91%)\n",
      "\n",
      "Round   7, Average loss 94.365 Test accuracy 15.910\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 172.7567 \n",
      "Accuracy: 1620/10000 (16.20%)\n",
      "\n",
      "Round   8, Average loss 172.757 Test accuracy 16.200\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 508.0292 \n",
      "Accuracy: 996/10000 (9.96%)\n",
      "\n",
      "Round   9, Average loss 508.029 Test accuracy 9.960\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 540.9026 \n",
      "Accuracy: 992/10000 (9.92%)\n",
      "\n",
      "Round  10, Average loss 540.903 Test accuracy 9.920\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1861.8981 \n",
      "Accuracy: 1176/10000 (11.76%)\n",
      "\n",
      "Round  11, Average loss 1861.898 Test accuracy 11.760\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2218.3093 \n",
      "Accuracy: 1176/10000 (11.76%)\n",
      "\n",
      "Round  12, Average loss 2218.309 Test accuracy 11.760\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 4003.2478 \n",
      "Accuracy: 1193/10000 (11.93%)\n",
      "\n",
      "Round  13, Average loss 4003.248 Test accuracy 11.930\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 8786.2119 \n",
      "Accuracy: 1202/10000 (12.02%)\n",
      "\n",
      "Round  14, Average loss 8786.212 Test accuracy 12.020\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 8336.7196 \n",
      "Accuracy: 1184/10000 (11.84%)\n",
      "\n",
      "Round  15, Average loss 8336.720 Test accuracy 11.840\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 11388.6120 \n",
      "Accuracy: 1205/10000 (12.05%)\n",
      "\n",
      "Round  16, Average loss 11388.612 Test accuracy 12.050\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 4833.9456 \n",
      "Accuracy: 1211/10000 (12.11%)\n",
      "\n",
      "Round  17, Average loss 4833.946 Test accuracy 12.110\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 12722.2701 \n",
      "Accuracy: 1232/10000 (12.32%)\n",
      "\n",
      "Round  18, Average loss 12722.270 Test accuracy 12.320\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 45332.8047 \n",
      "Accuracy: 1257/10000 (12.57%)\n",
      "\n",
      "Round  19, Average loss 45332.805 Test accuracy 12.570\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 35566.3274 \n",
      "Accuracy: 1221/10000 (12.21%)\n",
      "\n",
      "Round  20, Average loss 35566.327 Test accuracy 12.210\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 24022.0861 \n",
      "Accuracy: 1249/10000 (12.49%)\n",
      "\n",
      "Round  21, Average loss 24022.086 Test accuracy 12.490\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 22060.1032 \n",
      "Accuracy: 1252/10000 (12.52%)\n",
      "\n",
      "Round  22, Average loss 22060.103 Test accuracy 12.520\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 25651.1643 \n",
      "Accuracy: 1231/10000 (12.31%)\n",
      "\n",
      "Round  23, Average loss 25651.164 Test accuracy 12.310\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 159664.3567 \n",
      "Accuracy: 1261/10000 (12.61%)\n",
      "\n",
      "Round  24, Average loss 159664.357 Test accuracy 12.610\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 38588.2670 \n",
      "Accuracy: 1253/10000 (12.53%)\n",
      "\n",
      "Round  25, Average loss 38588.267 Test accuracy 12.530\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 66929.6689 \n",
      "Accuracy: 1238/10000 (12.38%)\n",
      "\n",
      "Round  26, Average loss 66929.669 Test accuracy 12.380\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 49492.3415 \n",
      "Accuracy: 1220/10000 (12.20%)\n",
      "\n",
      "Round  27, Average loss 49492.342 Test accuracy 12.200\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 68919.7133 \n",
      "Accuracy: 1233/10000 (12.33%)\n",
      "\n",
      "Round  28, Average loss 68919.713 Test accuracy 12.330\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 330365.1169 \n",
      "Accuracy: 1246/10000 (12.46%)\n",
      "\n",
      "Round  29, Average loss 330365.117 Test accuracy 12.460\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 184150.2378 \n",
      "Accuracy: 1141/10000 (11.41%)\n",
      "\n",
      "Round  30, Average loss 184150.238 Test accuracy 11.410\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 198955.5059 \n",
      "Accuracy: 1231/10000 (12.31%)\n",
      "\n",
      "Round  31, Average loss 198955.506 Test accuracy 12.310\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 540381.8161 \n",
      "Accuracy: 1267/10000 (12.67%)\n",
      "\n",
      "Round  32, Average loss 540381.816 Test accuracy 12.670\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 429518.1701 \n",
      "Accuracy: 1250/10000 (12.50%)\n",
      "\n",
      "Round  33, Average loss 429518.170 Test accuracy 12.500\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 287778.7970 \n",
      "Accuracy: 1142/10000 (11.42%)\n",
      "\n",
      "Round  34, Average loss 287778.797 Test accuracy 11.420\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 176435.6797 \n",
      "Accuracy: 1238/10000 (12.38%)\n",
      "\n",
      "Round  35, Average loss 176435.680 Test accuracy 12.380\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 568963.7575 \n",
      "Accuracy: 1265/10000 (12.65%)\n",
      "\n",
      "Round  36, Average loss 568963.758 Test accuracy 12.650\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 647507.4812 \n",
      "Accuracy: 1270/10000 (12.70%)\n",
      "\n",
      "Round  37, Average loss 647507.481 Test accuracy 12.700\n",
      "selected users: [0 1]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-7164cf7100a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    106\u001b[0m                     \u001b[0mlocal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLocalUpdate_with_BACC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_tilde\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_tilde\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;31m#                     w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m                     \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlocal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet_glob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m                     \u001b[0mw_locals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m                     \u001b[0mloss_locals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Jinhyun_DESKTOP\\06.Github\\CodedPrivateNN\\models\\Update.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, net)\u001b[0m\n\u001b[0;32m    166\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m                 '''\n\u001b[0;32m    170\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#self.args.verbose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    105\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eps'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m                     \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eps'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "K = 2\n",
    "T = 0\n",
    "sigma = 1\n",
    "Noise_Alloc = []\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "\n",
    "N_array = [2]\n",
    "B_array = [0.5]\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 50\n",
    "\n",
    "\n",
    "\n",
    "loss_test_arr_K2_G1_T0 = np.zeros((len(N_array),len(B_array),N_trials,N_epochs))\n",
    "acc_test_arr_K2_G1_T0  = np.zeros((len(N_array),len(B_array),N_trials,N_epochs))\n",
    "\n",
    "for N_idx in range(len(N_array)):\n",
    "    \n",
    "    N = N_array[N_idx]\n",
    "    \n",
    "    \n",
    "           \n",
    "        \n",
    "    # print(\"alpha_array: \",alpha_array,'\\n')\n",
    "    \n",
    "    \n",
    "    for B_idx in range(len(B_array)):\n",
    "        \n",
    "        B = B_array[B_idx]\n",
    "        z_array = []\n",
    "#         while(len(z_array)<N):\n",
    "#             z_tmp = np.random.uniform(-1,1,1)\n",
    "#             MIS_tmp = MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_tmp], 1,sigma)\n",
    "#             if MIS_tmp < B and MIS_tmp > 0.1:\n",
    "#                 z_array.append(z_tmp[0])\n",
    "#         \n",
    "#         z_array = np.sort(z_array)\n",
    "#         print(N_idx,'!!!')\n",
    "#         if N_idx==0:\n",
    "# #             z_array = np.array([-0.94,-0.534,0.534, 0.94])\n",
    "# #         elif N_idx==1:\n",
    "# #             z_array = np.array([-0.94, -0.73, 0.73, 0.94])\n",
    "#         elif N_idx==1:\n",
    "#             z_array = np.array([-0.94, -0.125, 0.125, 0.94])\n",
    "#         else:\n",
    "# #             z_array = np.array([-0.94, -0.73, -0.534, -0.125, 0.125, 0.534, 0.73, 0.94])\n",
    "# #             z_array = np.array([-0.9, -0.81, -0.22, -0.20, 0.20, 0.22, 0.81, 0.9])\n",
    "        \n",
    "        i_array = np.array(range(N))\n",
    "#         z_array = np.cos(i_array*2*math.pi/N/2) # np.cos(i_array*2*math.pi/N/2)\n",
    "        z_array = alpha_array - 0.05\n",
    "        \n",
    "        print('z_array:',z_array)\n",
    "#         for j in range(len(z_array)):\n",
    "#             print(MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_array[j]], 1,sigma))\n",
    "        \n",
    "        \n",
    "        _Noise_label = np.ones((15000*T,10)) * 0.1\n",
    "\n",
    "        X_tilde,a,b = BACC_Enc_Data_v3(encoding_input_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde,a,b = BACC_Enc_Data_v3(encoding_label_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "\n",
    "        m = N # m is the number of received result @ master\n",
    "    #     print('number of results:',m)\n",
    "\n",
    "        for trial_idx in range(N_trials):\n",
    "            print('(T, sigma)=',T,sigma,') ',trial_idx,'-th Trial!!')\n",
    "\n",
    "            net_glob = CNNCifar(args=args)\n",
    "            net_glob.cuda()\n",
    "            net_glob.train()\n",
    "\n",
    "            # copy weights\n",
    "            w_glob = net_glob.state_dict()\n",
    "\n",
    "            for iter in range(N_epochs): #args.epochs\n",
    "                w_locals, loss_locals = [], []\n",
    "                idxs_users = np.random.choice(range(N), m, replace=False)\n",
    "                idxs_users = np.sort(idxs_users)\n",
    "                print('selected users:',idxs_users)\n",
    "                \n",
    "#                 coded_net = BACC_Enc_Model_withNoise_v4(net_glob.cuda(), N, K, T, 1, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "                dec_z_array = []\n",
    "                for idx in idxs_users: #for idx in range(N):\n",
    "            #         print(idx)\n",
    "                    local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[idx,:,:], label=y_tilde[idx,:,:])\n",
    "#                     w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "                    w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                    w_locals.append(copy.deepcopy(w))\n",
    "                    loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                    dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "                # update global weights\n",
    "                #w_glob = FedAvg(w_locals)\n",
    "                w_glob = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "\n",
    "                # copy weight to net_glob\n",
    "                net_glob.load_state_dict(w_glob)\n",
    "\n",
    "                # print loss\n",
    "            #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "            #     loss_train_arr.append(loss_train)\n",
    "\n",
    "                acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "                acc_test_arr_K2_G1_T0[N_idx][B_idx][trial_idx][iter] = acc_test\n",
    "                loss_test_arr_K2_G1_T0[N_idx][B_idx][trial_idx][iter] = loss_test\n",
    "                if iter % 1 ==0:\n",
    "                    print('Round {:3d}, Average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_test, acc_test))\n",
    "                #print(loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lr_array = [0.1, 0.01, 0.005, 0.001,0.0005]\n",
    "\n",
    "plt.plot(acc_test_arr_K4_G1[0,0,0,0:30],label='w/o Grouping, K=4, N=4, G=1, lr=0.1' )\n",
    "plt.plot(acc_test_arr_K4_G1[0,1,0,0:30],label='w/o Grouping, K=4, N=4, G=1, lr=0.01' )\n",
    "plt.plot(acc_test_arr_K4_G1[0,2,0,0:30],label='w/o Grouping, K=4, N=4, G=1, lr=0.005' )\n",
    "plt.plot(acc_test_arr_K4_G1[0,3,0,0:30],label='w/o Grouping, K=4, N=4, G=1, lr=0.001' )\n",
    "plt.plot(acc_test_arr_K4_G1[0,4,0,0:30],label='w/o Grouping, K=4, N=4, G=1, lr=0.0005' )\n",
    "\n",
    "\n",
    "# plt.plot(acc_test_arr_uncoded,'r',label='uncoded')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "# plt.title(title_name)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Test accuracy(%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(acc_test_arr_K4_G1_v1[0,0,0,0:30],label='w/o Grouping, K=4, N=4, G=1, sigma=0' )\n",
    "plt.plot(acc_test_arr_K4_G1_v1[1,0,0,0:30],label='w/o Grouping, K=4, N=4, G=1, sigma=0.1' )\n",
    "plt.plot(acc_test_arr_K4_G1_v1[2,0,0,0:30],label='w/o Grouping, K=4, N=4, G=1, sigma=0.3' )\n",
    "plt.plot(acc_test_arr_K4_G1_v1[3,0,0,0:30],label='w/o Grouping, K=4, N=4, G=1, sigma=0.5' )\n",
    "plt.plot(acc_test_arr_K4_G1_v1[4,0,0,0:30],label='w/o Grouping, K=4, N=4, G=1, sigma=1' )\n",
    "\n",
    "\n",
    "# plt.plot(acc_test_arr_uncoded,'r',label='uncoded')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "# plt.title(title_name)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Test accuracy(%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. K=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "class my_argument:    \n",
    "    epochs    = 16    #\"rounds of training\"\n",
    "    num_users = 4  # \"number of users: K\"\n",
    "    frac      = 0.5 #\"the fraction of clients: C\"\n",
    "    local_ep  = 5 #\"the number of local epochs: E\"\n",
    "    local_bs  = 50  #\"local batch size: B\"\n",
    "    bs        = 50 #\"test batch size\"\n",
    "    lr        = 0.001 #\"learning rate\"\n",
    "    momentum  = 0.5 # \"SGD momentum (default: 0.5)\"\n",
    "    weight_decay = 5e-4\n",
    "    split     = 'user' # \"train-test split type, user or sample\"\n",
    "    opt='ADAM'\n",
    "    loss='Custom' # 'Custom' or 'Default'\n",
    "\n",
    "    # model arguments\n",
    "    model = 'cnn'\n",
    "    kernel_num=9 #, help='number of each kind of kernel')\n",
    "    kernel_sizes='3,4,5' #  help='comma-separated kernel size to use for convolution')\n",
    "    norm='batch_norm' #, help=\"batch_norm, layer_norm, or None\")\n",
    "    num_filters=32 #, help=\"number of filters for conv nets\")\n",
    "    max_pool='True' #help=\"Whether use max pooling rather than strided convolutions\")\n",
    "\n",
    "    # other arguments\n",
    "    dataset='cifar' #, help=\"name of dataset\")\n",
    "    iid=1\n",
    "    num_classes=10#, help=\"number of classes\")\n",
    "    num_channels=1#, help=\"number of channels of imges\")\n",
    "    gpu=1#, help=\"GPU ID, -1 for CPU\")\n",
    "    stopping_rounds=10#, help='rounds of early stopping')\n",
    "    verbose='False'#, help='verbose print')\n",
    "    seed=1#, help='random seed (default: 1)')\n",
    "    \n",
    "args = my_argument()\n",
    "\n",
    "args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)\n",
    "args.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(args.device)\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Normalize the test set same as training set without augmentation\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# trainset = torchvision.datasets.CIFAR10(\n",
    "#     root=\"./data/cifar\", train=True, download=True, transform=transform_train)\n",
    "# trainloader = torch.utils.data.DataLoader(\n",
    "#     trainset, batch_size=args.bs, shuffle=True, num_workers=2)\n",
    "\n",
    "# testset = torchvision.datasets.CIFAR10(\n",
    "#     root=\"./data/cifar\", train=False, download=True, transform=transform_test)\n",
    "# testloader = torch.utils.data.DataLoader(\n",
    "#     testset, batch_size=args.bs, shuffle=False, num_workers=2)\n",
    "\n",
    "dataset_train = datasets.CIFAR10('../data/cifar', train=True, download=True, transform=transform_train)\n",
    "dataset_test = datasets.CIFAR10('../data/cifar', train=False, download=True, transform=transform_test)\n",
    "if args.iid:\n",
    "    dict_users = cifar_iid(dataset_train, args.num_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of X: (50000, 3072)\n",
      "size of Y: (50000, 10)\n",
      "(50000, 10)\n"
     ]
    }
   ],
   "source": [
    "encoding_input_array_np = np.empty((len(dataset_train),32*32*3))\n",
    "encoding_label_array_np = np.empty((len(dataset_train),args.num_classes))\n",
    "print(\"size of X:\" ,encoding_input_array_np.shape)\n",
    "print(\"size of Y:\" ,encoding_label_array_np.shape)\n",
    "\n",
    "Size_submatrices = int(50000/args.num_users)\n",
    "\n",
    "\n",
    "for i in range(args.num_users):\n",
    "    \n",
    "    stt_pos = i*Size_submatrices\n",
    "    end_pos = (i+1)*Size_submatrices\n",
    "#     print(i,stt_pos,end_pos)\n",
    "    Temp_train = DataLoader(DatasetSplit(dataset_train, dict_users[i]), batch_size=Size_submatrices, shuffle=True)\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(Temp_train):\n",
    "        \n",
    "        images_np = images.detach().cpu().numpy()\n",
    "        \n",
    "#         print(np.size(images_np))\n",
    "        encoding_input_array_np[stt_pos:end_pos,:] = np.reshape(images_np, (Size_submatrices,32*32*3))\n",
    "#         print(encoding_input_array_np[stt_pos:end_pos,:].shape)\n",
    "\n",
    "        onehot_labels = torch.nn.functional.one_hot(labels,num_classes=args.num_classes)\n",
    "        labels_np = onehot_labels.detach().cpu().numpy()\n",
    "#         print(labels_np.shape)\n",
    "        encoding_label_array_np[stt_pos:end_pos,:] = labels_np\n",
    "    \n",
    "print(np.shape(encoding_label_array_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z_array: [-0.94  -0.125  0.125  0.94 ]\n",
      "0.4486236179368535\n",
      "0.4857187736123886\n",
      "0.48571877361238697\n",
      "0.4486236179368556\n",
      "@BACC_Enc: N,K,T, m_i= 4 4 5 12500 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 4 5 12500 \n",
      "\n",
      "(T, sigma)= 5 1 )  0 -th Trial!!\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0046825546688503685\n",
      "conv1.bias 0.0057450756430625916\n",
      "conv2.weight 0.00221137801806132\n",
      "conv2.bias 0.001973040634766221\n",
      "fc1.weight 0.0008330729007720947\n",
      "fc1.bias 0.0008774697780609131\n",
      "fc2.weight 0.002807859768943181\n",
      "fc2.bias 0.002543147121156965\n",
      "fc3.weight 0.00389044341586885\n",
      "fc3.bias 0.002784029766917229\n",
      "\n",
      "Test set: Average loss: 2.1409 \n",
      "Accuracy: 2177/10000 (21.77%)\n",
      "\n",
      "Round   0, Average loss 2.141 Test accuracy 21.770\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0012689701716105143\n",
      "conv1.bias 0.0008203949158390363\n",
      "conv2.weight 0.000493132472038269\n",
      "conv2.bias 0.0007007064996287227\n",
      "fc1.weight 8.147788047790527e-05\n",
      "fc1.bias 0.00018834128665427366\n",
      "fc2.weight 0.0001605666464283353\n",
      "fc2.bias 0.0005881305606592269\n",
      "fc3.weight 0.00067035782904852\n",
      "fc3.bias 0.0006155683659017086\n",
      "\n",
      "Test set: Average loss: 2.0036 \n",
      "Accuracy: 2779/10000 (27.79%)\n",
      "\n",
      "Round   1, Average loss 2.004 Test accuracy 27.790\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.000797226693895128\n",
      "conv1.bias 0.0005986041311795512\n",
      "conv2.weight 0.0005207357803980509\n",
      "conv2.bias 0.0008279180619865656\n",
      "fc1.weight 0.00015845056374867757\n",
      "fc1.bias 0.00033464459702372553\n",
      "fc2.weight 0.0002456718020968967\n",
      "fc2.bias 0.0005809331667565164\n",
      "fc3.weight 0.0004932999965690431\n",
      "fc3.bias 0.0003578869858756661\n",
      "\n",
      "Test set: Average loss: 1.9450 \n",
      "Accuracy: 3027/10000 (30.27%)\n",
      "\n",
      "Round   2, Average loss 1.945 Test accuracy 30.270\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0007235194577111139\n",
      "conv1.bias 0.0007034947630017996\n",
      "conv2.weight 0.0005559872090816498\n",
      "conv2.bias 0.0009023770689964294\n",
      "fc1.weight 0.00019958130518595376\n",
      "fc1.bias 0.0004015230884154638\n",
      "fc2.weight 0.0003423659810944209\n",
      "fc2.bias 0.0006225176510356722\n",
      "fc3.weight 0.0005208441189357213\n",
      "fc3.bias 0.00016236078226938844\n",
      "\n",
      "Test set: Average loss: 1.9179 \n",
      "Accuracy: 3134/10000 (31.34%)\n",
      "\n",
      "Round   3, Average loss 1.918 Test accuracy 31.340\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0006977638933393691\n",
      "conv1.bias 0.0007971962913870811\n",
      "conv2.weight 0.0005548523863156637\n",
      "conv2.bias 0.0009425694006495178\n",
      "fc1.weight 0.00021601943174997965\n",
      "fc1.bias 0.00045265074198444684\n",
      "fc2.weight 0.0004053277155709645\n",
      "fc2.bias 0.0007183596580511047\n",
      "fc3.weight 0.0005803313283693223\n",
      "fc3.bias 0.0001430822303518653\n",
      "\n",
      "Test set: Average loss: 1.8986 \n",
      "Accuracy: 3203/10000 (32.03%)\n",
      "\n",
      "Round   4, Average loss 1.899 Test accuracy 32.030\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0006683819161521064\n",
      "conv1.bias 0.0008473232543716828\n",
      "conv2.weight 0.0005447060863176982\n",
      "conv2.bias 0.0009695700136944652\n",
      "fc1.weight 0.00022613819440205893\n",
      "fc1.bias 0.0005013928438226382\n",
      "fc2.weight 0.000440176422633822\n",
      "fc2.bias 0.0008379484393766948\n",
      "fc3.weight 0.000625251020703997\n",
      "fc3.bias 0.00020644504111260175\n",
      "\n",
      "Test set: Average loss: 1.8884 \n",
      "Accuracy: 3220/10000 (32.20%)\n",
      "\n",
      "Round   5, Average loss 1.888 Test accuracy 32.200\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0006540633572472466\n",
      "conv1.bias 0.0008519589124868313\n",
      "conv2.weight 0.0005351121723651886\n",
      "conv2.bias 0.0009856082033365965\n",
      "fc1.weight 0.0002297598918279012\n",
      "fc1.bias 0.0005385757734378179\n",
      "fc2.weight 0.00045705163289630225\n",
      "fc2.bias 0.0009332765780744099\n",
      "fc3.weight 0.0006499712665875753\n",
      "fc3.bias 0.000275668827816844\n",
      "\n",
      "Test set: Average loss: 1.8835 \n",
      "Accuracy: 3238/10000 (32.38%)\n",
      "\n",
      "Round   6, Average loss 1.883 Test accuracy 32.380\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0006497791740629408\n",
      "conv1.bias 0.000850155483931303\n",
      "conv2.weight 0.0005302668611208598\n",
      "conv2.bias 0.0009821890853345394\n",
      "fc1.weight 0.00023268473148345948\n",
      "fc1.bias 0.0005632716541488966\n",
      "fc2.weight 0.00047188628287542434\n",
      "fc2.bias 0.0010305657273247128\n",
      "fc3.weight 0.0006724088674499875\n",
      "fc3.bias 0.00032374062575399873\n",
      "\n",
      "Test set: Average loss: 1.8808 \n",
      "Accuracy: 3236/10000 (32.36%)\n",
      "\n",
      "Round   7, Average loss 1.881 Test accuracy 32.360\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0006522013081444634\n",
      "conv1.bias 0.0008534745623668035\n",
      "conv2.weight 0.0005297335982322693\n",
      "conv2.bias 0.0009846034226939082\n",
      "fc1.weight 0.0002338178555170695\n",
      "fc1.bias 0.0005870946993430455\n",
      "fc2.weight 0.000480589507118104\n",
      "fc2.bias 0.001111339156826337\n",
      "fc3.weight 0.0006927515779222761\n",
      "fc3.bias 0.00035028671845793724\n",
      "\n",
      "Test set: Average loss: 1.8780 \n",
      "Accuracy: 3252/10000 (32.52%)\n",
      "\n",
      "Round   8, Average loss 1.878 Test accuracy 32.520\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0006554339329401652\n",
      "conv1.bias 0.0008455694187432528\n",
      "conv2.weight 0.000529110978047053\n",
      "conv2.bias 0.0009837718680500984\n",
      "fc1.weight 0.00023447291056315103\n",
      "fc1.bias 0.0006045374398430188\n",
      "fc2.weight 0.0004889233717842707\n",
      "fc2.bias 0.0011749368693147386\n",
      "fc3.weight 0.000711648804800851\n",
      "fc3.bias 0.00036846038419753314\n",
      "\n",
      "Test set: Average loss: 1.8766 \n",
      "Accuracy: 3244/10000 (32.44%)\n",
      "\n",
      "Round   9, Average loss 1.877 Test accuracy 32.440\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0006597668594784207\n",
      "conv1.bias 0.0008449822198599577\n",
      "conv2.weight 0.0005315773685773214\n",
      "conv2.bias 0.0009699081419967115\n",
      "fc1.weight 0.00023618316650390626\n",
      "fc1.bias 0.0006257879237333933\n",
      "fc2.weight 0.0004993295858776759\n",
      "fc2.bias 0.0012221299998816989\n",
      "fc3.weight 0.0007295719214848109\n",
      "fc3.bias 0.0003790891729295254\n",
      "\n",
      "Test set: Average loss: 1.8747 \n",
      "Accuracy: 3248/10000 (32.48%)\n",
      "\n",
      "Round  10, Average loss 1.875 Test accuracy 32.480\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0006654984421200223\n",
      "conv1.bias 0.0008417729598780473\n",
      "conv2.weight 0.0005304373800754548\n",
      "conv2.bias 0.0009897586423903704\n",
      "fc1.weight 0.00023685936133066812\n",
      "fc1.bias 0.0006466502944628398\n",
      "fc2.weight 0.0005082822981334867\n",
      "fc2.bias 0.0012546869970503308\n",
      "fc3.weight 0.0007442961136500041\n",
      "fc3.bias 0.0003891151864081621\n",
      "\n",
      "Test set: Average loss: 1.8723 \n",
      "Accuracy: 3254/10000 (32.54%)\n",
      "\n",
      "Round  11, Average loss 1.872 Test accuracy 32.540\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0006740087270736694\n",
      "conv1.bias 0.000861052501325806\n",
      "conv2.weight 0.0005290946364402771\n",
      "conv2.bias 0.0010170992463827133\n",
      "fc1.weight 0.0002363933523495992\n",
      "fc1.bias 0.000663708212474982\n",
      "fc2.weight 0.0005113304134399172\n",
      "fc2.bias 0.0012637772375629062\n",
      "fc3.weight 0.0007553303525561378\n",
      "fc3.bias 0.0003973189275711775\n",
      "\n",
      "Test set: Average loss: 1.8722 \n",
      "Accuracy: 3235/10000 (32.35%)\n",
      "\n",
      "Round  12, Average loss 1.872 Test accuracy 32.350\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0006850867138968573\n",
      "conv1.bias 0.000905668285364906\n",
      "conv2.weight 0.0005257153511047364\n",
      "conv2.bias 0.00104029371868819\n",
      "fc1.weight 0.00023725601037343343\n",
      "fc1.bias 0.0006882275144259135\n",
      "fc2.weight 0.0005140211374040634\n",
      "fc2.bias 0.0012676729155438288\n",
      "fc3.weight 0.0007595150243668329\n",
      "fc3.bias 0.0003966605756431818\n",
      "\n",
      "Test set: Average loss: 1.8735 \n",
      "Accuracy: 3230/10000 (32.30%)\n",
      "\n",
      "Round  13, Average loss 1.874 Test accuracy 32.300\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0006961550977494981\n",
      "conv1.bias 0.0009180760631958643\n",
      "conv2.weight 0.0005213351051012675\n",
      "conv2.bias 0.0010443481151014566\n",
      "fc1.weight 0.00023830326398213705\n",
      "fc1.bias 0.0007023245717088382\n",
      "fc2.weight 0.0005148542305779836\n",
      "fc2.bias 0.0012759305536746979\n",
      "fc3.weight 0.0007634530464808146\n",
      "fc3.bias 0.00039492552168667315\n",
      "\n",
      "Test set: Average loss: 1.8728 \n",
      "Accuracy: 3230/10000 (32.30%)\n",
      "\n",
      "Round  14, Average loss 1.873 Test accuracy 32.300\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0007079006565941704\n",
      "conv1.bias 0.000929664121940732\n",
      "conv2.weight 0.0005175993839899699\n",
      "conv2.bias 0.0010379496961832047\n",
      "fc1.weight 0.00024032402038574218\n",
      "fc1.bias 0.0007170669734477997\n",
      "fc2.weight 0.0005187820820581346\n",
      "fc2.bias 0.0012731153872751054\n",
      "fc3.weight 0.0007691122946285067\n",
      "fc3.bias 0.0003940038848668337\n",
      "\n",
      "Test set: Average loss: 1.8745 \n",
      "Accuracy: 3224/10000 (32.24%)\n",
      "\n",
      "Round  15, Average loss 1.875 Test accuracy 32.240\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0007186543941497803\n",
      "conv1.bias 0.0009602485224604607\n",
      "conv2.weight 0.0005145664016405741\n",
      "conv2.bias 0.001034400425851345\n",
      "fc1.weight 0.00024294916788736979\n",
      "fc1.bias 0.000730843593676885\n",
      "fc2.weight 0.0005230952822972858\n",
      "fc2.bias 0.0012798538165433066\n",
      "fc3.weight 0.0007725712798890614\n",
      "fc3.bias 0.00039480621926486493\n",
      "\n",
      "Test set: Average loss: 1.8742 \n",
      "Accuracy: 3231/10000 (32.31%)\n",
      "\n",
      "Round  16, Average loss 1.874 Test accuracy 32.310\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0007223129934734768\n",
      "conv1.bias 0.0009748137866457304\n",
      "conv2.weight 0.0005145438015460968\n",
      "conv2.bias 0.0010229001054540277\n",
      "fc1.weight 0.000243849515914917\n",
      "fc1.bias 0.0007478305449088415\n",
      "fc2.weight 0.0005239520754132952\n",
      "fc2.bias 0.0012925471223535993\n",
      "fc3.weight 0.000776008339155288\n",
      "fc3.bias 0.000397820770740509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.8751 \n",
      "Accuracy: 3224/10000 (32.24%)\n",
      "\n",
      "Round  17, Average loss 1.875 Test accuracy 32.240\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0007295861509111193\n",
      "conv1.bias 0.0009937184707572062\n",
      "conv2.weight 0.0005137790739536285\n",
      "conv2.bias 0.0010149362497031689\n",
      "fc1.weight 0.00024565706650416056\n",
      "fc1.bias 0.0007606702546278636\n",
      "fc2.weight 0.0005244418269111996\n",
      "fc2.bias 0.0012977466519389833\n",
      "fc3.weight 0.0007799324535188221\n",
      "fc3.bias 0.0003959155175834894\n",
      "\n",
      "Test set: Average loss: 1.8764 \n",
      "Accuracy: 3212/10000 (32.12%)\n",
      "\n",
      "Round  18, Average loss 1.876 Test accuracy 32.120\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0007364657190110949\n",
      "conv1.bias 0.0009905201538155477\n",
      "conv2.weight 0.0005136850972970327\n",
      "conv2.bias 0.0010153441689908504\n",
      "fc1.weight 0.0002480927308400472\n",
      "fc1.bias 0.000774102658033371\n",
      "fc2.weight 0.0005308111981740073\n",
      "fc2.bias 0.001293356486019634\n",
      "fc3.weight 0.0007811974201883589\n",
      "fc3.bias 0.0003954425919800997\n",
      "\n",
      "Test set: Average loss: 1.8773 \n",
      "Accuracy: 3191/10000 (31.91%)\n",
      "\n",
      "Round  19, Average loss 1.877 Test accuracy 31.910\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.000740002261267768\n",
      "conv1.bias 0.0009891843268026908\n",
      "conv2.weight 0.0005143187940120697\n",
      "conv2.bias 0.0010060116183012724\n",
      "fc1.weight 0.00024906879663467405\n",
      "fc1.bias 0.0007922821988662084\n",
      "fc2.weight 0.0005291814368868632\n",
      "fc2.bias 0.0013098032878977911\n",
      "fc3.weight 0.0007849867854799543\n",
      "fc3.bias 0.00039593474939465524\n",
      "\n",
      "Test set: Average loss: 1.8781 \n",
      "Accuracy: 3206/10000 (32.06%)\n",
      "\n",
      "Round  20, Average loss 1.878 Test accuracy 32.060\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0007465045319663153\n",
      "conv1.bias 0.0009945559625824292\n",
      "conv2.weight 0.0005125803252061208\n",
      "conv2.bias 0.0009960205061361194\n",
      "fc1.weight 0.00025096788009007774\n",
      "fc1.bias 0.0008101631576816241\n",
      "fc2.weight 0.0005302497318812779\n",
      "fc2.bias 0.0013137701011839368\n",
      "fc3.weight 0.0007869879404703776\n",
      "fc3.bias 0.00038737046997994184\n",
      "\n",
      "Test set: Average loss: 1.8818 \n",
      "Accuracy: 3189/10000 (31.89%)\n",
      "\n",
      "Round  21, Average loss 1.882 Test accuracy 31.890\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0007512868775261773\n",
      "conv1.bias 0.000986716089149316\n",
      "conv2.weight 0.000513828694820404\n",
      "conv2.bias 0.0009890437358990312\n",
      "fc1.weight 0.00025172446171442667\n",
      "fc1.bias 0.0008175025383631388\n",
      "fc2.weight 0.0005307358408731127\n",
      "fc2.bias 0.0013220884083282379\n",
      "fc3.weight 0.00078860947063991\n",
      "fc3.bias 0.0003858934389427304\n",
      "\n",
      "Test set: Average loss: 1.8822 \n",
      "Accuracy: 3177/10000 (31.77%)\n",
      "\n",
      "Round  22, Average loss 1.882 Test accuracy 31.770\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0007569854789310031\n",
      "conv1.bias 0.001000658453752597\n",
      "conv2.weight 0.0005136861900488536\n",
      "conv2.bias 0.0009789969772100449\n",
      "fc1.weight 0.0002544713020324707\n",
      "fc1.bias 0.0008203497777382533\n",
      "fc2.weight 0.0005324023110525949\n",
      "fc2.bias 0.0013305706282456715\n",
      "fc3.weight 0.0007905051821754092\n",
      "fc3.bias 0.0003795140190050006\n",
      "\n",
      "Test set: Average loss: 1.8807 \n",
      "Accuracy: 3195/10000 (31.95%)\n",
      "\n",
      "Round  23, Average loss 1.881 Test accuracy 31.950\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0007564439376195271\n",
      "conv1.bias 0.000992392422631383\n",
      "conv2.weight 0.000514334241549174\n",
      "conv2.bias 0.0009861999424174428\n",
      "fc1.weight 0.00025475865602493284\n",
      "fc1.bias 0.0008413241555293401\n",
      "fc2.weight 0.0005348705110095796\n",
      "fc2.bias 0.0013424143017757508\n",
      "fc3.weight 0.0007921719834918068\n",
      "fc3.bias 0.00037776585668325424\n",
      "\n",
      "Test set: Average loss: 1.8787 \n",
      "Accuracy: 3212/10000 (32.12%)\n",
      "\n",
      "Round  24, Average loss 1.879 Test accuracy 32.120\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0007601872417661879\n",
      "conv1.bias 0.0009841011681904395\n",
      "conv2.weight 0.0005132229129473368\n",
      "conv2.bias 0.0009796911617740989\n",
      "fc1.weight 0.0002567021449406942\n",
      "fc1.bias 0.0008553499976793925\n",
      "fc2.weight 0.0005341330217936682\n",
      "fc2.bias 0.0013590212911367416\n",
      "fc3.weight 0.0007929272594906035\n",
      "fc3.bias 0.00037037294823676347\n",
      "\n",
      "Test set: Average loss: 1.8805 \n",
      "Accuracy: 3196/10000 (31.96%)\n",
      "\n",
      "Round  25, Average loss 1.880 Test accuracy 31.960\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0007607026894887288\n",
      "conv1.bias 0.0009882710874080658\n",
      "conv2.weight 0.0005128541588783264\n",
      "conv2.bias 0.0009638078045099974\n",
      "fc1.weight 0.0002568242351214091\n",
      "fc1.bias 0.0008653679241736729\n",
      "fc2.weight 0.0005371854891852727\n",
      "fc2.bias 0.0013497156046685718\n",
      "fc3.weight 0.0007945157942317781\n",
      "fc3.bias 0.00036785469856113193\n",
      "\n",
      "Test set: Average loss: 1.8790 \n",
      "Accuracy: 3199/10000 (31.99%)\n",
      "\n",
      "Round  26, Average loss 1.879 Test accuracy 31.990\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0007593304581112332\n",
      "conv1.bias 0.0009787598780045907\n",
      "conv2.weight 0.0005131346484025319\n",
      "conv2.bias 0.000949153327383101\n",
      "fc1.weight 0.00025664085149765015\n",
      "fc1.bias 0.000870691364010175\n",
      "fc2.weight 0.0005358956162891691\n",
      "fc2.bias 0.001343590606536184\n",
      "fc3.weight 0.0007939410351571583\n",
      "fc3.bias 0.00036539353895932437\n",
      "\n",
      "Test set: Average loss: 1.8815 \n",
      "Accuracy: 3190/10000 (31.90%)\n",
      "\n",
      "Round  27, Average loss 1.881 Test accuracy 31.900\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0007634303304884169\n",
      "conv1.bias 0.0009791417978703976\n",
      "conv2.weight 0.0005155850450197856\n",
      "conv2.bias 0.0009392711217515171\n",
      "fc1.weight 0.00025756335258483886\n",
      "fc1.bias 0.0008751658101876576\n",
      "fc2.weight 0.0005351959712921627\n",
      "fc2.bias 0.0013468034920238313\n",
      "fc3.weight 0.0007923676854088193\n",
      "fc3.bias 0.0003657607128843665\n",
      "\n",
      "Test set: Average loss: 1.8769 \n",
      "Accuracy: 3210/10000 (32.10%)\n",
      "\n",
      "Round  28, Average loss 1.877 Test accuracy 32.100\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0007660334640079075\n",
      "conv1.bias 0.0009818386752158403\n",
      "conv2.weight 0.0005160224437713623\n",
      "conv2.bias 0.0009201413486152887\n",
      "fc1.weight 0.00025845475991566975\n",
      "fc1.bias 0.0008722433199485143\n",
      "fc2.weight 0.0005354782891651941\n",
      "fc2.bias 0.0013475086362588974\n",
      "fc3.weight 0.0007940039038658142\n",
      "fc3.bias 0.00036894707009196284\n",
      "\n",
      "Test set: Average loss: 1.8784 \n",
      "Accuracy: 3205/10000 (32.05%)\n",
      "\n",
      "Round  29, Average loss 1.878 Test accuracy 32.050\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0007650687959459093\n",
      "conv1.bias 0.0009739571250975132\n",
      "conv2.weight 0.0005171632766723633\n",
      "conv2.bias 0.0009149383404292166\n",
      "fc1.weight 0.000259472926457723\n",
      "fc1.bias 0.0008616697043180466\n",
      "fc2.weight 0.0005351085511464921\n",
      "fc2.bias 0.0013408262637399492\n",
      "fc3.weight 0.0007937834376380557\n",
      "fc3.bias 0.00036863326095044614\n",
      "\n",
      "Test set: Average loss: 1.8759 \n",
      "Accuracy: 3224/10000 (32.24%)\n",
      "\n",
      "Round  30, Average loss 1.876 Test accuracy 32.240\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0007650257481469048\n",
      "conv1.bias 0.0009800132053593795\n",
      "conv2.weight 0.0005194746454556783\n",
      "conv2.bias 0.0009163112845271826\n",
      "fc1.weight 0.00025856324036916095\n",
      "fc1.bias 0.000858003521958987\n",
      "fc2.weight 0.000532586517788115\n",
      "fc2.bias 0.0013466388696715945\n",
      "fc3.weight 0.0007902691761652628\n",
      "fc3.bias 0.0003672654740512371\n",
      "\n",
      "Test set: Average loss: 1.8778 \n",
      "Accuracy: 3211/10000 (32.11%)\n",
      "\n",
      "Round  31, Average loss 1.878 Test accuracy 32.110\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0007628886567221748\n",
      "conv1.bias 0.0009817864435414474\n",
      "conv2.weight 0.0005209608376026153\n",
      "conv2.bias 0.0009047788335010409\n",
      "fc1.weight 0.0002581691741943359\n",
      "fc1.bias 0.0008551940942804018\n",
      "fc2.weight 0.0005316697888904148\n",
      "fc2.bias 0.001340706610963458\n",
      "fc3.weight 0.0007898234895297459\n",
      "fc3.bias 0.00037052882835268973\n",
      "\n",
      "Test set: Average loss: 1.8771 \n",
      "Accuracy: 3218/10000 (32.18%)\n",
      "\n",
      "Round  32, Average loss 1.877 Test accuracy 32.180\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.000764913558959961\n",
      "conv1.bias 0.000991003355011344\n",
      "conv2.weight 0.0005230735739072164\n",
      "conv2.bias 0.0009033602545969188\n",
      "fc1.weight 0.000257863978544871\n",
      "fc1.bias 0.0008536826819181443\n",
      "fc2.weight 0.0005304993145049564\n",
      "fc2.bias 0.0013238079845905304\n",
      "fc3.weight 0.0007890272708166213\n",
      "fc3.bias 0.0003732658689841628\n",
      "\n",
      "Test set: Average loss: 1.8768 \n",
      "Accuracy: 3220/10000 (32.20%)\n",
      "\n",
      "Round  33, Average loss 1.877 Test accuracy 32.200\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0007629513740539551\n",
      "conv1.bias 0.0009701043212165436\n",
      "conv2.weight 0.0005251036087671916\n",
      "conv2.bias 0.0008983273291960359\n",
      "fc1.weight 0.00025642935434977215\n",
      "fc1.bias 0.0008527102569739023\n",
      "fc2.weight 0.0005296103538028777\n",
      "fc2.bias 0.0013306353773389543\n",
      "fc3.weight 0.000788688375836327\n",
      "fc3.bias 0.00037476553115993737\n",
      "\n",
      "Test set: Average loss: 1.8738 \n",
      "Accuracy: 3237/10000 (32.37%)\n",
      "\n",
      "Round  34, Average loss 1.874 Test accuracy 32.370\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0007615576187769572\n",
      "conv1.bias 0.0009569937052826086\n",
      "conv2.weight 0.0005300715565681458\n",
      "conv2.bias 0.0008940899861045182\n",
      "fc1.weight 0.00025745626290639244\n",
      "fc1.bias 0.0008441776658097903\n",
      "fc2.weight 0.0005297564324878511\n",
      "fc2.bias 0.0013226372677655448\n",
      "fc3.weight 0.0007897704839706421\n",
      "fc3.bias 0.00038035186007618905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.8732 \n",
      "Accuracy: 3256/10000 (32.56%)\n",
      "\n",
      "Round  35, Average loss 1.873 Test accuracy 32.560\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0007622079054514567\n",
      "conv1.bias 0.0009548254311084747\n",
      "conv2.weight 0.0005302322407563527\n",
      "conv2.bias 0.000896253390237689\n",
      "fc1.weight 0.0002558035850524902\n",
      "fc1.bias 0.0008439296235640844\n",
      "fc2.weight 0.0005269993392248003\n",
      "fc2.bias 0.0013279451855591365\n",
      "fc3.weight 0.0007866613212085906\n",
      "fc3.bias 0.00038237974513322115\n",
      "\n",
      "Test set: Average loss: 1.8745 \n",
      "Accuracy: 3229/10000 (32.29%)\n",
      "\n",
      "Round  36, Average loss 1.875 Test accuracy 32.290\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0007633494668536716\n",
      "conv1.bias 0.0009622796593854824\n",
      "conv2.weight 0.0005311388770739237\n",
      "conv2.bias 0.0008986147586256266\n",
      "fc1.weight 0.00025518161058425904\n",
      "fc1.bias 0.0008421518529454867\n",
      "fc2.weight 0.0005283672185171218\n",
      "fc2.bias 0.0013114006391593388\n",
      "fc3.weight 0.0007876724714324588\n",
      "fc3.bias 0.00038437526673078536\n",
      "\n",
      "Test set: Average loss: 1.8738 \n",
      "Accuracy: 3238/10000 (32.38%)\n",
      "\n",
      "Round  37, Average loss 1.874 Test accuracy 32.380\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0007640948560502794\n",
      "conv1.bias 0.0009551329227785269\n",
      "conv2.weight 0.000530820886294047\n",
      "conv2.bias 0.000899042293895036\n",
      "fc1.weight 0.00025468913714090985\n",
      "fc1.bias 0.0008444750681519508\n",
      "fc2.weight 0.0005295529252006894\n",
      "fc2.bias 0.0013094454826343628\n",
      "fc3.weight 0.0007896006107330322\n",
      "fc3.bias 0.0003808264620602131\n",
      "\n",
      "Test set: Average loss: 1.8739 \n",
      "Accuracy: 3237/10000 (32.37%)\n",
      "\n",
      "Round  38, Average loss 1.874 Test accuracy 32.370\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0007642302248213026\n",
      "conv1.bias 0.0009535329105953375\n",
      "conv2.weight 0.0005307990809281667\n",
      "conv2.bias 0.0009085781057365239\n",
      "fc1.weight 0.00025470401843388877\n",
      "fc1.bias 0.0008477967853347461\n",
      "fc2.weight 0.0005308189089336092\n",
      "fc2.bias 0.0013021423170963924\n",
      "fc3.weight 0.0007909854962712242\n",
      "fc3.bias 0.0003822504077106714\n",
      "\n",
      "Test set: Average loss: 1.8739 \n",
      "Accuracy: 3237/10000 (32.37%)\n",
      "\n",
      "Round  39, Average loss 1.874 Test accuracy 32.370\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0007662359873453776\n",
      "conv1.bias 0.0009461701847612858\n",
      "conv2.weight 0.0005297519763310751\n",
      "conv2.bias 0.0009171459241770208\n",
      "fc1.weight 0.00025360135237375897\n",
      "fc1.bias 0.000848271573583285\n",
      "fc2.weight 0.0005313188310653444\n",
      "fc2.bias 0.001296028672229676\n",
      "fc3.weight 0.0007881696735109601\n",
      "fc3.bias 0.00038930075243115425\n",
      "\n",
      "Test set: Average loss: 1.8757 \n",
      "Accuracy: 3229/10000 (32.29%)\n",
      "\n",
      "Round  40, Average loss 1.876 Test accuracy 32.290\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0007628824975755479\n",
      "conv1.bias 0.0009468178420017163\n",
      "conv2.weight 0.0005273764332135518\n",
      "conv2.bias 0.0009150846744887531\n",
      "fc1.weight 0.00025371098518371584\n",
      "fc1.bias 0.0008442269017299016\n",
      "fc2.weight 0.0005304675726663498\n",
      "fc2.bias 0.001289497794849532\n",
      "fc3.weight 0.0007894107273646763\n",
      "fc3.bias 0.0003862711600959301\n",
      "\n",
      "Test set: Average loss: 1.8761 \n",
      "Accuracy: 3216/10000 (32.16%)\n",
      "\n",
      "Round  41, Average loss 1.876 Test accuracy 32.160\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0007638782262802124\n",
      "conv1.bias 0.000922908696035544\n",
      "conv2.weight 0.0005274716019630432\n",
      "conv2.bias 0.0009172396967187524\n",
      "fc1.weight 0.0002538947264353434\n",
      "fc1.bias 0.0008411218722661336\n",
      "fc2.weight 0.0005290383857394021\n",
      "fc2.bias 0.0012951953602688654\n",
      "fc3.weight 0.0007877037638709659\n",
      "fc3.bias 0.00038979006931185725\n",
      "\n",
      "Test set: Average loss: 1.8765 \n",
      "Accuracy: 3220/10000 (32.20%)\n",
      "\n",
      "Round  42, Average loss 1.876 Test accuracy 32.200\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0007628018326229519\n",
      "conv1.bias 0.0009277020581066608\n",
      "conv2.weight 0.0005266984303792317\n",
      "conv2.bias 0.0009144230862148106\n",
      "fc1.weight 0.00025405073165893555\n",
      "fc1.bias 0.0008354333539803822\n",
      "fc2.weight 0.0005302668563903324\n",
      "fc2.bias 0.0012961855779091518\n",
      "fc3.weight 0.0007871491568429129\n",
      "fc3.bias 0.00038726585917174815\n",
      "\n",
      "Test set: Average loss: 1.8774 \n",
      "Accuracy: 3212/10000 (32.12%)\n",
      "\n",
      "Round  43, Average loss 1.877 Test accuracy 32.120\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0007639496193991767\n",
      "conv1.bias 0.0009368700751413902\n",
      "conv2.weight 0.0005260330935319264\n",
      "conv2.bias 0.0009315841598436236\n",
      "fc1.weight 0.0002543125549952189\n",
      "fc1.bias 0.0008324075490236283\n",
      "fc2.weight 0.000529211475735619\n",
      "fc2.bias 0.00128997578507378\n",
      "fc3.weight 0.0007880985736846924\n",
      "fc3.bias 0.0003921669907867908\n",
      "\n",
      "Test set: Average loss: 1.8749 \n",
      "Accuracy: 3223/10000 (32.23%)\n",
      "\n",
      "Round  44, Average loss 1.875 Test accuracy 32.230\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0007647485203213162\n",
      "conv1.bias 0.0009345051366835833\n",
      "conv2.weight 0.000527915358543396\n",
      "conv2.bias 0.0009270345326513052\n",
      "fc1.weight 0.0002559197545051575\n",
      "fc1.bias 0.0008350955943266551\n",
      "fc2.weight 0.0005313925326816619\n",
      "fc2.bias 0.0012895394826219196\n",
      "fc3.weight 0.0007901975086757115\n",
      "fc3.bias 0.00039433767087757585\n",
      "\n",
      "Test set: Average loss: 1.8784 \n",
      "Accuracy: 3223/10000 (32.23%)\n",
      "\n",
      "Round  45, Average loss 1.878 Test accuracy 32.230\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0007644295692443848\n",
      "conv1.bias 0.0009218224634726843\n",
      "conv2.weight 0.0005263801415761311\n",
      "conv2.bias 0.0009247616399079561\n",
      "fc1.weight 0.0002550360163052877\n",
      "fc1.bias 0.0008292385066548983\n",
      "fc2.weight 0.0005302211594960046\n",
      "fc2.bias 0.0012926623402606872\n",
      "fc3.weight 0.0007892131805419921\n",
      "fc3.bias 0.00039124079048633573\n",
      "\n",
      "Test set: Average loss: 1.8793 \n",
      "Accuracy: 3209/10000 (32.09%)\n",
      "\n",
      "Round  46, Average loss 1.879 Test accuracy 32.090\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0007698118024402194\n",
      "conv1.bias 0.0009086950061221918\n",
      "conv2.weight 0.0005267332990964254\n",
      "conv2.bias 0.0009244779357686639\n",
      "fc1.weight 0.00025636831919352213\n",
      "fc1.bias 0.0008271533995866775\n",
      "fc2.weight 0.0005308070826152015\n",
      "fc2.bias 0.001290286225931985\n",
      "fc3.weight 0.0007903590088798886\n",
      "fc3.bias 0.0003898927941918373\n",
      "\n",
      "Test set: Average loss: 1.8793 \n",
      "Accuracy: 3221/10000 (32.21%)\n",
      "\n",
      "Round  47, Average loss 1.879 Test accuracy 32.210\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0007716919978459677\n",
      "conv1.bias 0.0009161044533054034\n",
      "conv2.weight 0.000526486486196518\n",
      "conv2.bias 0.0009292347822338343\n",
      "fc1.weight 0.000257017453511556\n",
      "fc1.bias 0.0008242134004831314\n",
      "fc2.weight 0.0005341606480734689\n",
      "fc2.bias 0.001289196755914461\n",
      "fc3.weight 0.0007908588364010765\n",
      "fc3.bias 0.00039618848823010924\n",
      "\n",
      "Test set: Average loss: 1.8808 \n",
      "Accuracy: 3208/10000 (32.08%)\n",
      "\n",
      "Round  48, Average loss 1.881 Test accuracy 32.080\n",
      "selected users: [0 1 2 3]\n",
      "conv1.weight 0.0007718768384721545\n",
      "conv1.bias 0.0009225727990269661\n",
      "conv2.weight 0.0005258881549040477\n",
      "conv2.bias 0.0009154260042123497\n",
      "fc1.weight 0.0002568633755048116\n",
      "fc1.bias 0.0008261489992340405\n",
      "fc2.weight 0.0005334114271496969\n",
      "fc2.bias 0.0012844876341876529\n",
      "fc3.weight 0.0007899683855828785\n",
      "fc3.bias 0.0003951655700802803\n",
      "\n",
      "Test set: Average loss: 1.8805 \n",
      "Accuracy: 3211/10000 (32.11%)\n",
      "\n",
      "Round  49, Average loss 1.881 Test accuracy 32.110\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "K = 4\n",
    "T = 5\n",
    "sigma = 1\n",
    "Noise_Alloc = [0,2,4,6,8]\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "\n",
    "N_array = [4]\n",
    "B_array = [0.5]\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 50\n",
    "\n",
    "\n",
    "\n",
    "loss_test_arr_K4_G1 = np.zeros((len(N_array),len(B_array),N_trials,N_epochs))\n",
    "acc_test_arr_K4_G1  = np.zeros((len(N_array),len(B_array),N_trials,N_epochs))\n",
    "\n",
    "for N_idx in range(len(N_array)):\n",
    "    \n",
    "    N = N_array[N_idx]\n",
    "    \n",
    "    \n",
    "           \n",
    "        \n",
    "    # print(\"alpha_array: \",alpha_array,'\\n')\n",
    "    \n",
    "    \n",
    "    for B_idx in range(len(B_array)):\n",
    "        \n",
    "        B = B_array[B_idx]\n",
    "        z_array = []\n",
    "#         while(len(z_array)<N):\n",
    "#             z_tmp = np.random.uniform(-1,1,1)\n",
    "#             MIS_tmp = MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_tmp], 1,sigma)\n",
    "#             if MIS_tmp < B and MIS_tmp > 0.1:\n",
    "#                 z_array.append(z_tmp[0])\n",
    "#         \n",
    "#         z_array = np.sort(z_array)\n",
    "#         print(N_idx,'!!!')\n",
    "#         if N_idx==0:\n",
    "# #             z_array = np.array([-0.94,-0.534,0.534, 0.94])\n",
    "# #         elif N_idx==1:\n",
    "# #             z_array = np.array([-0.94, -0.73, 0.73, 0.94])\n",
    "#         elif N_idx==1:\n",
    "#             z_array = np.array([-0.94, -0.125, 0.125, 0.94])\n",
    "#         else:\n",
    "# #             z_array = np.array([-0.94, -0.73, -0.534, -0.125, 0.125, 0.534, 0.73, 0.94])\n",
    "# #             z_array = np.array([-0.9, -0.81, -0.22, -0.20, 0.20, 0.22, 0.81, 0.9])\n",
    "        \n",
    "        z_array = np.array([-0.94, -0.125, 0.125, 0.94])\n",
    "        \n",
    "        print('z_array:',z_array)\n",
    "        for j in range(len(z_array)):\n",
    "            print(MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_array[j]], 1,sigma))\n",
    "        \n",
    "        \n",
    "        _Noise_label = np.ones((15000*T,10)) * 0.1\n",
    "\n",
    "        X_tilde,a,b = BACC_Enc_Data_v3(encoding_input_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde,a,b = BACC_Enc_Data_v3(encoding_label_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "\n",
    "        m = N # m is the number of received result @ master\n",
    "    #     print('number of results:',m)\n",
    "\n",
    "        for trial_idx in range(N_trials):\n",
    "            print('(T, sigma)=',T,sigma,') ',trial_idx,'-th Trial!!')\n",
    "\n",
    "            net_glob = CNNCifar(args=args)\n",
    "            net_glob.cuda()\n",
    "            net_glob.train()\n",
    "\n",
    "            # copy weights\n",
    "            w_glob = net_glob.state_dict()\n",
    "\n",
    "            for iter in range(N_epochs): #args.epochs\n",
    "                w_locals, loss_locals = [], []\n",
    "                idxs_users = np.random.choice(range(N), m, replace=False)\n",
    "                idxs_users = np.sort(idxs_users)\n",
    "                print('selected users:',idxs_users)\n",
    "                \n",
    "                coded_net = BACC_Enc_Model_withNoise_v4(net_glob.cuda(), N, K, T, 1, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "                dec_z_array = []\n",
    "                for idx in idxs_users: #for idx in range(N):\n",
    "            #         print(idx)\n",
    "                    local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[idx,:,:], label=y_tilde[idx,:,:])\n",
    "                    w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "#                     w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                    w_locals.append(copy.deepcopy(w))\n",
    "                    loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                    dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "                # update global weights\n",
    "                #w_glob = FedAvg(w_locals)\n",
    "                w_glob = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "\n",
    "                # copy weight to net_glob\n",
    "                net_glob.load_state_dict(w_glob)\n",
    "\n",
    "                # print loss\n",
    "            #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "            #     loss_train_arr.append(loss_train)\n",
    "\n",
    "                acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "                acc_test_arr_K4_G1[N_idx][B_idx][trial_idx][iter] = acc_test\n",
    "                loss_test_arr_K4_G1[N_idx][B_idx][trial_idx][iter] = loss_test\n",
    "                if iter % 1 ==0:\n",
    "                    print('Round {:3d}, Average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_test, acc_test))\n",
    "                #print(loss_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3. K=4, N=4, T=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z_array: [ 1.          0.80901699  0.30901699 -0.30901699 -0.80901699]\n",
      "@BACC_Enc: N,K,T, m_i= 5 4 0 12500 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 5 4 0 12500 \n",
      "\n",
      "(T, sigma)= 0 1 )  0 -th Trial!!\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2.0013 \n",
      "Accuracy: 2079/10000 (20.79%)\n",
      "\n",
      "Round   0, Average loss 2.001 Test accuracy 20.790\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 22.4331 \n",
      "Accuracy: 2126/10000 (21.26%)\n",
      "\n",
      "Round   1, Average loss 22.433 Test accuracy 21.260\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 62.9370 \n",
      "Accuracy: 1621/10000 (16.21%)\n",
      "\n",
      "Round   2, Average loss 62.937 Test accuracy 16.210\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 142.2053 \n",
      "Accuracy: 1528/10000 (15.28%)\n",
      "\n",
      "Round   3, Average loss 142.205 Test accuracy 15.280\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 278.6917 \n",
      "Accuracy: 1094/10000 (10.94%)\n",
      "\n",
      "Round   4, Average loss 278.692 Test accuracy 10.940\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 464.2436 \n",
      "Accuracy: 994/10000 (9.94%)\n",
      "\n",
      "Round   5, Average loss 464.244 Test accuracy 9.940\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 731.8757 \n",
      "Accuracy: 1295/10000 (12.95%)\n",
      "\n",
      "Round   6, Average loss 731.876 Test accuracy 12.950\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 1015.5546 \n",
      "Accuracy: 1045/10000 (10.45%)\n",
      "\n",
      "Round   7, Average loss 1015.555 Test accuracy 10.450\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 927.3284 \n",
      "Accuracy: 1307/10000 (13.07%)\n",
      "\n",
      "Round   8, Average loss 927.328 Test accuracy 13.070\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 1318.9473 \n",
      "Accuracy: 1226/10000 (12.26%)\n",
      "\n",
      "Round   9, Average loss 1318.947 Test accuracy 12.260\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 1940.8309 \n",
      "Accuracy: 1241/10000 (12.41%)\n",
      "\n",
      "Round  10, Average loss 1940.831 Test accuracy 12.410\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 1930.8685 \n",
      "Accuracy: 1261/10000 (12.61%)\n",
      "\n",
      "Round  11, Average loss 1930.868 Test accuracy 12.610\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2573.0307 \n",
      "Accuracy: 1528/10000 (15.28%)\n",
      "\n",
      "Round  12, Average loss 2573.031 Test accuracy 15.280\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2119.8672 \n",
      "Accuracy: 1507/10000 (15.07%)\n",
      "\n",
      "Round  13, Average loss 2119.867 Test accuracy 15.070\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2354.4168 \n",
      "Accuracy: 1236/10000 (12.36%)\n",
      "\n",
      "Round  14, Average loss 2354.417 Test accuracy 12.360\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2906.5175 \n",
      "Accuracy: 1543/10000 (15.43%)\n",
      "\n",
      "Round  15, Average loss 2906.518 Test accuracy 15.430\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2534.7729 \n",
      "Accuracy: 1421/10000 (14.21%)\n",
      "\n",
      "Round  16, Average loss 2534.773 Test accuracy 14.210\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 3068.6160 \n",
      "Accuracy: 1431/10000 (14.31%)\n",
      "\n",
      "Round  17, Average loss 3068.616 Test accuracy 14.310\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 3550.0068 \n",
      "Accuracy: 1551/10000 (15.51%)\n",
      "\n",
      "Round  18, Average loss 3550.007 Test accuracy 15.510\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2569.2794 \n",
      "Accuracy: 1526/10000 (15.26%)\n",
      "\n",
      "Round  19, Average loss 2569.279 Test accuracy 15.260\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2762.7893 \n",
      "Accuracy: 1514/10000 (15.14%)\n",
      "\n",
      "Round  20, Average loss 2762.789 Test accuracy 15.140\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2563.3975 \n",
      "Accuracy: 1249/10000 (12.49%)\n",
      "\n",
      "Round  21, Average loss 2563.397 Test accuracy 12.490\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2752.4890 \n",
      "Accuracy: 1416/10000 (14.16%)\n",
      "\n",
      "Round  22, Average loss 2752.489 Test accuracy 14.160\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2627.6617 \n",
      "Accuracy: 1224/10000 (12.24%)\n",
      "\n",
      "Round  23, Average loss 2627.662 Test accuracy 12.240\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2355.0212 \n",
      "Accuracy: 1496/10000 (14.96%)\n",
      "\n",
      "Round  24, Average loss 2355.021 Test accuracy 14.960\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2735.5141 \n",
      "Accuracy: 1522/10000 (15.22%)\n",
      "\n",
      "Round  25, Average loss 2735.514 Test accuracy 15.220\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2336.3985 \n",
      "Accuracy: 1234/10000 (12.34%)\n",
      "\n",
      "Round  26, Average loss 2336.399 Test accuracy 12.340\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2626.6136 \n",
      "Accuracy: 1254/10000 (12.54%)\n",
      "\n",
      "Round  27, Average loss 2626.614 Test accuracy 12.540\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2403.0958 \n",
      "Accuracy: 1477/10000 (14.77%)\n",
      "\n",
      "Round  28, Average loss 2403.096 Test accuracy 14.770\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2083.6738 \n",
      "Accuracy: 1489/10000 (14.89%)\n",
      "\n",
      "Round  29, Average loss 2083.674 Test accuracy 14.890\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2242.6737 \n",
      "Accuracy: 1492/10000 (14.92%)\n",
      "\n",
      "Round  30, Average loss 2242.674 Test accuracy 14.920\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2407.4988 \n",
      "Accuracy: 1221/10000 (12.21%)\n",
      "\n",
      "Round  31, Average loss 2407.499 Test accuracy 12.210\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 1742.0270 \n",
      "Accuracy: 1464/10000 (14.64%)\n",
      "\n",
      "Round  32, Average loss 1742.027 Test accuracy 14.640\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2549.6610 \n",
      "Accuracy: 1502/10000 (15.02%)\n",
      "\n",
      "Round  33, Average loss 2549.661 Test accuracy 15.020\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2259.9596 \n",
      "Accuracy: 1492/10000 (14.92%)\n",
      "\n",
      "Round  34, Average loss 2259.960 Test accuracy 14.920\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2154.7875 \n",
      "Accuracy: 1488/10000 (14.88%)\n",
      "\n",
      "Round  35, Average loss 2154.787 Test accuracy 14.880\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2241.1927 \n",
      "Accuracy: 1497/10000 (14.97%)\n",
      "\n",
      "Round  36, Average loss 2241.193 Test accuracy 14.970\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2334.9361 \n",
      "Accuracy: 1502/10000 (15.02%)\n",
      "\n",
      "Round  37, Average loss 2334.936 Test accuracy 15.020\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2292.1609 \n",
      "Accuracy: 1501/10000 (15.01%)\n",
      "\n",
      "Round  38, Average loss 2292.161 Test accuracy 15.010\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 1993.5540 \n",
      "Accuracy: 1215/10000 (12.15%)\n",
      "\n",
      "Round  39, Average loss 1993.554 Test accuracy 12.150\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 1801.5985 \n",
      "Accuracy: 1477/10000 (14.77%)\n",
      "\n",
      "Round  40, Average loss 1801.598 Test accuracy 14.770\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 1975.0935 \n",
      "Accuracy: 1461/10000 (14.61%)\n",
      "\n",
      "Round  41, Average loss 1975.093 Test accuracy 14.610\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2435.0091 \n",
      "Accuracy: 1501/10000 (15.01%)\n",
      "\n",
      "Round  42, Average loss 2435.009 Test accuracy 15.010\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2283.9878 \n",
      "Accuracy: 1386/10000 (13.86%)\n",
      "\n",
      "Round  43, Average loss 2283.988 Test accuracy 13.860\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2302.2507 \n",
      "Accuracy: 1489/10000 (14.89%)\n",
      "\n",
      "Round  44, Average loss 2302.251 Test accuracy 14.890\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 1908.4606 \n",
      "Accuracy: 1213/10000 (12.13%)\n",
      "\n",
      "Round  45, Average loss 1908.461 Test accuracy 12.130\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2110.1607 \n",
      "Accuracy: 1386/10000 (13.86%)\n",
      "\n",
      "Round  46, Average loss 2110.161 Test accuracy 13.860\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2333.1768 \n",
      "Accuracy: 1497/10000 (14.97%)\n",
      "\n",
      "Round  47, Average loss 2333.177 Test accuracy 14.970\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2754.0662 \n",
      "Accuracy: 1525/10000 (15.25%)\n",
      "\n",
      "Round  48, Average loss 2754.066 Test accuracy 15.250\n",
      "selected users: [0 1 2 3 4]\n",
      "\n",
      "Test set: Average loss: 2333.7001 \n",
      "Accuracy: 1508/10000 (15.08%)\n",
      "\n",
      "Round  49, Average loss 2333.700 Test accuracy 15.080\n",
      "z_array: [ 1.          0.90096887  0.6234898   0.22252093 -0.22252093 -0.6234898\n",
      " -0.90096887]\n",
      "@BACC_Enc: N,K,T, m_i= 7 4 0 12500 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 7 4 0 12500 \n",
      "\n",
      "(T, sigma)= 0 1 )  0 -th Trial!!\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 31.1908 \n",
      "Accuracy: 1485/10000 (14.85%)\n",
      "\n",
      "Round   0, Average loss 31.191 Test accuracy 14.850\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 205.9583 \n",
      "Accuracy: 1420/10000 (14.20%)\n",
      "\n",
      "Round   1, Average loss 205.958 Test accuracy 14.200\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 354.6270 \n",
      "Accuracy: 1415/10000 (14.15%)\n",
      "\n",
      "Round   2, Average loss 354.627 Test accuracy 14.150\n",
      "selected users: [0 1 2 3 4 5 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 510.0565 \n",
      "Accuracy: 1251/10000 (12.51%)\n",
      "\n",
      "Round   3, Average loss 510.057 Test accuracy 12.510\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 587.4965 \n",
      "Accuracy: 1110/10000 (11.10%)\n",
      "\n",
      "Round   4, Average loss 587.496 Test accuracy 11.100\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 659.0045 \n",
      "Accuracy: 1268/10000 (12.68%)\n",
      "\n",
      "Round   5, Average loss 659.005 Test accuracy 12.680\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 735.8657 \n",
      "Accuracy: 1467/10000 (14.67%)\n",
      "\n",
      "Round   6, Average loss 735.866 Test accuracy 14.670\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 775.0035 \n",
      "Accuracy: 1334/10000 (13.34%)\n",
      "\n",
      "Round   7, Average loss 775.003 Test accuracy 13.340\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 802.8011 \n",
      "Accuracy: 1260/10000 (12.60%)\n",
      "\n",
      "Round   8, Average loss 802.801 Test accuracy 12.600\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 786.0416 \n",
      "Accuracy: 1192/10000 (11.92%)\n",
      "\n",
      "Round   9, Average loss 786.042 Test accuracy 11.920\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 844.7958 \n",
      "Accuracy: 1455/10000 (14.55%)\n",
      "\n",
      "Round  10, Average loss 844.796 Test accuracy 14.550\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 856.2704 \n",
      "Accuracy: 1339/10000 (13.39%)\n",
      "\n",
      "Round  11, Average loss 856.270 Test accuracy 13.390\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 913.2858 \n",
      "Accuracy: 1210/10000 (12.10%)\n",
      "\n",
      "Round  12, Average loss 913.286 Test accuracy 12.100\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 927.2470 \n",
      "Accuracy: 1245/10000 (12.45%)\n",
      "\n",
      "Round  13, Average loss 927.247 Test accuracy 12.450\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 976.4723 \n",
      "Accuracy: 1252/10000 (12.52%)\n",
      "\n",
      "Round  14, Average loss 976.472 Test accuracy 12.520\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1024.9172 \n",
      "Accuracy: 1453/10000 (14.53%)\n",
      "\n",
      "Round  15, Average loss 1024.917 Test accuracy 14.530\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1024.2809 \n",
      "Accuracy: 1449/10000 (14.49%)\n",
      "\n",
      "Round  16, Average loss 1024.281 Test accuracy 14.490\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1179.9972 \n",
      "Accuracy: 1261/10000 (12.61%)\n",
      "\n",
      "Round  17, Average loss 1179.997 Test accuracy 12.610\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1082.2636 \n",
      "Accuracy: 1249/10000 (12.49%)\n",
      "\n",
      "Round  18, Average loss 1082.264 Test accuracy 12.490\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1191.1273 \n",
      "Accuracy: 1463/10000 (14.63%)\n",
      "\n",
      "Round  19, Average loss 1191.127 Test accuracy 14.630\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1195.9824 \n",
      "Accuracy: 1438/10000 (14.38%)\n",
      "\n",
      "Round  20, Average loss 1195.982 Test accuracy 14.380\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1269.9107 \n",
      "Accuracy: 1179/10000 (11.79%)\n",
      "\n",
      "Round  21, Average loss 1269.911 Test accuracy 11.790\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1312.8683 \n",
      "Accuracy: 1456/10000 (14.56%)\n",
      "\n",
      "Round  22, Average loss 1312.868 Test accuracy 14.560\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1326.0638 \n",
      "Accuracy: 1458/10000 (14.58%)\n",
      "\n",
      "Round  23, Average loss 1326.064 Test accuracy 14.580\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1214.6747 \n",
      "Accuracy: 1307/10000 (13.07%)\n",
      "\n",
      "Round  24, Average loss 1214.675 Test accuracy 13.070\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1247.9809 \n",
      "Accuracy: 1236/10000 (12.36%)\n",
      "\n",
      "Round  25, Average loss 1247.981 Test accuracy 12.360\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1375.2655 \n",
      "Accuracy: 1254/10000 (12.54%)\n",
      "\n",
      "Round  26, Average loss 1375.265 Test accuracy 12.540\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1299.3356 \n",
      "Accuracy: 1444/10000 (14.44%)\n",
      "\n",
      "Round  27, Average loss 1299.336 Test accuracy 14.440\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1344.0458 \n",
      "Accuracy: 1316/10000 (13.16%)\n",
      "\n",
      "Round  28, Average loss 1344.046 Test accuracy 13.160\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1394.5425 \n",
      "Accuracy: 1248/10000 (12.48%)\n",
      "\n",
      "Round  29, Average loss 1394.542 Test accuracy 12.480\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1317.0398 \n",
      "Accuracy: 1243/10000 (12.43%)\n",
      "\n",
      "Round  30, Average loss 1317.040 Test accuracy 12.430\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1409.0819 \n",
      "Accuracy: 1325/10000 (13.25%)\n",
      "\n",
      "Round  31, Average loss 1409.082 Test accuracy 13.250\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1360.2535 \n",
      "Accuracy: 1315/10000 (13.15%)\n",
      "\n",
      "Round  32, Average loss 1360.253 Test accuracy 13.150\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1423.9918 \n",
      "Accuracy: 1453/10000 (14.53%)\n",
      "\n",
      "Round  33, Average loss 1423.992 Test accuracy 14.530\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1304.5999 \n",
      "Accuracy: 1239/10000 (12.39%)\n",
      "\n",
      "Round  34, Average loss 1304.600 Test accuracy 12.390\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1404.1398 \n",
      "Accuracy: 1452/10000 (14.52%)\n",
      "\n",
      "Round  35, Average loss 1404.140 Test accuracy 14.520\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1466.3629 \n",
      "Accuracy: 1457/10000 (14.57%)\n",
      "\n",
      "Round  36, Average loss 1466.363 Test accuracy 14.570\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1463.4525 \n",
      "Accuracy: 1250/10000 (12.50%)\n",
      "\n",
      "Round  37, Average loss 1463.453 Test accuracy 12.500\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1359.1480 \n",
      "Accuracy: 1418/10000 (14.18%)\n",
      "\n",
      "Round  38, Average loss 1359.148 Test accuracy 14.180\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1482.9653 \n",
      "Accuracy: 1457/10000 (14.57%)\n",
      "\n",
      "Round  39, Average loss 1482.965 Test accuracy 14.570\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1568.1809 \n",
      "Accuracy: 1458/10000 (14.58%)\n",
      "\n",
      "Round  40, Average loss 1568.181 Test accuracy 14.580\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1474.7328 \n",
      "Accuracy: 1451/10000 (14.51%)\n",
      "\n",
      "Round  41, Average loss 1474.733 Test accuracy 14.510\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1541.0887 \n",
      "Accuracy: 1251/10000 (12.51%)\n",
      "\n",
      "Round  42, Average loss 1541.089 Test accuracy 12.510\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1511.9371 \n",
      "Accuracy: 1253/10000 (12.53%)\n",
      "\n",
      "Round  43, Average loss 1511.937 Test accuracy 12.530\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1394.1591 \n",
      "Accuracy: 1235/10000 (12.35%)\n",
      "\n",
      "Round  44, Average loss 1394.159 Test accuracy 12.350\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1534.0807 \n",
      "Accuracy: 1251/10000 (12.51%)\n",
      "\n",
      "Round  45, Average loss 1534.081 Test accuracy 12.510\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1572.7553 \n",
      "Accuracy: 1255/10000 (12.55%)\n",
      "\n",
      "Round  46, Average loss 1572.755 Test accuracy 12.550\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1510.3011 \n",
      "Accuracy: 1251/10000 (12.51%)\n",
      "\n",
      "Round  47, Average loss 1510.301 Test accuracy 12.510\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1586.9163 \n",
      "Accuracy: 1252/10000 (12.52%)\n",
      "\n",
      "Round  48, Average loss 1586.916 Test accuracy 12.520\n",
      "selected users: [0 1 2 3 4 5 6]\n",
      "\n",
      "Test set: Average loss: 1594.6156 \n",
      "Accuracy: 1474/10000 (14.74%)\n",
      "\n",
      "Round  49, Average loss 1594.616 Test accuracy 14.740\n",
      "z_array: [ 1.          0.93969262  0.76604444  0.5         0.17364818 -0.17364818\n",
      " -0.5        -0.76604444 -0.93969262]\n",
      "@BACC_Enc: N,K,T, m_i= 9 4 0 12500 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 9 4 0 12500 \n",
      "\n",
      "(T, sigma)= 0 1 )  0 -th Trial!!\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 4.7435 \n",
      "Accuracy: 1702/10000 (17.02%)\n",
      "\n",
      "Round   0, Average loss 4.744 Test accuracy 17.020\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 10.1085 \n",
      "Accuracy: 1850/10000 (18.50%)\n",
      "\n",
      "Round   1, Average loss 10.108 Test accuracy 18.500\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 14.7920 \n",
      "Accuracy: 1436/10000 (14.36%)\n",
      "\n",
      "Round   2, Average loss 14.792 Test accuracy 14.360\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 16.5223 \n",
      "Accuracy: 1789/10000 (17.89%)\n",
      "\n",
      "Round   3, Average loss 16.522 Test accuracy 17.890\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 13.7797 \n",
      "Accuracy: 1183/10000 (11.83%)\n",
      "\n",
      "Round   4, Average loss 13.780 Test accuracy 11.830\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 17.8871 \n",
      "Accuracy: 1461/10000 (14.61%)\n",
      "\n",
      "Round   5, Average loss 17.887 Test accuracy 14.610\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 13.2764 \n",
      "Accuracy: 1775/10000 (17.75%)\n",
      "\n",
      "Round   6, Average loss 13.276 Test accuracy 17.750\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 15.9428 \n",
      "Accuracy: 1579/10000 (15.79%)\n",
      "\n",
      "Round   7, Average loss 15.943 Test accuracy 15.790\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 19.9954 \n",
      "Accuracy: 1795/10000 (17.95%)\n",
      "\n",
      "Round   8, Average loss 19.995 Test accuracy 17.950\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 24.7406 \n",
      "Accuracy: 1563/10000 (15.63%)\n",
      "\n",
      "Round   9, Average loss 24.741 Test accuracy 15.630\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 19.8102 \n",
      "Accuracy: 1564/10000 (15.64%)\n",
      "\n",
      "Round  10, Average loss 19.810 Test accuracy 15.640\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 7.8099 \n",
      "Accuracy: 1366/10000 (13.66%)\n",
      "\n",
      "Round  11, Average loss 7.810 Test accuracy 13.660\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 22.3433 \n",
      "Accuracy: 1245/10000 (12.45%)\n",
      "\n",
      "Round  12, Average loss 22.343 Test accuracy 12.450\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 25.3971 \n",
      "Accuracy: 1733/10000 (17.33%)\n",
      "\n",
      "Round  13, Average loss 25.397 Test accuracy 17.330\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 11.7445 \n",
      "Accuracy: 1432/10000 (14.32%)\n",
      "\n",
      "Round  14, Average loss 11.744 Test accuracy 14.320\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 18.2777 \n",
      "Accuracy: 1720/10000 (17.20%)\n",
      "\n",
      "Round  15, Average loss 18.278 Test accuracy 17.200\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 24.1944 \n",
      "Accuracy: 1299/10000 (12.99%)\n",
      "\n",
      "Round  16, Average loss 24.194 Test accuracy 12.990\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 23.5129 \n",
      "Accuracy: 1746/10000 (17.46%)\n",
      "\n",
      "Round  17, Average loss 23.513 Test accuracy 17.460\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 19.2197 \n",
      "Accuracy: 1381/10000 (13.81%)\n",
      "\n",
      "Round  18, Average loss 19.220 Test accuracy 13.810\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 26.0941 \n",
      "Accuracy: 1299/10000 (12.99%)\n",
      "\n",
      "Round  19, Average loss 26.094 Test accuracy 12.990\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 16.2975 \n",
      "Accuracy: 996/10000 (9.96%)\n",
      "\n",
      "Round  20, Average loss 16.297 Test accuracy 9.960\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 22.6457 \n",
      "Accuracy: 1786/10000 (17.86%)\n",
      "\n",
      "Round  21, Average loss 22.646 Test accuracy 17.860\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 24.6036 \n",
      "Accuracy: 719/10000 (7.19%)\n",
      "\n",
      "Round  22, Average loss 24.604 Test accuracy 7.190\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 16.9104 \n",
      "Accuracy: 1495/10000 (14.95%)\n",
      "\n",
      "Round  23, Average loss 16.910 Test accuracy 14.950\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 19.9233 \n",
      "Accuracy: 1040/10000 (10.40%)\n",
      "\n",
      "Round  24, Average loss 19.923 Test accuracy 10.400\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 13.2589 \n",
      "Accuracy: 1104/10000 (11.04%)\n",
      "\n",
      "Round  25, Average loss 13.259 Test accuracy 11.040\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 21.1113 \n",
      "Accuracy: 1237/10000 (12.37%)\n",
      "\n",
      "Round  26, Average loss 21.111 Test accuracy 12.370\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 22.5768 \n",
      "Accuracy: 1488/10000 (14.88%)\n",
      "\n",
      "Round  27, Average loss 22.577 Test accuracy 14.880\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 17.2608 \n",
      "Accuracy: 1041/10000 (10.41%)\n",
      "\n",
      "Round  28, Average loss 17.261 Test accuracy 10.410\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 16.1813 \n",
      "Accuracy: 1778/10000 (17.78%)\n",
      "\n",
      "Round  29, Average loss 16.181 Test accuracy 17.780\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 14.9773 \n",
      "Accuracy: 854/10000 (8.54%)\n",
      "\n",
      "Round  30, Average loss 14.977 Test accuracy 8.540\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 20.3241 \n",
      "Accuracy: 780/10000 (7.80%)\n",
      "\n",
      "Round  31, Average loss 20.324 Test accuracy 7.800\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 9.3785 \n",
      "Accuracy: 897/10000 (8.97%)\n",
      "\n",
      "Round  32, Average loss 9.378 Test accuracy 8.970\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 15.4156 \n",
      "Accuracy: 542/10000 (5.42%)\n",
      "\n",
      "Round  33, Average loss 15.416 Test accuracy 5.420\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 14.5373 \n",
      "Accuracy: 948/10000 (9.48%)\n",
      "\n",
      "Round  34, Average loss 14.537 Test accuracy 9.480\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 19.6934 \n",
      "Accuracy: 840/10000 (8.40%)\n",
      "\n",
      "Round  35, Average loss 19.693 Test accuracy 8.400\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 14.1269 \n",
      "Accuracy: 913/10000 (9.13%)\n",
      "\n",
      "Round  36, Average loss 14.127 Test accuracy 9.130\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 13.0321 \n",
      "Accuracy: 805/10000 (8.05%)\n",
      "\n",
      "Round  37, Average loss 13.032 Test accuracy 8.050\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 18.3588 \n",
      "Accuracy: 952/10000 (9.52%)\n",
      "\n",
      "Round  38, Average loss 18.359 Test accuracy 9.520\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 13.8616 \n",
      "Accuracy: 704/10000 (7.04%)\n",
      "\n",
      "Round  39, Average loss 13.862 Test accuracy 7.040\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 15.1838 \n",
      "Accuracy: 771/10000 (7.71%)\n",
      "\n",
      "Round  40, Average loss 15.184 Test accuracy 7.710\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 6.5382 \n",
      "Accuracy: 812/10000 (8.12%)\n",
      "\n",
      "Round  41, Average loss 6.538 Test accuracy 8.120\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 12.4725 \n",
      "Accuracy: 1296/10000 (12.96%)\n",
      "\n",
      "Round  42, Average loss 12.473 Test accuracy 12.960\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 27.6372 \n",
      "Accuracy: 860/10000 (8.60%)\n",
      "\n",
      "Round  43, Average loss 27.637 Test accuracy 8.600\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 27.9071 \n",
      "Accuracy: 503/10000 (5.03%)\n",
      "\n",
      "Round  44, Average loss 27.907 Test accuracy 5.030\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 10.4443 \n",
      "Accuracy: 833/10000 (8.33%)\n",
      "\n",
      "Round  45, Average loss 10.444 Test accuracy 8.330\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 11.0233 \n",
      "Accuracy: 942/10000 (9.42%)\n",
      "\n",
      "Round  46, Average loss 11.023 Test accuracy 9.420\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 13.8320 \n",
      "Accuracy: 755/10000 (7.55%)\n",
      "\n",
      "Round  47, Average loss 13.832 Test accuracy 7.550\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 18.4996 \n",
      "Accuracy: 950/10000 (9.50%)\n",
      "\n",
      "Round  48, Average loss 18.500 Test accuracy 9.500\n",
      "selected users: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Test set: Average loss: 25.1535 \n",
      "Accuracy: 842/10000 (8.42%)\n",
      "\n",
      "Round  49, Average loss 25.154 Test accuracy 8.420\n",
      "z_array: [ 1.          0.97094182  0.88545603  0.74851075  0.56806475  0.35460489\n",
      "  0.12053668 -0.12053668 -0.35460489 -0.56806475 -0.74851075 -0.88545603\n",
      " -0.97094182]\n",
      "@BACC_Enc: N,K,T, m_i= 13 4 0 12500 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 13 4 0 12500 \n",
      "\n",
      "(T, sigma)= 0 1 )  0 -th Trial!!\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3039 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round   0, Average loss 2.304 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3035 \n",
      "Accuracy: 1021/10000 (10.21%)\n",
      "\n",
      "Round   1, Average loss 2.303 Test accuracy 10.210\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3032 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round   2, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3027 \n",
      "Accuracy: 1339/10000 (13.39%)\n",
      "\n",
      "Round   3, Average loss 2.303 Test accuracy 13.390\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3031 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round   4, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3030 \n",
      "Accuracy: 1100/10000 (11.00%)\n",
      "\n",
      "Round   5, Average loss 2.303 Test accuracy 11.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3024 \n",
      "Accuracy: 1102/10000 (11.02%)\n",
      "\n",
      "Round   6, Average loss 2.302 Test accuracy 11.020\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3029 \n",
      "Accuracy: 1275/10000 (12.75%)\n",
      "\n",
      "Round   7, Average loss 2.303 Test accuracy 12.750\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.3027 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round   8, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3027 \n",
      "Accuracy: 1018/10000 (10.18%)\n",
      "\n",
      "Round   9, Average loss 2.303 Test accuracy 10.180\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3027 \n",
      "Accuracy: 1020/10000 (10.20%)\n",
      "\n",
      "Round  10, Average loss 2.303 Test accuracy 10.200\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3020 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  11, Average loss 2.302 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3021 \n",
      "Accuracy: 1005/10000 (10.05%)\n",
      "\n",
      "Round  12, Average loss 2.302 Test accuracy 10.050\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3025 \n",
      "Accuracy: 1125/10000 (11.25%)\n",
      "\n",
      "Round  13, Average loss 2.302 Test accuracy 11.250\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3029 \n",
      "Accuracy: 1025/10000 (10.25%)\n",
      "\n",
      "Round  14, Average loss 2.303 Test accuracy 10.250\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3033 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  15, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  16, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3029 \n",
      "Accuracy: 1054/10000 (10.54%)\n",
      "\n",
      "Round  17, Average loss 2.303 Test accuracy 10.540\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3032 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  18, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3034 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  19, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3033 \n",
      "Accuracy: 1001/10000 (10.01%)\n",
      "\n",
      "Round  20, Average loss 2.303 Test accuracy 10.010\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3010 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  21, Average loss 2.301 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3032 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  22, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  23, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  24, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3021 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  25, Average loss 2.302 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3031 \n",
      "Accuracy: 905/10000 (9.05%)\n",
      "\n",
      "Round  26, Average loss 2.303 Test accuracy 9.050\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3024 \n",
      "Accuracy: 1004/10000 (10.04%)\n",
      "\n",
      "Round  27, Average loss 2.302 Test accuracy 10.040\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3031 \n",
      "Accuracy: 1133/10000 (11.33%)\n",
      "\n",
      "Round  28, Average loss 2.303 Test accuracy 11.330\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3031 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  29, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3024 \n",
      "Accuracy: 1164/10000 (11.64%)\n",
      "\n",
      "Round  30, Average loss 2.302 Test accuracy 11.640\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3027 \n",
      "Accuracy: 1141/10000 (11.41%)\n",
      "\n",
      "Round  31, Average loss 2.303 Test accuracy 11.410\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3029 \n",
      "Accuracy: 1147/10000 (11.47%)\n",
      "\n",
      "Round  32, Average loss 2.303 Test accuracy 11.470\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3029 \n",
      "Accuracy: 1120/10000 (11.20%)\n",
      "\n",
      "Round  33, Average loss 2.303 Test accuracy 11.200\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  34, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3025 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  35, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3027 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  36, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3028 \n",
      "Accuracy: 1074/10000 (10.74%)\n",
      "\n",
      "Round  37, Average loss 2.303 Test accuracy 10.740\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3027 \n",
      "Accuracy: 1004/10000 (10.04%)\n",
      "\n",
      "Round  38, Average loss 2.303 Test accuracy 10.040\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  39, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3030 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  40, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3028 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  41, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3028 \n",
      "Accuracy: 1198/10000 (11.98%)\n",
      "\n",
      "Round  42, Average loss 2.303 Test accuracy 11.980\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3030 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  43, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3033 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  44, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3029 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  45, Average loss 2.303 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3031 \n",
      "Accuracy: 1001/10000 (10.01%)\n",
      "\n",
      "Round  46, Average loss 2.303 Test accuracy 10.010\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3028 \n",
      "Accuracy: 1054/10000 (10.54%)\n",
      "\n",
      "Round  47, Average loss 2.303 Test accuracy 10.540\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3001 \n",
      "Accuracy: 1000/10000 (10.00%)\n",
      "\n",
      "Round  48, Average loss 2.300 Test accuracy 10.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "Test set: Average loss: 2.3028 \n",
      "Accuracy: 1001/10000 (10.01%)\n",
      "\n",
      "Round  49, Average loss 2.303 Test accuracy 10.010\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "K = 4\n",
    "T = 0\n",
    "sigma = 1\n",
    "Noise_Alloc = []\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "\n",
    "N_array = [5,7,9,13]\n",
    "B_array = [0.5]\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 50\n",
    "\n",
    "\n",
    "\n",
    "loss_test_arr_K4_G1_T0 = np.zeros((len(N_array),len(B_array),N_trials,N_epochs))\n",
    "acc_test_arr_K4_G1_T0  = np.zeros((len(N_array),len(B_array),N_trials,N_epochs))\n",
    "\n",
    "for N_idx in range(len(N_array)):\n",
    "    \n",
    "    N = N_array[N_idx]\n",
    "    \n",
    "    \n",
    "           \n",
    "        \n",
    "    # print(\"alpha_array: \",alpha_array,'\\n')\n",
    "    \n",
    "    \n",
    "    for B_idx in range(len(B_array)):\n",
    "        \n",
    "        B = B_array[B_idx]\n",
    "        z_array = []\n",
    "#         while(len(z_array)<N):\n",
    "#             z_tmp = np.random.uniform(-1,1,1)\n",
    "#             MIS_tmp = MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_tmp], 1,sigma)\n",
    "#             if MIS_tmp < B and MIS_tmp > 0.1:\n",
    "#                 z_array.append(z_tmp[0])\n",
    "#         \n",
    "#         z_array = np.sort(z_array)\n",
    "#         print(N_idx,'!!!')\n",
    "#         if N_idx==0:\n",
    "# #             z_array = np.array([-0.94,-0.534,0.534, 0.94])\n",
    "# #         elif N_idx==1:\n",
    "# #             z_array = np.array([-0.94, -0.73, 0.73, 0.94])\n",
    "#         elif N_idx==1:\n",
    "#             z_array = np.array([-0.94, -0.125, 0.125, 0.94])\n",
    "#         else:\n",
    "# #             z_array = np.array([-0.94, -0.73, -0.534, -0.125, 0.125, 0.534, 0.73, 0.94])\n",
    "# #             z_array = np.array([-0.9, -0.81, -0.22, -0.20, 0.20, 0.22, 0.81, 0.9])\n",
    "        \n",
    "        i_array = np.array(range(N))\n",
    "        z_array = np.cos(i_array*2*math.pi/N/2) # np.cos(i_array*2*math.pi/N/2)\n",
    "        \n",
    "        print('z_array:',z_array)\n",
    "#         for j in range(len(z_array)):\n",
    "#             print(MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_array[j]], 1,sigma))\n",
    "        \n",
    "        \n",
    "        _Noise_label = np.ones((15000*T,10)) * 0.1\n",
    "\n",
    "        X_tilde,a,b = BACC_Enc_Data_v3(encoding_input_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde,a,b = BACC_Enc_Data_v3(encoding_label_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "\n",
    "        m = N # m is the number of received result @ master\n",
    "    #     print('number of results:',m)\n",
    "\n",
    "        for trial_idx in range(N_trials):\n",
    "            print('(T, sigma)=',T,sigma,') ',trial_idx,'-th Trial!!')\n",
    "\n",
    "            net_glob = CNNCifar(args=args)\n",
    "            net_glob.cuda()\n",
    "            net_glob.train()\n",
    "\n",
    "            # copy weights\n",
    "            w_glob = net_glob.state_dict()\n",
    "\n",
    "            for iter in range(N_epochs): #args.epochs\n",
    "                w_locals, loss_locals = [], []\n",
    "                idxs_users = np.random.choice(range(N), m, replace=False)\n",
    "                idxs_users = np.sort(idxs_users)\n",
    "                print('selected users:',idxs_users)\n",
    "                \n",
    "#                 coded_net = BACC_Enc_Model_withNoise_v4(net_glob.cuda(), N, K, T, 1, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "                dec_z_array = []\n",
    "                for idx in idxs_users: #for idx in range(N):\n",
    "            #         print(idx)\n",
    "                    local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[idx,:,:], label=y_tilde[idx,:,:])\n",
    "#                     w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "                    w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                    w_locals.append(copy.deepcopy(w))\n",
    "                    loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                    dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "                # update global weights\n",
    "                #w_glob = FedAvg(w_locals)\n",
    "                w_glob = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "\n",
    "                # copy weight to net_glob\n",
    "                net_glob.load_state_dict(w_glob)\n",
    "\n",
    "                # print loss\n",
    "            #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "            #     loss_train_arr.append(loss_train)\n",
    "\n",
    "                acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "                acc_test_arr_K4_G1_T0[N_idx][B_idx][trial_idx][iter] = acc_test\n",
    "                loss_test_arr_K4_G1_T0[N_idx][B_idx][trial_idx][iter] = loss_test\n",
    "                if iter % 1 ==0:\n",
    "                    print('Round {:3d}, Average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_test, acc_test))\n",
    "                #print(loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
