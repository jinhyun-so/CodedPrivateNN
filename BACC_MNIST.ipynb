{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "\n",
    "from utils.sampling import mnist_iid, mnist_noniid, cifar_iid\n",
    "from utils.options import args_parser\n",
    "from models.Update import LocalUpdate\n",
    "from models.Nets import MLP, CNNMnist, CNNCifar, LeNet, CNNMnist2\n",
    "from models.Fed import FedAvg\n",
    "from models.Fed import FedQAvg, Quantization, Quantization_Finite, my_score, my_score_Finite\n",
    "from models.test import test_img\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Define the Loss function\n",
    "\n",
    "As we encode the labels as well, cross entropy function should take the one-hot vector with softed value as an input.\n",
    "However, cross entorpy function supported by pytorch only takes one dimensional label (e.g [1,0,9,...] where entries presents class labels).\n",
    "Hence, now I define my cross entropy function here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_cross_entropy(input, target, size_average=True):\n",
    "    \"\"\" Cross entropy that accepts soft targets\n",
    "    Args:\n",
    "         pred: predictions for neural network\n",
    "         targets: targets, can be soft\n",
    "         size_average: if false, sum is returned instead of mean\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        input = torch.FloatTensor([[1.1, 2.8, 1.3], [1.1, 2.1, 4.8]])\n",
    "        input = torch.autograd.Variable(out, requires_grad=True)\n",
    "\n",
    "        target = torch.FloatTensor([[0.05, 0.9, 0.05], [0.05, 0.05, 0.9]])\n",
    "        target = torch.autograd.Variable(y1)\n",
    "        loss = cross_entropy(input, target)\n",
    "        loss.backward()\n",
    "    \"\"\"\n",
    "    logsoftmax = nn.LogSoftmax(dim=1)\n",
    "    if size_average:\n",
    "        return torch.mean(torch.sum(-target * logsoftmax(input) , dim=1))\n",
    "    else:\n",
    "        return torch.sum(torch.sum(-target * logsoftmax(input), dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1. Test my_cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "tensor([2, 4, 0])\n",
      "tensor([[0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 1],\n",
      "        [1, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn, autograd\n",
    "\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "# target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "target = torch.LongTensor([2,4,0])\n",
    "\n",
    "one_hot = torch.nn.functional.one_hot(target,num_classes=5)\n",
    "\n",
    "print(input.dim())\n",
    "print(target)\n",
    "print(one_hot)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot assign 'torch.FloatTensor' as parameter 'weight' (torch.nn.Parameter or None expected)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-117-54db371e0781>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m    611\u001b[0m                 raise TypeError(\"cannot assign '{}' as parameter '{}' \"\n\u001b[0;32m    612\u001b[0m                                 \u001b[1;34m\"(torch.nn.Parameter or None expected)\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 613\u001b[1;33m                                 .format(torch.typename(value), name))\n\u001b[0m\u001b[0;32m    614\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister_parameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    615\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot assign 'torch.FloatTensor' as parameter 'weight' (torch.nn.Parameter or None expected)"
     ]
    }
   ],
   "source": [
    "model = nn.Linear(2, 2)\n",
    "model.weight = torch.FloatTensor([[1,0],[0,1]])\n",
    "print(model.weight)\n",
    "print(model.bias)\n",
    "x = torch.randn(1, 2)\n",
    "# target = torch.randn(1, 2)\n",
    "output = model(x)\n",
    "print(x)\n",
    "print(output)\n",
    "# loss = my_loss(output, target)\n",
    "# loss.backward()\n",
    "# print(model.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "following outputs should be same.\n",
      "tensor(2.6336, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6336, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6336, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6336, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nfollowing outputs should be same.\")\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "loss_defalut = loss(input, target)\n",
    "print(loss_defalut)\n",
    "loss_defalut.backward()\n",
    "print(loss_defalut)\n",
    "\n",
    "\n",
    "print(loss(input, target))\n",
    "print(my_cross_entropy(input, one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [1., 0., 0.]])\n",
      "tensor(0.6178)\n",
      "tensor(0.5927)\n"
     ]
    }
   ],
   "source": [
    "out = torch.FloatTensor([[0.05, 0.9, 0.05], [0.05, 0.05, 0.9], [0.9, 0.05, 0.05]])\n",
    "out = torch.autograd.Variable(out)\n",
    "\n",
    "# Categorical targets\n",
    "y = torch.LongTensor([1, 2, 0])\n",
    "y = torch.autograd.Variable(y)\n",
    "\n",
    "# One-hot encoded targets\n",
    "y1 = torch.FloatTensor([[0, 1, 0], [0, 0, 1], [1, 0, 0]])\n",
    "y1 = torch.autograd.Variable(y1)\n",
    "\n",
    "print(y1)\n",
    "\n",
    "# Calculating the loss\n",
    "loss_val = nn.CrossEntropyLoss()(out, y)\n",
    "loss_val1 = nn.BCEWithLogitsLoss()(out, y1)\n",
    "\n",
    "print(loss_val)\n",
    "print(loss_val1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check whether my_cross_entropy function works properly with soft-valued one-hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 1],\n",
      "        [1, 0, 0, 0, 0]])\n",
      "tensor(2.6336, grad_fn=<MeanBackward0>)\n",
      "\n",
      "check the soft valued one-hot vector\n",
      "tensor([[ 0.0000,  0.1000,  0.9000,  0.1000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.1000,  0.9000],\n",
      "        [ 1.1000, -0.1000,  0.0000,  0.0000,  0.0000]])\n",
      "tensor(2.6009, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "target = torch.LongTensor([2,4,0])\n",
    "one_hot = torch.nn.functional.one_hot(target,num_classes=5)\n",
    "print(one_hot)\n",
    "print(my_cross_entropy(input, one_hot))\n",
    "print()\n",
    "\n",
    "print(\"check the soft valued one-hot vector\")\n",
    "one_hot = torch.FloatTensor([[0,0.1, 0.9, 0.1, 0],[0, 0, 0, 0.1, 0.9],[1.1, -0.1, 0,0,0]])\n",
    "print(one_hot)\n",
    "print(my_cross_entropy(input, one_hot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_argument:    \n",
    "    epochs = 200    #\"rounds of training\"\n",
    "    num_users = 15  # \"number of users: K\"\n",
    "    frac = 0.5 #\"the fraction of clients: C\"\n",
    "    local_ep = 1 #\"the number of local epochs: E\"\n",
    "    local_bs = 200 #\"local batch size: B\"\n",
    "    bs=128 #\"test batch size\"\n",
    "    lr=0.01 #\"learning rate\"\n",
    "    momentum=0.5 # \"SGD momentum (default: 0.5)\"\n",
    "    split='user' # \"train-test split type, user or sample\"\n",
    "    opt='ADAM'\n",
    "    loss='Default' # 'Custom' or 'Default'\n",
    "\n",
    "    # model arguments\n",
    "    model = 'cnn'\n",
    "    kernel_num=9 #, help='number of each kind of kernel')\n",
    "    kernel_sizes='3,4,5' #  help='comma-separated kernel size to use for convolution')\n",
    "    norm='None' #, help=\"batch_norm, layer_norm, or None\")\n",
    "    num_filters=32 #, help=\"number of filters for conv nets\")\n",
    "    max_pool='True' #help=\"Whether use max pooling rather than strided convolutions\")\n",
    "\n",
    "    # other arguments\n",
    "    dataset='mnist' #, help=\"name of dataset\")\n",
    "    iid=1\n",
    "    num_classes=10#, help=\"number of classes\")\n",
    "    num_channels=1#, help=\"number of channels of imges\")\n",
    "    gpu=1#, help=\"GPU ID, -1 for CPU\")\n",
    "    stopping_rounds=10#, help='rounds of early stopping')\n",
    "    verbose='False'#, help='verbose print')\n",
    "    seed=1#, help='random seed (default: 1)')\n",
    "    \n",
    "args = my_argument()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "# load dataset and split users\n",
    "if args.dataset == 'mnist':\n",
    "    trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    dataset_train = datasets.MNIST('../data/mnist/', train=True, download=True, transform=trans_mnist)\n",
    "    dataset_test = datasets.MNIST('../data/mnist/', train=False, download=True, transform=trans_mnist)\n",
    "    # sample users\n",
    "    if args.iid:\n",
    "        dict_users = mnist_iid(dataset_train, args.num_users)\n",
    "    else:\n",
    "        dict_users = mnist_noniid(dataset_train, args.num_users)\n",
    "elif args.dataset == 'cifar':\n",
    "    trans_cifar = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    dataset_train = datasets.CIFAR10('../data/cifar', train=True, download=True, transform=trans_cifar)\n",
    "    dataset_test = datasets.CIFAR10('../data/cifar', train=False, download=True, transform=trans_cifar)\n",
    "    if args.iid:\n",
    "        dict_users = cifar_iid(dataset_train, args.num_users)\n",
    "    else:\n",
    "        exit('Error: only consider IID setting in CIFAR10')\n",
    "else:\n",
    "    exit('Error: unrecognized dataset')\n",
    "img_size = dataset_train[0][0].shape\n",
    "\n",
    "# print(dataset_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train CNN with Torch's CrossEntropy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNMnist2(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=1024, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_glob = CNNMnist2(args=args)\n",
    "net_glob.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNMnist2(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=1024, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.329 Test accuracy 9.800\n",
      "\n",
      "Test set: Average loss: 2.1631 \n",
      "Accuracy: 6283/10000 (62.83%)\n",
      "\n",
      "Round   1, Average loss 1.881 Test accuracy 62.830\n",
      "\n",
      "Test set: Average loss: 0.2181 \n",
      "Accuracy: 9428/10000 (94.28%)\n",
      "\n",
      "Round   2, Average loss 0.829 Test accuracy 94.280\n",
      "\n",
      "Test set: Average loss: 0.1346 \n",
      "Accuracy: 9625/10000 (96.25%)\n",
      "\n",
      "Round   3, Average loss 0.428 Test accuracy 96.250\n",
      "\n",
      "Test set: Average loss: 0.1026 \n",
      "Accuracy: 9688/10000 (96.88%)\n",
      "\n",
      "Round   4, Average loss 0.329 Test accuracy 96.880\n",
      "\n",
      "Test set: Average loss: 0.0853 \n",
      "Accuracy: 9754/10000 (97.54%)\n",
      "\n",
      "Round   5, Average loss 0.286 Test accuracy 97.540\n",
      "\n",
      "Test set: Average loss: 0.0773 \n",
      "Accuracy: 9769/10000 (97.69%)\n",
      "\n",
      "Round   6, Average loss 0.258 Test accuracy 97.690\n",
      "\n",
      "Test set: Average loss: 0.0701 \n",
      "Accuracy: 9789/10000 (97.89%)\n",
      "\n",
      "Round   7, Average loss 0.244 Test accuracy 97.890\n",
      "\n",
      "Test set: Average loss: 0.0656 \n",
      "Accuracy: 9797/10000 (97.97%)\n",
      "\n",
      "Round   8, Average loss 0.229 Test accuracy 97.970\n",
      "\n",
      "Test set: Average loss: 0.0624 \n",
      "Accuracy: 9815/10000 (98.15%)\n",
      "\n",
      "Round   9, Average loss 0.221 Test accuracy 98.150\n",
      "\n",
      "Test set: Average loss: 0.0614 \n",
      "Accuracy: 9813/10000 (98.13%)\n",
      "\n",
      "Round  10, Average loss 0.216 Test accuracy 98.130\n",
      "\n",
      "Test set: Average loss: 0.0566 \n",
      "Accuracy: 9827/10000 (98.27%)\n",
      "\n",
      "Round  11, Average loss 0.208 Test accuracy 98.270\n",
      "\n",
      "Test set: Average loss: 0.0576 \n",
      "Accuracy: 9825/10000 (98.25%)\n",
      "\n",
      "Round  12, Average loss 0.200 Test accuracy 98.250\n",
      "\n",
      "Test set: Average loss: 0.0556 \n",
      "Accuracy: 9826/10000 (98.26%)\n",
      "\n",
      "Round  13, Average loss 0.200 Test accuracy 98.260\n",
      "\n",
      "Test set: Average loss: 0.0526 \n",
      "Accuracy: 9840/10000 (98.40%)\n",
      "\n",
      "Round  14, Average loss 0.197 Test accuracy 98.400\n",
      "\n",
      "Test set: Average loss: 0.0510 \n",
      "Accuracy: 9833/10000 (98.33%)\n",
      "\n",
      "Round  15, Average loss 0.190 Test accuracy 98.330\n",
      "\n",
      "Test set: Average loss: 0.0535 \n",
      "Accuracy: 9833/10000 (98.33%)\n",
      "\n",
      "Round  16, Average loss 0.185 Test accuracy 98.330\n",
      "\n",
      "Test set: Average loss: 0.0520 \n",
      "Accuracy: 9844/10000 (98.44%)\n",
      "\n",
      "Round  17, Average loss 0.187 Test accuracy 98.440\n",
      "\n",
      "Test set: Average loss: 0.0509 \n",
      "Accuracy: 9844/10000 (98.44%)\n",
      "\n",
      "Round  18, Average loss 0.184 Test accuracy 98.440\n",
      "\n",
      "Test set: Average loss: 0.0503 \n",
      "Accuracy: 9844/10000 (98.44%)\n",
      "\n",
      "Round  19, Average loss 0.181 Test accuracy 98.440\n",
      "\n",
      "Test set: Average loss: 0.0501 \n",
      "Accuracy: 9850/10000 (98.50%)\n",
      "\n",
      "Round  20, Average loss 0.187 Test accuracy 98.500\n",
      "\n",
      "Test set: Average loss: 0.0507 \n",
      "Accuracy: 9845/10000 (98.45%)\n",
      "\n",
      "Round  21, Average loss 0.185 Test accuracy 98.450\n",
      "\n",
      "Test set: Average loss: 0.0499 \n",
      "Accuracy: 9852/10000 (98.52%)\n",
      "\n",
      "Round  22, Average loss 0.180 Test accuracy 98.520\n",
      "\n",
      "Test set: Average loss: 0.0495 \n",
      "Accuracy: 9854/10000 (98.54%)\n",
      "\n",
      "Round  23, Average loss 0.181 Test accuracy 98.540\n",
      "\n",
      "Test set: Average loss: 0.0496 \n",
      "Accuracy: 9851/10000 (98.51%)\n",
      "\n",
      "Round  24, Average loss 0.180 Test accuracy 98.510\n",
      "\n",
      "Test set: Average loss: 0.0480 \n",
      "Accuracy: 9853/10000 (98.53%)\n",
      "\n",
      "Round  25, Average loss 0.177 Test accuracy 98.530\n",
      "\n",
      "Test set: Average loss: 0.0474 \n",
      "Accuracy: 9855/10000 (98.55%)\n",
      "\n",
      "Round  26, Average loss 0.177 Test accuracy 98.550\n",
      "\n",
      "Test set: Average loss: 0.0486 \n",
      "Accuracy: 9858/10000 (98.58%)\n",
      "\n",
      "Round  27, Average loss 0.179 Test accuracy 98.580\n",
      "\n",
      "Test set: Average loss: 0.0474 \n",
      "Accuracy: 9864/10000 (98.64%)\n",
      "\n",
      "Round  28, Average loss 0.174 Test accuracy 98.640\n",
      "\n",
      "Test set: Average loss: 0.0476 \n",
      "Accuracy: 9861/10000 (98.61%)\n",
      "\n",
      "Round  29, Average loss 0.176 Test accuracy 98.610\n"
     ]
    }
   ],
   "source": [
    "print(net_glob)\n",
    "net_glob.train()\n",
    "\n",
    "# copy weights\n",
    "w_glob = net_glob.state_dict()\n",
    "\n",
    "# training\n",
    "loss_train = []\n",
    "loss_test_arr_uncoded = []\n",
    "acc_test_arr_uncoded = []\n",
    "\n",
    "net_best = None\n",
    "best_loss = None\n",
    "val_acc_list, net_list = [], []\n",
    "\n",
    "for iter in range(30): #args.epochs\n",
    "    w_locals, loss_locals = [], []\n",
    "    m = 15\n",
    "    idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "    for idx in idxs_users:\n",
    "#         print(idx)\n",
    "        local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])\n",
    "        w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "        w_locals.append(copy.deepcopy(w))\n",
    "        loss_locals.append(copy.deepcopy(loss))\n",
    "    # update global weights\n",
    "    w_glob = FedAvg(w_locals)\n",
    "\n",
    "    # copy weight to net_glob\n",
    "    net_glob.load_state_dict(w_glob)\n",
    "\n",
    "    # print loss\n",
    "    loss_avg = sum(loss_locals) / len(loss_locals)\n",
    "    \n",
    "    loss_train.append(loss_avg)\n",
    "    \n",
    "    acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "    acc_test_arr_uncoded.append(acc_test)\n",
    "    loss_test_arr_uncoded.append(loss_test)\n",
    "    if iter % 1 ==0:\n",
    "        print('Round {:3d}, Average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_avg,acc_test))\n",
    "    #print(loss_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train CNN with Customized Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.loss='Custom' # 'Custom' or 'Default'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.309 Test accuracy 9.800\n",
      "\n",
      "Test set: Average loss: 1.9062 \n",
      "Accuracy: 8132/10000 (81.32%)\n",
      "\n",
      "Round   1, Average loss 1.646 Test accuracy 81.320\n",
      "\n",
      "Test set: Average loss: 0.1939 \n",
      "Accuracy: 9448/10000 (94.48%)\n",
      "\n",
      "Round   2, Average loss 0.732 Test accuracy 94.480\n",
      "\n",
      "Test set: Average loss: 0.1276 \n",
      "Accuracy: 9642/10000 (96.42%)\n",
      "\n",
      "Round   3, Average loss 0.396 Test accuracy 96.420\n",
      "\n",
      "Test set: Average loss: 0.1027 \n",
      "Accuracy: 9705/10000 (97.05%)\n",
      "\n",
      "Round   4, Average loss 0.320 Test accuracy 97.050\n",
      "\n",
      "Test set: Average loss: 0.0857 \n",
      "Accuracy: 9746/10000 (97.46%)\n",
      "\n",
      "Round   5, Average loss 0.277 Test accuracy 97.460\n",
      "\n",
      "Test set: Average loss: 0.0766 \n",
      "Accuracy: 9767/10000 (97.67%)\n",
      "\n",
      "Round   6, Average loss 0.249 Test accuracy 97.670\n",
      "\n",
      "Test set: Average loss: 0.0694 \n",
      "Accuracy: 9793/10000 (97.93%)\n",
      "\n",
      "Round   7, Average loss 0.235 Test accuracy 97.930\n",
      "\n",
      "Test set: Average loss: 0.0643 \n",
      "Accuracy: 9805/10000 (98.05%)\n",
      "\n",
      "Round   8, Average loss 0.222 Test accuracy 98.050\n",
      "\n",
      "Test set: Average loss: 0.0613 \n",
      "Accuracy: 9830/10000 (98.30%)\n",
      "\n",
      "Round   9, Average loss 0.213 Test accuracy 98.300\n"
     ]
    }
   ],
   "source": [
    "net_glob = CNNMnist2(args=args)\n",
    "net_glob.cuda()\n",
    "net_glob.train()\n",
    "\n",
    "# copy weights\n",
    "w_glob = net_glob.state_dict()\n",
    "\n",
    "# training\n",
    "loss_train = []\n",
    "loss_test_arr = []\n",
    "acc_test_arr = []\n",
    "cv_loss, cv_acc = [], []\n",
    "val_loss_pre, counter = 0, 0\n",
    "net_best = None\n",
    "best_loss = None\n",
    "val_acc_list, net_list = [], []\n",
    "\n",
    "for iter in range(10): #args.epochs\n",
    "    w_locals, loss_locals = [], []\n",
    "    m = 15\n",
    "    idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "    for idx in range(args.num_users):\n",
    "#         print(idx)\n",
    "        local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])\n",
    "        w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "        w_locals.append(copy.deepcopy(w))\n",
    "        loss_locals.append(copy.deepcopy(loss))\n",
    "    # update global weights\n",
    "    w_glob = FedAvg(w_locals)\n",
    "\n",
    "    # copy weight to net_glob\n",
    "    net_glob.load_state_dict(w_glob)\n",
    "\n",
    "    # print loss\n",
    "    loss_avg = sum(loss_locals) / len(loss_locals)\n",
    "    \n",
    "    loss_train.append(loss_avg)\n",
    "    \n",
    "    acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "    acc_test_arr.append(acc_test)\n",
    "    loss_test_arr.append(loss_test)\n",
    "    if iter % 1 ==0:\n",
    "        print('Round {:3d}, Average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_avg,acc_test))\n",
    "    #print(loss_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train CNN by utilizing BACC (N=15, K=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. BACC encoding for MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_argument:    \n",
    "    epochs = 200    #\"rounds of training\"\n",
    "    num_users = 15  # \"number of users: N\"\n",
    "    num_partition = 6 # \"number of users: K\"\n",
    "    frac = 0.5 #\"the fraction of clients: C\"\n",
    "    local_ep = 1 #\"the number of local epochs: E\"\n",
    "    local_bs = 200 #\"local batch size: B\"\n",
    "    bs=200 #\"test batch size\"\n",
    "    lr=0.01 #\"learning rate\"\n",
    "    momentum=0.5 # \"SGD momentum (default: 0.5)\"\n",
    "    split='user' # \"train-test split type, user or sample\"\n",
    "    opt='ADAM'\n",
    "    loss='Custom' # 'Custom' or 'Default'\n",
    "\n",
    "    # model arguments\n",
    "    model = 'cnn'\n",
    "    kernel_num=9 #, help='number of each kind of kernel')\n",
    "    kernel_sizes='3,4,5' #  help='comma-separated kernel size to use for convolution')\n",
    "    norm='None' #, help=\"batch_norm, layer_norm, or None\")\n",
    "    num_filters=32 #, help=\"number of filters for conv nets\")\n",
    "    max_pool='True' #help=\"Whether use max pooling rather than strided convolutions\")\n",
    "\n",
    "    # other arguments\n",
    "    dataset='mnist' #, help=\"name of dataset\")\n",
    "    iid=1\n",
    "    num_classes=10#, help=\"number of classes\")\n",
    "    num_channels=1#, help=\"number of channels of imges\")\n",
    "    gpu=1#, help=\"GPU ID, -1 for CPU\")\n",
    "    stopping_rounds=10#, help='rounds of early stopping')\n",
    "    verbose='False'#, help='verbose print')\n",
    "    seed=1#, help='random seed (default: 1)')\n",
    "    \n",
    "args = my_argument()\n",
    "\n",
    "args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "# load dataset and split users\n",
    "trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "dataset_train = datasets.MNIST('../data/mnist/', train=True, download=True, transform=trans_mnist)\n",
    "dataset_test = datasets.MNIST('../data/mnist/', train=False, download=True, transform=trans_mnist)\n",
    "\n",
    "dict_users = mnist_iid(dataset_train, args.num_partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of X: (60000, 784)\n",
      "size of Y: (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "encoding_input_array_np = np.empty((len(dataset_train),28*28))\n",
    "encoding_label_array_np = np.empty((len(dataset_train),args.num_classes))\n",
    "print(\"size of X:\" ,encoding_input_array_np.shape)\n",
    "print(\"size of Y:\" ,encoding_label_array_np.shape)\n",
    "\n",
    "Size_submatrices = int(60000/args.num_partition)\n",
    "\n",
    "for i in range(args.num_partition):\n",
    "    \n",
    "    stt_pos = i*Size_submatrices\n",
    "    end_pos = (i+1)*Size_submatrices\n",
    "#     print(i,stt_pos,end_pos)\n",
    "    Temp_train = DataLoader(DatasetSplit(dataset_train, dict_users[i]), batch_size=Size_submatrices, shuffle=True)\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(Temp_train):\n",
    "        \n",
    "        images_np = images.detach().cpu().numpy()\n",
    "        encoding_input_array_np[stt_pos:end_pos,:] = np.reshape(images_np, (Size_submatrices,28*28))\n",
    "#         print(encoding_input_array_np[stt_pos:end_pos,:].shape)\n",
    "\n",
    "        onehot_labels = torch.nn.functional.one_hot(labels,num_classes=args.num_classes)\n",
    "        labels_np = onehot_labels.detach().cpu().numpy()\n",
    "#         print(labels_np.shape)\n",
    "        encoding_label_array_np[stt_pos:end_pos,:] = labels_np\n",
    "\n",
    "\n",
    "# print(labels_np[0:10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha_array:  [ 0.96592583  0.70710678  0.25881905 -0.25881905 -0.70710678 -0.96592583] \n",
      "\n",
      "z_array:  [ 1.          0.9781476   0.91354546  0.80901699  0.66913061  0.5\n",
      "  0.30901699  0.10452846 -0.10452846 -0.30901699 -0.5        -0.66913061\n",
      " -0.80901699 -0.91354546 -0.9781476 ] \n",
      "\n",
      "@BACC_Enc: N,K, m_i= 15 6 10000 \n",
      "\n",
      "@BACC_Enc: N,K, m_i= 15 6 10000 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "N= args.num_users\n",
    "K= args.num_partition\n",
    "\n",
    "\n",
    "j_array = np.array(range(K))\n",
    "# print(\"j: \",(2*j_array+1)*math.pi/2/K,'\\n')\n",
    "\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*K)) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "i_array = np.array(range(N))\n",
    "z_array = np.cos(i_array*2*math.pi/N/2) # np.cos(i_array*2*math.pi/N/2)\n",
    "print(\"z_array: \",z_array,'\\n')\n",
    "\n",
    "X_tilde = BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "y_tilde = BACC_Enc(encoding_label_array_np, alpha_array, z_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 10000, 784)\n",
      "(15, 10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X_tilde.shape)\n",
    "print(y_tilde.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Train LeNet with BACC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of results: 15\n",
      "(m= 15 )  0 -th Trial!!\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "\n",
      "Test set: Average loss: 2.2771 \n",
      "Accuracy: 6257/10000 (62.57%)\n",
      "\n",
      "Round   1, Average loss 2.277 Test accuracy 62.570\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "\n",
      "Test set: Average loss: 1.2245 \n",
      "Accuracy: 9094/10000 (90.94%)\n",
      "\n",
      "Round   2, Average loss 1.225 Test accuracy 90.940\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "\n",
      "Test set: Average loss: 0.2009 \n",
      "Accuracy: 9503/10000 (95.03%)\n",
      "\n",
      "Round   3, Average loss 0.201 Test accuracy 95.030\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "\n",
      "Test set: Average loss: 0.1814 \n",
      "Accuracy: 9520/10000 (95.20%)\n",
      "\n",
      "Round   4, Average loss 0.181 Test accuracy 95.200\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "\n",
      "Test set: Average loss: 0.1419 \n",
      "Accuracy: 9574/10000 (95.74%)\n",
      "\n",
      "Round   5, Average loss 0.142 Test accuracy 95.740\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "\n",
      "Test set: Average loss: 0.1539 \n",
      "Accuracy: 9539/10000 (95.39%)\n",
      "\n",
      "Round   6, Average loss 0.154 Test accuracy 95.390\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "\n",
      "Test set: Average loss: 0.1473 \n",
      "Accuracy: 9572/10000 (95.72%)\n",
      "\n",
      "Round   7, Average loss 0.147 Test accuracy 95.720\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "\n",
      "Test set: Average loss: 0.1424 \n",
      "Accuracy: 9595/10000 (95.95%)\n",
      "\n",
      "Round   8, Average loss 0.142 Test accuracy 95.950\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "\n",
      "Test set: Average loss: 0.1466 \n",
      "Accuracy: 9593/10000 (95.93%)\n",
      "\n",
      "Round   9, Average loss 0.147 Test accuracy 95.930\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "\n",
      "Test set: Average loss: 0.1658 \n",
      "Accuracy: 9543/10000 (95.43%)\n",
      "\n",
      "Round  10, Average loss 0.166 Test accuracy 95.430\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "\n",
      "Test set: Average loss: 0.1632 \n",
      "Accuracy: 9500/10000 (95.00%)\n",
      "\n",
      "Round  11, Average loss 0.163 Test accuracy 95.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "\n",
      "Test set: Average loss: 0.1397 \n",
      "Accuracy: 9570/10000 (95.70%)\n",
      "\n",
      "Round  12, Average loss 0.140 Test accuracy 95.700\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "\n",
      "Test set: Average loss: 0.1464 \n",
      "Accuracy: 9572/10000 (95.72%)\n",
      "\n",
      "Round  13, Average loss 0.146 Test accuracy 95.720\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "\n",
      "Test set: Average loss: 0.1575 \n",
      "Accuracy: 9568/10000 (95.68%)\n",
      "\n",
      "Round  14, Average loss 0.158 Test accuracy 95.680\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "\n",
      "Test set: Average loss: 0.1483 \n",
      "Accuracy: 9565/10000 (95.65%)\n",
      "\n",
      "Round  15, Average loss 0.148 Test accuracy 95.650\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "\n",
      "Test set: Average loss: 0.1839 \n",
      "Accuracy: 9583/10000 (95.83%)\n",
      "\n",
      "Round  16, Average loss 0.184 Test accuracy 95.830\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "\n",
      "Test set: Average loss: 0.1772 \n",
      "Accuracy: 9585/10000 (95.85%)\n",
      "\n",
      "Round  17, Average loss 0.177 Test accuracy 95.850\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "\n",
      "Test set: Average loss: 0.1568 \n",
      "Accuracy: 9592/10000 (95.92%)\n",
      "\n",
      "Round  18, Average loss 0.157 Test accuracy 95.920\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "\n",
      "Test set: Average loss: 0.1726 \n",
      "Accuracy: 9571/10000 (95.71%)\n",
      "\n",
      "Round  19, Average loss 0.173 Test accuracy 95.710\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "\n",
      "Test set: Average loss: 0.2037 \n",
      "Accuracy: 9576/10000 (95.76%)\n",
      "\n",
      "Round  20, Average loss 0.204 Test accuracy 95.760\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "\n",
      "Test set: Average loss: 0.1514 \n",
      "Accuracy: 9594/10000 (95.94%)\n",
      "\n",
      "Round  21, Average loss 0.151 Test accuracy 95.940\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "\n",
      "Test set: Average loss: 0.1667 \n",
      "Accuracy: 9549/10000 (95.49%)\n",
      "\n",
      "Round  22, Average loss 0.167 Test accuracy 95.490\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "\n",
      "Test set: Average loss: 0.1869 \n",
      "Accuracy: 9570/10000 (95.70%)\n",
      "\n",
      "Round  23, Average loss 0.187 Test accuracy 95.700\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "\n",
      "Test set: Average loss: 0.1793 \n",
      "Accuracy: 9536/10000 (95.36%)\n",
      "\n",
      "Round  24, Average loss 0.179 Test accuracy 95.360\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "\n",
      "Test set: Average loss: 0.1772 \n",
      "Accuracy: 9559/10000 (95.59%)\n",
      "\n",
      "Round  25, Average loss 0.177 Test accuracy 95.590\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "\n",
      "Test set: Average loss: 0.1842 \n",
      "Accuracy: 9564/10000 (95.64%)\n",
      "\n",
      "Round  26, Average loss 0.184 Test accuracy 95.640\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "\n",
      "Test set: Average loss: 0.1868 \n",
      "Accuracy: 9557/10000 (95.57%)\n",
      "\n",
      "Round  27, Average loss 0.187 Test accuracy 95.570\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "\n",
      "Test set: Average loss: 0.1660 \n",
      "Accuracy: 9586/10000 (95.86%)\n",
      "\n",
      "Round  28, Average loss 0.166 Test accuracy 95.860\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "\n",
      "Test set: Average loss: 0.1782 \n",
      "Accuracy: 9591/10000 (95.91%)\n",
      "\n",
      "Round  29, Average loss 0.178 Test accuracy 95.910\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "# training\n",
    "loss_train_arr = []\n",
    "loss_test_arr = []\n",
    "acc_test_arr = []\n",
    "net_best = None\n",
    "best_loss = None\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 30\n",
    "\n",
    "# m_array = np.array(range(4,16)) # m is the number of received result @ master\n",
    "m_array = np.array([15]) # m is the number of received result @ master\n",
    "loss_test_arr = np.empty((len(m_array),N_trials,N_epochs))\n",
    "acc_test_arr  = np.empty((len(m_array),N_trials,N_epochs))\n",
    "\n",
    "for m_idx in range(len(m_array)):   \n",
    "    \n",
    "    m = m_array[m_idx] # m is the number of received result @ master\n",
    "    print('number of results:',m)\n",
    "    \n",
    "    for trial_idx in range(N_trials):\n",
    "        print('(m=',m,') ',trial_idx,'-th Trial!!')\n",
    "        \n",
    "        net_glob = CNNMnist2(args=args)\n",
    "        net_glob.cuda()\n",
    "        net_glob.train()\n",
    "\n",
    "        # copy weights\n",
    "        w_glob = net_glob.state_dict()\n",
    "\n",
    "        for iter in range(N_epochs): #args.epochs\n",
    "            w_locals, loss_locals = [], []\n",
    "            idxs_users = np.random.choice(range(N), m, replace=False)\n",
    "            idxs_users = np.sort(idxs_users)\n",
    "            print('selected users:',idxs_users)\n",
    "\n",
    "            dec_z_array = []\n",
    "            for idx in idxs_users: #for idx in range(N):\n",
    "        #         print(idx)\n",
    "                local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[idx,:,:], label=y_tilde[idx,:,:])\n",
    "                w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                w_locals.append(copy.deepcopy(w))\n",
    "                loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "            # update global weights\n",
    "            #w_glob = FedAvg(w_locals)\n",
    "            w_glob = FedAvg_with_BACC_Dec(w_locals, alpha_array, dec_z_array)\n",
    "\n",
    "            # copy weight to net_glob\n",
    "            net_glob.load_state_dict(w_glob)\n",
    "\n",
    "            # print loss\n",
    "        #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "        #     loss_train_arr.append(loss_train)\n",
    "\n",
    "            acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "            acc_test_arr[m_idx][trial_idx][iter] = acc_test\n",
    "            loss_test_arr[m_idx][trial_idx][iter] = loss_test\n",
    "            if iter % 1 ==0:\n",
    "                print('Round {:3d}, Average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_test, acc_test))\n",
    "            #print(loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "filehandler = open(\"./plot/MNIST_LeNet_N15_K6_m15_test_acc\",\"wb\")\n",
    "pickle.dump(acc_test_arr,filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 30)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "m_array = np.array([4,6,8,10,15,31]) # m is the number of received result @ master\n",
    "\n",
    "plot_acc = np.mean(acc_test_arr, axis=1)\n",
    "\n",
    "raw = pickle.load(open('./plot/MNIST_LeNet_N30_K6_m30_test_acc','rb'))\n",
    "plot_acc_N30 = np.mean(raw, axis=1)\n",
    "print(raw.shape)\n",
    "\n",
    "\n",
    "plt.plot(plot_acc[0,:],'b',label='N=15, K=6, s=0')\n",
    "plt.plot(plot_acc_N30[0,:],'g',label='N=30, K=6, s=0')\n",
    "plt.plot(acc_test_arr_uncoded,'r',label='uncoded')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Test accuracy(%)')\n",
    "plt.show()\n",
    "\n",
    "import pickle\n",
    "\n",
    "filehandler = open(\"./plot/MNIST_LeNet_N15_K6_m15_test_acc\",\"wb\")\n",
    "pickle.dump(acc_test_arr,filehandler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N=15, K=6 (without averaging, i.e., N_trials = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1, 30)\n",
      "(5, 30)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEGCAYAAABhMDI9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOydd3hUVf7/X2daZiYzSSY9JEBoAqFDAEVAQBFUFNZVcC1gWXT3a1fQta2ra8W6ru6qWNAVe++gaMBKL1IktABJCCSTnkzLzPn9cTPpZWYyqb95P8997p07t5xz5855n08XUkpCCCGEEEIIwQtVZzcghBBCCCGEroUQMYQQQgghhFAPIWIIIYQQQgihHkLEEEIIIYQQQj2EiCGEEEIIIYR60HR2A9qC2NhYmZqaGtC5FRUVhIeHB7dBnYye1qee1h/oeX3qaf2BntenpvqzefPmAillXHPndGtiSE1NZdOmTQGdm5GRwbRp04LboE5GT+tTT+sP9Lw+9bT+QM/rU1P9EUIcbumckCophBBCCCGEeggRQwghhBBCCPXQbsQghHhFCHFCCLGzzr5oIcQ3Qoh91WtL9X4hhHhGCLFfCLFDCDG2vdoVQgghhBBCy2hPiWEFMLvBvr8Ba6SUg4A11Z8BzgIGVS9XA/9tx3aFEEIIIYTQAtqNGKSU64DCBrvnAq9Vb78GzKuz/3Wp4FcgSgiR1F5tCyGEEEIIoXmI9kyiJ4RIBT6XUg6v/lwspYyq832RlNIihPgceERK+WP1/jXA7VLKRi5HQoirUaQKEhISxr399tsBta28vByTyRTQuV0VPa1PPa0/0PP61NP6Az2vT031Z/r06ZullOnNndNV3FVFE/uaZCwp5YvAiwDp6ekyULeynuaSBj2vTz2tP9Dz+tTT+gM9r0+B9KejieG4ECJJSnmsWlV0onp/NtC7znEpQG4Hty2EELoP3G5wOpUl2FK/lODxQFWVch/v0sRnU2YmWCygUoFaXX9puK/udRoudfe73bVtkLLpxeNR2iqEsqhUteumtoVofH4z17Zs21b7XFtbvNfxeFrebu45NtyWsvW+eLdPPx1Gjgzub1+NjiaGT4FFwCPV60/q7L9OCPE2MBEokVIe6+C2hRAoPB5wuaCysnax2ep/rrvP46n9s9b9UzdYEvfsgT17wG5XFoej/rrutpQ+XbNmgICm13W3nc6W7+nddrlAo2m8qNWNPqdXVoIvagqPp3bgb2rxDoydjGZ1Ed0Yozq7Ab7iv//tfsQghHgLmAbECiGygXtRCOFdIcRVwBHgwurDvwTOBvYDlcAV7dWuHgu7HW1RERw5UjtoNbXYbMpSUaEM0hUVTS/e75xOZSbjctVf191up0FqSFM7dTrQ65UlLKx23XBG2NKs0EsQ0PTau+29V1iYMpjHxta/p7cdGk3TM+Em9tny8zHFxrbeeSGUe+h0LS9arUKCwUbD2b+X6Bp8/m3nTkYMG1Z/Rux2N54lezxKO30h0IYz/YZLQymg7uy8pdm7L5MGIdi6bRtjxo5t8Zh612ppZu89rm4/m3qm3n43Jdk01y+9Pvi/ezXajRiklH9q5qvTmzhWAte2V1t6DEpL4cAB2L+/du1dcnI4NZBrhoWB0Qjh4fUXiwWSk5XvtVrlxfWu627XXRuNTS8GQ/1tlaplsbx6+eXXXzll+vTaQVina59BsAOxq4fpr61RUdCD+gNQ4nbDqQH9m4KDupMTtbpTmtBVjM8hgDIgHj8OBw8qA//Bg8riHfxPnKh/fEICDBwIZ5wBAwaQWVDASSNHKoOowVA7o/UudfeFhysDtabrvgKOrCyljyGEEEKHouuOCj0ZR47Ajh21A3/dxWarPU4IZdY+aBCcd55CAt6lf38wm+tdNjcjg5N62OwthBBC6HiEiKGj4HbDl1/Cc8/BqlW1+8PDlUF+4ECYNUvZ9i59+wZdjyilYi+tq8Hx7m9qXVdt6nU0qas67U7w2nPr9seXPkhZa1LxLnU/S9lYOAu2BkBK5V51TUXeba9t32saaupzRYWikRs3DiZMUF63jtLKSam0Myys62gCq6qgsBAKCsBqrV0XFsKRI8kcOgSRkcoSEVF/W6/vfu++vwgRQ3sjPx9efhmefx4OH4ZeveC++2DmTBgwAOLigv6WORyQlQWHDilCiHft3S4pCc59hKglC28XfCEbvV4Rdkymltd5eX34+efW2yGlMviVlkJZWdPr0lIoL2/as7Ou/bAu+dW1r/sLrbYxWej1UFGRTnh4616QbndjEgjUK9VgUOYfFRW1AmlUFIwfrywTJihLko+5Biorldc6Px82bIgmJ6fxAFtQUH/b4VCeaUyMYsNvaQkPr99vr79Ew892e33bdt2l4T6brX77iotb6uGgFvuv1SokERUF0dHKEhPT/DoqSnmHmvP3qLvPZlOeldchzrtuat/f/w4LFvj2m/mLEDG0B6SEDRsU6eDdd5Vfcfp0XEuXkmsTvPS6nuzXCqjSOXDpzDjV4bjURlwqA04RhkuG4ZRaHE4VVVWN7b1N2X7Vati7dzSFhZCTU38Q0euhXz9lmTxZGQC8M9rWHHS8NmFf/nxNOfY03OedPZaVKQO1d52fr5BW3f0eT3+fH7larczmzObatcUCffoonyMMNsx532CwZyGH/xF3eHKj/jTsk1qtPN+6z7qpBep7sDZ0Aqv7OT/fTlycqVUHGbW6aTNRU4vXru81G9Xd1utrZ+lVVbB7N2zcqLyeGzbAo48qfQZISVGIYtw4pQ35+cpA6iUB73ZdbSfUuksKoQyGsbHKgJiaqlwrNlb5LSorawmjoAD27YNfflG2/SFflar22XideeoSelNEHxamtKN//9r21V17ty0W+O67Hxk5cjIlJcokqrSUJreLi2uljsxMhXDaMukyGJTF64zW1DoysvazxRL4vVpDiBiCCZsN3n5bIYTNm8Fsxn3hhZT06UvR9u18/tBeHj3xN45V9SJCV4lWONHhQCvt6HCgE2XohLNmMekkGr0WaTQh9WY8nnDc0ohbGnA4VPU8R6uqlJfl9NOVl79fv1qNVEJC1xHhfYWU8O23aznttNN8Ol6rbUbwkhJ2fYT8cik/97GxJzyS4RXLGHnGQxjHLgxuo31ARsbOTvNK0mgUt/eRI+Gqq5R9lZWwbVstUWzcCB99pHzn9dCNi4PERBg+XNn27ouLg8OHtzBr1tiamXEgKjQplQHXSxgVFbW+El4CqLut1QLuKsq++QZtnz4Yhg0L2jMCiIioIsDCkFRVQVGRQhhe1VRRkdJmr8NfU06ARmPX+o+GiCFY+Ow1uPgaKHfgGTSIivkLKCgtwb5xE/t+KuHRykf4+fgwRqS5ePu/MHWqETACIN1u3EVFVOXnU5VfVr3Op+pEPq7cXByHDuI6mg2VHiXKQ6VCm5JCWP/+6Ab0J2zAQMIG9GfD4cNMnTMH0cIbdrTsKHkVeYxPHN96n9wuUGuD83z8hBCg1Up0ujZcpPwErs9vZvXR73g1LoG9KuV5Y7Kg2bGMtN9fYuzAcxmXNJ4x8WOIDIv0+xZSSqTNhspobENDOw9GI0yapCxelJbWhnC0hoyMUk46qW1tEKJWhz9gQOvHVxUUkHPLrVRu2ACAYfRoLJdcQsSsMxFtemHaDo2mljS7M0LEECR4XnoOYXOSO2IgpQ4V7NiBa8QUnuv7FC+vGUxEhOC55+Dqq7WNPESFWo0mNhZNbCwMHUqhvZCd+TvYceAQZQW5LJjxDIPDU3FmZeE8cADHgYM4Dh7Auf8AFT/9hHS5AIgH9t51N5rERLQJCco6MQFNQiLOGDOflP7IW0XfUGqEXy/dgE5d/Sdy2aAgE07sUZb83+HEbig+Cuc+DeMu79Bn6S+chw/jyskBlRqhVnQzjgPfkvHby3xh0HBcE0OyPZErB81j4vDZ7HFlsfmXp9hStJuVe15nxZ7XARgYNZBxCeMYlzCOsfFjSQhv2VXW43CQc/MtVPz8MzFXXUXMn69CZTAEpU9SSkQnWTgjInw7rj0TcDaHys2bybnpZtxlZST+836kzUbRyjfJXbqU448+imX+hUQtWIC2nd2cbdu2UbpqNZ7KSqTdhqfShsduR9pseGzKtsdWibTZkQ4Huv79MYwZg3HMaAxjx6JNTGzX9rUVIWIIEpybDoBGR1Wkm9gZp/Fh5JPc/XAEVitccw088ICiw2wIl8dFZlEmO/J3sD1/Ozvyd3C07CgAGilRS8nbn13A6X1OZ/HIxQwbfHa982VVFa7sbBwHD7J77Tr6RZhxHcujKi8P2/btlK7KUyyowNTqxa6Fox/NJDJFQ3hkAXrNEYSojl5WaSH2JEgZD2V5kJ/Zjk+t7bBt307WJZc2qaQegqY6etoNHAb+RX7Y84x89FEmz/8ENr+G44tb+S0mhS1j57Ol/AifHfiMd/a+A8DcAXO5++S70WsaT53d5RVkX3stlRs2YJwwgYLnnqP4ww+JX3IrEWef3WhQz6/M55mtz9Db3ptpTGuyL1JKyjMysC5/CVduLr3/+x/0Q4e26fm0B3LKc/j8wOd8fvBzqmxVzfYHwF1cTOHKlZR8+BGGUaOIu+lGdH36+H1PKSWFr73GicceR5eSQu+XXkI/WBFVLJdeSsVPP1O0ciUF/32eghdexDxzJtGXXoJh3LigEqyUkqI3VnL80UcRKhUqsxmVwYDKoEfoDagMBtSxMWirt4VBj1BrcGRmUvzeexT9738AaJKSFJIYPQbDmDHohwxGaDtHOm8KIWIIAqqKilAVlOFOMJE7+yquf+5sNh+LYPJkeOYZGDMGPNJDfqWVYxXHyC3PZXfhbraf2M5u627sbjsAcYY4RsWN4sI+ZzLypxdIq5I4zAmsdOWx8th61hxZw+TkyVw98mrGxI8BQGg06FJT0aWmYlOpiK/WX0spWXV4Ff/a+BSl+TnMMIzi0pjZHM94iS3H8pl9MJf83zXkAyp9bwxp/QmfeDLGabPRDx+BUKvhiaHgCJILUzvAU1FBztLb0MTHobnnFlZtf4mfi/dRBYzVJXDm0IvpFzkAPG6k2wMeN4WvvU7OTTfhvPlmYq5eTFjcYNLfuZT07/8Df1xO1enPsrdoL6sOrWLFrhVkFmXy1PSnSDYl19zXXVzMkWuuwb5zF72WPUrkuedSuWkTeQ89RO6tSyha+SYJd9yBYcRwADYc28Bt627DareiF3rOKD6D/lG1hnVZVUXpV19jXb4cR2Ymml5JICWHL72MlOeeJfzkk9v2oBxl8OvzcPB75bPwupGJ2m2hUhaEoj487XZIqjUslzhK+ObwN3x24DO2nNgCQLQ+mkJXIS6PC62q/qBWVVBA4YoVFL35Fp7KSowTJlD2/feUrl6NZf6FxP71r2h81Le4y8s5dtfdlK1ahXnmGSQ99BDqOjE8QqXCNGUypimTcR49StFbb1P8wQeUff01YYMHY7nkYiLnzGmzus9jt5N3772UfPIppunT6bXs0XrtaA3S5cK+NxPb1q3Ytm6lcutWSr/8SumDXo9hxAgMo0ehT0tDP3Qo2j59WlQLtyfatR5DeyM9PV1u2tSoZINPCGZq3YJ/P0vMDdfzbq/LuCj3dSxRJ5gz9xGSL3GR5y7nWMUxjlcep8pTO6vVqDSkRacxMm4ko+JGMSpuFInhiQhHKbwyW1HjXPk1SA+8eBpl6VfyTp+hvL7rdYocRYxPHM/VI69mYuLEmhmRt0/bTmzjsU2PsSN/B4Msg1gybgmTkifBvm9Y++GlXJcYz1uzX2OIug+VGzdSsX49lRs24jx4EACVyYQxPR2j6xd0fXujnnMf6qgoZYmIQHRQtHRrv1HuXXdR8tHHfHXjWF7Tb0MtJefKcBbNWEb/vk0brT0OB8fuuJPSL78k8g9/IOm+fyBsJ+DtS+DYdphxN0y5FYRgXfY6/rbub6hUKpZNXcakXpOoys/nyFV/xpmVRfLTT2GeMaPm2tLtpuSjjzjx1NO4CwuJ+MM8Vs+O4+msFfSN6Mtt42/jtu9uIyo8ijfPfpMIYaDko4+wvvwKrqNH0Q0cQOzixUScfTZVVitHFy/GmXWYXsseJeKss/x/gM5K2PgS/PgU2AoheRxoDMo7JT2ArN6W9T/n74W0ubjmPscPOT/w+cHPyTiagcvjol9kP87tfy7n9D+HH7J/4IH1D/Ddhd8RZ1QGeVdODtaXX6H4gw+QLhcRs2cTc9UV6Lfdh+v4cQo2OijeXopQq4ie0puYM9JQR0SCzgg6E2iNENUH+k0FlRp7ZiY5N9yI8+hR4m+9legrLvdJAvDYbJR+8QWFK9/EsWcPaouF2OuuxTJ/frMzc1uVjSc2PcGA0gH86cz6GX2c2TlkX389jt9/J/b8U4kdLREeB4y+BAbNBFVggSuuY8ewbdtG5datVG7ZiuP332ukX1V4OGFDh6AfmqaQxeBBhEU4ESd+g9xtMHIB9JnY6j2a+h8JIVqsxxAihjZCOp0cnnQqqZs3cQlvsPpsO3HnvYAurJJ4qSIpfgSJ5hSSwpOUxZREgjGB1MhUwtRh9S9W5YSVF8Dhn+CS92HAdGX/ZzfBltfhrz9RaenLB/s+YMXOFZywnWBk3EiuHnE1U1Om8v637/OL9he+OfwNcYY4rh9zPecNOA+1Sg3OCnjuZLbodSwyOnlx5ouc0uuUerd3nThB5caNVK7fQOWGDTizsprssyoiopYooiJRR0WhTUwifNIkjGPHBM0A2NJvVLp6NTk33MjqqWZWTKrksnI7l47+P+In3dSqe4eUkoJ/P0vBf/6Dcfx4Uv79DOrwMPj0evjtPRj2B5j7HOjCOVJ6hJsybuJA8QGWpCzilEdWUVVQQO/nniX8lFOavL67vJycfz9N6Rtv4lRLds0Zyvl3vUJ4eBSvfP0KL2f/m4WZCUz/uQJ3QQH6kSOJveZqTNOnI2yF8P1DkLsVd/I0jr62A9tve0i4806iL7vUtwdX5VDel3WPQ3keDJgB0++GlHFNHu6RHipdlZQ5yyhzlWFd8w/WnNjEquh4ih0lROujOavfWZzb/1zSYtJqBubVWau5de2tfHDeB/Qt0mJdvpySzz4DIYicex6xf/4zutRUyFwFb86H3hNBqHGeKCH/p1JK97lRh0li0sqxDCyrP7ZG9qakajLHVq5HZTaR8uSTGMf74DDRAFJKbFu2kP/Mv6lcvx5dairxS5dgmjGjEcHc89M9fLz/Y86NOpeH5j6kEGb+Xiq+WEnOvz9GVrnodXIh5l4OCI9XTqo4AZF9IP1yGLMQTL5JQR7pIaski98KfmNnwU52WXext3AvKfpE3h66DMfuXdi3/oJ952/YD+chnYpPsVBJwqJc6GMEkZcuxrjg9lbvFQgxhFRJbUTpqlWojykZwg9q+7D6VRNJ4Z8Se/x3NP87H8LS4KyHW/dFk1IZmA6thXnP15ICwIx7YNeH8PXfMF72MZelXcb8wfP5ZP8nvLLzFa777jpSI1I5WnoUnUbH/43+PxalLcKorSM6f/8QlBzBdOZLsOl+ypxljZqgjY8n8pxziDznHACq/nsuroJi3NMexl1cXH8pKVHWhUU4Dx6i9PjXWF98EZXRiPGUUzBNmYJp6hS0vXq1+Rk3hDMvj8N3/o3DSYJVp+n53+FDDD//NRhydusnA0II4m64Hl2/VI7deRdZCy6i9wvPozt/OSQMh2//Adb9MPpS+qg1vNHrbJ488Rmpty+nvEpN778vJjy2Ag6uVdQu5kSIrlUN7bQdYMmgtWiu1vGPTamkf7iHY5sWEHfjDYz49jdeWAfqisPkDktk/OOvYpw4EeF2wfr/Qsaj4KqAxJGoNz5Fn5MkORV9Of7gg1Qd3kvcXfc3P2N2V8H2t2Dto1ByFPpMggteoSRpOKuyVrH1h/eVwb+aAMqd5criKkc2qIsVZtAyw9SPOZMXc0qvUxqpigAsegt9j0vsdzzIwbUbETodlosuIuaqK9HWjZbb9RGERcKiz0GjQwckA9G7dpH/5FOc+OknCo8NIu6aK4mcNRV5ZCPHlz1O8Za1GOOdJC8cjCa6vDZDqx8QQmAcN44+K16lfO1aTjz2ONnXXocxPZ3422+vUfd9tO8jPt7/MQCeku3wzmXIrJ8p3GLnxA4zYVGClGvGoRt3BqROhpiB4KmC37+ATS/Dmvvh+4chbS6Mvwr6nFLjPy2lJLciVyGAgl3stO5kt3U3Fa4KAIwaI2nmvow3JvNT+SHyfr2Ofnl7IMwG40BONOEMS8PuTMJerMOeW0bpviyMhvazP4UkhjZASknWhfMRm38iNesYafHZ7D5eq4tm/Qvw1W0w9TaYcVfLF1vzT/jhcWVmd9rSxt//+jx8fTssWAlD59TsdnlcfHnwS97NfBeTzcQDZz1QI9bXIHcbLJ8OYxeSN+MOZr4/k/sm3cf5g85vuU3vLoQTv8N1G1p5Eoq+v2L9esrXraNi3Q+4cpU6S7oBA2pIwpCejsoPaaKp36jEXsyGBecQe7CQD++cxJKoaCI3rYDbs0AX7vO1vajcvJns664Hj4fkfz9D+IQJkLkaPlwMdiU81l6k4UhGDHaV4O4/acDi5ukT+fR31TF49z0VmX4VK1XlPLH5aRLCE3hi2hMMixlG+Y8/cfyRh3HuP4AUgoiZM/l6spF/2T7n7ol3s0BlgVV3KGQ08AyY9TDEnQSlx2DPp8gdH5D38e8UHwgnMi2MpBsuQ4y6ACx9qx++B3Z+ABkPQ+EB6DWWqul38LNez6cHP+X7I9/j9DhJMCYQrY/GpDNh0pow68yYdeaabZPWhElnwqw2MHLlpZiGnAvznmvyudl37ybrqWXIH9bjMYQRd+lCoi9fhKahh0WVAx4bCEPPhXn/afJaFb/+yoknnsT+22/oBg5ApTdg37mT6IvPJ368ROx4C7etkEMxqewZOIXdEbHsLj3E0bKjXDH8Ci5Lu8zn31tWVVH8/vvkP/NvRd03eQRlU4xcKjcz2uEkVy0Y5nDyaIWR3I3RlP2Wj3nGZHotewpVS3U08jNh0yuw7U3FLhefBulXYk+bxx9XX86RsiMAaFVaBoenMExjZoTDyfCiY6Qe34va7SRLo+Hc3r24xxPF/KTJ0GuMskQPaESIsjo83he1bkiV5AeCQQyVmzdz+JJLKVHnMXCv4MwhR9mys87gJCV8ch1sewPm/w/Szmv6Qptehc9vgrEL4dxnmo7Ucrvg+cmKa+m1G0Db2FOmyT65q+Cl06E0F67bQLlayylvncKS9CUsGrao5Q5+ch3s/xZu/b3l4xpASonz0KEakqjcuBHpciEMBsInTiTm6sUYx45t9ToN+7PLuotPH1jMvK+KOPSX2Zx145OI/5wC5gRY+EnzF2oFziNHOPqXv+I8epSk++8n6g/zFLWes5zKLVs5etPtqMKN9H3qPrbr81my/V/Y3Q4eGnI5p1uGQd4Oyja+xL26Sr4JNzLdkMI/Z/yLyNhaB3/pclG+bh3b8vOZctFFuD1ubvj6Kn7K38wLx44zMby3Qggnndn0My0+SsHDd1HwyUbCk+yknFqEqu9YGHQm7P5EcS+OH8a+k6/iU3cRnx/6ggJbAZYwC2f3P5u5A+YyJHqI7x46H14DmV/D0v31Ylnse/dS8OyzlH3zLcJs4u3RlQy9+hYWjL+q6evs/QreukhRjQ6a2eztpJSUrf6G/KeeoqqgAHnXdewZHsGewj3sLtjJXuse7FIhYr1HMlhjRmVKYGvpAe4ZfQPz+5+LYkwXTayBykI4tg1ytypqusPbse4QFO414UTw3XgVF885ndvEAXS5Lv62OgzHgYPE33oL0Vde6ftzc1YoJL3xZTi2jYPGCOYmRLFAm8AfSooZdOIAOlntAWiwQOJISBoFSaOQCSM4I+OvjEtIZ9lpy3y7nw8IqZI6GPkrXqXCoCKy0sNRdX+iYxsYoISAOU8qcQEf/UURPxPS6h+TuQq+uAUGzoRznmo+b5JaC7Mfgf/Ng1+ehalLfGvkhheUP8QFr4LBglF6UAlVk6qkRgiLUDxa/IQQgrD+/Qnr35+Yyy/HU1lJxYYNVKz7gbJvv+XwZQuJX7qE6EWLfPrDSSl5e+/bvPP5o/xztQP35HSFFEpzIX8PjL7Y7zbWha5PH1LfepPsG2/i2B134MzKIu7GG6jYuofs65eiTUigzysvo+3Vi/HAO32ncmvGrdy06wWuGn4VM4fM5Lbjq8gpy2GJiGHh7l8Qv58CQ+bAhMXQ91SEVov59NNxZ2SArQj12mU8uvlzLk2K55bkPrw15x36WJqP7hJRvYl79HU0Y94h7/77Oby5N71jHGgyHqYodgBfTrmaTyuPsPu3p9EIDVNSpjB34FymJk9FG0iQYtpc2PE2HFoHA0/HsW8f+c/9h7Kvv0ZlMhF77bVEXnYJH35yGtdo7c1fZ9dHoI+C/tNavJ0QgohZZ5LRr5JHf/wnpaWPw8+KmmVI9BAuGHIRaTFpDCWMfnu/Rb39bVyO3dwSH8sDW/+FYfXfObeisvV+afSQOAL1hIuJmzua/2SuJvH9TZy5XlK8dxOzJ0QzKCObKkMkfV5aTnjdyD9foAtXJnhjF0LOZgp+eRIqdjCzIJdhsSNhyPm1ZBCZUu//LoD0xPGsP7a+U+NYIEQMAcOZnU3Ft9+x+mRY+nEVH4lBxMQ38QfUhMGCN+DF0+DtP8Hi78EYrXyXswXeuxwSR8CFK0Ddys8xYLoy2PzwpDIYRrSivy8+At89CINmKQZVQCVUhGvDfSMGfQQ4y8HjDtjrAkBlNGKeNg3ztGnE3XwTuXfcwYlHHsW2fTtJ/3wAtal5FVC5s5x7f76X7/ev4pnPw9BZYhjw2DPKn+bAGuWggWcE3DYv1JGR9Fn+Inn334/1hRewbduGbcsWdAMG0Oel5UrwYTUSwxN5dfarPLLhEV7e+TIv73yZeGM8r561QnEjLjyozBi3vgG7P4a4oYreecSF9Mr5Cp65AmxFmMYt4t/jr+BPGddx/dpbeOPsNzDrWnZ/tFy0AE1sDDm3LmHvlxbe+fMf+cy5larsrxkaPZTbx9/O2f3PJlof3bYHMmAG6Ew4vl9JwX++pPSrr1AZDMT89S/ELFqEOioKALCQCAAAACAASURBVKPKSJG9qOlruOzw+5cwbJ7PEfQbTmwCg55HJt5PWkwafSP6ohIN7AoDzoQz/oF23yoeryjg2iMfcY9QYUw5m9PN1eQqJYqXVfVaZ4JeoyFuSE1b3v79LT6wb+HGv99MP9UpnFj2GKNWrScrQcWIle+jS0mmTUgeR8HYS+CHHcQt/BzquCg3h/GJ4/ny0JccKj1E/0jfc4UFGyFiCBCHX/kvbiHRnDEN7Yrn2a0dRkxDicGLiCSFHFacA+9fqYjVJUcVT43wWLj4PQjzoQ4wwJkPwHMT4Zt74Y/Lmz9OSvhiCSDhnMfrzUwidBGUu8pbv1dY9SDlKANDlG/tawXv53zJ8SsGMSTJQ8rKVRTt2gYP3kbUkOFEhUVh0ppqZkrZzmwe+/wxcspzeHLXSGLytpK8/F9ovNnD9q8Bcy+ID44RTmi1JN5/P7rUfpx4/HEMo0bR+4XnUUc2TpWhU+v4+yl/Z2TcSDblbeKW9FtqB+Po/jDrQZh+V7VaYTl8uQS+uo2TpAf6TobZD0PSSHoDT572JNd8cw23rbuNZ2c8q3iRNQOXx8Xa/nbW/jmF+S8f4KzHjjP8/AmMnHoBgycELyWEI/sYBdv7U7r9Z4TBSMzixURfcXnts69GuCq8eWLY/y04y2omJb6g0F5IijmFc/qf0/KBOiMM+wNhwDNjLmHxN4tZmruaZ2c8q7hmt4KdBTtZtnEZU5KncOXwK1EJFX1WvMrbnz7EYwVvMT2xiWjUAJBvywcgxuDb9SYkTgBgU96mEDF0N1SVl1Px4SdsTdOyuPc84Hm2uUYxoqXfvvcEOOcJxfPoq6WKiO52weVfKjpyXxHdDyZdrxiqx18FfZoJftr1EexbBbMeUvzC68CsM1PqLG39XkEmBofbwQPrH1A+pMCwiwQ3fpxH2FW38M+zVfwyVIVGaIjSRxEVFkVWcRbRhmhejboB/VePYVl4GaYpk5Xz3VVKwNbQc4OatlwIQcxVV2I+fQaapCRUYWEtHj9v4DzmDZzX9Jc6I4y9DMZcCjmbYeeH7CoNZ9iFd9Zr84SkCdwx8Q7++es/eXrL09yafmujS5U4Sngv8z3e+v0tTlSeIDUlldzHriXt8c+Ien098vX17NXeSVjaUAwjR2EYORLDqJFoe/duUSUh3W6qjh/HlZODMycHV04Ojr2ZlH37LUKrJnpIBTH3Po5mzJwmzzepTRQ5miGGXR+BIVqJSfARhfZCv6Udo9bIf07/D39e/Wdu/P5GXpj5AmMTmrdhlThKWLJ2CXGGOB6a/FCNRCKEQJ+WhutngdVurRfUGCisNis6lY4InW95RnqbexNvjGdj3kbmD57f5vsHihAxBIBNLz9CpN1NxKWXYjmizAh+ZwjTWpsUjF0Ix3YoM0h1GCz8WPE+8RdTblG8H766TVFNNZxh2orgq9shaTRMuKbR6SatiXKnLxJD9cvs8IFEfIDVZgXg3lPuZWbfmZQ4Sii64ADi7se5+eOD/KlqLL/NH0NhVSkljhJiXbE8dMrtFC24As2gQcTfWmfAzN0C9hIY0KiEeFCgCzS9ZlMQAlLSISWd/IyMJols/uD57C/ez4pdKxgQNaCGbLJKsnhjzxt8euBTbFU2JiZN5N5T7mVy8mRUQoU841qqjh3DtmMHth2/YduxneL3369JvaC2WNCPHIFhxEi0SUm48o7hysnFVU0Crry8RulENPHxRF92GTGXX4LmlQmQ+z00RwwqU9MSg8umGJ5HXuhXIsZCeyEDonzIpNcAkWGRPH/G81z+9eVcu+ZaXpr1EsNiGmddlVJy9493c7zyOK/Nfo0off0JT6xBURkW2AqCQgz5tnxiDbE+2wuEEIxPHM+vub92qp0hRAx+otJRjuvtTzjc18B5590K996PFHBQ9ifal4nO7IeVgXzADOjrp2HLC104zLwfPvyzosce18C76Nt/QGUBXPJek3YLs85Mbnlu6/epKzEEAV5iiDXEEhkWSWRYJH3S+iDfPpXjjz0O//sf/XI9JD/1FNqEeDK+/x77A0/iKSmh18sv1Z+97/9WSeHQf1pQ2tYVcNv42zhYcpD7f7kfp9vJD9k/sDZ7LRqVhnP6n8OlQy9lcPTgeucIIdD26oW2Vy8iZs8GFJdMx759NURh37GDgnU/1BTp0MTHo01OxjB6NBHJyWiTe6FNTkaXnNxYShp0Juz5DM5a1qSdyaw2k23PbtyZfd8o8Rh+qJGklBTaC4nRB6bGiTHEsPzM5Vz+9eX85Zu/8OqsVxloGVjvmBW7VpCRncHfJvyNkXEjG12jLjEEA/m2fGKNsa0fWAcTEifwxcEvOFRyqF7qlI5EiBj8xKev3cOooipMN/5VCfr5fQ8OswFXqa7JJHmNoNbCWY+2vSEjLlDSHay5X/Eg8ap6Dv8Cm1fAKdcpxrYmYNaZfbMx6Kt160EihkJ7IUCjP77Q6Ui8604Mo0dx7J6/c+iPfyT5yScw/PAj5d9/T8Idf0M/uP6AyP41SooHYxuNrF0IGpWGJ057gou/uJh//vpPLGEWrhl1DQsGL6gZsHyB0GjQDx2KfuhQLAsUdYS7vBy31YomMbFV9Vg9pJ2nGNCPrm9yImNSmShxlOCp9narwa4PwRir2FN8RGVVJQ63o02G88TwRJbPXM6irxdx9TdX89rs1+gd0RuALce38K8t/2Jm35lcPKRpTzavLcA7iWkrrDYrfSP6+nXO+AQlwntj3sZOI4YuVBqi6+NI6RG076+mPMbA6AuuVnbu3U2pSRlAfSKGYEEIhWAqrbBW8XkWHhd8dqMSoj/9zmZP9dvGYA9OIj2rXfmzNWeIizznHPq98zZqs5kjV1yJ+d13CZ80CctlDQKYKgsVnX07qZE6E5Fhkbx05kssm7qM1Res5trR1/pFCs1BbTKh69vXP1IARWLQ6JVYiSYQrg7HLd31vdycFYobdtp5rXva1UGhTZk4RBvaRva9I3qz/MzluDwu/rz6z+RV5GG1WVm6dinJpmTum3Rfsyoai96CQARXYvDz90sxp5BgTGBDXuuBpe2FEDH4CCklL31wF2lHPMQvvEKJOPR4YP8hrNU/fIcSAygSwdjLlFiF/L30OfIBFOxVYidaiAL22hg83kCb5hBkVZJXYmhpRhg2aBCp772L+cyZeMxmkh5+uHGGyYPfAzIobqpdEUmmJM7qd1aT6b47HGFm5Tnv/lR53xvApFK86by/LQD7VoOr0i81EtROHNrsagsMiBrA8zOfp9RZyuLVi7lt3W0UO4p5YtoTLboEa1VawlXhQSEGp9up2Mr8JAavnWHT8U2dUvMCQsTgMzKOZtDri8249VpS/lSt08/NBZud3DAlnqDDiQFgxt9BGw4f/YW+h9+D4X9sMcIUFIlBIql0tRIQ1A7G53BteKsDntpkIuWppyh48AG0CfGND9i/RgmaSm49ejqEICBtLpTlQk7jLAMmtUIM9QzQuz5Sksz1PdWv2/gycfAHw2KG8dzpz5FXkceGvA3cOfFOhkQPafW8CHVEUIihrk3NX0xInEChvZCDJQfb3I5AECIGH2CvsvPfNQ9y6h5J9B8vRO0tcbV3LwCH1X0RQtKEu3v7wxQH026H3C241WFKdHQr8M6YWg1y04UrBt5gGZ/tVv/+9E0lTJNSIYYB09sUdBeCHzhpFqh1TaqTGhGDo1zJNZU21+/fJ5gSgxdjE8ay/Mzl3DXxrtZzg1UjQh0RFBuDl1ziDP7X+UxPVLJVbMzb2OZ2BIIQMfiAl3e+zIgfc1F7BLEL6xSQz1Sqm+3XpGGxiIAKoQcFE66GkQvYO/gGMDUxw26AGmJwtTLgC6GoEoKlSrIF7nFSg+O7qlNJ9zz7QpeFPlLxotv9aY1nkxdeVVJNLEPm11Bl81uNBHVsDEEkBoDR8aO5aMhFPrt+mtXmoEgM3uC2QCSGFFMKieGJnWZnCBFDKzhaepT/bXmZOdu1mKdPR9e3jodBZiaEqTlIWueokbxQa+H8FymI863Sl0mr/Jl9zpdkD5IqyV+JoSnUpMEIEUOHIm0ulBxRktDVQSOJYddHYEpsPvCyBRTaCzFrzbW1yDsJXlVSW/X7XnIJhBiEEExInMCmvM6xM4SIoRUs27SMKbsl+nIn0XWlBVBUSfEGrM74ziUGP+GNwvQ5yC1INoZCe6HPqQGaxf5vlZTGreWJCiG4GHwWqDSN1ElaocWoMSr2AUeZEr8QgBoJqqOe2+iRFAxEqCNwepytS9StoMBWgEAE3Kf0hHSKHEUcKD7QpnYEghAxtACP9LDu6Fou2KonbMgQjBMn1D8gMxNiBFZbdLciBpNOmeX57LIaBGKo8lRRZC9qm8TgKIcjv4akhc6AwQL9TlOIocEM1qK3KKqkvV+D2wHDfdPlN0Qg6TDaAxFqZeLUVjtDvi0fi97SZJEjXzA+UYln6Ax1UogYWkC5q5xhh9xE5BQTvXBhfR2lwwGHDkGEE2t5ZLciBq+NwedEekGwMRQ7ipHItkkMWT+C2xmyL3QW0uZC0SE4vrPe7mh9tKJK2vWhktQwZUIzF2gZXYUYzCrl/9FWO0OBraBNMSgp5hR6hfdi0/HAas60BSFiaAEl9hLO2SipijIRMadBtseDBxW/7hgV1jKjb+kwugjMWh+9kkBJvR0EYvDOvtr0xz+wRikW36fpWsshtDOGzAGhbqROsugtFNkKFDXfsHl+l9/0oqsQQ7AkhoLKthEDKN5Jm/I2tR5zFGSEiKEFlDiKGXlI4pgxoXFJymqPJGeUjgqbtltJDFq1Fr1a76ONwRwU43Nz6TD8wv5vlXq7TVSvC6EDEB6jPP9dH9dTJ0WFRVFUnqdIcwF4IwG4Pe62qxqDBC8xtFlisLedGMYnju8UO0OIGFpASfEJNB4IS0hs/GV1DEOhURnouhMxgGJn8M3GECSJoZV0GK2i8KCy9NBo526DtLlg3adUJaxGtD6aIlcZMiIFkputFtkivKrGrkAMRpURjUrTJmKQUrZZlQSdZ2cIEUMLKLfmAaC3NPHjZmZCdAQFHiV4pbsRg8+J9MIiFL90t6tN92uzj/r+ajfVkH2hczG0urZyHXWSRa3HgcQ2dE6b1EjQ9jxJwYAQglhDbJuIodhRTJWnKqDgtrpINiWTbEpmU17H2hlCxNACKguPA2C0NBE0tncv9IrCaleK7HQ7YtCafYxjCE6+JKvdikal8blgSSMc+A6i+kKM/7n6QwgiTPFKltW6xFB4BIDCgdMDvmxQVI1BRKw+lgJ74MRQE8PgZ8rtppCekM6m4x1rZ+gUYhBC3CyE2CWE2CmEeEsIoRdC9BNCrBdC7BNCvCOE6NwoF8BerKg/zDFNqJIyMyFBjxUlLW53Mj5DtcTgi41BH5x8SV7DYkCFR6qcNUXpg1mtLYQAkTYXTuyGgn0AROduB6A4KvDCNsHOk9RWxBpi22R8rol61redGMYnjqfYUcy+on1tvpav6HBiEEIkAzcA6VLK4YAauAh4FHhKSjkIKAKu6ui2NYSzWInm1EXVr3NLcTGcOAHRKgo9SiR0d5MYfLcxeFNvt40YrDZr4LPBo+vBWR5SI3UVDD1XWe/+BI2rjKicbQAUNlfi0wd0NWKIMcS0SZXkJZU4Y9tUSVBrZ+hIt9XOUiVpAIMQQgMYgWPADOD96u9fA5oppNtxqCopBkAV0SA7XrVHEhF2rFXKLKm7EYNfNgYIiiopYP3x/m+VqFs/ageH0I6I6AW9J8LuT4gtWE90lWJ/arLEp4+w2qyohIrIsM7IRNkYsYZYCu2FuD3ugM5vS56khuhl6kWyKblDE+p1eAU3KWWOEOJx4AhgA1YDm4FiKaW3+Gw20KRcKoS4GrgaICEhgYyMjIDaUV5e3uq5ZXmK8fmX33Yg99eKcQnffMNQwG0o5lC2Ea3Ww4YN6zpdy+FLn7woKiqixF7S6vHm0n2MA37b9BPWrMAN0MeKj2HSm/z6vbz9Gbf9E9zmwWz7dUvA9+8q8Oc36spI0Q1j4NFX6FVciEOrDH6bdm0iMjuwgX2XdRfhIpx1a9cFs5kBoby8nMKyQjzSwxfff1HjvuoPthVuQyd0bPwpOIN5b9mbX7J/4bvvv6tfKc8HBPLOdTgxCCEswFygH1AMvAec1cShTWaOklK+CLwIkJ6eLqdNmxZQOzIyMmjt3H3v34dHwNRZs+oXi1mzBlQq1FFuXNr+xMWpmD49sHYEE770yYt9O/axZusaJk2Z1HLSsoIU2AIjTkqFkb5duyGklJS/Uc6w1GFMS/f9GhkZGUwbNxQyDsHpf2falMDu35Xgz2/UpVE8AJ5+hQh7NvLUm9DmfYYlxcK0cdMCutyH331IojaxSzybjIwMJvWbxHsZ7zF4zOBGdbZ9wZdrvySBhKD1p+xAGb/++CvJo5L9bk8g71xnqJLOAA5JKfOllC7gQ2ASEFWtWgJIAXyoVt++UJVX4jRqG1cQy8yEPsmgFlgro7qdGglq8yW16pkUBONzuascl8cVWAzDge+Udci+0LUQ1VupuQ2I4edj0VsothcHfLmuEvXshVcFFKidocBe0GZX1bpIT+jY+gydQQxHgJOFEEahuKicDuwGvgcuqD5mEdB0kdkOhLrcQVV4EzVyMzOhbxIAheXh3c4jCfzIlxQE43Ob0mHs/xbC4yBxZMD3D6GdMOkGjsdPhcSRWMIsbbIxFNqDUKsjiPBOYgIlhvzK/LZnEq6DJFMSKaaUnksMUsr1KEbmLcBv1W14EbgduEUIsR+IAV7u6LbVhUd6CKt04jYZGnzhUYihVxQA1lJDt5QYfM6XpNGDStsm43PAPurSrUgMA2YEHDgVQjti2Dz2pN0KQmDRWyh0FLZ+TjPoahKD910NWGKwBVdiAJiQNKHD4hk65d8mpbxXSjlESjlcSnmZlNIhpTwopZwgpRwopbxQSunojLZ5Ue4qx2iXYA6v/0VuLlRWQrweEFiLNN2TGHwt71lTxa0NEkOA6TDMZQfBVhhKg9ENYNEHLjHYq+xUuCqCOsNuK4xaI+Ha8ICIwVZlo9xVHhRX1bpIT0in1FlKZlFmUK/bFELTsGZQYi/BZAdVRAOPhOocScQIZHgCVqvolsTgs40B2px6O9B0GNGF1dXC+gceURtCx6Am9XYA8J7XlSQGCDzIzUsmwVaNeeMZOkKdFCKGZlDiLCHcDppmYxgclOkGUFXV/WIYoE4VN19iGfRtK+/plRgseksrR9ZHdOEWSBoNpuDOvEIIPixhFsXJIICcWl0tuM2LGH1MQGkxghncVheJ4Yn0NvcOEUNnosReTLgdtA2jnjMzwWgETRGFqkFA90uHAX6okqDNGVatNitRYVFoVH54R9uKiSjdG6rW1k3gJf2iAKKfvROHLkcMAUY/BzO4rSEmJCp2hkAD73xFiBiaQUlJdcptSwNxYO9eOOkkKMvF6kkFuqfEYNQYUQmVH6m3A5cYAvI4ObQWgSdkX+gmqCGGANRJXVViCDTDak0CvXYghvTEdMqcZe1uZwgRQzOoqMms2kAczMyEgf3BXoK1qg/QPYlBCIFJa/K9WE8bjc9+p8PI3ohHaCFlfMD3DaHjYAlTiME7yPuDrkwMZc4yHG7//GDyK/NRCVXNMwkmxid0jJ2hwyOfuwtsRQrrh0cn1O701nmeNxsAq0uJZeiOxACKOqlDjM/2QoZGD/XvpKIsbIYEwtVKIXWXy0V2djZ2uz3gdnQFREZGsmfPns5uRiPo9XpSUlLQagMrXO8d1AOSGGyFGDQGjFpjQPduL3hn/FablV6mXj6fV2ArIEYfg1qlDnqbEsIT6BvRl415G1k4bGHQr+9FiBiagaM65Xa9zKreOs/JUVACVodSp6FbE4PLx7rP9lKlnGMACaGsNqv/rohFWdj1CXidhbOzszGbzaSmpgaWuruLoKysDLPZ3NnNqAcpJVarlezsbPr16xfQNdpiY+hqMQxe1I1+9pcY2kON5EV6Qjqrs1bj9rjbhXwgpEpqFq7qlNv13FW9HknxStBboU35M3RH4zOASWvyXWLwuKDK/9ASh9tBuavcvz++lFB0GJuhtg6G3W4nJiamW5NCV4UQgpiYmDZJYxG6CAQiYBtDVySGQKOf25sYxieOp8xVxt6ive12jxAxNAN3iaJTV0fWcVf1xjBUCxHW8ggiIkDTTeUun4v1tCH1tjeGwS/js60IHKXY9Qn1dodIof3Q1merVqmJCovqUcTgLbLj9ZryFQW2gqC7qtZFR8QzhIihGcgyZRBUN5QY4uNBFoE+EmuxttuqkcAfG0PgifQCckUsOgRQT2LozsjIyEAIwWeffVazb86cOe2Sfvv9999HCMGmTR1bIxiqo58DdFftisTgdZjwR2Jwe9xY7W0oSuUD4o3xrDx7JX8a8qd2u0eLxCCESBJC3CSE+EAI8YsQ4jshxDNCiFmih0/fRFklUoDKZKrduXcvDB4MZccgIhmrtfvaF8APG0NN3Wf/iaEmT5I/NoaiLADs+p5BDAApKSk8+OCD7XqPsrIynnnmGSZOnNiu92kOFr3Fb68kKWWXlRi0Ki2WMItf0c9FjiI80tOuEgPAyLiRLafLbyOaJQYhxHLgjepj/gVcAdwC/IhSXe0nIcTkdmtZJ0NdbsNl1NVPuZ2ZqcQwlOaAOanbE4PXXbXVpFze1NsBRD8HlFm1mhhshoSWj+tgZGVlMXToUBYvXsywYcM488wzsdlsPp07atQoIiMj+e6771o87tixY0ydOpXRo0czfPhwfvjhB5/bd88993Dbbbeh1+t9PieYiNZH+516u8xVRpWnqksSA/gf5NaeMQwdiZa0489KKbc3sX8b8K4QQg/0aZ9mdS480oO20klVeB1pwVvn+aSToPQHSBiO1QqDBnVeO9sKs86MRFLpqqzJndQkaiQG/20MgamSsiA8Ho+66QHuvs92sTu3bTWoGyKtVwT3njus1eP27dvHW2+9xfLly5k/fz4ffPABx44dY+XKlY2OnTp1Ks8880zN57vvvps77riDuXPnNnv9N998k1mzZnHXXXfhdruprKwEYMGCBezd29jYeMstt7Bw4UK2bt3K0aNHmTNnDo8//rgvXQ46osKi/FYl1eTRCrTsazvD3yC3/Eol6jnYmVU7Gs0SQ1OkIIToCxillHuklHag/dP8dQLKnGUYbRJpruNX7fVIGjQAth+HiF4UFnZfjySonxajZWII3PhstVn991EvygJLqt/36gj069eP0aNHAzBu3DiysrK4++67Wbp0aavnTpkyBaBFKWD8+PFceeWVuFwu5s2bV3Ovd955p9lzPB4PN998MytWrPCjJ8GHRW+h2FGMR3p8Lj/ZVYPbvIg1xLL1xFafj///QWKoByHE7UA64BFC2KSUl7dbqzoZpY5STHYJ8XUGSy8xJEfDdkmVMZni4u6tSqohBlcZSSQ1f2AbjM8BpcMoyoLeJzf7tS8z+/ZCWFht4Sa1Wo3NZuOxxx7zSWIAWLJkCQ8++CCaale29evXc8011wBw//33c95557Fu3Tq++OILLrvsMpYuXcrChQtblBjmzp3Lzp07a8o35uXlcd555/Hpp5+Snp4erK63imh9NB7pocRR4nPCxIBrdXQQvBlWpZQ+eW71eGIQQvwVeEHKGgX0WCnlhdXf7eiIxnUWvJlV63kk7d2rFIuJUQw+RbL7psPwwqT1MfV2G4zPVrufwW1uF5Rkw8hUv+/VWVi6dKlPEgPA6aefzsMPP0xurlK5duLEiWzbtq3m+8OHD5OcnMzixYupqKhgy5YtLFy4sEWJAaCgoFbdMW3aNB5//PEOJQWoTYtRZC/ymxi6ssRgdyv1IlqUqqtRYCvArDWj13SOnSdYaEneswFfCyHOqv68ptor6XtgTfs3rfNQ4lCIQRtZ5+XOzIR+/cCu6BCtrmSgexNDTert1mIZNDqlklsAxme/PU5KjoL0dFlVUjBw1113kZ2d3eR3GRkZjB49mjFjxvDBBx9w4403dnDrAoeXDPzxTPLaoKL0Ue3SprbC3yC3fFtwS3p2FlqyMawQQrwL3C6EuBq4B3gL0Ekp/a9e0Y1QbC+ijx10lgbEcNJJUKrM9KxOxZWyOxODdwbkW4bVwPIlWW1WRsWN8v2Eao8kLKlQ4n9u//ZEamoqO3furPm8ZMkSn86bNm1ajZoH4LzzzkNK2eSxixYtYtGiRW1qZ3vER/gCLzEUO3z3TCq0FRIZFolWFViOpvZG3bQYqZGprR5vtVnb3VW1I9Cahag38BpwHXArsAxon+QcXQhlpQVoPGDwZlb11nkePBjKckGjp7BCUa/0BOOzT8V6AqjJ4Pa4KXYUB+Sq2pMlhp6KQDKsdtUYBi+80c++FuzJt+XXnNOd0ZKN4WUgHDAAu6WUVwgh0oFXhRA/Sikf7qhGdjQqi04AdVJue+s8n3QSlG5UYhgKFUNUd5YYzFp/ivX4n3rb66Hil2GxKAvUOjAnAfv8ul8InYtAajJ0eWKok2HVFxTYCog1dn9iaEliSJdSXiSlnAvMBpBSbpJSnkMPdVP1wlakvAQ11du83iBeVVJ11DN0b2LQqrXo1Xo/ajL4JzHUxDD446NelAVRfRVDfwjdCjq1DpPW5FcsQ1cnhoiwCDQqjU82hgpXBbYqW7f3SIKWieHbamPzj0A9lwgp5Qft26zOhbNYEYVrvJK8rqqDB1cTgxL1rNFAXcel7giTzuSbjUEf6bfxOSBXxMJDITVSN4a/aTG6OjGohEqp/ewDMfSU4DZo2fh8qxAiGnBLKUs6sE2djqoSxXimiqjOrOqt85yUVJ0nqRdWq2Jf6O4Zo8w6s482hgAkhmrx22dikLI6hqFzcv2E0HZYwiw+p8Wo8lRR7CjusjEMXvga/dxTYhig5VxJFwFFzZGCECJVCDGp3VrWifCUVGdWjawWB7x1nu1F4HaCWYl67s5qJC/MWj8yrPpptZRfFgAAIABJREFUY/A7gV51uu2QxNB94U+GVa/3UleWGKA2yK01/H9BDEAysFUI8aIQ4hohxPlCiIuFEH8XQnwHPA30TLfVcmUGXU+V5E2eB/Ukhu4O32syVEsMzbhZNgWrzYpGaGq8n1pFD/VI6qi02++++y5paWkMGzaMiy++OKjX9hX+qJJqEix20TxJXvgrMfR0VdITQoh/ATOBU4EJKEFve4CrpJSHOqaJHQ+VN+W22Vxb5/lPf4LSY8oB1cSQmtqpzQwKTDoTOeU5rR8YZgbpBlcl6MJbP55a/bGveXN6KjFAbdrtuvEMwcS+fft4+OGH+emnn7BYLJw4caJd7tMaLHoLRfYin1JIdPWoZy9iDDEU2gtbLaWZb8tHo9IQGRbZ7DHdBS3+Y6WUVcAvUsq7pZRXSSmvk1I+15NJwSM9aCocVBnDlJTb3jrPgwc3khh6hCrJ12I9AaTe9jsdRg0x9PX9nA5EV067vXz5cq699los1UGZ8fHxPp0XbESHRePyuKhwVbR6bHchhlhDLG7pbjVwz1vSsyeUqvElid5mIcQG4FUp5er2blBno8xZRrhd4g5X6jrXeCSddBKUrQahgvD4nkMMWl+Nz3UzrLaQcK8OrDY/K3MVZYExtjY3U3P46m+Q95vv1/UFiSPgrEdaPayrpt3OrH5PTz31VNxuN//4xz+YPXt2q/0JNrypLYocRa3mFuouxOA1jhfYClqc6BTYCnpEcBv4RgyDgFnAYiHEcyhpMV6TUh5o15Z1EkodpYTbAXO1uqRuDEPGCjAlYnNqsNt7CDHozDjcDpxuZ8sVoQJIvV1oL2RA1ADfG9OF02170RXTbgNUVVWxb98+MjIyyM7OZsqUKezcuZOoqI7NQeQd5IvsRfQ2927x2EJ7IRqhqcnZ1VXha5Bbvi2fZFNyRzSp3dEqMVRnV/0K+EoIMQ1YCdxcLUXcIaXc0L5N7FgomVUlqvjqWau3znNUVL0YBugZxmfvrK7MWday2qcmw6pvnstSysAkhpTxrR/nw8y+vdAV024vXLiQlJQUTj75ZLRaLf369WPw4MHs27eP8eN9eJ5BRN0Mq63Ba4Pq6qqXmnxJraTFsNqsjI4b3RFNane0SgxCiCjgEmAhUATcDHwEjEMJfOvXng3saBQ7ipWU25HVBiSvqyooxBB3Uo+IevaibrEe34jBN4mhwlWB0+P03Ufdm257xIW+Hd+F0BXSbs+bN4+33nqLyy+/nIKCAjIzM+nfv3/gnQoQ/mRYLbQVdnmPJKifSK85uDwuCu2FPcJVFVpPogewEYgH5kspZ0sp35VSuqSUvwLL27d5HY8SRwkmO2gjq0Vwb/I8UILbzL16FjFofUyk56fx2e90GCXZitdTF1clBQPtkXZ71qxZxMTEkJaWxvTp03nssceI6YQXtEaV5EMsQ1ePevbCqDVi0BhaJAZvidKeQgy+2BgG1ynWUw9SyoeC3J5OR4m9mN520FtiwelU6jz36aPMlB2likdStddqjyCGaomh1bQYfkoMfqfD6Aauql057bYQgif/X3tnHt9Wdeb979HuRd5jZ3H2PXEcZ6VAgJQlQKFAWTttKUtLCzNtYYZ5W952ysBM551Op6Xt0JkWWsrSMqWFQFnKGogbQikhzk72xQl2HMe2bHmVrOW8f9x7ZSu2pSvvks7389FH0tXVvedI9n10nuX3PPQQDz30UMLvHU4ybBk4LA5T1c9Nviam5SRH2/h4RW6pVNwG5lYMr+ruJACEEPlCiD+N4JjGlLYOD/YQZOQXaUYBoKQkqobBo6+SU8kwxC1ySzD4HCleMvuL0DAMBSnlmUw7hBCmi9ySZcUA5g1DKhS3gTnDMFFKGTH/UspmYPLIDWls6fJoQlj23Hyor9c2lpT0qWGA1Ag+944xxMRiBXuWaVmMhOUwouS2FclMgasgriupM9BJV7ArqQxDLFdSQ1dDZL9UwIxhCAkhSo0nQoghr/2EEHlCiOeEEPuFEPuEEGcLIQqEEG8JIQ7p9+aaxg4zPl1y25qbE71iaNNXDG4tKykzE1zJ3dYVSLRZj/meDMavK7O9fzW57WmaAVIkNUb1cyySpYbBoNBVGDMrKR0Nw/3Ae0KIx4UQjwObgG8P8bw/BV6XUi4AlqLJbNwHvC2lnIvWU/q+IZ5jUAS82h+0JSenZ8VQXJySVc8AmbZMLMJiUno7J6Hgc0ItG5OghkFhDjOupIRXlGNMUUYRXr+X7lB3v683dTWR58zDbh2fLUoTJa5hkFL+CU0n6UXgJWC1lPK1wZ5QCJEDnA88ph+/W3dVXY3WRhT9/prBnmMohLxanr41J/cMV1IdZOSDPSOlDIMQgmx79rA36/H4PIl3blOGISXId+bHlY9IthWDsRIYyOA1dDakzGoBzGUlAfiAE4ALmCOEmCOl/MsgzzkLaEBrEboUqALuBkqklHUAUso6IUS/Yi9CiK8AXwEoKSkZtEJle3t7v+/1NWoukK379zFj61Ymu1y8u3UrZcd24bLksrWykmPHluF0hqms3Dmoc48UA80pHraQjUMfH6KyK/Z7yztDWNtq2G7iHEdOHcGCxdR4bIF21vhaOOwJUdNr/97zyc3Npa0tsX4Q45FQKDRu5+Hz+RL+++nvb67F20JHoIO3Nr6FXfT/C/r9tvcBOLjjII02c/2UR4v+5lTXqbmS39j8BtOdfbW8jtYfxWlxDrti7nAwqOuClDLmDbgd+AhoAd5FMxKV8d4X43grgSBwlv78p8C/Ai1n7Ncc71grVqyQg2Xjxo39br//7hVy7/wFMtjcLOXnPy/lzJnaC784T8rfXCellHL+fClvvHHQpx4xBppTPK5/6Xr5tbe/Fn/HZ74g5c9Wmzrmlc9fKe+tvNfcAGq3S/nPOVLufSlqc+/57N2719yxxiEbN26UgHzppZdka2urlFLKK664YtDfV38cP35crl27VlZUVMglS5bIP/3pTwkfYzCfcX9z+MOBP8iyJ8pkXXvdgO/75a5fyrInymRnoDPhc440/c1pT8MeWfZEmdx4ou9rUkq57tl18r5N943swAZJf/MBtsoY11YzMYa/1y/m1VLK89AqnusSMz9R1AA1UsoP9OfPAcuBeiHEJAD9ftR1g8MyjK1dU8u0uN2aK8lQqWw9CTlaMlaq9GIwyLZnJ9Csx3y6asKpqinsSjJkt0eK733ve9x4441s376dZ555hr/9278dsXPFo8DZo5c0EE1dTWTatMKxZMCIhfSXmSSlpLGrMWVSVcFc8NknpewCEEI4pJQfAQsGe0Ip5SngYyGEXk7MRcBetPiFUdlzC1pMY1Rp624j0yc1yW2rVTMMJSUQ7IaOBsiZTDhMynRvMzDdrMdk8Lk71E1boC3x4ra88Sm3bTCeZbeFELS2at+N1+tl8uSxyyg3MtFiGYZkqmGAaIXVM2ntbtXkX5IkkG4GMzGGOr3A7WXgDSGEB6gf4nm/DjwthHAAR4Hb0IzUH4QQX0KLZ4y6aI6hrCrdmdqG+nr4xCd6UlVzJuP1au0ZUs0wmFsxuKG7TfsALAP/pogEFs3KYTRXQ2Zhj+xGHP5jy3+w37Pf3LFNsqBgAd9a/a24+41X2e0HHniAdevW8fDDD9PR0cGGDRvMTHtE6C29PRAeX3LoJBnYrXbynHn9GgYjNTuVVgxm1FWv0h9+VwhxEZALDKnyWUq5A809dSYXDeW4Q8Xbrekk4c6GUAgaG8+oYUgtnSQDt8NNW8CkKwk04+AauEuVoZOU0IohSdxI41V22xDQu/fee3n//fe5+eab2bNnD5YYBnykMONK8vg8TM5OrjrZgaqfU62GAeIYBiGEFdgmpVwKIKV8e1RGNUZoyqoS6wS3FkgIh/tUPXuOaw9TyTAY6aphGY7dhrO3XlIsw6D/8yRU9TxlucnRYuqX/UgxXmW3H3vsMV5//XUAzj77bHw+H42NjWPSyS3HmYNVWGPWMnh8HpYULRnFUQ2dwozCflcMEcOQmSaGQUoZEkLsFUJMkVKaaAyc3Hj9XrJ8YMvNO6O4zXAlTUrZFYNE0hnojN11y6SQXkI6SaEgeD+GsmvNDnfcMR5kt6dNm8bbb7/Nrbfeyr59+/D5fEyYMDauDYuwkOvMHdCVFJZhmn3NSRVjAG1FsPN03xT1tHQlAUXAPiHE+0CkkauUMnn/kwfA6/cy3QeO/IIzitt2gD0TXHkppZNk0FsvKaZhMCm9nZCyamsNhINJ40oaDr7zne8MGGeorKzkP//zP7Hb7WRnZ/PUU0+ZOuaPfvQj7rjjDn784x8jhOCJJ54Y0wY4Ba6CAV1Jrf5WQjKUfIbBVUSTrwkpZdRn29DZgNPqJNseu5VpMmHGMIxdu6xRxujF4MorijYMu09q4m5CpOyKAaAt0MakWP2cTSqsNvmayLBlkGnPjH/yJEpVHc+y24sWLeK9995L+H0jRZ4zb0DDkGxVzwZFGUV0BbvoDHaSZc+KbG/0NVKUUTTuO9Elgpngc0rHFXrT1u7BEdSb9PQW0HsvuoZBCK3TZ6pg/NKJm5kUMQyx23smlIqYRIZBYZ58Vz6HWw73+1rCTZzGCb1rGaIMQ2djSgWewUQdgxCiTQjRqt86hRB+IYQ5JbUkw9esBZasubqAnt2u93quixgGjwfy88GaQiKgRjP2+D0ZzMcYEspIstggJzWaqCs0YrmSknnFAH1rGVKtuA3MrRjcxmMhhAW4Fk0RNeXwt2i/ZCLKqsXFICW0Ra8YUsmNBETiCsPVxS2hVEQlt52S5Lvy8fq9hMIhrGd8t6lmGBq6Glg5sb/s++QloSRnKWVYSvkccMkIjWdMCZ6prFpSAp2NWnDUnZpyGJBAsx5HNiDiBp8TXjEoN1LKke/MRyL7VVn1+DwIBHnO5PLH9mcY/CE/rd2t6bdiEEJc1eupBa0wLXWiLL0I65ICkSY9Z9QwgGYYJk4cqxGODG67yWY9Fktc6e1QOESzP4FUxOZqmLzM5EgVyYIhi9Hib+lTz+Lp8pDnzMNmMSvuPD7IdeZiFdaoIrdIqmpmmhkGoqUpgkA1Wu+E1KNNuzBaDVfSkiVRNQygGYbFi8dqgCOD3WrHZXUlIKQ38IrB2+0lLMPmitu6WqCrWa0YUhDDMHh8HmYzO+q1ZNNJMrAIi9bJrdeKIRWrnsFco56be91uk1I+qAvhpRRhGcZiKKvm6CuGqM5tWnA01QT0DLIdZhVWY7f3jFQ9m3Eltehl5CluGCorKxFC8PLLL0e2XXnllcOq3b9p0yaWL1+OzWbjueeei3rtySefZO7cucydO5cnn3xygCMML/nOgYX0kk0nqTdnVj8bj9POMAghHtNF9Izn+UKIX47ssEaftu42Mru03HJrKATd3T06ScIKWRPo7oa2ttQ0DAkJ6cVwJSXUsjGSqjrTxAiTm5GW3Z42bRpPPPEEn/vc56K2ezweHnzwQT744AO2bNnCgw8+SHNz7H7Mw4GxIhjQMCThigE0AxBlGDrT1DAAy6XWehMAKWUzWk+GlEJTVpWEMpwIo4qtpETrw+CeBBYrHl36JdWCz6DFGeLGGCCu9HZCchgRwzC+5bYNxrPs9owZMygvL+8jmvfGG29wySWXUFBQQH5+PpdccklEU2kkMRRWPf6+eklNvgR6dYwzijKKInUYoBW3CUTSzmcgzMQYLEKIXCmlF7QVA5AaHa970eJv0ZVVs6KL22pPRsUXIHVXDHHTVUFbMTQfH/DlhOQwmqshoyCmIF9/nPp//w//vuGV3XYuXMDEb3877n7jVXZ7IGpra5k6dWrkeWlpKbW1Iy97ZrfYcdvdtPiis5ICoQBt3W1JeyEtyijC0+WJCE42dDaQ78pPukB6PMzM5ifA+0KI3wMS+CzwgxEd1Rjg7dYE9ESOO1pAb99JKFkEpLZhyHZkU9tu4oIRJ/jc5GvCKqzkOE30VkjCVNXxKrs9EP1Jb4yWdEO+K7+PKylZaxgMCjMKCcogXr+XfFd+Sha3gbkCt8eFEFXAhWhpqjdJKXeP+MhGGU1ZVWIrzo02DG11MOdiILUNw3DGGApcBbHluw2aq2FShflB6pj5ZT9SjFfZ7YEoLS2NCnLX1NRE6TaNJPmu/D6upIRiUOOQ3rUMhmFIJbltAzN1DKuAfVLKXfpztxBipZRy64iPbhTx+r0U+sCep0tuWyyQbYPudsgtBYjEGFLSMJiOMeRCoFOTy7b2/fMx3es5HIKWE7DomkGMdnwxHmS3B+LSSy/l29/+diTg/Oabb/Lv//7vgzpWouS78jnZfjJqW0KuxnFIb8MwN38uDV0NzMmbM8ajGn7MBJ8fBTp7Pe8AHhmZ4YwdhrKqM69QizEUFUG7XsOQq6WqpvqKwR/y0x3qjr1jRBajf3dSU1eTuV+DrbVpJ7dt8J3vfIeampp+X6usrKSiooJly5axfv167r77blPH/PDDDyktLeXZZ5/lq1/9Kov1YpuCggK++93vsmrVKlatWsX9999PwShlT+Q7U8+V1NswhGUYT5cn5YrbwGTwWUoZNp5IKcNCiJQLPhsxBltuHuzdqwWevbrPPVcL3jU1gcMBmSbUpJON3rIYMS/svfWSMvv+c3t8Hmbkzoh/wiRUVR3PsturVq0a0Njcfvvt3H777Qkfc6jku/Jp9jdH9S9IFcPQ1NVEi7+FoAymXKoqmFsxHBNC3CWEsAohLEKIv0Orfk4p2to9OIO9lFWLi7XOYhBxJRkCeikkux7BENIzL73dd8UgpaTJZ1InKQkNgyIxClwFBMPBKBdlk68Jh8URJVudTGTaMsmwZdDY1ZiyxW1gzjB8FbgIqNdvFwB3jOSgxgJfi/YlR5RVS0rAWwMWO2RpfXNTUVnVICK9HS/OEENhtTPYiT/kN1/cpuS2UxpDFqO3O8nTpVU9J2tTGyGEJovha0zZ4jYwl5VUD1w/CmMZUwItWr61NSe3R0DPW6OJ5+lFQ6kqhwE9zXri1jIY7T37MQwJF7flTu03gK1IDQxZDI/Pw7ScaZHHyepGMjCqnxt9mmFIy3RVIYQTuBVYDLiM7VLKr4zcsEafkKGs6rBDR4de9bw3El8AbcUwf/5YjXBkMWIM8Zv1DNz3OWE5DOVGSmn6XTGkiGGobq2moTM1BfTAnCvpKWAGcCXwATAb8I3gmMYEQ3LbFghoG4qLtRWDHl+A1OzFYGC6J0OMrKSEVwzKMKQ0vaW3DVLBMBhCeo1djWTaMs31Nk8yzBiGeVLK/wu0SykfAy4DykZ2WKNLWIYRbVpGrsWn27yiIk0nSTcMUqZ2jCGyYogbYxg4+GxoyMQNPvtaobNJGYYUp7crCbTkBI/Pk7Q1DAZFGUW0+Fuo66hLyVRVMGcY9J/QtAghFgJuIDlUz0zS1t1Glk9XVjVE0XKsIEMRw9DeDoFA6hqGTFsmFmGJH2OwZ2hqs/3FGHwmVwxplpE01rLbVquViooKKioquOqqqwY4wvCTac/EZXVFXElGckKyrxgM19EBz4GkN3IDYSby95gunPfPwBtAJnD/iI5qlNGUVbXHljb9gucKavdpUPUMWrZFtj07foxBCC0APUDwOceRg90ap8wlzQwD9Mhuj5QchSG7/cMf/rDPaxkZGVHV1aOJUcsAWkYSkLS9GAwMw1DTXsPiohTr2qVjplHPI1LKZinlRinlNCllkZTyf0ZjcKOFpqwqCWc4EY261rq9Q7vvVcMAqWsYIEG9pAGCz4nJbc9IaHxjTTLKbo81ec68iCvJ9IpynNM72JyKgWcwt2JIeSLKqu5srYYhLw86dSG9nNSXwzBwO9y0Bcy29+x/xWA6I8mVBxmDawZ/zz0w3D+AKyrgJz+Jv1+yyW4D+Hw+Vq5cic1m47777uOaa0ZPn6rAVRCR3k72qmcDZRjSBE1ZtZ/iNlduJG/fMAypmpUEWi3DUPo+e3we5uXPi//+JM5ISjbZbYATJ04wefJkjh49yoUXXsiSJUuYPXt2/DcOA/mufKpbq4HUMQy9x5+2hkEIYZNSBuNtS2Za/C265HYe1NT2Km6LTlWF1F8xnKmG2S9ON7T3bfttujNXczVMXJL4AHXM/LIfKZJNdhtg8uTJAMyaNYu1a9eyffv2UTUMhkFIFcPgsDrIdebi9XtTsrgNzK0YtgDLTWxLWlr9rRT5wJGXD1XboLwcWqNrGFK5raeB6RiDKweaDkVtMjpzxXUlGXLbCz89hJGOL8az7HZzczOZmZk4nU4aGxt57733+OY3vzmoYw2GAlcBXcEufEEfHp8Ht92Nw+oYtfOPFEWuIrx+b8quGAaMVAkhioUQS4EMIcQSIUS5fluDlpmUMni7vWT7hKasGhHQ61vclpMD9pTTle1hKMFn04HF1pMQDiStK2k4GE3Z7X379rFy5UqWLl3KJz/5Se677z4WLVo0bHOJR55TiyM1+5ojOkmpgGEQUtUwxFoxXAHcDpQC/43WvQ2gDfjuCI9rVDFiDNbMTGhpgcJ86GruYxhS2Y0EWoyhPdAe6Wc7IP10cTMth5GkGUmQnLLb55xzDrt3j13DxYgshr85JaqeDQozCrEKa2R+qcaAhkFK+TjwuBDiRinlH0ZxTKNOa4cHZ0Bit+i2L8eumb80kcMwcDvcSCQdgY5IJXS/OHMg5IegH2yaz92Qw4hb8GMYhoKZwzBixXjHMATNvmaafE1Mz0mN2thzJp9DSIbMtbBNQszMqlgIkQMghPiFEGKLEOKiER7XqBJo0QpwrGG9H5EhFZ9mK4aEhfR6rRpMy2E0V2uV070C+4rUpbcsRiqtGK6eczU/vKBvMWGqYMYwfEVK2SqEWIfmVroL+MFQT6w3/tkuhHhFfz5TCPGBEOKQEOL3QohRi1AFvFqetS2gJ1q5dBWQM4LP6WIYzEtv9+wXyTiJ50NuroY8JbedLhiuFo/PQ4u/JWUMQ6pjxjAYztDLgcellFUm3xePu4F9vZ7/B/BjKeVcoBn40jCcwxShVi8AVr+ui2HrBGEB96TIPumwYjB6Mphu1tMrAN3U1YTL6iLTFicvIYlrGBSJ43a4sQorx7zHCMuwMgxJgpkL/E4hxKvAp4HXhBDZ9BiLQSGEKEULbv9Kfy6ACwFD/etJYFTKM8MyDK2a/EVEQM/q1YyCrvkTCukx6RQ3DEYXN/PS2z37eXweCjMK43fmUoYhrbAIC3nOPI60HAGSXycpXTCznr8NWAEcllJ2CiGKGPqv+Z8A30RTagUoBFp6Fc3VAP32fBRCfAX4CkBJScmgFSrb29uprKykM9QZUVatP3yYSS4XrfX7sZDNdv3YXq8dKc/F4zlEZWXtoM43GhhzGiynA6cB+HDnh3Bk4P2y246wEthd9ReajocAOFx/GFvYFvP81mAH53U2cqQ5zMcmxtl7Prm5ubS1mUilHeeEQqFxOw+fz5fw34+ZvzlHyMGBRq047+P9H1NZndg5Rpuh/h+NNwY1Hyll3BvwWeA7+uOpwAoz7xvgWFcC/6M/Xgu8AkxAMzz0OsfueMdasWKFHCwbN26UUkp53Htc3vu1RXLv/AUydN11Us6aJeVPK6T8w62RfffvlxKk/O1vB326UcGY02Bp6mqSZU+Uyaf3Ph17x8bDUv5zjpQ7fhfZdN2L18mvbfha7Ped2KK9b9+fTI2n93z27t1r6j3jndbWVlP7bdy4UV5xxRUJHfuCCy6QH3744WCGJaUc3Gds5m/u9tdvl2VPlMmyJ8rk4ebDgxjZ6DLU/6PxRn/zAbbKGNfWuK4kIcTPgE8CX9A3dQC/SMz8RHEucJUQohp4Bs2F9BMgTwhhrGBKARPaDEPH6/eSrYcWRHOzXtxW2ycjCVLfleS2m2zW48rV7nvHGHxN8d0EjQe1+wkp2h9V0S+9c/1VjCE5MBNjOEdK+VX0dp5SSg8w6IwhKeX/lVKWSilnoK1E3pFSfh7YCFyv73YL8OJgz5EImrKqRGY4EadPQ2GelqPfq9dzqvdiMLBb7bisroTbe4ZlmGZfc/xU1cYDYHVAXnLmsldXV1NW1tO88Ic//CEPPPAAa9eu5Vvf+harV69m3rx5EZG8UCjEP/7jP7JkyRLKy8t5+OGHAXj77bdZtmwZS5Ys4fbbb8fv9wPw+uuvs2DBAtasWcPzzz8fOU9HRwe33347q1atYtmyZbz4ovav0dXVxWc/+1nKy8u56aabTEuAjzZG9bNFWMh15o7xaBRmMBNjCAghLOgBZyFEIRAegbF8C3hGCPE9YDvw2Aicow9G1bNwu+HAQVg6V3shtyfEkS4rBoBshwmFVZtTu8DrwWev30tIhuL/Gmw4CAWzh56qOpa62wMQDAbZsmULr776Kg8++CAbNmzg0Ucf5dixY2zfvh2bzYbH48Hn83Hrrbfy9ttvM2/ePL74xS/y85//nDvvvJM77riDd955hzlz5nDTTTdFjv1v//ZvXHjhhfz617+mpaWF1atXc/HFF/PII4+QmZnJrl272LVrF8uXj0/5MuPvIt+Zn7IFYalGLK0k47/3v4H1wAQhxIPAZrTU0iEjpayUUl6pPz4qpVwtpZwjpbxBSukfjnPEQ2vSA7acHGhsBLcuhpSGriRIRC+pR3o7UvUcTw6j8SBMMCHLnYRce+21QI8UN8CGDRu48847I0qqBQUFHDp0iJkzZzJvnvY53HLLLWzatIn9+/czc+ZM5s6dixCCL3zhC5Fjv/nmm3z/+9+noqKCtWvX4vP5OHHiBJs2bYrsV15eTnl5+SjO2DyGK0llJCUPsX66bQGWSymfEkJUARej6SXdIKXcE+N9SUWrv5Vin8SZ44JwGDL1TNxerqSmJrBaNRG9VMdtd8ePMUCUXpIpOeWgH5qPQdl1Qx/kGOlu22w2wuGexbLP54s8NuS4rVYrwaCWXCel7JO+KwfQSAIGTPWVUrJ+/Xrmz+8bm4mbHjwOiBgGFV9IGmKt6yJ/cVLKj6SUP5VS/iSVjAJ3oxykAAAgAElEQVRoMQa334rDpq8UXAGwZ0JGT8DM0ElKgv/BIZOQ9LYefDYlh9F0BGQYipJ3xVBSUsLp06dpamrC7/fzyiuvxNx/3bp1/OIXv4gYCo/Hw7x586iurubw4cMA/OY3v+GCCy5gwYIFHDt2jCNHtDzh3/3ud5HjXHrppTz88MMRo7J9+3ZA6/dg9IHYs2cPu3btGt4JDxMFTs0gKMOQPMRaMUwQQvzDQC9KKR8agfGMOi3+FrL9YDME9OwdmhuplxVIBzkMA7fDTW27iVqNXu09TclhRDKSktcw2O127r//fs466yxmzpzJggULYu7/5S9/mYMHD1JeXo7dbueOO+7glltu4fHHH+eGG24gGAyyatUq7rzzTpxOJ48++ihXXHEFRUVFrFmzJqLk+t3vfpd77rmH8vJypJTMmDGDV155hbvuuovbbruN8vJyKioqWL169Wh8DAmT59KCz3GTExTjhliGwQpk02vlkIp4/V6yuiR2w0Vg8Ub6PBukgxyGgangM2iupJaPATjRegKX1RXJPukXwzAUzh2GUY4d3/jGN/jGN74x4OtFRUWRGIPNZuOhhx7ioYd6fkO1tbVx0UUXRX719+ayyy5j//79fbZnZGTwyCOP9Lv9mWeeGcQsRhdjpaBWDMlDLMNQJ6X8l1EbyRjR3unF0R3Gqi/3EY2Quypqn6YmmJ6cGZYJ43aYjTHkgF/TmNp2ehtLJyyNnXHScAByp4EjpXo8KUxQ6Crkm6u+yUXTUkqUOaUxFWNIZbpbNDeI1d8NDgcEG6ICz5AevRgM3HY3/pCf7lB37B314LPX7+WA5wArJq6IvX8KZyQpYiOE4OZFNzM5e/JYD0VhkliGIS3Me9BQVvV1QZEeYc6N7hWQTq4kQ3o7rjtJDz7vOL0diWRlycqB9w2HofEQFKmKZ4UiGRjQMOgVzimNpqyqXQAt7e1QoFdl9ipu6+rSbuliGLIdmvS2qepnGaLq5AfYLDaWFC0ZeF/vxxDsgqKhxRdipXoqhob6bBW9SesyxLbuNjJ1ZVWL1wt5GdoLaSiHYWBIb5vtyVBVv5UlRUtw2VwD79t4SLsfgkaSy+WiqalJXcBGACklTU1NuFwxvkNFWpHWbbQMOQwA0dICM3QXUk6PLzSdqp6hp1lP3C5uzlw6heCjloPcXhZHhV2XXB6KK6m0tJSamhoaGhoGfYzxgM/nG5cXYJfLRWmpareq0Eh7w5DtA6TULEDmZMiaAPaMyD7pZhhMxxicbnY4nYRkmBUlcQLPDQcgsxCyBv8h2u12Zs6cOej3jxcqKytZtmzZWA9DoYhJWruSNGVVsITDiEBAq3rup4YB0igrSTcM7d3xpLdzqHI5sWKhorgi9r6Nh5K64lmhSDfS2jC0+FvI8klsVv1jMKqee5FuMYZEVgxbM5wszJpElj0r9r6NB5RhUCiSiLQ2DEaMwe7Q20tYvf3WMED6GIZMWyYWYaEtENsw+G1OdjudrMjotwNrDx1N0NmkmvMoFElEWhuGVn8rWT5w2HUBPYev3xqGzEwYh/HCEUEIQbY9vizG7o5aAkKwwmmiOQ+oFYNCkUSktWHwdnvJ9VuxW/SPITu9i9sM3A533BhDVfN+hJQst7hjH8zQSFKGQaFIGtLaMLT4W3D7BTYpwWKBjP4NQ7oEng3MSG9vPb2NuYEQuQFfzP1oOAi2jD4uOoVCMX5Ja8NgxBhswSDkZYOlr2FIJ8ltg2x7dswYQyAcYGfDTlaERER6e0AaD0DRHM3wKhSKpCCt/1tb/a1k+MJYu7sh1wUWO2QVR+2Trq6kWCuGfU376Ap2sUI6TRiGg0ojSaFIMtLbMHQ24/SFsHZ1QbZN00g645dtuhqGWDGGqvoqAFZYe/o+90t3p9azQWUkKRRJRVobhkBrCwCWjg6t13NO3/hCQwNMTTP3eLwVw9b6rczImUGRMzf2iqHpECCHLJ6nUChGl7Q1DGEZJmwoq7a2gqu7T3yhslK7X7t2dMc21mTbs2kPtGvqs2cQCofYXr9dk8Ho1fe5XwzxPOVKUiiSirQ1DF3hLjJ9EhEOI/x+sHf2MQzvvANZWbBq1QAHSVHcDjcSSUego89rh1oO0RZo0wxDr77P/dJwAIQFCmeP4GgVCsVwk7aGoTPcSbZPahlJAFmyX8Nw/vlg1L+lC7H0koz4wqqJqyJd3Aak8QDkzwCbcySGqVAoRoi0NQwd4Q4tVTUU0jZkiahc+5MnYf9+uPDCMRrgGGIYhv6kt7ee2sqU7ClMzJqorxha6fR3s6+uH5eS6tqmUCQlaWsYOsOdZPnAaqwYsi1Rnds2btTu09EwGD0ZzmzWI6Wkqr6qR2bb6QYkP3l1B1f9bDOejl59okNBaDo86D7PwVCYZl/fGIdCoRh50tYwdIQ6yPaBLWS4kkSU5PY770B+PixdOkYDHEOMLm5nZiYd8x6j2d/c09/Zpe1XuesIgZBkw776np1bjkOoe9BSGP/19iHu/XMXv9x0VHVtUyhGmbRt1KOtGGTPB1CQF7nQgWYY1q4Fq3UsRje2DNT3eWv9VoAzVgwQ9rXisObx5kenuHGl7o6LaCQl7kqSUvLHHSexCfi3V/exo6aFH1xXTpZz8H+uUkpe2F7LW3vryXLayM2wR91yMmy9HtvJcdlx2dPwy1coSGPD0BHuoNAHdosVshxQOC3y2rFjUF0N9947duMbSwbqybC1fisTMiYw1a1f/J2aIZ2eFWJN+TT+d8sJOvxB7QLeYKiqJl7D8NHJVk54OrmtzEHJ1Fn84PX9HKpv45GbVzKzKE7vh3742NPJt1/YzbuHGpmc60IC3q4And2hmO+7cEEx966bx+LJuQmfU6FIZtLWMHSGO8nz27DJMGRb+7iRID3jCwBuu56V1CvG0Du+IIQAwBNyUQBcOieTaWUTeeIv1fz5YAOfWjJJWzFkl0BGXsLnf21PHVaLYEWxjSsvmE3Z5Fy+/rttXPXwZn58UwUXLyoxdZxQWPL4e8f40ZsHsQj416sX8/mzpmOxaOPvDoZp8wXwdkXfWrsC1Lb4+N2WE1zxX5u5onwS/3DJPGZPyE54LgpFMpK2hqEj3EFOt1XLSsoOR6WqvvMOlJTAwoVjOMAEOVjfRldweHzxdqsdl9UVtWKoaa/hdOfpnvgC8NaRTm4CLpjupGhGAQVZDt746FSPYRhEfEFKyWu7T3H2rEKyHV0ArJlbxMtfX8Ndv93Gl5/ayjcumss9F82NXOD7Y19dK/et38XOGi8XLijme9eUMTkvI2ofh81CYbaTwuz+02nvWjubX717lMc2H+O13XVct7yUuy+eS2l+ZsLzUiiSibQNPneGOsn2oQnoZYYihkFKzTBceCGIga87g2ZPrZe///0O3jvcOGzH9HR0c+XDm3n2YHf8nU1ypizG1lPR8QUpJS/s1VJUSxzdWC2CixcW887+03QHQprc9iA0kg7Wt3O0sYPLl0yM2l6an8mzd57NDStK+a+3D/GlJz/E2xno835fIMQP3zjApx/eTE1zF//1N8t47JaVfYyCGXIz7Ny7bj6bvvlJbjt3Ji/uPMknf1jJP7+4h9NtceTGFYokJm1XDEbw2errgsyeGob9++HUqeF3Ix0+3c6P3zrIn3bXAZrf+9w5RcNy7Fd2naQ7GOaDujDdwTAO29DtfbYjuotbVX0Vec48ZuXNAmDHxy181AS4iBS5Xbp4In/YWkPV3v2c7fcOasXw6u46hIB1iybyUdWxqNdcdis/uL6ciml5PPDSR3z6Z5t55OYVLJykxTq2HPNw3/O7ONrQwbXLp/DdKxaRn+UY3AfQi6JsJ9+9chFfPm8m//X2YZ7+4AS/3/oxt54zkzsvmEVe5tDPkUxIKQmGJf5gGK9fcqyxgzZfgDZfUL9pj9v9PY/b/EFCIYlEIiVItB9hUkr9sezZ1us80efV74neLhCRH3FCCATajzpxxnP0/QRgEfpjob2fXvs3nPbxfN32nmPq243HvR8IEvv1GH3M6GP0zMHcsa6umMInZo2MwmfaGoaOcAcZ7QEs/m7IdkZqGIY7vlDT3MlPNxxi/bYaMuxWvnHRXELhMP+98QjVjR3MGEQw9UzWb6slw26lIxBi08EG0z74WLgd7qgYQ1V9FcuLl2MRmtF5tqqGkF13qeiG4dw5RWQ5rHy0aytnw6AMw2t76lg9o4AJ7v7dO0IIPn/WdBZOyuGu31bxmf95j3+5qoydNS08/cEJSvMzeOr21Zw/b0LC547HpNwM/v3aJdx5wSx+suEQj2w6wtN/Pc7fnDWNhZPczCzKZmZhFrmZI18qL6WkpTNAY7sfT0c3zZ3dNHcG8HR009LZjacjoN13dtOib+/sDmKzWLBbBXarBZt+r92E9prNgt0iCIQl3cEw/mBIvw/jD4ToDmmPo67ZGysHHGeG3Uq2y4bbacNmFVEX8TMvzsZFG9Fzue19IdWeR1+g+zUqUc91QyR79gUISxm1TeoHk0BnZ5hT3S29jm+cS+qfPVH3iRI5X4xjm2Hl9JHrIJa2hqEz2E5Gh197ktXToOedd2D6dJg5c2jHP93m4382HuHpD44jhOD2c2dy19rZFGY7OeX18fPKI6zfVsO964ZWGXykoZ2dH7fwzcvm8/O3D/DCjtrhMQx2d6Ty+VTHKWraa/jcws8Bmrvm5Z0nubRsChxxR6S3XXYra+cX03Rkg3aQBF1Jh0+3cbC+nQevWhx33+XT8nnl6+fxd/+7jW+u34VFwJfWzOTedfPIdIzsn/X0wix+fFMFd14wm4feOsCv3j1KuNc/dEGWgxmFmcwoymJWURYzirKYWZTFjMIsgmFJV7d2gQ2GwgT1C3AwLAmGwvp2SUd3kMb2bhrb/DS0+2ls89PYbjzuprHdTzDc/1Ukw26lIMtBXqadgiwHpfmZFGTayXTaCIbCBEKSgH6eQFh7rm3veS3TasFp024OmwWnzRp53nvbieojrFiyiGynDbfLRrbLRo7LjttlI8tpw25NPm91ZWUla9NNOfMMRt0wCCGmAk8BE4Ew8KiU8qdCiALg98AMoBq4UUrZPBJjCMswls6unuK2bCu4JxEOa4qqV189+PiCtzPAI5uO8Ph71XSHwty4cirfuGgOk3J7fNwTc12cO6eI57fV8vcXz4sZRI3HC9tqsQi4fnkpVXuPsGFvPW2+AG7X0H61uh1uattrgV79F/T4whsfnaLNF+SGFaVQ447qybBucQnN+44TysjG6p6U0Dlf230KgMvKJsbZU2OC28nTXz6Lp/96nGXT8lk6NfEMqKEwf6KbR25eiT8Y4mNPJ8caO6lu7OBoYwfVjR385XATz2+r7fvGN19P6Dw2i6Ao20mR20FRtpOFE3Mocju1bdkOCrOc5GdpRiA/0zGq9ReV4ROsXTYl/o6KpGIsVgxB4F4p5TYhhBuoEkK8BdwKvC2l/L4Q4j7gPuBbIzGAtu42Mn0Sa1DPYy+eAFY7u3ZorTwH40byBUI8tvkYv/jzEdr9Qa5aOpl7Lp43YN799StKufuZHfz1WBPnzB5crCEc1oq21sydQHGOi7Mn2Xj7hI83Pqrn+hWl8Q8Qg94xhqr6KrLt2czP11YAz1XVMCUvQ/NvniG9/ckFxey2nKTeMY3JCVrX1/acYsX0fEpyXKbfY7dauPXcIS7vhojTZmVOsZs5xe4+r3V2B6lu7KS6qYPqpg4OHznKvDmzsVkEDpsFm0Vz6TiiXDsCl93KhGzt4p+bYR/SjweFIlFG3TBIKeuAOv1xmxBiHzAFuBpYq+/2JFDJCBkGr98bLYcxqceNBPDJTyZ+zIfeOsijm45y8cJi7l03PxIQHYhLF0/E7bSxvqp20IZhS7WH2pYu/s+l2gV7dp6FaQWZ/HF77ZANQ++spKr6KpYVL8NqsXKypYvNhxv5+oV6uugZCqs5LjsL7afY4l/CJCkjPuF4VDd2sLeulX+6IolyhE2Q6bCxaHIOiybr8iHUsPYCJUOuGN+MaYxBCDEDWAZ8AJToRgMpZZ0QoniA93wF+ApASUkJlUY3nQSo9ldrchj6iuG0M4u9lZU8++wSpk7N4NChLRw6ZP54XUHJU+91ctZEK1+Y3kH9gW3UH4j/vmUT4JWdNVxS4MFlS/wX4a/3+HFZIaPpIJWVh+jo6KAi38HLhxt54fV3yHcN3r/b4G2gO9zNCxte4Kj3KGWijMrKSl4+0o2UUBqoobLyJOUdAWzeGrbp34M12Ml5oUZ2BkpoeGUjU93mxvCno1qqbX57NZWVJwBob28f1Pc7nkm1OaXafCD15jSY+YyZYRBCZAPrgXuklK1mf1lKKR8FHgVYuXKlHEyQ6N2ad/nLO2ANBZF2QfHiVeSfu5Y9e+Dmm0k48PSrd4/iC+3jO9d/gvJS837urBkeNv3ifdrz53JZgr/wfYEQX9+4gSuWlnLpxZrSX2VlJXdfs5KXfvRnmrJm8JnzZiV0zN6c2n+KVz54he7SbqiFG86+gaUTlvLAh5WcNdPNjZ86W9vx9Aw4vbfnM6utgs1whEm4MqZy81pzkhgP7dnM0lK47vI1kW2pGARMtTml2nwg9eY0mPmMScqAEMKOZhSellI+r2+uF0JM0l+fBJweqfN7u3VXUjAU6cNQVQXt7YnHF4KhMI+/V83qGQUJGQWAldPzmV6YyfqqmsROCry1t542f5Brl0cH/mZPyGbJlFz+uKOfoGcCGEJ6G2s24rK6WFy4mK3Hm6lu6ox2U53ZrKdBE89zTVzEGx+dMnWumuZOdtV4uXxJYsFqhUIxMoy6YRDa0uAxYJ+U8qFeL70E3KI/vgV4caTG4PV79SY9wUiqqhFfSPSHwmt7TlHb0sWXz0s8ACqE4Lrlpbx/tIma5s6E3vvC9lom5br6LXC5ZtkU9tS2cvh0jO5qcTCkt7fUbWFp8VLsVjvPba0h02HVJC8MXLnRfZ8bD4DFTvmSpeyta+VjT/x5vb5HMyCXm8xGUigUI8tYrBjOBW4GLhRC7NBvnwK+D1wihDgEXKI/HxEKXYWUBvKwhkIIt4DcKbzzjtZ7oSiBOLCUkl+9e5QZhZlcvHBwtQOf0VP9XugvrXEAGtr8/PlgA1dXTMHaT7bKp5dOwiLgj9tPDmpM0NOsJxAOsKJkBZ3dQV7ZdZJPLZkULX/tdEOgA8J6hlfjISiYxbolWiW5mVXDq7vrWDw5h+mFQy/2UygUQ2fUDYOUcrOUUkgpy6WUFfrtVSllk5TyIinlXP3eM1JjuGzmZawUCzUBvSyBzzmV995L3I30YXUzO2u8fGnNzEGnE04tyOQTswpYv63GdEOal3eeJBSWfdxIBsVurU7ixZ21g25yY0hvA6wsWcnre07R0R3Sahd6o/dkiNQyNByACfOYVpjJgolu3vyonljUebvYdqJFrRYUinFE8pUlDhOW9natrWeOg7/uzMfnS9ww/Ordo+Rl2rl+xdT4O8fg+hVTqW7qpOq4uXq+F7bXUjYlh3klffPmDa6pmMLHni62nRhcjaBhGOwWO0uKlvDs1hqmFWSyeuYZZfh6Twb8bRDsBs/RSHOeSxdP5MPjHhrb/QOe5w3DjaTiCwrFuCFtDYO9pUXTWykq5J2NAqsVzj/f/PuPNXbw1r56vnDWdDIcQ6s0vbxsIpkOK+u3xQ9CH6pvY3etl88si53FdGnZRFx2Cy9sH1wQ2jAMS4qW0NAa5v2jTVy/orRvXUJkxdAGzcdAhiIaSZcunoiUsGHvwKuGV/ecYn6JW/U6UCjGEWlrGBxer/agpIR33oGVKyEndk1aFL/efAy7xcIXz54+5LFkOW1cVjaRV3bW4QvE7ir2/PZarBbBVUsnx9wv22nj4oUl/GlXHYFQOOExZdoyyXPmce6Uc1m/rQYh4Lr+UmqNdqi+1p6ubRM0w7BwkpupBRkDxhlOt/n4sNrTR2JboVCMLWlrGOxtWsZOZ+EMPvggMTdSS2c3z1Z9zFUVkylOQL4hFtcvL6XNH4wZrA2HJX/cXsv5c4sGVB/tzWeWTaG5M8Cmgw0Jj0cIwUvXvMSti25j/bYazpldyJT+ehr0diU16oahcG7kGJcumsh7h5to8/XtnfDGR/VICZeXKTeSQjGeSF/D0K5JSm/tOIdgMDHD8PQHJ/AFwoNKUR2IT8zSLrzrY2Qn/fVoE3VeH59Zbq4Y7vx5E8jPtA/anZTvyqfqeCsfe7q4YaA4SsQwtGoZSTml4OxxC11aNpHuUJjKA32N02u765g1IYt5JcqNpFCMJ9LSMMhQCHuX1jZyQ83ZOBxwzjnm3usPhnjiL9WcN7eIBRMT8D3FwWIRXLt8CpsPNVDf2n93sPXbanE7bawzKattt1q4onwSG/bV0+4PDmpcz1Z9jNtp49LFA7h7emcl6RlJvVk+LZ+ibEeflVBTu58Pjnn4VNkk03pKCoVidEhLwxBua9PkMCzw6u6FnH02ZJps4/vyzjoa2vx8eQhyEwNx7fJSwpJ+f+F3dYd4fU8dly+ZmJCs8meWTcEXCEeyfxKh3R/ktd2nuHLppIED7IZh8Hm1FUNRdA8Gq0VwyaISKg804A/2xE/e2ltPKCxNS2wrFIrRIy0NQ6i1FVswSMhlY9tet2k3klHQNr/Ezflzh6ctZ29mFmWxYno+z1X1rWl4c69WRxAvG+lMlk/LZ2pBxqAkMl7dVUdXIBRbqdWRBcKirRYCHVDUVxtp3eKJtPuD/OVwU8+x95xiWkEmiycP36pLoVAMD+lpGLyt2EIhvLZcpBSmDcPmw43sP9XGl86bOWLuj+tXlHL4dDu7arxR29dvq2VKXgZnnVlHEAchBFcvncJ7hxsTamAfCkue+fAEsyZksXxafqwTaKuGmq3a8366tp0zu5Bspy3iTvJ2BvjL4UYuXzJRuZEUinFIehqGVi+2YJCTlJKZCatXm3vfr949RlG2k6srYqeKDoUryifhtFmiahpOt/rYfKiBa5ZNHlSF9TXLJhOWmhvMDHXeLj73y7+y7UQLX/zE9PgXb2cuNOk65UV9DYPTZuWTC4oj7qO39tUTDEs+pbKRFIpxSVoahnBrK9ZQiEPdcznvPHA44r/nYH0bfz7YwC1nT8dpG7nWiTkuO+sWT+SlnScjPvkXd5wkLEnYjWQwp9hN2ZQc/mgiO+mNj05x+U/fZXetl/+8vpxbzpkR/wRGnMGVB1n9u9guXVxCU0c3VcebeW13HVPyMigvzU1gFgqFYrRIS8MQavFiDYY44Jtj2o30q3eP4rJb+Pwnhl7QFo/rlk+hpTPAxv2a8vjz22tZWprLnOLBp3VeUzGF3bVejjS09/u6LxDiOy/s5qu/qWJqfiavfH0NN6ycas7VYxiGCfMHbJa9dn4xDpuF9VU1vHuokcvKlBtJoRivpKVhCJ+qw4KknhJThqGhzc8ft5/kuuWlFGSZWF4MkfPmTqDY7eS5qhr21bWyr641osI6WK5aOhmLgBf7WTXsP9XKVT/bzNMfnOAr589i/V3nMCsRiQqj+rlo3oC7ZDttrJlTxO+3fkx3KMynVLWzQjFuSUvDkLdK63jW5ixg2bL4+//m/WoC4TBfWjM6TeetFsFnlk+h8kADv3z3KDaL4NNxJDDiUZzj4pzZRfxxx8lIxpOUkqfer+aqn72HpyPAU7ev5tufWojDluCfRe8VQwyM+ouSHCfLpsYIaCsUijElLQ2D9fRxACYvzMAaJ1zQ1R3iN389zkULShL7FT1Erl9eSjAseX5bLWvnT6AwO74ERjyuWTaFE55Otp1owdPRzR1PVXH/ix9x7uxCXr/nPM6fN2FwB3bGXzEAXLyoBJtFcHnZpEHLlCsUipFnzHo+jyUNO6qZAMw/J34rzue319DcGeCOYZS/MMPcEjflpbnsqomvpGqWSxeX8J0XLDz01gEOn26nuSPA/Vcu4rZzZwzN32+sGOIYhqJsJy/87bnMKDJZTahQKMaEtDQMH26s51PAy9LLkw/9Oea+dV4fS6bk9u1DMArccd4sHt10lIsWFg/L8dwuOxcv0hRXZ0/I4te3rmLx5GHIDJq8DCYthbxpcXddojKRFIpxT1oahsCUUt7NPo+is6ZSaIv9EcwrcQ/9F/Ug+fTSyUOOLZzJ/1k3n7LJudxyznQyHcP09Zddq90UCkVKkJaG4epn/onKyjX899qzxnooo86MoizuWjt7rIehUCjGMWkZfFYoFArFwCjDoFAoFIoolGFQKBQKRRTKMCgUCoUiCmUYFAqFQhGFMgwKhUKhiEIZBoVCoVBEoQyDQqFQKKIQZ/YWTiaEEA3A8UG+vQhoHMbhjAdSbU6pNh9IvTml2nwg9ebU33ymSykHVM1MasMwFIQQW6WUK8d6HMNJqs0p1eYDqTenVJsPpN6cBjMf5UpSKBQKRRTKMCgUCoUiinQ2DI+O9QBGgFSbU6rNB1JvTqk2H0i9OSU8n7SNMSgUCoWif9J5xaBQKBSKflCGQaFQKBRRpKVhEEJcJoQ4IIQ4LIS4b6zHM1SEENVCiN1CiB1CiK1jPZ7BIIT4tRDitBBiT69tBUKIt4QQh/T7/LEcYyIMMJ8HhBC1+ve0QwjxqbEcY6IIIaYKITYKIfYJIT4SQtytb0/K7ynGfJL2exJCuIQQW4QQO/U5PahvnymE+ED/jn4vhHDEPE66xRiEEFbgIHAJUAN8CPyNlHLvmA5sCAghqoGVUsqkLcoRQpwPtANPSSnL9G0/ADxSyu/rBjxfSvmtsRynWQaYzwNAu5Tyh2M5tsEihJgETJJSbhNCuIEq4BrgVpLwe4oxnxtJ0u9JaD2Is6SU7UIIO7AZuBv4B+B5KeUzQohfADullD8f6DjpuGJYDRyWUh6VUnYDzwBXj/GY0h4p5SbAc8bmq4En9cdPov3TJgUDzCepkVLWSSm36Y/bgH3AFJL0e4oxn6RFarTrT+36TQIXAs/p2+N+R+loGKYAH/d6XkOS/zGgfYctv9YAAAQkSURBVPFvCiGqhBBfGevBDCMlUso60P6JgeIxHs9w8DUhxC7d1ZQULpf+EELMAJYBH5AC39MZ84Ek/p6EEFYhxA7gNPAWcARokVIG9V3iXvPS0TCIfrYluz/tXCnlcuBy4O90N4Zi/PFzYDZQAdQBPxrb4QwOIUQ2sB64R0rZOtbjGSr9zCepvycpZUhKWQGUonlIFva3W6xjpKNhqAGm9npeCpwco7EMC1LKk/r9aeAFtD+GVKBe9wMb/uDTYzyeISGlrNf/acPAL0nC70n3W68HnpZSPq9vTtrvqb/5pML3BCClbAEqgU8AeUIIm/5S3GteOhqGD4G5epTeAXwWeGmMxzRohBBZeuAMIUQWsA7YE/tdScNLwC3641uAF8dwLEPGuHjqfIYk+570wOZjwD4p5UO9XkrK72mg+STz9ySEmCCEyNMfZwAXo8VONgLX67vF/Y7SLisJQE8/+wlgBX4tpfy3MR7SoBFCzEJbJQDYgP9NxvkIIX4HrEWTCK4H/hn4I/AHYBpwArhBSpkUAd0B5rMWzT0hgWrgq4ZvPhkQQqwB3gV2A2F987fR/PJJ9z3FmM/fkKTfkxCiHC24bEX74f8HKeW/6NeJZ4ACYDvwBSmlf8DjpKNhUCgUCsXApKMrSaFQKBQxUIZBoVAoFFEow6BQKBSKKJRhUCgUCkUUyjAoFAqFIgplGBSKUUQIsVYI8cpYj0OhiIUyDAqFQqGIQhkGhaIfhBBf0HXtdwghHtGFydqFED8SQmwTQrwthJig71shhPirLrr2giG6JoSYI4TYoGvjbxNCzNYPny2EeE4IsV8I8bRegYsQ4vtCiL36cZJO8lmROijDoFCcgRBiIXATmjhhBRACPg9kAdt0wcI/o1UzAzwFfEtKWY5WRWtsfxr4bynlUuAcNEE20FQ87wEWAbOAc4UQBWjyC4v143xvZGepUAyMMgwKRV8uAlYAH+ryxRehXcDDwO/1fX4LrBFC5AJ5Uso/69ufBM7X9aumSClfAJBS+qSUnfo+W6SUNbpI2w5gBtAK+IBfCSGuBYx9FYpRRxkGhaIvAnhSSlmh3+ZLKR/oZ79YejL9ybsb9NaoCQE2XSt/NZrS5zXA6wmOWaEYNpRhUCj68jZwvRCiGCI9jaej/b8YCpWfAzZLKb1AsxDiPH37zcCfdV3/GiHENfoxnEKIzIFOqPcEyJVSvormZqoYiYkpFGawxd9FoUgvpJR7hRD/hNYVzwIEgL8DOoDFQogqwIsWhwBNxvgX+oX/KHCbvv1m4BEhxL/ox7ghxmndwItCCBfaauPvh3laCoVplLqqQmESIUS7lDJ7rMehUIw0ypWkUCgUiijUikGhUCgUUagVg0KhUCiiUIZBoVAoFFEow6BQKBSKKJRhUCgUCkUUyjAoFAqFIor/D7HnwXxminAnAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "m_array = np.array([4,6,8,10,15])\n",
    "\n",
    "plot_acc = np.mean(acc_test_arr, axis=1)\n",
    "print(acc_test_arr.shape)\n",
    "print(plot_acc.shape)\n",
    "\n",
    "plt.plot(plot_acc[0,:],label='n=N-s=4')\n",
    "plt.plot(plot_acc[1,:],label='n=N-s=6')\n",
    "plt.plot(plot_acc[2,:],label='n=N-s=8')\n",
    "plt.plot(plot_acc[3,:],label='n=N-s=10')\n",
    "plt.plot(plot_acc[4,:],'b',label='n=N-s=15')\n",
    "plt.plot(acc_test_arr_uncoded,'r',label='uncoded')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Test accuracy(%)')\n",
    "plt.show()\n",
    "\n",
    "import pickle\n",
    "\n",
    "filehandler = open(\"./plot/MNIST_LeNet_N15_K6_m_4_6_8_10_15_test_acc\",\"wb\")\n",
    "pickle.dump(acc_test_arr,filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 10, 30)\n",
      "(12, 30)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sel_m=10\n",
    "sel_trial=0\n",
    "\n",
    "plot_acc = np.mean(acc_test_arr, axis=1)\n",
    "print(acc_test_arr.shape)\n",
    "print(plot_acc.shape)\n",
    "\n",
    "plt.plot(plot_acc[0,:],label='n=N-s=4')\n",
    "plt.plot(plot_acc[2,:],label='n=N-s=6')\n",
    "plt.plot(plot_acc[4,:],label='n=N-s=8')\n",
    "plt.plot(plot_acc[6,:],label='n=N-s=10')\n",
    "plt.plot(plot_acc[11,:],label='n=N-s=15')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Test accuracy(%)')\n",
    "plt.show()\n",
    "\n",
    "import pickle\n",
    "\n",
    "filehandler = open(\"./plot/MNIST_LeNet_N15_K6_test_acc\",\"wb\")\n",
    "pickle.dump(acc_test_arr,filehandler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Train CNN by utilizing BACC (N=31, K=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of X: (60000, 784)\n",
      "size of Y: (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "class my_argument:    \n",
    "    epochs = 200    #\"rounds of training\"\n",
    "    num_users = 31  # \"number of users: N\"\n",
    "    num_partition = 12 # \"number of users: K\"\n",
    "    frac = 0.5 #\"the fraction of clients: C\"\n",
    "    local_ep = 1 #\"the number of local epochs: E\"\n",
    "    local_bs = 200 #\"local batch size: B\"\n",
    "    bs=200 #\"test batch size\"\n",
    "    lr=0.01 #\"learning rate\"\n",
    "    momentum=0.5 # \"SGD momentum (default: 0.5)\"\n",
    "    split='user' # \"train-test split type, user or sample\"\n",
    "    opt='ADAM'\n",
    "    loss='Custom' # 'Custom' or 'Default'\n",
    "\n",
    "    # model arguments\n",
    "    model = 'cnn'\n",
    "    kernel_num=9 #, help='number of each kind of kernel')\n",
    "    kernel_sizes='3,4,5' #  help='comma-separated kernel size to use for convolution')\n",
    "    norm='None' #, help=\"batch_norm, layer_norm, or None\")\n",
    "    num_filters=32 #, help=\"number of filters for conv nets\")\n",
    "    max_pool='True' #help=\"Whether use max pooling rather than strided convolutions\")\n",
    "\n",
    "    # other arguments\n",
    "    dataset='mnist' #, help=\"name of dataset\")\n",
    "    iid=1\n",
    "    num_classes=10#, help=\"number of classes\")\n",
    "    num_channels=1#, help=\"number of channels of imges\")\n",
    "    gpu=1#, help=\"GPU ID, -1 for CPU\")\n",
    "    stopping_rounds=10#, help='rounds of early stopping')\n",
    "    verbose='False'#, help='verbose print')\n",
    "    seed=1#, help='random seed (default: 1)')\n",
    "    \n",
    "args = my_argument()\n",
    "\n",
    "args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "# load dataset and split users\n",
    "trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "dataset_train = datasets.MNIST('../data/mnist/', train=True, download=True, transform=trans_mnist)\n",
    "dataset_test = datasets.MNIST('../data/mnist/', train=False, download=True, transform=trans_mnist)\n",
    "\n",
    "dict_users = mnist_iid(dataset_train, args.num_partition)\n",
    "\n",
    "encoding_input_array_np = np.empty((len(dataset_train),28*28))\n",
    "encoding_label_array_np = np.empty((len(dataset_train),args.num_classes))\n",
    "print(\"size of X:\" ,encoding_input_array_np.shape)\n",
    "print(\"size of Y:\" ,encoding_label_array_np.shape)\n",
    "\n",
    "Size_submatrices = int(60000/args.num_partition)\n",
    "\n",
    "for i in range(args.num_partition):\n",
    "    \n",
    "    stt_pos = i*Size_submatrices\n",
    "    end_pos = (i+1)*Size_submatrices\n",
    "#     print(i,stt_pos,end_pos)\n",
    "    Temp_train = DataLoader(DatasetSplit(dataset_train, dict_users[i]), batch_size=Size_submatrices, shuffle=True)\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(Temp_train):\n",
    "        \n",
    "        images_np = images.detach().cpu().numpy()\n",
    "        encoding_input_array_np[stt_pos:end_pos,:] = np.reshape(images_np, (Size_submatrices,28*28))\n",
    "#         print(encoding_input_array_np[stt_pos:end_pos,:].shape)\n",
    "\n",
    "        onehot_labels = torch.nn.functional.one_hot(labels,num_classes=args.num_classes)\n",
    "        labels_np = onehot_labels.detach().cpu().numpy()\n",
    "#         print(labels_np.shape)\n",
    "        encoding_label_array_np[stt_pos:end_pos,:] = labels_np\n",
    "\n",
    "\n",
    "# print(labels_np[0:10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha_array:  [ 0.99144486  0.92387953  0.79335334  0.60876143  0.38268343  0.13052619\n",
      " -0.13052619 -0.38268343 -0.60876143 -0.79335334 -0.92387953 -0.99144486] \n",
      "\n",
      "z_array:  [ 1.          0.99486932  0.97952994  0.95413926  0.91895781  0.87434662\n",
      "  0.82076344  0.75875812  0.68896692  0.61210598  0.52896401  0.44039415\n",
      "  0.34730525  0.25065253  0.15142778  0.05064917 -0.05064917 -0.15142778\n",
      " -0.25065253 -0.34730525 -0.44039415 -0.52896401 -0.61210598 -0.68896692\n",
      " -0.75875812 -0.82076344 -0.87434662 -0.91895781 -0.95413926 -0.97952994\n",
      " -0.99486932] \n",
      "\n",
      "@BACC_Enc: N,K, m_i= 31 12 5000 \n",
      "\n",
      "@BACC_Enc: N,K, m_i= 31 12 5000 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "N= args.num_users\n",
    "K= args.num_partition\n",
    "\n",
    "\n",
    "j_array = np.array(range(K))\n",
    "# print(\"j: \",(2*j_array+1)*math.pi/2/K,'\\n')\n",
    "\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*K)) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "i_array = np.array(range(N))\n",
    "z_array = np.cos(i_array*2*math.pi/N/2) # np.cos(i_array*2*math.pi/N/2)\n",
    "print(\"z_array: \",z_array,'\\n')\n",
    "\n",
    "X_tilde = BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "y_tilde = BACC_Enc(encoding_label_array_np, alpha_array, z_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of results: 6\n",
      "(m= 6 )  0 -th Trial!!\n",
      "selected users: [15 16 19 20 23 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 5  6 10 11 18 19]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   1, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 4  7  9 15 19 24]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   2, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  4  5 12 13 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   3, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 5 18 20 21 25 28]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   4, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 5  6 19 23 25 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   5, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 3 13 14 18 21 26]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   6, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 3  4  8 12 24 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   7, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 8 11 12 15 19 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   8, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  9 14 17 24 25]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   9, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 3  6 12 16 20 24]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  10, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [10 11 14 16 17 26]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  11, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 8  9 14 26 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  12, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0 10 12 15 16 22]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  13, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 9 10 13 15 18 25]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  14, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 7 11 15 18 21 23]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  15, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 3  8 16 19 24 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  16, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [10 13 21 23 24 28]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  17, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 2  5  8 24 28 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  18, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 1  2  4 11 14 26]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  19, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  4  8 10 17 18]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  20, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 8  9 12 14 17 28]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  21, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 2  8 13 17 20 26]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  22, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 1  2  6 13 15 24]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  23, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [11 12 18 23 26 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  24, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 6  7  9 19 25 27]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  25, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 2  5  8 26 27 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  26, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 6 11 13 21 22 23]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  27, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 1  6 11 19 21 25]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  28, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  5 13 19 24 25]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  29, Average loss 2.303 Test accuracy 9.800\n",
      "(m= 6 )  1 -th Trial!!\n",
      "selected users: [ 2  7 17 18 20 25]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 2  4  5 25 26 28]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   1, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 2  9 13 15 21 27]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   2, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 5  7 12 16 21 24]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   3, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  5  9 19 22 23]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   4, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 6 10 12 18 23 27]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   5, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 4  7 11 13 16 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   6, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [14 16 17 19 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   7, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 6  7 10 12 14 15]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   8, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 7  8  9 16 20 26]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   9, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 5 10 13 15 28 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  10, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 1 12 15 17 20 26]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  11, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0 11 21 24 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  12, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  3  4 12 17 19]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  13, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 1  9 19 21 22 23]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  14, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 8 11 16 21 22 27]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  15, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 1  8  9 10 13 28]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  16, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 1  4  7 13 20 25]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  17, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  2  3 11 13 19]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  18, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  3  4  9 10 20]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  19, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 6  9 10 17 22 28]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  20, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 7  9 12 19 23 26]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  21, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 9 11 18 26 27 28]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  22, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 4  5 23 27 28 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  23, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 4  6 20 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  24, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [11 12 17 21 23 27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  25, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 2  3  5 10 14 16]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  26, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 2 12 15 18 21 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  27, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 3  6 10 17 20 25]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  28, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 8 13 14 20 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  29, Average loss 2.303 Test accuracy 9.800\n",
      "(m= 6 )  2 -th Trial!!\n",
      "selected users: [ 4  8 10 22 25 26]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 5  6 12 19 26 28]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   1, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 2 10 20 25 26 27]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   2, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 1  5 15 21 22 26]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   3, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 2  4 13 16 17 21]\n",
      "\n",
      "Test set: Average loss: 2.1726 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   4, Average loss 2.173 Test accuracy 9.800\n",
      "selected users: [ 0  1  5 12 16 22]\n",
      "\n",
      "Test set: Average loss: 1.9796 \n",
      "Accuracy: 1940/10000 (19.40%)\n",
      "\n",
      "Round   5, Average loss 1.980 Test accuracy 19.400\n",
      "selected users: [ 4 12 13 19 23 27]\n",
      "\n",
      "Test set: Average loss: 2.0899 \n",
      "Accuracy: 1648/10000 (16.48%)\n",
      "\n",
      "Round   6, Average loss 2.090 Test accuracy 16.480\n",
      "selected users: [ 7  9 13 17 21 28]\n",
      "\n",
      "Test set: Average loss: 2.3019 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   7, Average loss 2.302 Test accuracy 9.800\n",
      "selected users: [ 3  6  8 16 17 25]\n",
      "\n",
      "Test set: Average loss: 2.3012 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   8, Average loss 2.301 Test accuracy 9.800\n",
      "selected users: [ 1  2  5  8 15 28]\n",
      "\n",
      "Test set: Average loss: 2.2904 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   9, Average loss 2.290 Test accuracy 9.800\n",
      "selected users: [ 2  6 19 22 26 30]\n",
      "\n",
      "Test set: Average loss: 2.2593 \n",
      "Accuracy: 1067/10000 (10.67%)\n",
      "\n",
      "Round  10, Average loss 2.259 Test accuracy 10.670\n",
      "selected users: [ 2  4  5 13 15 21]\n",
      "\n",
      "Test set: Average loss: 2.2528 \n",
      "Accuracy: 1094/10000 (10.94%)\n",
      "\n",
      "Round  11, Average loss 2.253 Test accuracy 10.940\n",
      "selected users: [ 2  9 11 19 24 28]\n",
      "\n",
      "Test set: Average loss: 2.2756 \n",
      "Accuracy: 1018/10000 (10.18%)\n",
      "\n",
      "Round  12, Average loss 2.276 Test accuracy 10.180\n",
      "selected users: [ 0  5  8 16 18 24]\n",
      "\n",
      "Test set: Average loss: 2.2750 \n",
      "Accuracy: 1023/10000 (10.23%)\n",
      "\n",
      "Round  13, Average loss 2.275 Test accuracy 10.230\n",
      "selected users: [ 7  8 10 15 19 30]\n",
      "\n",
      "Test set: Average loss: 2.2754 \n",
      "Accuracy: 1022/10000 (10.22%)\n",
      "\n",
      "Round  14, Average loss 2.275 Test accuracy 10.220\n",
      "selected users: [ 0  4  5  7 11 22]\n",
      "\n",
      "Test set: Average loss: 2.2779 \n",
      "Accuracy: 1014/10000 (10.14%)\n",
      "\n",
      "Round  15, Average loss 2.278 Test accuracy 10.140\n",
      "selected users: [ 2 16 18 20 22 26]\n",
      "\n",
      "Test set: Average loss: 2.2784 \n",
      "Accuracy: 1012/10000 (10.12%)\n",
      "\n",
      "Round  16, Average loss 2.278 Test accuracy 10.120\n",
      "selected users: [ 0  2  8 13 18 29]\n",
      "\n",
      "Test set: Average loss: 2.2786 \n",
      "Accuracy: 1011/10000 (10.11%)\n",
      "\n",
      "Round  17, Average loss 2.279 Test accuracy 10.110\n",
      "selected users: [ 0  4  7 16 20 30]\n",
      "\n",
      "Test set: Average loss: 2.2786 \n",
      "Accuracy: 1011/10000 (10.11%)\n",
      "\n",
      "Round  18, Average loss 2.279 Test accuracy 10.110\n",
      "selected users: [ 9 10 20 25 26 27]\n",
      "\n",
      "Test set: Average loss: 2.2786 \n",
      "Accuracy: 1011/10000 (10.11%)\n",
      "\n",
      "Round  19, Average loss 2.279 Test accuracy 10.110\n",
      "selected users: [ 2  7 20 22 27 28]\n",
      "\n",
      "Test set: Average loss: 2.2786 \n",
      "Accuracy: 1011/10000 (10.11%)\n",
      "\n",
      "Round  20, Average loss 2.279 Test accuracy 10.110\n",
      "selected users: [ 1  7 11 20 22 24]\n",
      "\n",
      "Test set: Average loss: 2.2786 \n",
      "Accuracy: 1011/10000 (10.11%)\n",
      "\n",
      "Round  21, Average loss 2.279 Test accuracy 10.110\n",
      "selected users: [ 3 10 14 21 24 27]\n",
      "\n",
      "Test set: Average loss: 2.2786 \n",
      "Accuracy: 1011/10000 (10.11%)\n",
      "\n",
      "Round  22, Average loss 2.279 Test accuracy 10.110\n",
      "selected users: [ 0  1 10 15 16 22]\n",
      "\n",
      "Test set: Average loss: 2.2786 \n",
      "Accuracy: 1011/10000 (10.11%)\n",
      "\n",
      "Round  23, Average loss 2.279 Test accuracy 10.110\n",
      "selected users: [ 3  7 12 13 21 23]\n",
      "\n",
      "Test set: Average loss: 2.2786 \n",
      "Accuracy: 1011/10000 (10.11%)\n",
      "\n",
      "Round  24, Average loss 2.279 Test accuracy 10.110\n",
      "selected users: [ 4  5 13 14 19 21]\n",
      "\n",
      "Test set: Average loss: 2.2786 \n",
      "Accuracy: 1011/10000 (10.11%)\n",
      "\n",
      "Round  25, Average loss 2.279 Test accuracy 10.110\n",
      "selected users: [ 2  7 12 23 25 26]\n",
      "\n",
      "Test set: Average loss: 2.2786 \n",
      "Accuracy: 1011/10000 (10.11%)\n",
      "\n",
      "Round  26, Average loss 2.279 Test accuracy 10.110\n",
      "selected users: [ 0  1 16 18 20 26]\n",
      "\n",
      "Test set: Average loss: 2.2786 \n",
      "Accuracy: 1011/10000 (10.11%)\n",
      "\n",
      "Round  27, Average loss 2.279 Test accuracy 10.110\n",
      "selected users: [ 6 11 17 22 25 27]\n",
      "\n",
      "Test set: Average loss: 2.2786 \n",
      "Accuracy: 1011/10000 (10.11%)\n",
      "\n",
      "Round  28, Average loss 2.279 Test accuracy 10.110\n",
      "selected users: [ 5  9 23 26 29 30]\n",
      "\n",
      "Test set: Average loss: 2.2786 \n",
      "Accuracy: 1011/10000 (10.11%)\n",
      "\n",
      "Round  29, Average loss 2.279 Test accuracy 10.110\n",
      "(m= 6 )  3 -th Trial!!\n",
      "selected users: [ 8 14 20 21 25 26]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 6 11 13 16 27 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   1, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  2 22 24 26 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   2, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  4 10 26 27 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   3, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 4 11 12 25 26 28]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   4, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 1  6  9 13 24 25]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   5, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 6  9 10 16 17 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   6, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 4  5 16 25 26 27]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   7, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 3 14 17 20 22 24]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   8, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  1  2  6  8 28]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   9, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 3 12 17 19 24 26]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  10, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 9 11 14 21 26 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  11, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  7 10 12 14 28]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  12, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 1 10 11 12 14 27]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  13, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 8 10 14 15 25 28]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  14, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 5 11 14 17 25 27]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  15, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  5 14 17 19 27]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  16, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [13 14 20 24 26 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  17, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  5 13 14 20 22]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  18, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  1  4 16 19 26]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  19, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 4  6 17 20 26 28]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  20, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 2  4 12 14 25 28]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  21, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 1  3  5  7 28 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  22, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 4  9 12 13 20 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  23, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 4 19 20 24 25 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  24, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 6 12 13 18 19 22]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  25, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  8 17 20 22 23]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  26, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  8  9 10 19 23]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  27, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [11 12 16 17 23 28]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  28, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 2  4 10 13 26 28]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  29, Average loss 2.303 Test accuracy 9.800\n",
      "(m= 6 )  4 -th Trial!!\n",
      "selected users: [14 16 20 26 27 28]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [11 15 18 23 27 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   1, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 4  5  9 13 18 24]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   2, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  3 12 16 25 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   3, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 8  9 13 20 23 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   4, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 5  6  8 15 19 28]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   5, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 1  4  6 21 25 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   6, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 4  6 11 12 25 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   7, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 1  4  8 11 17 21]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   8, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 8 12 20 22 24 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   9, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 6  7  8 10 12 25]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  10, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 1 12 17 19 24 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  11, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  3  6  9 20 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  12, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 7 12 13 18 20 23]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  13, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 8 15 19 20 25 27]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  14, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 2  7 10 12 15 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  15, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 1  5 11 14 16 23]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  16, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  1 11 17 23 24]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  17, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 2  3  7 14 24 25]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  18, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  1 11 23 24 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  19, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 5  8  9 11 19 22]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  20, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 2  7  9 17 19 27]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  21, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 6  7 10 12 18 23]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  22, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 4  7  8 17 19 22]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  23, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  7 10 20 26 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  24, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  5 12 18 24 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  25, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 5  8 13 16 28 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  26, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 2 15 18 22 25 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  27, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 3  9 11 13 20 28]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  28, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  7  9 10 14 27]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  29, Average loss 2.303 Test accuracy 9.800\n",
      "(m= 6 )  5 -th Trial!!\n",
      "selected users: [ 0  3 13 19 26 28]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 2  5 10 17 18 23]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   1, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  6  8 11 25 27]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   2, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 1  4 17 24 27 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   3, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 7  8 10 20 25 26]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   4, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  2 12 20 27 28]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   5, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 5 10 11 23 26 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   6, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 9 14 15 16 18 25]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   7, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [11 12 16 17 20 23]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   8, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0 12 14 24 27 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   9, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 9 16 18 24 27 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  10, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 4  8 16 19 24 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  11, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 6  9 12 19 20 27]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  12, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 6 10 16 22 23 26]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  13, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 6  8 10 17 20 26]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  14, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 4  5  6  7 13 17]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  15, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  6  8 10 22 27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  16, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 9 15 16 20 21 22]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  17, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 1  2  6 12 26 27]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  18, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 3  7  8 12 15 26]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  19, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 2  7 15 22 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  20, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 7 14 16 19 20 22]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  21, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  1  4  8 13 19]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  22, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 4 11 13 23 26 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  23, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 2  3  4 16 22 27]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  24, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 5  9 12 16 22 28]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  25, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 1  3  7  8 10 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  26, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 1  7 10 15 26 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  27, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 3 13 14 17 23 24]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  28, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 2 10 11 16 24 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  29, Average loss 2.303 Test accuracy 9.800\n",
      "(m= 6 )  6 -th Trial!!\n",
      "selected users: [ 1 11 14 16 22 27]\n",
      "\n",
      "Test set: Average loss: 2.3027 \n",
      "Accuracy: 1022/10000 (10.22%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 10.220\n",
      "selected users: [ 1 16 23 24 26 27]\n",
      "\n",
      "Test set: Average loss: 0.7935 \n",
      "Accuracy: 7761/10000 (77.61%)\n",
      "\n",
      "Round   1, Average loss 0.794 Test accuracy 77.610\n",
      "selected users: [ 3  5  6 14 15 30]\n",
      "\n",
      "Test set: Average loss: 0.7013 \n",
      "Accuracy: 7908/10000 (79.08%)\n",
      "\n",
      "Round   2, Average loss 0.701 Test accuracy 79.080\n",
      "selected users: [ 3 10 13 15 16 24]\n",
      "\n",
      "Test set: Average loss: 0.6906 \n",
      "Accuracy: 8380/10000 (83.80%)\n",
      "\n",
      "Round   3, Average loss 0.691 Test accuracy 83.800\n",
      "selected users: [ 8  9 10 19 20 24]\n",
      "\n",
      "Test set: Average loss: 1.9652 \n",
      "Accuracy: 7390/10000 (73.90%)\n",
      "\n",
      "Round   4, Average loss 1.965 Test accuracy 73.900\n",
      "selected users: [ 1  2 11 16 23 25]\n",
      "\n",
      "Test set: Average loss: 2.0385 \n",
      "Accuracy: 5700/10000 (57.00%)\n",
      "\n",
      "Round   5, Average loss 2.039 Test accuracy 57.000\n",
      "selected users: [ 7  9 11 12 20 26]\n",
      "\n",
      "Test set: Average loss: 6.3415 \n",
      "Accuracy: 1354/10000 (13.54%)\n",
      "\n",
      "Round   6, Average loss 6.341 Test accuracy 13.540\n",
      "selected users: [ 1  4  8 19 20 24]\n",
      "\n",
      "Test set: Average loss: 10.7983 \n",
      "Accuracy: 1905/10000 (19.05%)\n",
      "\n",
      "Round   7, Average loss 10.798 Test accuracy 19.050\n",
      "selected users: [ 0 15 19 20 22 27]\n",
      "\n",
      "Test set: Average loss: 155.8669 \n",
      "Accuracy: 1661/10000 (16.61%)\n",
      "\n",
      "Round   8, Average loss 155.867 Test accuracy 16.610\n",
      "selected users: [ 1  5  7 14 17 26]\n",
      "\n",
      "Test set: Average loss: 1385.5515 \n",
      "Accuracy: 1699/10000 (16.99%)\n",
      "\n",
      "Round   9, Average loss 1385.551 Test accuracy 16.990\n",
      "selected users: [ 3  5  7 14 25 27]\n",
      "\n",
      "Test set: Average loss: 10549.7662 \n",
      "Accuracy: 1755/10000 (17.55%)\n",
      "\n",
      "Round  10, Average loss 10549.766 Test accuracy 17.550\n",
      "selected users: [ 3  7  9 19 27 30]\n",
      "\n",
      "Test set: Average loss: 82976.8077 \n",
      "Accuracy: 1750/10000 (17.50%)\n",
      "\n",
      "Round  11, Average loss 82976.808 Test accuracy 17.500\n",
      "selected users: [ 6 15 18 21 23 24]\n",
      "\n",
      "Test set: Average loss: 706153.7710 \n",
      "Accuracy: 1771/10000 (17.71%)\n",
      "\n",
      "Round  12, Average loss 706153.771 Test accuracy 17.710\n",
      "selected users: [ 8 11 15 17 27 30]\n",
      "\n",
      "Test set: Average loss: 5670032.9536 \n",
      "Accuracy: 1764/10000 (17.64%)\n",
      "\n",
      "Round  13, Average loss 5670032.954 Test accuracy 17.640\n",
      "selected users: [ 0  1 11 17 20 25]\n",
      "\n",
      "Test set: Average loss: 45739678.5555 \n",
      "Accuracy: 1568/10000 (15.68%)\n",
      "\n",
      "Round  14, Average loss 45739678.556 Test accuracy 15.680\n",
      "selected users: [ 3 12 17 20 27 30]\n",
      "\n",
      "Test set: Average loss: 364992539.5215 \n",
      "Accuracy: 1769/10000 (17.69%)\n",
      "\n",
      "Round  15, Average loss 364992539.522 Test accuracy 17.690\n",
      "selected users: [ 1  2  3  4  8 14]\n",
      "\n",
      "Test set: Average loss: 2880796695.8512 \n",
      "Accuracy: 1617/10000 (16.17%)\n",
      "\n",
      "Round  16, Average loss 2880796695.851 Test accuracy 16.170\n",
      "selected users: [ 6 12 16 17 24 25]\n",
      "\n",
      "Test set: Average loss: 22994244304.9936 \n",
      "Accuracy: 1745/10000 (17.45%)\n",
      "\n",
      "Round  17, Average loss 22994244304.994 Test accuracy 17.450\n",
      "selected users: [ 6 10 13 14 24 29]\n",
      "\n",
      "Test set: Average loss: 183908421001.4976 \n",
      "Accuracy: 1828/10000 (18.28%)\n",
      "\n",
      "Round  18, Average loss 183908421001.498 Test accuracy 18.280\n",
      "selected users: [ 7 14 16 17 18 19]\n",
      "\n",
      "Test set: Average loss: 1468079162425.3440 \n",
      "Accuracy: 1646/10000 (16.46%)\n",
      "\n",
      "Round  19, Average loss 1468079162425.344 Test accuracy 16.460\n",
      "selected users: [ 5 14 15 21 23 24]\n",
      "\n",
      "Test set: Average loss: 11745428282367.3848 \n",
      "Accuracy: 1819/10000 (18.19%)\n",
      "\n",
      "Round  20, Average loss 11745428282367.385 Test accuracy 18.190\n",
      "selected users: [ 7  9 15 18 22 24]\n",
      "\n",
      "Test set: Average loss: 93954614210271.2344 \n",
      "Accuracy: 1671/10000 (16.71%)\n",
      "\n",
      "Round  21, Average loss 93954614210271.234 Test accuracy 16.710\n",
      "selected users: [ 7 12 21 24 28 30]\n",
      "\n",
      "Test set: Average loss: 751570431608727.1250 \n",
      "Accuracy: 1759/10000 (17.59%)\n",
      "\n",
      "Round  22, Average loss 751570431608727.125 Test accuracy 17.590\n",
      "selected users: [ 5  9 10 15 23 25]\n",
      "\n",
      "Test set: Average loss: 6012528547610749.0000 \n",
      "Accuracy: 1934/10000 (19.34%)\n",
      "\n",
      "Round  23, Average loss 6012528547610749.000 Test accuracy 19.340\n",
      "selected users: [ 4  9 10 14 21 24]\n",
      "\n",
      "Test set: Average loss: 48099128108585296.0000 \n",
      "Accuracy: 1823/10000 (18.23%)\n",
      "\n",
      "Round  24, Average loss 48099128108585296.000 Test accuracy 18.230\n",
      "selected users: [ 0  5 10 12 17 18]\n",
      "\n",
      "Test set: Average loss: 384791621408714112.0000 \n",
      "Accuracy: 1777/10000 (17.77%)\n",
      "\n",
      "Round  25, Average loss 384791621408714112.000 Test accuracy 17.770\n",
      "selected users: [ 7  8 15 17 20 24]\n",
      "\n",
      "Test set: Average loss: 3078328545035580928.0000 \n",
      "Accuracy: 1840/10000 (18.40%)\n",
      "\n",
      "Round  26, Average loss 3078328545035580928.000 Test accuracy 18.400\n",
      "selected users: [ 8 11 15 16 20 29]\n",
      "\n",
      "Test set: Average loss: 24626623309381832704.0000 \n",
      "Accuracy: 1864/10000 (18.64%)\n",
      "\n",
      "Round  27, Average loss 24626623309381832704.000 Test accuracy 18.640\n",
      "selected users: [11 15 20 26 27 29]\n",
      "\n",
      "Test set: Average loss: 197012961913750781952.0000 \n",
      "Accuracy: 1883/10000 (18.83%)\n",
      "\n",
      "Round  28, Average loss 197012961913750781952.000 Test accuracy 18.830\n",
      "selected users: [ 4  5 18 22 28 30]\n",
      "\n",
      "Test set: Average loss: 1576103847052852854784.0000 \n",
      "Accuracy: 1759/10000 (17.59%)\n",
      "\n",
      "Round  29, Average loss 1576103847052852854784.000 Test accuracy 17.590\n",
      "(m= 6 )  7 -th Trial!!\n",
      "selected users: [ 3  4 13 21 23 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 5  6 12 19 20 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   1, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  1  3 13 20 27]\n",
      "\n",
      "Test set: Average loss: 2.0900 \n",
      "Accuracy: 1996/10000 (19.96%)\n",
      "\n",
      "Round   2, Average loss 2.090 Test accuracy 19.960\n",
      "selected users: [ 4 11 12 20 25 30]\n",
      "\n",
      "Test set: Average loss: 1.8451 \n",
      "Accuracy: 2700/10000 (27.00%)\n",
      "\n",
      "Round   3, Average loss 1.845 Test accuracy 27.000\n",
      "selected users: [ 2  3  7 13 14 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   4, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 7  9 13 15 21 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   5, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 5  8 18 25 27 28]\n",
      "\n",
      "Test set: Average loss: 2.3001 \n",
      "Accuracy: 991/10000 (9.91%)\n",
      "\n",
      "Round   6, Average loss 2.300 Test accuracy 9.910\n",
      "selected users: [ 1  5  6  7  9 22]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   7, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 6 12 15 16 20 30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.2975 \n",
      "Accuracy: 1002/10000 (10.02%)\n",
      "\n",
      "Round   8, Average loss 2.298 Test accuracy 10.020\n",
      "selected users: [ 0  3  4  5  8 18]\n",
      "\n",
      "Test set: Average loss: 2.2984 \n",
      "Accuracy: 998/10000 (9.98%)\n",
      "\n",
      "Round   9, Average loss 2.298 Test accuracy 9.980\n",
      "selected users: [ 2  3 13 17 22 23]\n",
      "\n",
      "Test set: Average loss: 4050.7462 \n",
      "Accuracy: 1229/10000 (12.29%)\n",
      "\n",
      "Round  10, Average loss 4050.746 Test accuracy 12.290\n",
      "selected users: [ 3  4 16 17 21 24]\n",
      "\n",
      "Test set: Average loss: 2.2821 \n",
      "Accuracy: 1069/10000 (10.69%)\n",
      "\n",
      "Round  11, Average loss 2.282 Test accuracy 10.690\n",
      "selected users: [ 1  2  8 12 18 29]\n",
      "\n",
      "Test set: Average loss: 2.2865 \n",
      "Accuracy: 1050/10000 (10.50%)\n",
      "\n",
      "Round  12, Average loss 2.286 Test accuracy 10.500\n",
      "selected users: [ 0  3 14 17 22 26]\n",
      "\n",
      "Test set: Average loss: 2.2899 \n",
      "Accuracy: 1035/10000 (10.35%)\n",
      "\n",
      "Round  13, Average loss 2.290 Test accuracy 10.350\n",
      "selected users: [ 0  8 12 21 25 30]\n",
      "\n",
      "Test set: Average loss: 2.2888 \n",
      "Accuracy: 1040/10000 (10.40%)\n",
      "\n",
      "Round  14, Average loss 2.289 Test accuracy 10.400\n",
      "selected users: [ 4  5  8 22 29 30]\n",
      "\n",
      "Test set: Average loss: 2.2897 \n",
      "Accuracy: 1036/10000 (10.36%)\n",
      "\n",
      "Round  15, Average loss 2.290 Test accuracy 10.360\n",
      "selected users: [ 4  8 11 21 23 28]\n",
      "\n",
      "Test set: Average loss: 2.2897 \n",
      "Accuracy: 1036/10000 (10.36%)\n",
      "\n",
      "Round  16, Average loss 2.290 Test accuracy 10.360\n",
      "selected users: [11 13 14 17 19 22]\n",
      "\n",
      "Test set: Average loss: 2.2899 \n",
      "Accuracy: 1035/10000 (10.35%)\n",
      "\n",
      "Round  17, Average loss 2.290 Test accuracy 10.350\n",
      "selected users: [ 3 10 15 16 17 27]\n",
      "\n",
      "Test set: Average loss: 2.2899 \n",
      "Accuracy: 1035/10000 (10.35%)\n",
      "\n",
      "Round  18, Average loss 2.290 Test accuracy 10.350\n",
      "selected users: [ 0  5  6 10 21 30]\n",
      "\n",
      "Test set: Average loss: 2.2899 \n",
      "Accuracy: 1035/10000 (10.35%)\n",
      "\n",
      "Round  19, Average loss 2.290 Test accuracy 10.350\n",
      "selected users: [10 13 15 16 19 26]\n",
      "\n",
      "Test set: Average loss: 2.2899 \n",
      "Accuracy: 1035/10000 (10.35%)\n",
      "\n",
      "Round  20, Average loss 2.290 Test accuracy 10.350\n",
      "selected users: [ 0  8 10 16 27 29]\n",
      "\n",
      "Test set: Average loss: 2.2899 \n",
      "Accuracy: 1035/10000 (10.35%)\n",
      "\n",
      "Round  21, Average loss 2.290 Test accuracy 10.350\n",
      "selected users: [ 1 13 17 21 22 29]\n",
      "\n",
      "Test set: Average loss: 2.2899 \n",
      "Accuracy: 1035/10000 (10.35%)\n",
      "\n",
      "Round  22, Average loss 2.290 Test accuracy 10.350\n",
      "selected users: [12 20 22 23 25 28]\n",
      "\n",
      "Test set: Average loss: 2.2899 \n",
      "Accuracy: 1035/10000 (10.35%)\n",
      "\n",
      "Round  23, Average loss 2.290 Test accuracy 10.350\n",
      "selected users: [ 2  4  7 17 23 27]\n",
      "\n",
      "Test set: Average loss: 2.2899 \n",
      "Accuracy: 1035/10000 (10.35%)\n",
      "\n",
      "Round  24, Average loss 2.290 Test accuracy 10.350\n",
      "selected users: [ 4  9 12 17 19 22]\n",
      "\n",
      "Test set: Average loss: 2.2899 \n",
      "Accuracy: 1035/10000 (10.35%)\n",
      "\n",
      "Round  25, Average loss 2.290 Test accuracy 10.350\n",
      "selected users: [ 4  5 16 24 25 28]\n",
      "\n",
      "Test set: Average loss: 2.2899 \n",
      "Accuracy: 1035/10000 (10.35%)\n",
      "\n",
      "Round  26, Average loss 2.290 Test accuracy 10.350\n",
      "selected users: [16 17 19 20 22 26]\n",
      "\n",
      "Test set: Average loss: 2.2899 \n",
      "Accuracy: 1035/10000 (10.35%)\n",
      "\n",
      "Round  27, Average loss 2.290 Test accuracy 10.350\n",
      "selected users: [ 0  6 11 19 27 29]\n",
      "\n",
      "Test set: Average loss: 2.2899 \n",
      "Accuracy: 1035/10000 (10.35%)\n",
      "\n",
      "Round  28, Average loss 2.290 Test accuracy 10.350\n",
      "selected users: [12 18 22 27 28 30]\n",
      "\n",
      "Test set: Average loss: 2.2899 \n",
      "Accuracy: 1035/10000 (10.35%)\n",
      "\n",
      "Round  29, Average loss 2.290 Test accuracy 10.350\n",
      "(m= 6 )  8 -th Trial!!\n",
      "selected users: [ 4 12 17 18 21 26]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 5  6 13 21 23 26]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   1, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 1  3  5  9 24 28]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   2, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  2  7 11 14 24]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   3, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 4 19 20 25 28 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   4, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 3  5 11 21 23 28]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   5, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 1 13 18 19 24 26]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   6, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  2  4 10 27 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   7, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 4  6 10 12 19 25]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   8, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  2 11 14 16 24]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   9, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 1 11 22 25 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  10, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 2  8 13 15 25 28]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  11, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 1  2 14 16 19 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  12, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 6  8 17 19 26 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  13, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 5  7  9 10 25 27]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  14, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 6  7 11 21 22 27]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  15, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 3 12 14 19 22 28]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  16, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 1  2  3  5 11 26]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  17, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 3  6 12 15 17 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  18, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 9 12 14 15 24 25]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  19, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 3  7 15 16 22 24]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  20, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 1  2  6 18 21 26]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  21, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 5  6  9 21 23 27]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  22, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [14 15 19 20 25 26]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  23, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 2  5 15 18 25 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  24, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 1  5 18 23 24 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  25, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0 12 18 22 25 26]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  26, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 3  5  8 17 19 26]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  27, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 2  7 10 11 26 27]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  28, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0 10 16 18 22 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  29, Average loss 2.303 Test accuracy 9.800\n",
      "(m= 6 )  9 -th Trial!!\n",
      "selected users: [ 0  4  6 12 14 15]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0 12 13 15 21 23]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   1, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 1  5  6  9 15 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   2, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 5 10 11 12 20 23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   3, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  3  4 19 26 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   4, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [11 14 20 23 26 28]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   5, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 5 15 23 24 25 27]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   6, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [10 16 21 22 26 28]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   7, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 1  2 11 13 21 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   8, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 9 12 16 17 21 28]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   9, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 1  7  9 17 28 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  10, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 1  6  9 11 24 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  11, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 4  5 11 17 21 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  12, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 9 15 23 25 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  13, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  3  4  7 11 23]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  14, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 6  9 10 16 17 18]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  15, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 2  7 12 15 21 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  16, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  9 11 12 18 26]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  17, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 3  7 14 16 25 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  18, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 3 11 14 16 18 27]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  19, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 2  5  6 10 17 28]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  20, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 8 11 15 24 25 28]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  21, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [12 13 14 16 19 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  22, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 6  7 13 17 23 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  23, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 7 15 19 22 25 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  24, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 8 12 13 18 23 26]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  25, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 2  3 14 22 24 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  26, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  3  6 11 15 20]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  27, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0 17 19 20 21 26]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  28, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0 19 21 22 24 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round  29, Average loss 2.303 Test accuracy 9.800\n",
      "number of results: 9\n",
      "(m= 9 )  0 -th Trial!!\n",
      "selected users: [ 0  7  8 12 15 16 20 21 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 3  7  9 10 11 12 16 20 25]\n",
      "\n",
      "Test set: Average loss: 1.8978 \n",
      "Accuracy: 6279/10000 (62.79%)\n",
      "\n",
      "Round   1, Average loss 1.898 Test accuracy 62.790\n",
      "selected users: [ 4  6 11 12 13 16 17 21 28]\n",
      "\n",
      "Test set: Average loss: 0.4064 \n",
      "Accuracy: 9248/10000 (92.48%)\n",
      "\n",
      "Round   2, Average loss 0.406 Test accuracy 92.480\n",
      "selected users: [ 0 13 14 15 17 21 23 26 27]\n",
      "\n",
      "Test set: Average loss: 6.4056 \n",
      "Accuracy: 9282/10000 (92.82%)\n",
      "\n",
      "Round   3, Average loss 6.406 Test accuracy 92.820\n",
      "selected users: [ 1  2  3  6  7 15 20 24 27]\n",
      "\n",
      "Test set: Average loss: 9.6734 \n",
      "Accuracy: 9425/10000 (94.25%)\n",
      "\n",
      "Round   4, Average loss 9.673 Test accuracy 94.250\n",
      "selected users: [ 5  7  9 12 15 17 19 26 27]\n",
      "\n",
      "Test set: Average loss: 102.2848 \n",
      "Accuracy: 9290/10000 (92.90%)\n",
      "\n",
      "Round   5, Average loss 102.285 Test accuracy 92.900\n",
      "selected users: [ 9 10 11 13 17 21 22 24 27]\n",
      "\n",
      "Test set: Average loss: 25.5520 \n",
      "Accuracy: 9515/10000 (95.15%)\n",
      "\n",
      "Round   6, Average loss 25.552 Test accuracy 95.150\n",
      "selected users: [ 2  8 13 15 22 23 24 26 27]\n",
      "\n",
      "Test set: Average loss: 29.8938 \n",
      "Accuracy: 9514/10000 (95.14%)\n",
      "\n",
      "Round   7, Average loss 29.894 Test accuracy 95.140\n",
      "selected users: [ 1  4  5  6  9 14 16 19 29]\n",
      "\n",
      "Test set: Average loss: 23.1856 \n",
      "Accuracy: 9482/10000 (94.82%)\n",
      "\n",
      "Round   8, Average loss 23.186 Test accuracy 94.820\n",
      "selected users: [ 1  8 11 14 15 17 21 24 30]\n",
      "\n",
      "Test set: Average loss: 202.7492 \n",
      "Accuracy: 9553/10000 (95.53%)\n",
      "\n",
      "Round   9, Average loss 202.749 Test accuracy 95.530\n",
      "selected users: [ 9 10 12 14 18 19 21 27 29]\n",
      "\n",
      "Test set: Average loss: 210.4888 \n",
      "Accuracy: 9599/10000 (95.99%)\n",
      "\n",
      "Round  10, Average loss 210.489 Test accuracy 95.990\n",
      "selected users: [ 2  4  6  8 17 18 21 24 28]\n",
      "\n",
      "Test set: Average loss: 493.3104 \n",
      "Accuracy: 9585/10000 (95.85%)\n",
      "\n",
      "Round  11, Average loss 493.310 Test accuracy 95.850\n",
      "selected users: [ 2  5 12 15 17 18 22 25 26]\n",
      "\n",
      "Test set: Average loss: 2091.5387 \n",
      "Accuracy: 9597/10000 (95.97%)\n",
      "\n",
      "Round  12, Average loss 2091.539 Test accuracy 95.970\n",
      "selected users: [ 1  2  3  4  6 14 16 18 24]\n",
      "\n",
      "Test set: Average loss: 5962.9693 \n",
      "Accuracy: 9594/10000 (95.94%)\n",
      "\n",
      "Round  13, Average loss 5962.969 Test accuracy 95.940\n",
      "selected users: [ 0  3 10 12 17 19 23 25 27]\n",
      "\n",
      "Test set: Average loss: 10751.4158 \n",
      "Accuracy: 9598/10000 (95.98%)\n",
      "\n",
      "Round  14, Average loss 10751.416 Test accuracy 95.980\n",
      "selected users: [ 0  3  4 13 15 18 23 25 29]\n",
      "\n",
      "Test set: Average loss: 38370.6742 \n",
      "Accuracy: 9588/10000 (95.88%)\n",
      "\n",
      "Round  15, Average loss 38370.674 Test accuracy 95.880\n",
      "selected users: [ 2  3  5  7 11 12 14 22 26]\n",
      "\n",
      "Test set: Average loss: 96793.0044 \n",
      "Accuracy: 9580/10000 (95.80%)\n",
      "\n",
      "Round  16, Average loss 96793.004 Test accuracy 95.800\n",
      "selected users: [ 0  1  3  9 13 17 20 24 30]\n",
      "\n",
      "Test set: Average loss: 219218.5964 \n",
      "Accuracy: 9591/10000 (95.91%)\n",
      "\n",
      "Round  17, Average loss 219218.596 Test accuracy 95.910\n",
      "selected users: [ 0  1  2 11 12 15 22 23 26]\n",
      "\n",
      "Test set: Average loss: 595183.1124 \n",
      "Accuracy: 9574/10000 (95.74%)\n",
      "\n",
      "Round  18, Average loss 595183.112 Test accuracy 95.740\n",
      "selected users: [ 0  7 13 16 18 19 22 26 30]\n",
      "\n",
      "Test set: Average loss: 1488977.0176 \n",
      "Accuracy: 9580/10000 (95.80%)\n",
      "\n",
      "Round  19, Average loss 1488977.018 Test accuracy 95.800\n",
      "selected users: [ 0  1  2  5  7  8 11 16 26]\n",
      "\n",
      "Test set: Average loss: 3984594.6592 \n",
      "Accuracy: 9566/10000 (95.66%)\n",
      "\n",
      "Round  20, Average loss 3984594.659 Test accuracy 95.660\n",
      "selected users: [ 3  9 14 15 16 17 18 20 29]\n",
      "\n",
      "Test set: Average loss: 8738211.2960 \n",
      "Accuracy: 9583/10000 (95.83%)\n",
      "\n",
      "Round  21, Average loss 8738211.296 Test accuracy 95.830\n",
      "selected users: [ 3 13 14 16 18 20 23 25 28]\n",
      "\n",
      "Test set: Average loss: 20918051.8144 \n",
      "Accuracy: 9577/10000 (95.77%)\n",
      "\n",
      "Round  22, Average loss 20918051.814 Test accuracy 95.770\n",
      "selected users: [ 0  8 11 15 17 23 25 27 28]\n",
      "\n",
      "Test set: Average loss: 50257649.7664 \n",
      "Accuracy: 9577/10000 (95.77%)\n",
      "\n",
      "Round  23, Average loss 50257649.766 Test accuracy 95.770\n",
      "selected users: [14 16 19 21 22 23 26 27 29]\n",
      "\n",
      "Test set: Average loss: 110995467.4688 \n",
      "Accuracy: 9582/10000 (95.82%)\n",
      "\n",
      "Round  24, Average loss 110995467.469 Test accuracy 95.820\n",
      "selected users: [ 7 10 12 15 18 22 23 27 30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 263001412.4032 \n",
      "Accuracy: 9583/10000 (95.83%)\n",
      "\n",
      "Round  25, Average loss 263001412.403 Test accuracy 95.830\n",
      "selected users: [ 0  4  6 12 13 15 16 24 29]\n",
      "\n",
      "Test set: Average loss: 628724646.7072 \n",
      "Accuracy: 9582/10000 (95.82%)\n",
      "\n",
      "Round  26, Average loss 628724646.707 Test accuracy 95.820\n",
      "selected users: [ 7  8 15 20 21 23 24 27 30]\n",
      "\n",
      "Test set: Average loss: 1503109686.8864 \n",
      "Accuracy: 9582/10000 (95.82%)\n",
      "\n",
      "Round  27, Average loss 1503109686.886 Test accuracy 95.820\n",
      "selected users: [ 2  3  4  7 10 11 18 23 26]\n",
      "\n",
      "Test set: Average loss: 3586858382.1312 \n",
      "Accuracy: 9578/10000 (95.78%)\n",
      "\n",
      "Round  28, Average loss 3586858382.131 Test accuracy 95.780\n",
      "selected users: [ 3  7 10 11 13 17 21 22 28]\n",
      "\n",
      "Test set: Average loss: 8466582935.9616 \n",
      "Accuracy: 9577/10000 (95.77%)\n",
      "\n",
      "Round  29, Average loss 8466582935.962 Test accuracy 95.770\n",
      "(m= 9 )  1 -th Trial!!\n",
      "selected users: [ 0  1  2  4  6 20 22 24 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 1  2  4 11 19 24 25 28 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   1, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  2  8 15 18 20 21 22 29]\n",
      "\n",
      "Test set: Average loss: 1.5497 \n",
      "Accuracy: 7272/10000 (72.72%)\n",
      "\n",
      "Round   2, Average loss 1.550 Test accuracy 72.720\n",
      "selected users: [ 0  5  7 16 20 21 25 29 30]\n",
      "\n",
      "Test set: Average loss: 0.8340 \n",
      "Accuracy: 9388/10000 (93.88%)\n",
      "\n",
      "Round   3, Average loss 0.834 Test accuracy 93.880\n",
      "selected users: [ 0  1  5  6  9 10 16 19 26]\n",
      "\n",
      "Test set: Average loss: 15.3930 \n",
      "Accuracy: 9232/10000 (92.32%)\n",
      "\n",
      "Round   4, Average loss 15.393 Test accuracy 92.320\n",
      "selected users: [ 0  8  9 13 15 23 24 27 30]\n",
      "\n",
      "Test set: Average loss: 40.2715 \n",
      "Accuracy: 9398/10000 (93.98%)\n",
      "\n",
      "Round   5, Average loss 40.271 Test accuracy 93.980\n",
      "selected users: [ 1  2  9 10 14 17 21 25 30]\n",
      "\n",
      "Test set: Average loss: 8.0224 \n",
      "Accuracy: 9258/10000 (92.58%)\n",
      "\n",
      "Round   6, Average loss 8.022 Test accuracy 92.580\n",
      "selected users: [ 0  2  3 13 20 24 25 26 27]\n",
      "\n",
      "Test set: Average loss: 105.9850 \n",
      "Accuracy: 9449/10000 (94.49%)\n",
      "\n",
      "Round   7, Average loss 105.985 Test accuracy 94.490\n",
      "selected users: [ 0  6  8 18 22 24 25 26 27]\n",
      "\n",
      "Test set: Average loss: 305.7090 \n",
      "Accuracy: 9475/10000 (94.75%)\n",
      "\n",
      "Round   8, Average loss 305.709 Test accuracy 94.750\n",
      "selected users: [ 0  1  4  6 21 25 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1883.7806 \n",
      "Accuracy: 9409/10000 (94.09%)\n",
      "\n",
      "Round   9, Average loss 1883.781 Test accuracy 94.090\n",
      "selected users: [ 1  2  4 11 19 24 25 28 29]\n",
      "\n",
      "Test set: Average loss: 3885.2556 \n",
      "Accuracy: 9436/10000 (94.36%)\n",
      "\n",
      "Round  10, Average loss 3885.256 Test accuracy 94.360\n",
      "selected users: [ 0  7 12 13 14 17 22 24 30]\n",
      "\n",
      "Test set: Average loss: 6990.8537 \n",
      "Accuracy: 9478/10000 (94.78%)\n",
      "\n",
      "Round  11, Average loss 6990.854 Test accuracy 94.780\n",
      "selected users: [ 2  6 15 17 18 20 21 22 23]\n",
      "\n",
      "Test set: Average loss: 26311.9642 \n",
      "Accuracy: 9362/10000 (93.62%)\n",
      "\n",
      "Round  12, Average loss 26311.964 Test accuracy 93.620\n",
      "selected users: [ 1  5 11 12 20 25 26 27 29]\n",
      "\n",
      "Test set: Average loss: 65029.2121 \n",
      "Accuracy: 9405/10000 (94.05%)\n",
      "\n",
      "Round  13, Average loss 65029.212 Test accuracy 94.050\n",
      "selected users: [ 4 10 11 14 16 20 22 28 30]\n",
      "\n",
      "Test set: Average loss: 148757.4639 \n",
      "Accuracy: 9424/10000 (94.24%)\n",
      "\n",
      "Round  14, Average loss 148757.464 Test accuracy 94.240\n",
      "selected users: [ 0  3  6 12 13 20 23 26 30]\n",
      "\n",
      "Test set: Average loss: 403065.0120 \n",
      "Accuracy: 9389/10000 (93.89%)\n",
      "\n",
      "Round  15, Average loss 403065.012 Test accuracy 93.890\n",
      "selected users: [ 0  7 10 13 14 16 18 20 23]\n",
      "\n",
      "Test set: Average loss: 964098.9696 \n",
      "Accuracy: 9390/10000 (93.90%)\n",
      "\n",
      "Round  16, Average loss 964098.970 Test accuracy 93.900\n",
      "selected users: [ 6 11 15 16 18 21 22 28 30]\n",
      "\n",
      "Test set: Average loss: 2215735.0160 \n",
      "Accuracy: 9397/10000 (93.97%)\n",
      "\n",
      "Round  17, Average loss 2215735.016 Test accuracy 93.970\n",
      "selected users: [ 1  3  5 11 18 19 21 24 26]\n",
      "\n",
      "Test set: Average loss: 5646480.5440 \n",
      "Accuracy: 9383/10000 (93.83%)\n",
      "\n",
      "Round  18, Average loss 5646480.544 Test accuracy 93.830\n",
      "selected users: [ 0  3  7 16 19 23 24 27 29]\n",
      "\n",
      "Test set: Average loss: 13131603.3408 \n",
      "Accuracy: 9395/10000 (93.95%)\n",
      "\n",
      "Round  19, Average loss 13131603.341 Test accuracy 93.950\n",
      "selected users: [ 1  3  5  6 13 18 23 25 30]\n",
      "\n",
      "Test set: Average loss: 31350664.4992 \n",
      "Accuracy: 9388/10000 (93.88%)\n",
      "\n",
      "Round  20, Average loss 31350664.499 Test accuracy 93.880\n",
      "selected users: [ 5  8 12 13 14 16 18 22 28]\n",
      "\n",
      "Test set: Average loss: 73086544.7936 \n",
      "Accuracy: 9393/10000 (93.93%)\n",
      "\n",
      "Round  21, Average loss 73086544.794 Test accuracy 93.930\n",
      "selected users: [ 0  2  8  9 13 19 21 25 29]\n",
      "\n",
      "Test set: Average loss: 171045118.1568 \n",
      "Accuracy: 9389/10000 (93.89%)\n",
      "\n",
      "Round  22, Average loss 171045118.157 Test accuracy 93.890\n",
      "selected users: [ 6 11 14 17 19 20 24 29 30]\n",
      "\n",
      "Test set: Average loss: 400344055.3984 \n",
      "Accuracy: 9398/10000 (93.98%)\n",
      "\n",
      "Round  23, Average loss 400344055.398 Test accuracy 93.980\n",
      "selected users: [ 1  2  6 11 15 18 20 23 27]\n",
      "\n",
      "Test set: Average loss: 947740457.3696 \n",
      "Accuracy: 9395/10000 (93.95%)\n",
      "\n",
      "Round  24, Average loss 947740457.370 Test accuracy 93.950\n",
      "selected users: [ 6  9 13 15 16 22 24 27 28]\n",
      "\n",
      "Test set: Average loss: 2245154493.2352 \n",
      "Accuracy: 9395/10000 (93.95%)\n",
      "\n",
      "Round  25, Average loss 2245154493.235 Test accuracy 93.950\n",
      "selected users: [ 4  5 12 16 17 19 26 29 30]\n",
      "\n",
      "Test set: Average loss: 5320917591.6544 \n",
      "Accuracy: 9399/10000 (93.99%)\n",
      "\n",
      "Round  26, Average loss 5320917591.654 Test accuracy 93.990\n",
      "selected users: [ 4  5  7 10 13 22 25 26 29]\n",
      "\n",
      "Test set: Average loss: 12560708245.9136 \n",
      "Accuracy: 9398/10000 (93.98%)\n",
      "\n",
      "Round  27, Average loss 12560708245.914 Test accuracy 93.980\n",
      "selected users: [ 2  4  7 13 14 15 17 19 24]\n",
      "\n",
      "Test set: Average loss: 29669341187.2768 \n",
      "Accuracy: 9399/10000 (93.99%)\n",
      "\n",
      "Round  28, Average loss 29669341187.277 Test accuracy 93.990\n",
      "selected users: [ 2  4  6 15 21 23 24 28 30]\n",
      "\n",
      "Test set: Average loss: 70325499330.5600 \n",
      "Accuracy: 9399/10000 (93.99%)\n",
      "\n",
      "Round  29, Average loss 70325499330.560 Test accuracy 93.990\n",
      "(m= 9 )  2 -th Trial!!\n",
      "selected users: [ 0  1  9 10 12 14 20 21 23]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  2  6  7 17 18 19 22 28]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   1, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  5  8 11 13 14 15 20 23]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   2, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  2  3 10 11 14 19 27 30]\n",
      "\n",
      "Test set: Average loss: 2.0521 \n",
      "Accuracy: 3334/10000 (33.34%)\n",
      "\n",
      "Round   3, Average loss 2.052 Test accuracy 33.340\n",
      "selected users: [ 1  3  4  8 10 12 26 27 30]\n",
      "\n",
      "Test set: Average loss: 0.5617 \n",
      "Accuracy: 8615/10000 (86.15%)\n",
      "\n",
      "Round   4, Average loss 0.562 Test accuracy 86.150\n",
      "selected users: [ 1  2 13 15 23 24 25 26 30]\n",
      "\n",
      "Test set: Average loss: 0.4163 \n",
      "Accuracy: 8749/10000 (87.49%)\n",
      "\n",
      "Round   5, Average loss 0.416 Test accuracy 87.490\n",
      "selected users: [ 0  3  4  9 11 13 14 23 27]\n",
      "\n",
      "Test set: Average loss: 1.0221 \n",
      "Accuracy: 9573/10000 (95.73%)\n",
      "\n",
      "Round   6, Average loss 1.022 Test accuracy 95.730\n",
      "selected users: [ 3  7  8 10 12 20 21 28 29]\n",
      "\n",
      "Test set: Average loss: 0.3532 \n",
      "Accuracy: 9375/10000 (93.75%)\n",
      "\n",
      "Round   7, Average loss 0.353 Test accuracy 93.750\n",
      "selected users: [ 1  7 13 17 21 23 25 26 30]\n",
      "\n",
      "Test set: Average loss: 1.8315 \n",
      "Accuracy: 9612/10000 (96.12%)\n",
      "\n",
      "Round   8, Average loss 1.831 Test accuracy 96.120\n",
      "selected users: [ 2  7  9 10 22 24 26 27 29]\n",
      "\n",
      "Test set: Average loss: 12.5502 \n",
      "Accuracy: 9396/10000 (93.96%)\n",
      "\n",
      "Round   9, Average loss 12.550 Test accuracy 93.960\n",
      "selected users: [ 2  7  9 14 22 24 26 28 30]\n",
      "\n",
      "Test set: Average loss: 4.9341 \n",
      "Accuracy: 9548/10000 (95.48%)\n",
      "\n",
      "Round  10, Average loss 4.934 Test accuracy 95.480\n",
      "selected users: [ 2  5 10 13 15 23 27 29 30]\n",
      "\n",
      "Test set: Average loss: 131.7736 \n",
      "Accuracy: 9497/10000 (94.97%)\n",
      "\n",
      "Round  11, Average loss 131.774 Test accuracy 94.970\n",
      "selected users: [ 1  5  8 11 13 16 19 24 27]\n",
      "\n",
      "Test set: Average loss: 386.3015 \n",
      "Accuracy: 9511/10000 (95.11%)\n",
      "\n",
      "Round  12, Average loss 386.301 Test accuracy 95.110\n",
      "selected users: [ 1  2  3  5 10 16 17 20 26]\n",
      "\n",
      "Test set: Average loss: 3395.8173 \n",
      "Accuracy: 9291/10000 (92.91%)\n",
      "\n",
      "Round  13, Average loss 3395.817 Test accuracy 92.910\n",
      "selected users: [ 0  2  3  5 20 23 24 26 30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 10654.3964 \n",
      "Accuracy: 9298/10000 (92.98%)\n",
      "\n",
      "Round  14, Average loss 10654.396 Test accuracy 92.980\n",
      "selected users: [ 1  5  7  8 12 15 17 26 29]\n",
      "\n",
      "Test set: Average loss: 23878.9069 \n",
      "Accuracy: 9316/10000 (93.16%)\n",
      "\n",
      "Round  15, Average loss 23878.907 Test accuracy 93.160\n",
      "selected users: [ 1  4  6  9 13 14 23 24 25]\n",
      "\n",
      "Test set: Average loss: 51602.2263 \n",
      "Accuracy: 9357/10000 (93.57%)\n",
      "\n",
      "Round  16, Average loss 51602.226 Test accuracy 93.570\n",
      "selected users: [ 0  6  8  9 12 15 25 27 29]\n",
      "\n",
      "Test set: Average loss: 125313.2770 \n",
      "Accuracy: 9356/10000 (93.56%)\n",
      "\n",
      "Round  17, Average loss 125313.277 Test accuracy 93.560\n",
      "selected users: [ 1  9 12 14 16 19 20 23 28]\n",
      "\n",
      "Test set: Average loss: 254749.8428 \n",
      "Accuracy: 9394/10000 (93.94%)\n",
      "\n",
      "Round  18, Average loss 254749.843 Test accuracy 93.940\n",
      "selected users: [ 1  4  5  6 11 14 22 23 26]\n",
      "\n",
      "Test set: Average loss: 628315.3952 \n",
      "Accuracy: 9383/10000 (93.83%)\n",
      "\n",
      "Round  19, Average loss 628315.395 Test accuracy 93.830\n",
      "selected users: [ 2  3  9 10 12 14 19 22 28]\n",
      "\n",
      "Test set: Average loss: 1253561.5328 \n",
      "Accuracy: 9417/10000 (94.17%)\n",
      "\n",
      "Round  20, Average loss 1253561.533 Test accuracy 94.170\n",
      "selected users: [ 3  8 13 20 21 22 23 27 28]\n",
      "\n",
      "Test set: Average loss: 2847076.3456 \n",
      "Accuracy: 9419/10000 (94.19%)\n",
      "\n",
      "Round  21, Average loss 2847076.346 Test accuracy 94.190\n",
      "selected users: [10 13 15 18 20 23 25 28 30]\n",
      "\n",
      "Test set: Average loss: 7302751.9168 \n",
      "Accuracy: 9421/10000 (94.21%)\n",
      "\n",
      "Round  22, Average loss 7302751.917 Test accuracy 94.210\n",
      "selected users: [ 2  4  8 13 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 17060867.7248 \n",
      "Accuracy: 9426/10000 (94.26%)\n",
      "\n",
      "Round  23, Average loss 17060867.725 Test accuracy 94.260\n",
      "selected users: [ 2  7 10 12 19 20 23 29 30]\n",
      "\n",
      "Test set: Average loss: 38013825.5360 \n",
      "Accuracy: 9435/10000 (94.35%)\n",
      "\n",
      "Round  24, Average loss 38013825.536 Test accuracy 94.350\n",
      "selected users: [ 0  5  6  8 11 21 26 27 29]\n",
      "\n",
      "Test set: Average loss: 92776617.0112 \n",
      "Accuracy: 9431/10000 (94.31%)\n",
      "\n",
      "Round  25, Average loss 92776617.011 Test accuracy 94.310\n",
      "selected users: [ 1  6  7 10 13 16 18 20 29]\n",
      "\n",
      "Test set: Average loss: 221109497.4464 \n",
      "Accuracy: 9430/10000 (94.30%)\n",
      "\n",
      "Round  26, Average loss 221109497.446 Test accuracy 94.300\n",
      "selected users: [ 1  6  8  9 16 21 23 24 30]\n",
      "\n",
      "Test set: Average loss: 527633467.8016 \n",
      "Accuracy: 9434/10000 (94.34%)\n",
      "\n",
      "Round  27, Average loss 527633467.802 Test accuracy 94.340\n",
      "selected users: [ 2  4  9 11 16 17 19 20 24]\n",
      "\n",
      "Test set: Average loss: 1246102731.9808 \n",
      "Accuracy: 9431/10000 (94.31%)\n",
      "\n",
      "Round  28, Average loss 1246102731.981 Test accuracy 94.310\n",
      "selected users: [ 0  1  7 11 12 24 26 27 28]\n",
      "\n",
      "Test set: Average loss: 2904159250.0224 \n",
      "Accuracy: 9434/10000 (94.34%)\n",
      "\n",
      "Round  29, Average loss 2904159250.022 Test accuracy 94.340\n",
      "(m= 9 )  3 -th Trial!!\n",
      "selected users: [ 7 10 13 14 22 23 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 1  5 10 20 22 23 27 28 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   1, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 1  2  7 11 15 18 21 26 27]\n",
      "\n",
      "Test set: Average loss: 1.7965 \n",
      "Accuracy: 7086/10000 (70.86%)\n",
      "\n",
      "Round   2, Average loss 1.797 Test accuracy 70.860\n",
      "selected users: [ 0  7 10 11 12 22 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.8279 \n",
      "Accuracy: 9424/10000 (94.24%)\n",
      "\n",
      "Round   3, Average loss 0.828 Test accuracy 94.240\n",
      "selected users: [ 4  6  7  9 12 16 18 20 30]\n",
      "\n",
      "Test set: Average loss: 2.0500 \n",
      "Accuracy: 9547/10000 (95.47%)\n",
      "\n",
      "Round   4, Average loss 2.050 Test accuracy 95.470\n",
      "selected users: [ 3 10 11 12 15 17 22 25 30]\n",
      "\n",
      "Test set: Average loss: 1.4740 \n",
      "Accuracy: 9469/10000 (94.69%)\n",
      "\n",
      "Round   5, Average loss 1.474 Test accuracy 94.690\n",
      "selected users: [ 0  1  9 10 13 15 19 23 30]\n",
      "\n",
      "Test set: Average loss: 0.9459 \n",
      "Accuracy: 7102/10000 (71.02%)\n",
      "\n",
      "Round   6, Average loss 0.946 Test accuracy 71.020\n",
      "selected users: [ 1  2  8  9 14 19 22 26 30]\n",
      "\n",
      "Test set: Average loss: 0.4238 \n",
      "Accuracy: 8934/10000 (89.34%)\n",
      "\n",
      "Round   7, Average loss 0.424 Test accuracy 89.340\n",
      "selected users: [ 0  2  3  7 12 13 15 18 23]\n",
      "\n",
      "Test set: Average loss: 0.9528 \n",
      "Accuracy: 9421/10000 (94.21%)\n",
      "\n",
      "Round   8, Average loss 0.953 Test accuracy 94.210\n",
      "selected users: [ 6  9 11 15 20 22 23 27 30]\n",
      "\n",
      "Test set: Average loss: 17.5556 \n",
      "Accuracy: 9541/10000 (95.41%)\n",
      "\n",
      "Round   9, Average loss 17.556 Test accuracy 95.410\n",
      "selected users: [ 4  5  9 11 13 15 17 18 27]\n",
      "\n",
      "Test set: Average loss: 55.0090 \n",
      "Accuracy: 9593/10000 (95.93%)\n",
      "\n",
      "Round  10, Average loss 55.009 Test accuracy 95.930\n",
      "selected users: [ 1  6  7  9 20 22 24 26 30]\n",
      "\n",
      "Test set: Average loss: 181.0762 \n",
      "Accuracy: 9576/10000 (95.76%)\n",
      "\n",
      "Round  11, Average loss 181.076 Test accuracy 95.760\n",
      "selected users: [ 2  4  8 14 16 17 19 21 27]\n",
      "\n",
      "Test set: Average loss: 301.1772 \n",
      "Accuracy: 9629/10000 (96.29%)\n",
      "\n",
      "Round  12, Average loss 301.177 Test accuracy 96.290\n",
      "selected users: [ 0  3 11 13 14 17 19 24 30]\n",
      "\n",
      "Test set: Average loss: 339.5629 \n",
      "Accuracy: 9617/10000 (96.17%)\n",
      "\n",
      "Round  13, Average loss 339.563 Test accuracy 96.170\n",
      "selected users: [ 0  2  6 13 17 19 22 23 27]\n",
      "\n",
      "Test set: Average loss: 976.2892 \n",
      "Accuracy: 9638/10000 (96.38%)\n",
      "\n",
      "Round  14, Average loss 976.289 Test accuracy 96.380\n",
      "selected users: [ 0  1  2  4  5  7  9 10 19]\n",
      "\n",
      "Test set: Average loss: 2056.7332 \n",
      "Accuracy: 9642/10000 (96.42%)\n",
      "\n",
      "Round  15, Average loss 2056.733 Test accuracy 96.420\n",
      "selected users: [ 1  3  8 16 18 21 23 24 29]\n",
      "\n",
      "Test set: Average loss: 4945.8763 \n",
      "Accuracy: 9623/10000 (96.23%)\n",
      "\n",
      "Round  16, Average loss 4945.876 Test accuracy 96.230\n",
      "selected users: [ 2 10 11 17 20 22 25 27 29]\n",
      "\n",
      "Test set: Average loss: 8463.3119 \n",
      "Accuracy: 9645/10000 (96.45%)\n",
      "\n",
      "Round  17, Average loss 8463.312 Test accuracy 96.450\n",
      "selected users: [ 2  7 10 11 12 16 23 25 26]\n",
      "\n",
      "Test set: Average loss: 23504.1104 \n",
      "Accuracy: 9625/10000 (96.25%)\n",
      "\n",
      "Round  18, Average loss 23504.110 Test accuracy 96.250\n",
      "selected users: [ 3  4  7 10 14 16 28 29 30]\n",
      "\n",
      "Test set: Average loss: 70118.5583 \n",
      "Accuracy: 9642/10000 (96.42%)\n",
      "\n",
      "Round  19, Average loss 70118.558 Test accuracy 96.420\n",
      "selected users: [ 1  5  7 10 14 17 18 25 27]\n",
      "\n",
      "Test set: Average loss: 184027.1376 \n",
      "Accuracy: 9639/10000 (96.39%)\n",
      "\n",
      "Round  20, Average loss 184027.138 Test accuracy 96.390\n",
      "selected users: [ 1  8  9 12 15 22 25 26 27]\n",
      "\n",
      "Test set: Average loss: 449987.6888 \n",
      "Accuracy: 9643/10000 (96.43%)\n",
      "\n",
      "Round  21, Average loss 449987.689 Test accuracy 96.430\n",
      "selected users: [ 2  7 16 17 21 23 24 27 29]\n",
      "\n",
      "Test set: Average loss: 1095817.2432 \n",
      "Accuracy: 9637/10000 (96.37%)\n",
      "\n",
      "Round  22, Average loss 1095817.243 Test accuracy 96.370\n",
      "selected users: [ 1  2 11 13 14 19 22 23 28]\n",
      "\n",
      "Test set: Average loss: 2300600.4352 \n",
      "Accuracy: 9636/10000 (96.36%)\n",
      "\n",
      "Round  23, Average loss 2300600.435 Test accuracy 96.360\n",
      "selected users: [ 0  1  4  5 13 18 21 23 25]\n",
      "\n",
      "Test set: Average loss: 5857845.4784 \n",
      "Accuracy: 9636/10000 (96.36%)\n",
      "\n",
      "Round  24, Average loss 5857845.478 Test accuracy 96.360\n",
      "selected users: [ 0  6 11 16 19 22 23 29 30]\n",
      "\n",
      "Test set: Average loss: 13826708.0960 \n",
      "Accuracy: 9631/10000 (96.31%)\n",
      "\n",
      "Round  25, Average loss 13826708.096 Test accuracy 96.310\n",
      "selected users: [ 1  3  6  8  9 12 20 24 30]\n",
      "\n",
      "Test set: Average loss: 32871346.6880 \n",
      "Accuracy: 9636/10000 (96.36%)\n",
      "\n",
      "Round  26, Average loss 32871346.688 Test accuracy 96.360\n",
      "selected users: [ 1  4  9 13 16 19 21 28 29]\n",
      "\n",
      "Test set: Average loss: 78549871.0016 \n",
      "Accuracy: 9637/10000 (96.37%)\n",
      "\n",
      "Round  27, Average loss 78549871.002 Test accuracy 96.370\n",
      "selected users: [ 6  7  9 10 11 13 26 27 28]\n",
      "\n",
      "Test set: Average loss: 187667893.2480 \n",
      "Accuracy: 9639/10000 (96.39%)\n",
      "\n",
      "Round  28, Average loss 187667893.248 Test accuracy 96.390\n",
      "selected users: [ 2  3  4  5  6  8 12 16 17]\n",
      "\n",
      "Test set: Average loss: 433526267.9040 \n",
      "Accuracy: 9633/10000 (96.33%)\n",
      "\n",
      "Round  29, Average loss 433526267.904 Test accuracy 96.330\n",
      "(m= 9 )  4 -th Trial!!\n",
      "selected users: [ 0  2  5  8 11 18 23 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 3  4  5  6  7 12 13 19 23]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   1, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 2  3  6 12 23 25 26 27 29]\n",
      "\n",
      "Test set: Average loss: 2.2166 \n",
      "Accuracy: 2655/10000 (26.55%)\n",
      "\n",
      "Round   2, Average loss 2.217 Test accuracy 26.550\n",
      "selected users: [ 0  2  7  9 13 17 19 21 25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.5572 \n",
      "Accuracy: 8252/10000 (82.52%)\n",
      "\n",
      "Round   3, Average loss 0.557 Test accuracy 82.520\n",
      "selected users: [ 1  3 10 12 13 18 20 22 30]\n",
      "\n",
      "Test set: Average loss: 0.6204 \n",
      "Accuracy: 9494/10000 (94.94%)\n",
      "\n",
      "Round   4, Average loss 0.620 Test accuracy 94.940\n",
      "selected users: [ 3  7  9 11 21 23 25 26 29]\n",
      "\n",
      "Test set: Average loss: 6.0137 \n",
      "Accuracy: 9325/10000 (93.25%)\n",
      "\n",
      "Round   5, Average loss 6.014 Test accuracy 93.250\n",
      "selected users: [ 1  4  7  8 12 19 21 28 29]\n",
      "\n",
      "Test set: Average loss: 4.6866 \n",
      "Accuracy: 9502/10000 (95.02%)\n",
      "\n",
      "Round   6, Average loss 4.687 Test accuracy 95.020\n",
      "selected users: [ 0  2 10 14 16 24 26 28 29]\n",
      "\n",
      "Test set: Average loss: 10.8125 \n",
      "Accuracy: 9297/10000 (92.97%)\n",
      "\n",
      "Round   7, Average loss 10.813 Test accuracy 92.970\n",
      "selected users: [ 0  3  4 16 17 18 22 23 27]\n",
      "\n",
      "Test set: Average loss: 490.0241 \n",
      "Accuracy: 9193/10000 (91.93%)\n",
      "\n",
      "Round   8, Average loss 490.024 Test accuracy 91.930\n",
      "selected users: [ 3  4  5 15 17 22 25 27 29]\n",
      "\n",
      "Test set: Average loss: 780.5161 \n",
      "Accuracy: 9364/10000 (93.64%)\n",
      "\n",
      "Round   9, Average loss 780.516 Test accuracy 93.640\n",
      "selected users: [ 0  4  9 10 12 13 20 24 30]\n",
      "\n",
      "Test set: Average loss: 2140.7408 \n",
      "Accuracy: 9384/10000 (93.84%)\n",
      "\n",
      "Round  10, Average loss 2140.741 Test accuracy 93.840\n",
      "selected users: [ 1 10 11 12 18 22 28 29 30]\n",
      "\n",
      "Test set: Average loss: 4433.6799 \n",
      "Accuracy: 9424/10000 (94.24%)\n",
      "\n",
      "Round  11, Average loss 4433.680 Test accuracy 94.240\n",
      "selected users: [ 2  4  8 10 13 16 23 25 28]\n",
      "\n",
      "Test set: Average loss: 9921.5765 \n",
      "Accuracy: 9413/10000 (94.13%)\n",
      "\n",
      "Round  12, Average loss 9921.576 Test accuracy 94.130\n",
      "selected users: [ 6  7  9 14 22 24 25 27 28]\n",
      "\n",
      "Test set: Average loss: 18653.8259 \n",
      "Accuracy: 9472/10000 (94.72%)\n",
      "\n",
      "Round  13, Average loss 18653.826 Test accuracy 94.720\n",
      "selected users: [ 0  6 15 17 20 21 26 27 28]\n",
      "\n",
      "Test set: Average loss: 55819.6297 \n",
      "Accuracy: 9437/10000 (94.37%)\n",
      "\n",
      "Round  14, Average loss 55819.630 Test accuracy 94.370\n",
      "selected users: [ 0  1  7  8 13 16 17 18 20]\n",
      "\n",
      "Test set: Average loss: 119532.5606 \n",
      "Accuracy: 9460/10000 (94.60%)\n",
      "\n",
      "Round  15, Average loss 119532.561 Test accuracy 94.600\n",
      "selected users: [ 0  2  4 10 11 14 16 25 28]\n",
      "\n",
      "Test set: Average loss: 366110.1180 \n",
      "Accuracy: 9427/10000 (94.27%)\n",
      "\n",
      "Round  16, Average loss 366110.118 Test accuracy 94.270\n",
      "selected users: [ 1  5  6  8 10 11 20 27 29]\n",
      "\n",
      "Test set: Average loss: 920448.6616 \n",
      "Accuracy: 9418/10000 (94.18%)\n",
      "\n",
      "Round  17, Average loss 920448.662 Test accuracy 94.180\n",
      "selected users: [ 0  1 14 16 22 24 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1744398.4144 \n",
      "Accuracy: 9477/10000 (94.77%)\n",
      "\n",
      "Round  18, Average loss 1744398.414 Test accuracy 94.770\n",
      "selected users: [ 4 10 15 17 18 20 21 29 30]\n",
      "\n",
      "Test set: Average loss: 4092777.5296 \n",
      "Accuracy: 9467/10000 (94.67%)\n",
      "\n",
      "Round  19, Average loss 4092777.530 Test accuracy 94.670\n",
      "selected users: [ 0  8  9 14 15 17 19 20 23]\n",
      "\n",
      "Test set: Average loss: 9597506.5920 \n",
      "Accuracy: 9465/10000 (94.65%)\n",
      "\n",
      "Round  20, Average loss 9597506.592 Test accuracy 94.650\n",
      "selected users: [ 0  3  5  6  8 12 16 19 24]\n",
      "\n",
      "Test set: Average loss: 22651917.0816 \n",
      "Accuracy: 9469/10000 (94.69%)\n",
      "\n",
      "Round  21, Average loss 22651917.082 Test accuracy 94.690\n",
      "selected users: [ 0  2  3  4 11 18 24 26 30]\n",
      "\n",
      "Test set: Average loss: 54357819.5456 \n",
      "Accuracy: 9466/10000 (94.66%)\n",
      "\n",
      "Round  22, Average loss 54357819.546 Test accuracy 94.660\n",
      "selected users: [ 5  7 12 15 17 18 19 21 28]\n",
      "\n",
      "Test set: Average loss: 128752681.8816 \n",
      "Accuracy: 9469/10000 (94.69%)\n",
      "\n",
      "Round  23, Average loss 128752681.882 Test accuracy 94.690\n",
      "selected users: [ 1  6  9 10 20 21 23 27 30]\n",
      "\n",
      "Test set: Average loss: 310793197.1584 \n",
      "Accuracy: 9464/10000 (94.64%)\n",
      "\n",
      "Round  24, Average loss 310793197.158 Test accuracy 94.640\n",
      "selected users: [ 0  4  7 12 18 22 23 25 27]\n",
      "\n",
      "Test set: Average loss: 736860685.9264 \n",
      "Accuracy: 9463/10000 (94.63%)\n",
      "\n",
      "Round  25, Average loss 736860685.926 Test accuracy 94.630\n",
      "selected users: [ 5 10 11 13 17 22 23 24 30]\n",
      "\n",
      "Test set: Average loss: 1750233785.9584 \n",
      "Accuracy: 9464/10000 (94.64%)\n",
      "\n",
      "Round  26, Average loss 1750233785.958 Test accuracy 94.640\n",
      "selected users: [ 0  2  4  7  8 13 14 23 26]\n",
      "\n",
      "Test set: Average loss: 4161683868.8768 \n",
      "Accuracy: 9463/10000 (94.63%)\n",
      "\n",
      "Round  27, Average loss 4161683868.877 Test accuracy 94.630\n",
      "selected users: [ 1  2  5  8 11 15 20 21 29]\n",
      "\n",
      "Test set: Average loss: 9864010550.4768 \n",
      "Accuracy: 9462/10000 (94.62%)\n",
      "\n",
      "Round  28, Average loss 9864010550.477 Test accuracy 94.620\n",
      "selected users: [ 3  5 13 17 23 24 25 26 29]\n",
      "\n",
      "Test set: Average loss: 23398907353.4976 \n",
      "Accuracy: 9463/10000 (94.63%)\n",
      "\n",
      "Round  29, Average loss 23398907353.498 Test accuracy 94.630\n",
      "(m= 9 )  5 -th Trial!!\n",
      "selected users: [ 2  3  8  9 10 13 14 17 28]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 1  4 14 17 23 25 26 27 28]\n",
      "\n",
      "Test set: Average loss: 1.6645 \n",
      "Accuracy: 5098/10000 (50.98%)\n",
      "\n",
      "Round   1, Average loss 1.664 Test accuracy 50.980\n",
      "selected users: [ 8 10 12 13 19 24 25 26 30]\n",
      "\n",
      "Test set: Average loss: 0.2391 \n",
      "Accuracy: 9339/10000 (93.39%)\n",
      "\n",
      "Round   2, Average loss 0.239 Test accuracy 93.390\n",
      "selected users: [ 2  4 14 18 19 21 22 23 27]\n",
      "\n",
      "Test set: Average loss: 0.4494 \n",
      "Accuracy: 9586/10000 (95.86%)\n",
      "\n",
      "Round   3, Average loss 0.449 Test accuracy 95.860\n",
      "selected users: [ 1  5  6 13 15 16 17 23 28]\n",
      "\n",
      "Test set: Average loss: 0.2754 \n",
      "Accuracy: 9400/10000 (94.00%)\n",
      "\n",
      "Round   4, Average loss 0.275 Test accuracy 94.000\n",
      "selected users: [ 1  3  5  6 14 21 26 28 30]\n",
      "\n",
      "Test set: Average loss: 1.8704 \n",
      "Accuracy: 9605/10000 (96.05%)\n",
      "\n",
      "Round   5, Average loss 1.870 Test accuracy 96.050\n",
      "selected users: [14 18 19 20 23 25 26 28 29]\n",
      "\n",
      "Test set: Average loss: 0.2772 \n",
      "Accuracy: 9195/10000 (91.95%)\n",
      "\n",
      "Round   6, Average loss 0.277 Test accuracy 91.950\n",
      "selected users: [ 8 12 13 17 24 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.4783 \n",
      "Accuracy: 9539/10000 (95.39%)\n",
      "\n",
      "Round   7, Average loss 0.478 Test accuracy 95.390\n",
      "selected users: [ 0  7  8 12 13 15 20 23 29]\n",
      "\n",
      "Test set: Average loss: 4.1187 \n",
      "Accuracy: 9549/10000 (95.49%)\n",
      "\n",
      "Round   8, Average loss 4.119 Test accuracy 95.490\n",
      "selected users: [ 3  6  8 10 11 15 16 19 29]\n",
      "\n",
      "Test set: Average loss: 1.7433 \n",
      "Accuracy: 9541/10000 (95.41%)\n",
      "\n",
      "Round   9, Average loss 1.743 Test accuracy 95.410\n",
      "selected users: [ 8 11 13 16 17 18 19 26 29]\n",
      "\n",
      "Test set: Average loss: 6.5653 \n",
      "Accuracy: 9588/10000 (95.88%)\n",
      "\n",
      "Round  10, Average loss 6.565 Test accuracy 95.880\n",
      "selected users: [ 2  9 11 14 16 17 21 25 30]\n",
      "\n",
      "Test set: Average loss: 19.6775 \n",
      "Accuracy: 9604/10000 (96.04%)\n",
      "\n",
      "Round  11, Average loss 19.678 Test accuracy 96.040\n",
      "selected users: [ 3  5 13 18 19 21 25 27 29]\n",
      "\n",
      "Test set: Average loss: 236.5422 \n",
      "Accuracy: 9544/10000 (95.44%)\n",
      "\n",
      "Round  12, Average loss 236.542 Test accuracy 95.440\n",
      "selected users: [ 0  2  3  8  9 10 19 26 27]\n",
      "\n",
      "Test set: Average loss: 759.9806 \n",
      "Accuracy: 9564/10000 (95.64%)\n",
      "\n",
      "Round  13, Average loss 759.981 Test accuracy 95.640\n",
      "selected users: [ 2  7  8 15 20 23 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1521.8007 \n",
      "Accuracy: 9560/10000 (95.60%)\n",
      "\n",
      "Round  14, Average loss 1521.801 Test accuracy 95.600\n",
      "selected users: [ 0  3  4  6 14 17 23 25 29]\n",
      "\n",
      "Test set: Average loss: 1784.5789 \n",
      "Accuracy: 9589/10000 (95.89%)\n",
      "\n",
      "Round  15, Average loss 1784.579 Test accuracy 95.890\n",
      "selected users: [ 5 11 13 14 15 17 21 24 28]\n",
      "\n",
      "Test set: Average loss: 6031.1822 \n",
      "Accuracy: 9565/10000 (95.65%)\n",
      "\n",
      "Round  16, Average loss 6031.182 Test accuracy 95.650\n",
      "selected users: [ 0  2  6  9 12 19 21 22 27]\n",
      "\n",
      "Test set: Average loss: 12298.9996 \n",
      "Accuracy: 9602/10000 (96.02%)\n",
      "\n",
      "Round  17, Average loss 12299.000 Test accuracy 96.020\n",
      "selected users: [ 0  1  4 13 14 15 16 24 28]\n",
      "\n",
      "Test set: Average loss: 35820.1056 \n",
      "Accuracy: 9594/10000 (95.94%)\n",
      "\n",
      "Round  18, Average loss 35820.106 Test accuracy 95.940\n",
      "selected users: [ 0  3 13 14 16 17 23 25 26]\n",
      "\n",
      "Test set: Average loss: 79854.0795 \n",
      "Accuracy: 9534/10000 (95.34%)\n",
      "\n",
      "Round  19, Average loss 79854.080 Test accuracy 95.340\n",
      "selected users: [ 1  3  7 12 15 25 27 29 30]\n",
      "\n",
      "Test set: Average loss: 183658.2100 \n",
      "Accuracy: 9583/10000 (95.83%)\n",
      "\n",
      "Round  20, Average loss 183658.210 Test accuracy 95.830\n",
      "selected users: [ 5  7  9 11 18 22 24 26 30]\n",
      "\n",
      "Test set: Average loss: 493836.5672 \n",
      "Accuracy: 9575/10000 (95.75%)\n",
      "\n",
      "Round  21, Average loss 493836.567 Test accuracy 95.750\n",
      "selected users: [ 7 11 13 18 20 22 25 26 30]\n",
      "\n",
      "Test set: Average loss: 1192730.0912 \n",
      "Accuracy: 9575/10000 (95.75%)\n",
      "\n",
      "Round  22, Average loss 1192730.091 Test accuracy 95.750\n",
      "selected users: [ 3  5  7 11 15 20 21 22 27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2847331.0464 \n",
      "Accuracy: 9583/10000 (95.83%)\n",
      "\n",
      "Round  23, Average loss 2847331.046 Test accuracy 95.830\n",
      "selected users: [ 1  6  9 14 17 18 19 20 22]\n",
      "\n",
      "Test set: Average loss: 6182209.9520 \n",
      "Accuracy: 9606/10000 (96.06%)\n",
      "\n",
      "Round  24, Average loss 6182209.952 Test accuracy 96.060\n",
      "selected users: [ 4  5  6  7  9 15 21 23 25]\n",
      "\n",
      "Test set: Average loss: 15576707.9680 \n",
      "Accuracy: 9597/10000 (95.97%)\n",
      "\n",
      "Round  25, Average loss 15576707.968 Test accuracy 95.970\n",
      "selected users: [ 0  2  3  6  7 12 15 19 22]\n",
      "\n",
      "Test set: Average loss: 36657109.4016 \n",
      "Accuracy: 9600/10000 (96.00%)\n",
      "\n",
      "Round  26, Average loss 36657109.402 Test accuracy 96.000\n",
      "selected users: [ 9 11 12 16 17 23 25 26 29]\n",
      "\n",
      "Test set: Average loss: 83118491.6480 \n",
      "Accuracy: 9605/10000 (96.05%)\n",
      "\n",
      "Round  27, Average loss 83118491.648 Test accuracy 96.050\n",
      "selected users: [ 0  3  5  8  9 12 17 18 24]\n",
      "\n",
      "Test set: Average loss: 195906632.0896 \n",
      "Accuracy: 9603/10000 (96.03%)\n",
      "\n",
      "Round  28, Average loss 195906632.090 Test accuracy 96.030\n",
      "selected users: [ 1  3  5  7 12 13 17 18 26]\n",
      "\n",
      "Test set: Average loss: 466712603.0336 \n",
      "Accuracy: 9598/10000 (95.98%)\n",
      "\n",
      "Round  29, Average loss 466712603.034 Test accuracy 95.980\n",
      "(m= 9 )  6 -th Trial!!\n",
      "selected users: [ 3 10 11 16 19 21 23 27 29]\n",
      "\n",
      "Test set: Average loss: 2.3024 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.302 Test accuracy 9.800\n",
      "selected users: [ 4  5  7 16 18 21 25 26 30]\n",
      "\n",
      "Test set: Average loss: 2.0176 \n",
      "Accuracy: 4500/10000 (45.00%)\n",
      "\n",
      "Round   1, Average loss 2.018 Test accuracy 45.000\n",
      "selected users: [ 1  4  6  8 12 15 20 21 24]\n",
      "\n",
      "Test set: Average loss: 0.3340 \n",
      "Accuracy: 9077/10000 (90.77%)\n",
      "\n",
      "Round   2, Average loss 0.334 Test accuracy 90.770\n",
      "selected users: [ 2  7 15 16 17 19 23 26 28]\n",
      "\n",
      "Test set: Average loss: 0.2287 \n",
      "Accuracy: 9366/10000 (93.66%)\n",
      "\n",
      "Round   3, Average loss 0.229 Test accuracy 93.660\n",
      "selected users: [ 5  6  7  8 13 14 21 24 28]\n",
      "\n",
      "Test set: Average loss: 3.7653 \n",
      "Accuracy: 9169/10000 (91.69%)\n",
      "\n",
      "Round   4, Average loss 3.765 Test accuracy 91.690\n",
      "selected users: [ 0  4  7 10 16 22 23 27 28]\n",
      "\n",
      "Test set: Average loss: 11.0592 \n",
      "Accuracy: 9371/10000 (93.71%)\n",
      "\n",
      "Round   5, Average loss 11.059 Test accuracy 93.710\n",
      "selected users: [ 3  8  9 12 14 17 18 19 30]\n",
      "\n",
      "Test set: Average loss: 1.3283 \n",
      "Accuracy: 9135/10000 (91.35%)\n",
      "\n",
      "Round   6, Average loss 1.328 Test accuracy 91.350\n",
      "selected users: [ 0  5  6 12 18 22 24 26 30]\n",
      "\n",
      "Test set: Average loss: 31.3132 \n",
      "Accuracy: 9415/10000 (94.15%)\n",
      "\n",
      "Round   7, Average loss 31.313 Test accuracy 94.150\n",
      "selected users: [ 5  6  9 10 13 17 19 27 30]\n",
      "\n",
      "Test set: Average loss: 81.2049 \n",
      "Accuracy: 9472/10000 (94.72%)\n",
      "\n",
      "Round   8, Average loss 81.205 Test accuracy 94.720\n",
      "selected users: [ 0  3  4 13 17 21 24 26 28]\n",
      "\n",
      "Test set: Average loss: 436.0860 \n",
      "Accuracy: 9423/10000 (94.23%)\n",
      "\n",
      "Round   9, Average loss 436.086 Test accuracy 94.230\n",
      "selected users: [ 0  4  7 10 11 17 22 28 29]\n",
      "\n",
      "Test set: Average loss: 510.4229 \n",
      "Accuracy: 9502/10000 (95.02%)\n",
      "\n",
      "Round  10, Average loss 510.423 Test accuracy 95.020\n",
      "selected users: [ 1  3  8  9 15 18 22 23 25]\n",
      "\n",
      "Test set: Average loss: 1089.5641 \n",
      "Accuracy: 9512/10000 (95.12%)\n",
      "\n",
      "Round  11, Average loss 1089.564 Test accuracy 95.120\n",
      "selected users: [ 0  1  2  6  8  9 13 17 28]\n",
      "\n",
      "Test set: Average loss: 1292.0583 \n",
      "Accuracy: 9541/10000 (95.41%)\n",
      "\n",
      "Round  12, Average loss 1292.058 Test accuracy 95.410\n",
      "selected users: [ 2  5  7  8 12 19 22 26 27]\n",
      "\n",
      "Test set: Average loss: 2585.6798 \n",
      "Accuracy: 9526/10000 (95.26%)\n",
      "\n",
      "Round  13, Average loss 2585.680 Test accuracy 95.260\n",
      "selected users: [ 1  9 10 11 13 17 18 29 30]\n",
      "\n",
      "Test set: Average loss: 3489.1472 \n",
      "Accuracy: 9284/10000 (92.84%)\n",
      "\n",
      "Round  14, Average loss 3489.147 Test accuracy 92.840\n",
      "selected users: [ 1  3  8 11 14 15 16 21 24]\n",
      "\n",
      "Test set: Average loss: 7457.8884 \n",
      "Accuracy: 9484/10000 (94.84%)\n",
      "\n",
      "Round  15, Average loss 7457.888 Test accuracy 94.840\n",
      "selected users: [ 2  3  9 13 17 18 19 22 27]\n",
      "\n",
      "Test set: Average loss: 15058.9555 \n",
      "Accuracy: 9507/10000 (95.07%)\n",
      "\n",
      "Round  16, Average loss 15058.955 Test accuracy 95.070\n",
      "selected users: [ 1  6  7 11 12 14 17 25 26]\n",
      "\n",
      "Test set: Average loss: 43731.2970 \n",
      "Accuracy: 9511/10000 (95.11%)\n",
      "\n",
      "Round  17, Average loss 43731.297 Test accuracy 95.110\n",
      "selected users: [ 1  7  8 15 17 21 22 25 26]\n",
      "\n",
      "Test set: Average loss: 136084.8301 \n",
      "Accuracy: 9524/10000 (95.24%)\n",
      "\n",
      "Round  18, Average loss 136084.830 Test accuracy 95.240\n",
      "selected users: [ 8 12 13 14 17 18 25 27 29]\n",
      "\n",
      "Test set: Average loss: 330862.8366 \n",
      "Accuracy: 9512/10000 (95.12%)\n",
      "\n",
      "Round  19, Average loss 330862.837 Test accuracy 95.120\n",
      "selected users: [ 2  3  7  8  9 11 18 19 29]\n",
      "\n",
      "Test set: Average loss: 759168.6244 \n",
      "Accuracy: 9512/10000 (95.12%)\n",
      "\n",
      "Round  20, Average loss 759168.624 Test accuracy 95.120\n",
      "selected users: [ 1  3  5 15 17 22 25 26 28]\n",
      "\n",
      "Test set: Average loss: 1920716.8400 \n",
      "Accuracy: 9520/10000 (95.20%)\n",
      "\n",
      "Round  21, Average loss 1920716.840 Test accuracy 95.200\n",
      "selected users: [ 3  5  9 10 12 16 20 26 27]\n",
      "\n",
      "Test set: Average loss: 4693916.6848 \n",
      "Accuracy: 9521/10000 (95.21%)\n",
      "\n",
      "Round  22, Average loss 4693916.685 Test accuracy 95.210\n",
      "selected users: [ 3  5  8 16 18 19 23 27 29]\n",
      "\n",
      "Test set: Average loss: 11272416.3328 \n",
      "Accuracy: 9511/10000 (95.11%)\n",
      "\n",
      "Round  23, Average loss 11272416.333 Test accuracy 95.110\n",
      "selected users: [ 0  2  3  4  5  6 10 14 25]\n",
      "\n",
      "Test set: Average loss: 27340090.7264 \n",
      "Accuracy: 9521/10000 (95.21%)\n",
      "\n",
      "Round  24, Average loss 27340090.726 Test accuracy 95.210\n",
      "selected users: [ 3  6  7  8  9 11 20 22 24]\n",
      "\n",
      "Test set: Average loss: 67305659.7504 \n",
      "Accuracy: 9511/10000 (95.11%)\n",
      "\n",
      "Round  25, Average loss 67305659.750 Test accuracy 95.110\n",
      "selected users: [ 2  6 10 14 15 18 22 23 26]\n",
      "\n",
      "Test set: Average loss: 161852963.3280 \n",
      "Accuracy: 9512/10000 (95.12%)\n",
      "\n",
      "Round  26, Average loss 161852963.328 Test accuracy 95.120\n",
      "selected users: [ 1  6  7 10 11 18 22 23 30]\n",
      "\n",
      "Test set: Average loss: 390177162.4448 \n",
      "Accuracy: 9514/10000 (95.14%)\n",
      "\n",
      "Round  27, Average loss 390177162.445 Test accuracy 95.140\n",
      "selected users: [ 0  1  5  6  9 16 17 20 30]\n",
      "\n",
      "Test set: Average loss: 933664192.1024 \n",
      "Accuracy: 9518/10000 (95.18%)\n",
      "\n",
      "Round  28, Average loss 933664192.102 Test accuracy 95.180\n",
      "selected users: [ 1  2  4  6 14 16 18 27 30]\n",
      "\n",
      "Test set: Average loss: 2217287560.3968 \n",
      "Accuracy: 9517/10000 (95.17%)\n",
      "\n",
      "Round  29, Average loss 2217287560.397 Test accuracy 95.170\n",
      "(m= 9 )  7 -th Trial!!\n",
      "selected users: [ 3  6 11 14 18 19 21 26 27]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 2  4  5  7  9 16 18 19 25]\n",
      "\n",
      "Test set: Average loss: 1.2800 \n",
      "Accuracy: 7950/10000 (79.50%)\n",
      "\n",
      "Round   1, Average loss 1.280 Test accuracy 79.500\n",
      "selected users: [ 0  6 11 13 14 17 18 22 23]\n",
      "\n",
      "Test set: Average loss: 0.7811 \n",
      "Accuracy: 9109/10000 (91.09%)\n",
      "\n",
      "Round   2, Average loss 0.781 Test accuracy 91.090\n",
      "selected users: [ 2  9 10 14 21 22 23 26 27]\n",
      "\n",
      "Test set: Average loss: 0.4964 \n",
      "Accuracy: 9461/10000 (94.61%)\n",
      "\n",
      "Round   3, Average loss 0.496 Test accuracy 94.610\n",
      "selected users: [ 3  8  9 16 21 22 25 26 27]\n",
      "\n",
      "Test set: Average loss: 2.2133 \n",
      "Accuracy: 9460/10000 (94.60%)\n",
      "\n",
      "Round   4, Average loss 2.213 Test accuracy 94.600\n",
      "selected users: [ 0  7 10 11 19 23 24 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4257 \n",
      "Accuracy: 9245/10000 (92.45%)\n",
      "\n",
      "Round   5, Average loss 0.426 Test accuracy 92.450\n",
      "selected users: [ 0  2  8 12 16 21 23 24 25]\n",
      "\n",
      "Test set: Average loss: 11.5463 \n",
      "Accuracy: 9425/10000 (94.25%)\n",
      "\n",
      "Round   6, Average loss 11.546 Test accuracy 94.250\n",
      "selected users: [ 1  3  5  7 10 14 24 25 29]\n",
      "\n",
      "Test set: Average loss: 1.2198 \n",
      "Accuracy: 6349/10000 (63.49%)\n",
      "\n",
      "Round   7, Average loss 1.220 Test accuracy 63.490\n",
      "selected users: [ 1  2  4 10 11 15 22 25 29]\n",
      "\n",
      "Test set: Average loss: 10.2502 \n",
      "Accuracy: 9465/10000 (94.65%)\n",
      "\n",
      "Round   8, Average loss 10.250 Test accuracy 94.650\n",
      "selected users: [ 2  6  8  9 15 23 25 27 28]\n",
      "\n",
      "Test set: Average loss: 55.1730 \n",
      "Accuracy: 9521/10000 (95.21%)\n",
      "\n",
      "Round   9, Average loss 55.173 Test accuracy 95.210\n",
      "selected users: [ 0  2  3 11 13 18 24 26 30]\n",
      "\n",
      "Test set: Average loss: 456.1216 \n",
      "Accuracy: 9402/10000 (94.02%)\n",
      "\n",
      "Round  10, Average loss 456.122 Test accuracy 94.020\n",
      "selected users: [ 8 12 13 18 21 23 24 28 30]\n",
      "\n",
      "Test set: Average loss: 703.8963 \n",
      "Accuracy: 9453/10000 (94.53%)\n",
      "\n",
      "Round  11, Average loss 703.896 Test accuracy 94.530\n",
      "selected users: [ 1  4  6 11 13 15 18 23 25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 3348.3394 \n",
      "Accuracy: 9407/10000 (94.07%)\n",
      "\n",
      "Round  12, Average loss 3348.339 Test accuracy 94.070\n",
      "selected users: [ 0  1  4  5  6 15 24 26 30]\n",
      "\n",
      "Test set: Average loss: 9510.9690 \n",
      "Accuracy: 9404/10000 (94.04%)\n",
      "\n",
      "Round  13, Average loss 9510.969 Test accuracy 94.040\n",
      "selected users: [ 2  6  7 14 17 19 23 27 30]\n",
      "\n",
      "Test set: Average loss: 15096.7941 \n",
      "Accuracy: 9465/10000 (94.65%)\n",
      "\n",
      "Round  14, Average loss 15096.794 Test accuracy 94.650\n",
      "selected users: [ 2  3  4  7  8 13 23 26 28]\n",
      "\n",
      "Test set: Average loss: 29668.8815 \n",
      "Accuracy: 9478/10000 (94.78%)\n",
      "\n",
      "Round  15, Average loss 29668.881 Test accuracy 94.780\n",
      "selected users: [ 1  3  4 11 13 18 19 25 30]\n",
      "\n",
      "Test set: Average loss: 91639.9292 \n",
      "Accuracy: 9477/10000 (94.77%)\n",
      "\n",
      "Round  16, Average loss 91639.929 Test accuracy 94.770\n",
      "selected users: [ 1  3 10 14 16 18 22 24 28]\n",
      "\n",
      "Test set: Average loss: 195951.7388 \n",
      "Accuracy: 9470/10000 (94.70%)\n",
      "\n",
      "Round  17, Average loss 195951.739 Test accuracy 94.700\n",
      "selected users: [ 3  5 15 16 18 22 24 28 30]\n",
      "\n",
      "Test set: Average loss: 526458.6668 \n",
      "Accuracy: 9445/10000 (94.45%)\n",
      "\n",
      "Round  18, Average loss 526458.667 Test accuracy 94.450\n",
      "selected users: [ 4  7 11 15 16 20 21 27 28]\n",
      "\n",
      "Test set: Average loss: 1376676.6000 \n",
      "Accuracy: 9466/10000 (94.66%)\n",
      "\n",
      "Round  19, Average loss 1376676.600 Test accuracy 94.660\n",
      "selected users: [ 0  3  4  9 11 17 18 20 22]\n",
      "\n",
      "Test set: Average loss: 3353749.0112 \n",
      "Accuracy: 9456/10000 (94.56%)\n",
      "\n",
      "Round  20, Average loss 3353749.011 Test accuracy 94.560\n",
      "selected users: [ 2  3  6  8  9 12 16 19 24]\n",
      "\n",
      "Test set: Average loss: 7785744.6400 \n",
      "Accuracy: 9463/10000 (94.63%)\n",
      "\n",
      "Round  21, Average loss 7785744.640 Test accuracy 94.630\n",
      "selected users: [ 8  9 10 12 17 20 23 27 28]\n",
      "\n",
      "Test set: Average loss: 18367059.1744 \n",
      "Accuracy: 9457/10000 (94.57%)\n",
      "\n",
      "Round  22, Average loss 18367059.174 Test accuracy 94.570\n",
      "selected users: [ 0  6 10 11 15 16 28 29 30]\n",
      "\n",
      "Test set: Average loss: 44870432.1792 \n",
      "Accuracy: 9455/10000 (94.55%)\n",
      "\n",
      "Round  23, Average loss 44870432.179 Test accuracy 94.550\n",
      "selected users: [ 1  9 11 14 15 22 23 24 30]\n",
      "\n",
      "Test set: Average loss: 105634373.7344 \n",
      "Accuracy: 9462/10000 (94.62%)\n",
      "\n",
      "Round  24, Average loss 105634373.734 Test accuracy 94.620\n",
      "selected users: [ 4  8 10 12 13 14 22 25 26]\n",
      "\n",
      "Test set: Average loss: 248955421.2864 \n",
      "Accuracy: 9464/10000 (94.64%)\n",
      "\n",
      "Round  25, Average loss 248955421.286 Test accuracy 94.640\n",
      "selected users: [ 2  3  5  7 12 18 21 25 29]\n",
      "\n",
      "Test set: Average loss: 589142583.2960 \n",
      "Accuracy: 9463/10000 (94.63%)\n",
      "\n",
      "Round  26, Average loss 589142583.296 Test accuracy 94.630\n",
      "selected users: [ 0  4  6 10 14 15 19 22 23]\n",
      "\n",
      "Test set: Average loss: 1401595103.6416 \n",
      "Accuracy: 9459/10000 (94.59%)\n",
      "\n",
      "Round  27, Average loss 1401595103.642 Test accuracy 94.590\n",
      "selected users: [ 2  4  6 13 15 16 18 20 25]\n",
      "\n",
      "Test set: Average loss: 3337395739.0336 \n",
      "Accuracy: 9455/10000 (94.55%)\n",
      "\n",
      "Round  28, Average loss 3337395739.034 Test accuracy 94.550\n",
      "selected users: [ 1  7 17 20 21 22 27 28 29]\n",
      "\n",
      "Test set: Average loss: 7872329318.4000 \n",
      "Accuracy: 9461/10000 (94.61%)\n",
      "\n",
      "Round  29, Average loss 7872329318.400 Test accuracy 94.610\n",
      "(m= 9 )  8 -th Trial!!\n",
      "selected users: [ 0  5 11 15 17 18 20 28 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 983/10000 (9.83%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.830\n",
      "selected users: [ 3  8 10 14 15 16 22 23 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   1, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 1  3 21 22 24 25 26 27 29]\n",
      "\n",
      "Test set: Average loss: 1.5358 \n",
      "Accuracy: 5833/10000 (58.33%)\n",
      "\n",
      "Round   2, Average loss 1.536 Test accuracy 58.330\n",
      "selected users: [ 1  2  5 11 12 17 19 20 21]\n",
      "\n",
      "Test set: Average loss: 3.9582 \n",
      "Accuracy: 9122/10000 (91.22%)\n",
      "\n",
      "Round   3, Average loss 3.958 Test accuracy 91.220\n",
      "selected users: [ 1  5  7  8 18 22 25 29 30]\n",
      "\n",
      "Test set: Average loss: 8.8912 \n",
      "Accuracy: 9253/10000 (92.53%)\n",
      "\n",
      "Round   4, Average loss 8.891 Test accuracy 92.530\n",
      "selected users: [ 4  6 10 14 15 18 20 22 29]\n",
      "\n",
      "Test set: Average loss: 11.4614 \n",
      "Accuracy: 9404/10000 (94.04%)\n",
      "\n",
      "Round   5, Average loss 11.461 Test accuracy 94.040\n",
      "selected users: [ 4  8  9 14 23 25 27 28 29]\n",
      "\n",
      "Test set: Average loss: 9.1638 \n",
      "Accuracy: 9517/10000 (95.17%)\n",
      "\n",
      "Round   6, Average loss 9.164 Test accuracy 95.170\n",
      "selected users: [ 0  1  6  8 12 14 15 27 28]\n",
      "\n",
      "Test set: Average loss: 486.4696 \n",
      "Accuracy: 9260/10000 (92.60%)\n",
      "\n",
      "Round   7, Average loss 486.470 Test accuracy 92.600\n",
      "selected users: [ 5  6  7 13 19 20 21 26 29]\n",
      "\n",
      "Test set: Average loss: 1085.3265 \n",
      "Accuracy: 9319/10000 (93.19%)\n",
      "\n",
      "Round   8, Average loss 1085.327 Test accuracy 93.190\n",
      "selected users: [ 1  2  3  7  9 14 18 23 28]\n",
      "\n",
      "Test set: Average loss: 1167.0281 \n",
      "Accuracy: 9457/10000 (94.57%)\n",
      "\n",
      "Round   9, Average loss 1167.028 Test accuracy 94.570\n",
      "selected users: [ 0  1  3  8  9 12 18 28 29]\n",
      "\n",
      "Test set: Average loss: 1791.6883 \n",
      "Accuracy: 9488/10000 (94.88%)\n",
      "\n",
      "Round  10, Average loss 1791.688 Test accuracy 94.880\n",
      "selected users: [ 0  4  5  7 10 17 19 29 30]\n",
      "\n",
      "Test set: Average loss: 2421.3727 \n",
      "Accuracy: 9467/10000 (94.67%)\n",
      "\n",
      "Round  11, Average loss 2421.373 Test accuracy 94.670\n",
      "selected users: [ 1  6  9 10 18 19 20 23 26]\n",
      "\n",
      "Test set: Average loss: 14831.1824 \n",
      "Accuracy: 9419/10000 (94.19%)\n",
      "\n",
      "Round  12, Average loss 14831.182 Test accuracy 94.190\n",
      "selected users: [ 7  8  9 13 19 22 28 29 30]\n",
      "\n",
      "Test set: Average loss: 27643.3219 \n",
      "Accuracy: 9466/10000 (94.66%)\n",
      "\n",
      "Round  13, Average loss 27643.322 Test accuracy 94.660\n",
      "selected users: [ 4  9 11 12 13 17 21 24 30]\n",
      "\n",
      "Test set: Average loss: 66930.1086 \n",
      "Accuracy: 9479/10000 (94.79%)\n",
      "\n",
      "Round  14, Average loss 66930.109 Test accuracy 94.790\n",
      "selected users: [ 0  4  6 11 15 22 27 28 30]\n",
      "\n",
      "Test set: Average loss: 180074.4020 \n",
      "Accuracy: 9468/10000 (94.68%)\n",
      "\n",
      "Round  15, Average loss 180074.402 Test accuracy 94.680\n",
      "selected users: [ 0  7  8 10 17 21 26 29 30]\n",
      "\n",
      "Test set: Average loss: 484087.6148 \n",
      "Accuracy: 9446/10000 (94.46%)\n",
      "\n",
      "Round  16, Average loss 484087.615 Test accuracy 94.460\n",
      "selected users: [ 1  2  4 18 19 20 22 23 26]\n",
      "\n",
      "Test set: Average loss: 1347507.6320 \n",
      "Accuracy: 9426/10000 (94.26%)\n",
      "\n",
      "Round  17, Average loss 1347507.632 Test accuracy 94.260\n",
      "selected users: [ 1  5 10 11 16 17 20 22 24]\n",
      "\n",
      "Test set: Average loss: 3491752.0896 \n",
      "Accuracy: 9419/10000 (94.19%)\n",
      "\n",
      "Round  18, Average loss 3491752.090 Test accuracy 94.190\n",
      "selected users: [ 4  6  7 12 17 21 25 28 29]\n",
      "\n",
      "Test set: Average loss: 8078327.9808 \n",
      "Accuracy: 9425/10000 (94.25%)\n",
      "\n",
      "Round  19, Average loss 8078327.981 Test accuracy 94.250\n",
      "selected users: [ 0  3  4  8 13 15 20 21 27]\n",
      "\n",
      "Test set: Average loss: 19464610.2528 \n",
      "Accuracy: 9427/10000 (94.27%)\n",
      "\n",
      "Round  20, Average loss 19464610.253 Test accuracy 94.270\n",
      "selected users: [ 4  7 11 13 17 23 24 25 27]\n",
      "\n",
      "Test set: Average loss: 46036520.6016 \n",
      "Accuracy: 9429/10000 (94.29%)\n",
      "\n",
      "Round  21, Average loss 46036520.602 Test accuracy 94.290\n",
      "selected users: [ 6  8  9 10 13 14 19 21 24]\n",
      "\n",
      "Test set: Average loss: 108006284.8000 \n",
      "Accuracy: 9429/10000 (94.29%)\n",
      "\n",
      "Round  22, Average loss 108006284.800 Test accuracy 94.290\n",
      "selected users: [ 6 10 12 14 18 21 25 28 29]\n",
      "\n",
      "Test set: Average loss: 257408916.0704 \n",
      "Accuracy: 9425/10000 (94.25%)\n",
      "\n",
      "Round  23, Average loss 257408916.070 Test accuracy 94.250\n",
      "selected users: [ 2  5  8 11 12 13 21 24 28]\n",
      "\n",
      "Test set: Average loss: 613621895.1680 \n",
      "Accuracy: 9423/10000 (94.23%)\n",
      "\n",
      "Round  24, Average loss 613621895.168 Test accuracy 94.230\n",
      "selected users: [ 0  7  8 11 12 19 24 26 29]\n",
      "\n",
      "Test set: Average loss: 1451520478.4128 \n",
      "Accuracy: 9425/10000 (94.25%)\n",
      "\n",
      "Round  25, Average loss 1451520478.413 Test accuracy 94.250\n",
      "selected users: [ 0  2  4 14 17 21 22 23 24]\n",
      "\n",
      "Test set: Average loss: 3435429252.3008 \n",
      "Accuracy: 9427/10000 (94.27%)\n",
      "\n",
      "Round  26, Average loss 3435429252.301 Test accuracy 94.270\n",
      "selected users: [ 1  6  7 13 14 23 24 28 30]\n",
      "\n",
      "Test set: Average loss: 8091150686.6176 \n",
      "Accuracy: 9426/10000 (94.26%)\n",
      "\n",
      "Round  27, Average loss 8091150686.618 Test accuracy 94.260\n",
      "selected users: [ 2  9 10 15 16 18 22 23 26]\n",
      "\n",
      "Test set: Average loss: 19179866069.4016 \n",
      "Accuracy: 9426/10000 (94.26%)\n",
      "\n",
      "Round  28, Average loss 19179866069.402 Test accuracy 94.260\n",
      "selected users: [ 4  6  7 13 15 17 18 21 28]\n",
      "\n",
      "Test set: Average loss: 45439105315.6352 \n",
      "Accuracy: 9426/10000 (94.26%)\n",
      "\n",
      "Round  29, Average loss 45439105315.635 Test accuracy 94.260\n",
      "(m= 9 )  9 -th Trial!!\n",
      "selected users: [ 0  3 11 14 19 21 23 26 30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 5  8  9 11 13 20 26 28 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   1, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  5  8 11 16 17 24 26 30]\n",
      "\n",
      "Test set: Average loss: 0.7852 \n",
      "Accuracy: 7844/10000 (78.44%)\n",
      "\n",
      "Round   2, Average loss 0.785 Test accuracy 78.440\n",
      "selected users: [ 1 11 14 17 19 21 25 26 27]\n",
      "\n",
      "Test set: Average loss: 0.3937 \n",
      "Accuracy: 9532/10000 (95.32%)\n",
      "\n",
      "Round   3, Average loss 0.394 Test accuracy 95.320\n",
      "selected users: [ 1  6  8  9 10 14 26 27 30]\n",
      "\n",
      "Test set: Average loss: 5.0299 \n",
      "Accuracy: 9471/10000 (94.71%)\n",
      "\n",
      "Round   4, Average loss 5.030 Test accuracy 94.710\n",
      "selected users: [ 1  2  6 11 14 20 22 25 28]\n",
      "\n",
      "Test set: Average loss: 8.3829 \n",
      "Accuracy: 9500/10000 (95.00%)\n",
      "\n",
      "Round   5, Average loss 8.383 Test accuracy 95.000\n",
      "selected users: [ 0  1  5 15 17 19 26 27 29]\n",
      "\n",
      "Test set: Average loss: 101.0847 \n",
      "Accuracy: 9340/10000 (93.40%)\n",
      "\n",
      "Round   6, Average loss 101.085 Test accuracy 93.400\n",
      "selected users: [ 0  5  6  9 10 16 19 22 30]\n",
      "\n",
      "Test set: Average loss: 356.0858 \n",
      "Accuracy: 9398/10000 (93.98%)\n",
      "\n",
      "Round   7, Average loss 356.086 Test accuracy 93.980\n",
      "selected users: [ 3  4  7  8 11 14 15 16 23]\n",
      "\n",
      "Test set: Average loss: 518.5341 \n",
      "Accuracy: 9433/10000 (94.33%)\n",
      "\n",
      "Round   8, Average loss 518.534 Test accuracy 94.330\n",
      "selected users: [ 1  9 16 17 18 22 24 28 30]\n",
      "\n",
      "Test set: Average loss: 877.4088 \n",
      "Accuracy: 9507/10000 (95.07%)\n",
      "\n",
      "Round   9, Average loss 877.409 Test accuracy 95.070\n",
      "selected users: [ 1  4  5  6  7  8  9 10 16]\n",
      "\n",
      "Test set: Average loss: 3306.2055 \n",
      "Accuracy: 9479/10000 (94.79%)\n",
      "\n",
      "Round  10, Average loss 3306.205 Test accuracy 94.790\n",
      "selected users: [ 0  2  6  7  8 13 14 22 23]\n",
      "\n",
      "Test set: Average loss: 8372.0491 \n",
      "Accuracy: 9478/10000 (94.78%)\n",
      "\n",
      "Round  11, Average loss 8372.049 Test accuracy 94.780\n",
      "selected users: [ 2  3  5 12 13 15 22 23 29]\n",
      "\n",
      "Test set: Average loss: 16921.7081 \n",
      "Accuracy: 9493/10000 (94.93%)\n",
      "\n",
      "Round  12, Average loss 16921.708 Test accuracy 94.930\n",
      "selected users: [ 0  7  8  9 11 18 20 25 29]\n",
      "\n",
      "Test set: Average loss: 50336.0799 \n",
      "Accuracy: 9479/10000 (94.79%)\n",
      "\n",
      "Round  13, Average loss 50336.080 Test accuracy 94.790\n",
      "selected users: [ 2  4  9 17 19 20 21 23 29]\n",
      "\n",
      "Test set: Average loss: 94343.8719 \n",
      "Accuracy: 9498/10000 (94.98%)\n",
      "\n",
      "Round  14, Average loss 94343.872 Test accuracy 94.980\n",
      "selected users: [ 3  4  8  9 10 11 18 19 27]\n",
      "\n",
      "Test set: Average loss: 231580.0112 \n",
      "Accuracy: 9513/10000 (95.13%)\n",
      "\n",
      "Round  15, Average loss 231580.011 Test accuracy 95.130\n",
      "selected users: [ 1 11 16 18 22 23 26 27 29]\n",
      "\n",
      "Test set: Average loss: 581550.9112 \n",
      "Accuracy: 9498/10000 (94.98%)\n",
      "\n",
      "Round  16, Average loss 581550.911 Test accuracy 94.980\n",
      "selected users: [ 7  9 12 17 19 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1320501.5992 \n",
      "Accuracy: 9501/10000 (95.01%)\n",
      "\n",
      "Round  17, Average loss 1320501.599 Test accuracy 95.010\n",
      "selected users: [ 4  6 10 11 15 16 18 21 25]\n",
      "\n",
      "Test set: Average loss: 3393783.8080 \n",
      "Accuracy: 9487/10000 (94.87%)\n",
      "\n",
      "Round  18, Average loss 3393783.808 Test accuracy 94.870\n",
      "selected users: [ 1  2  3  6  9 11 16 20 28]\n",
      "\n",
      "Test set: Average loss: 8221061.2224 \n",
      "Accuracy: 9493/10000 (94.93%)\n",
      "\n",
      "Round  19, Average loss 8221061.222 Test accuracy 94.930\n",
      "selected users: [ 0  7 11 12 15 18 22 24 26]\n",
      "\n",
      "Test set: Average loss: 19905569.1264 \n",
      "Accuracy: 9490/10000 (94.90%)\n",
      "\n",
      "Round  20, Average loss 19905569.126 Test accuracy 94.900\n",
      "selected users: [ 1  2  4 11 16 18 20 26 27]\n",
      "\n",
      "Test set: Average loss: 48508677.6320 \n",
      "Accuracy: 9492/10000 (94.92%)\n",
      "\n",
      "Round  21, Average loss 48508677.632 Test accuracy 94.920\n",
      "selected users: [ 2  8 13 15 21 24 26 28 29]\n",
      "\n",
      "Test set: Average loss: 116086057.3696 \n",
      "Accuracy: 9491/10000 (94.91%)\n",
      "\n",
      "Round  22, Average loss 116086057.370 Test accuracy 94.910\n",
      "selected users: [ 1  2  3  5  6  7  9 29 30]\n",
      "\n",
      "Test set: Average loss: 261769151.2832 \n",
      "Accuracy: 9492/10000 (94.92%)\n",
      "\n",
      "Round  23, Average loss 261769151.283 Test accuracy 94.920\n",
      "selected users: [ 0  2  5  7 11 12 15 24 25]\n",
      "\n",
      "Test set: Average loss: 629548668.1088 \n",
      "Accuracy: 9490/10000 (94.90%)\n",
      "\n",
      "Round  24, Average loss 629548668.109 Test accuracy 94.900\n",
      "selected users: [ 0  2  6  7 14 17 22 24 30]\n",
      "\n",
      "Test set: Average loss: 1470862499.8400 \n",
      "Accuracy: 9492/10000 (94.92%)\n",
      "\n",
      "Round  25, Average loss 1470862499.840 Test accuracy 94.920\n",
      "selected users: [ 0  2  4  9 13 19 21 24 26]\n",
      "\n",
      "Test set: Average loss: 3500349148.3648 \n",
      "Accuracy: 9490/10000 (94.90%)\n",
      "\n",
      "Round  26, Average loss 3500349148.365 Test accuracy 94.900\n",
      "selected users: [ 3  6  8 12 16 18 20 21 28]\n",
      "\n",
      "Test set: Average loss: 8305337656.9344 \n",
      "Accuracy: 9492/10000 (94.92%)\n",
      "\n",
      "Round  27, Average loss 8305337656.934 Test accuracy 94.920\n",
      "selected users: [ 3  8 10 11 15 18 22 28 30]\n",
      "\n",
      "Test set: Average loss: 19678299540.6848 \n",
      "Accuracy: 9491/10000 (94.91%)\n",
      "\n",
      "Round  28, Average loss 19678299540.685 Test accuracy 94.910\n",
      "selected users: [ 5 13 14 15 17 18 23 25 29]\n",
      "\n",
      "Test set: Average loss: 46781514095.0016 \n",
      "Accuracy: 9490/10000 (94.90%)\n",
      "\n",
      "Round  29, Average loss 46781514095.002 Test accuracy 94.900\n",
      "number of results: 12\n",
      "(m= 12 )  0 -th Trial!!\n",
      "selected users: [ 1  2  7 14 15 16 17 20 22 24 26 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 6  8  9 11 14 20 21 22 24 27 28 29]\n",
      "\n",
      "Test set: Average loss: 2.2901 \n",
      "Accuracy: 2026/10000 (20.26%)\n",
      "\n",
      "Round   1, Average loss 2.290 Test accuracy 20.260\n",
      "selected users: [ 4  5  7  9 10 16 17 18 24 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.8005 \n",
      "Accuracy: 8305/10000 (83.05%)\n",
      "\n",
      "Round   2, Average loss 0.801 Test accuracy 83.050\n",
      "selected users: [ 3  8 11 13 14 15 17 19 20 25 26 30]\n",
      "\n",
      "Test set: Average loss: 0.2095 \n",
      "Accuracy: 9431/10000 (94.31%)\n",
      "\n",
      "Round   3, Average loss 0.209 Test accuracy 94.310\n",
      "selected users: [ 0  2  3  5  9 12 15 21 23 26 29 30]\n",
      "\n",
      "Test set: Average loss: 4.4341 \n",
      "Accuracy: 9110/10000 (91.10%)\n",
      "\n",
      "Round   4, Average loss 4.434 Test accuracy 91.100\n",
      "selected users: [ 1  2  3  4  9 11 13 17 21 22 25 26]\n",
      "\n",
      "Test set: Average loss: 4.1393 \n",
      "Accuracy: 9062/10000 (90.62%)\n",
      "\n",
      "Round   5, Average loss 4.139 Test accuracy 90.620\n",
      "selected users: [ 2  4 10 11 14 15 16 21 23 28 29 30]\n",
      "\n",
      "Test set: Average loss: 7.8211 \n",
      "Accuracy: 9279/10000 (92.79%)\n",
      "\n",
      "Round   6, Average loss 7.821 Test accuracy 92.790\n",
      "selected users: [ 3  4  6  7  8  9 11 16 17 18 19 23]\n",
      "\n",
      "Test set: Average loss: 3.1968 \n",
      "Accuracy: 9300/10000 (93.00%)\n",
      "\n",
      "Round   7, Average loss 3.197 Test accuracy 93.000\n",
      "selected users: [ 3  7  8 12 16 18 19 20 22 23 24 26]\n",
      "\n",
      "Test set: Average loss: 11.8897 \n",
      "Accuracy: 9097/10000 (90.97%)\n",
      "\n",
      "Round   8, Average loss 11.890 Test accuracy 90.970\n",
      "selected users: [ 4  5  6  7  9 15 18 20 21 25 27 29]\n",
      "\n",
      "Test set: Average loss: 12.4555 \n",
      "Accuracy: 9321/10000 (93.21%)\n",
      "\n",
      "Round   9, Average loss 12.455 Test accuracy 93.210\n",
      "selected users: [ 1 13 15 17 21 22 24 25 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 11.8367 \n",
      "Accuracy: 9475/10000 (94.75%)\n",
      "\n",
      "Round  10, Average loss 11.837 Test accuracy 94.750\n",
      "selected users: [ 3  8 10 11 13 14 18 22 24 27 28 30]\n",
      "\n",
      "Test set: Average loss: 5.7939 \n",
      "Accuracy: 9371/10000 (93.71%)\n",
      "\n",
      "Round  11, Average loss 5.794 Test accuracy 93.710\n",
      "selected users: [ 3  5  7  8 12 14 17 18 22 24 28 30]\n",
      "\n",
      "Test set: Average loss: 1.0332 \n",
      "Accuracy: 8928/10000 (89.28%)\n",
      "\n",
      "Round  12, Average loss 1.033 Test accuracy 89.280\n",
      "selected users: [ 0  3  5  9 11 13 15 17 19 20 22 24]\n",
      "\n",
      "Test set: Average loss: 16.1779 \n",
      "Accuracy: 9288/10000 (92.88%)\n",
      "\n",
      "Round  13, Average loss 16.178 Test accuracy 92.880\n",
      "selected users: [11 13 15 16 17 18 19 21 22 25 26 29]\n",
      "\n",
      "Test set: Average loss: 12.6881 \n",
      "Accuracy: 9211/10000 (92.11%)\n",
      "\n",
      "Round  14, Average loss 12.688 Test accuracy 92.110\n",
      "selected users: [ 0  3  6  7  8  9 10 12 14 15 18 29]\n",
      "\n",
      "Test set: Average loss: 5.2800 \n",
      "Accuracy: 9318/10000 (93.18%)\n",
      "\n",
      "Round  15, Average loss 5.280 Test accuracy 93.180\n",
      "selected users: [ 0  1  6  8 13 14 18 19 22 23 27 29]\n",
      "\n",
      "Test set: Average loss: 10.8801 \n",
      "Accuracy: 9488/10000 (94.88%)\n",
      "\n",
      "Round  16, Average loss 10.880 Test accuracy 94.880\n",
      "selected users: [ 0  3  6  7  8 10 13 14 21 22 23 26]\n",
      "\n",
      "Test set: Average loss: 72.0732 \n",
      "Accuracy: 9190/10000 (91.90%)\n",
      "\n",
      "Round  17, Average loss 72.073 Test accuracy 91.900\n",
      "selected users: [ 0  1  5  7 11 14 17 20 23 25 28 30]\n",
      "\n",
      "Test set: Average loss: 45.5447 \n",
      "Accuracy: 9341/10000 (93.41%)\n",
      "\n",
      "Round  18, Average loss 45.545 Test accuracy 93.410\n",
      "selected users: [ 4  5 12 13 17 19 22 23 24 25 26 29]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 39.2936 \n",
      "Accuracy: 9332/10000 (93.32%)\n",
      "\n",
      "Round  19, Average loss 39.294 Test accuracy 93.320\n",
      "selected users: [ 1  8  9 11 15 16 18 19 20 21 29 30]\n",
      "\n",
      "Test set: Average loss: 30.8454 \n",
      "Accuracy: 8967/10000 (89.67%)\n",
      "\n",
      "Round  20, Average loss 30.845 Test accuracy 89.670\n",
      "selected users: [ 0  1  4  6  8 11 13 15 17 24 26 29]\n",
      "\n",
      "Test set: Average loss: 27.3152 \n",
      "Accuracy: 9368/10000 (93.68%)\n",
      "\n",
      "Round  21, Average loss 27.315 Test accuracy 93.680\n",
      "selected users: [ 0  2  3  5  7 10 14 15 21 23 25 26]\n",
      "\n",
      "Test set: Average loss: 556.6399 \n",
      "Accuracy: 8866/10000 (88.66%)\n",
      "\n",
      "Round  22, Average loss 556.640 Test accuracy 88.660\n",
      "selected users: [ 0  2  3  7 11 14 19 21 22 23 24 27]\n",
      "\n",
      "Test set: Average loss: 137.3840 \n",
      "Accuracy: 9309/10000 (93.09%)\n",
      "\n",
      "Round  23, Average loss 137.384 Test accuracy 93.090\n",
      "selected users: [ 1  2  4  5 10 11 12 15 20 25 29 30]\n",
      "\n",
      "Test set: Average loss: 331.2988 \n",
      "Accuracy: 9117/10000 (91.17%)\n",
      "\n",
      "Round  24, Average loss 331.299 Test accuracy 91.170\n",
      "selected users: [ 2  4  5 10 12 13 15 17 19 20 21 22]\n",
      "\n",
      "Test set: Average loss: 69.0455 \n",
      "Accuracy: 9336/10000 (93.36%)\n",
      "\n",
      "Round  25, Average loss 69.046 Test accuracy 93.360\n",
      "selected users: [ 0  2  3  5 12 13 16 17 19 25 26 30]\n",
      "\n",
      "Test set: Average loss: 384.5805 \n",
      "Accuracy: 9189/10000 (91.89%)\n",
      "\n",
      "Round  26, Average loss 384.580 Test accuracy 91.890\n",
      "selected users: [ 1  4  5  6  9 12 16 23 24 27 28 29]\n",
      "\n",
      "Test set: Average loss: 315.1198 \n",
      "Accuracy: 9320/10000 (93.20%)\n",
      "\n",
      "Round  27, Average loss 315.120 Test accuracy 93.200\n",
      "selected users: [ 0  2  3  7  8  9 12 15 20 23 24 25]\n",
      "\n",
      "Test set: Average loss: 555.0830 \n",
      "Accuracy: 9226/10000 (92.26%)\n",
      "\n",
      "Round  28, Average loss 555.083 Test accuracy 92.260\n",
      "selected users: [ 0  1  5  9 12 13 19 21 23 24 26 28]\n",
      "\n",
      "Test set: Average loss: 338.9402 \n",
      "Accuracy: 9339/10000 (93.39%)\n",
      "\n",
      "Round  29, Average loss 338.940 Test accuracy 93.390\n",
      "(m= 12 )  1 -th Trial!!\n",
      "selected users: [ 2  3  5  6  9 11 14 15 21 27 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 2  3  4  7  8  9 10 12 21 23 27 30]\n",
      "\n",
      "Test set: Average loss: 2.2987 \n",
      "Accuracy: 2146/10000 (21.46%)\n",
      "\n",
      "Round   1, Average loss 2.299 Test accuracy 21.460\n",
      "selected users: [ 0  2  6  9 13 15 16 18 20 21 23 25]\n",
      "\n",
      "Test set: Average loss: 1.1915 \n",
      "Accuracy: 7344/10000 (73.44%)\n",
      "\n",
      "Round   2, Average loss 1.191 Test accuracy 73.440\n",
      "selected users: [ 4  5  7 10 11 16 19 21 22 23 25 26]\n",
      "\n",
      "Test set: Average loss: 0.6971 \n",
      "Accuracy: 9202/10000 (92.02%)\n",
      "\n",
      "Round   3, Average loss 0.697 Test accuracy 92.020\n",
      "selected users: [ 0  3  5 10 11 12 14 18 19 26 27 29]\n",
      "\n",
      "Test set: Average loss: 6.0745 \n",
      "Accuracy: 8915/10000 (89.15%)\n",
      "\n",
      "Round   4, Average loss 6.074 Test accuracy 89.150\n",
      "selected users: [ 0  2  3  4  6  7 16 17 20 24 29 30]\n",
      "\n",
      "Test set: Average loss: 0.9211 \n",
      "Accuracy: 8050/10000 (80.50%)\n",
      "\n",
      "Round   5, Average loss 0.921 Test accuracy 80.500\n",
      "selected users: [ 4  8  9 12 14 15 18 19 21 22 24 28]\n",
      "\n",
      "Test set: Average loss: 0.7287 \n",
      "Accuracy: 9151/10000 (91.51%)\n",
      "\n",
      "Round   6, Average loss 0.729 Test accuracy 91.510\n",
      "selected users: [ 1  4  5  6 14 15 16 18 19 26 28 30]\n",
      "\n",
      "Test set: Average loss: 1.1213 \n",
      "Accuracy: 9418/10000 (94.18%)\n",
      "\n",
      "Round   7, Average loss 1.121 Test accuracy 94.180\n",
      "selected users: [ 1  8 12 13 14 15 18 19 20 21 27 30]\n",
      "\n",
      "Test set: Average loss: 2.6370 \n",
      "Accuracy: 9448/10000 (94.48%)\n",
      "\n",
      "Round   8, Average loss 2.637 Test accuracy 94.480\n",
      "selected users: [ 3  4  9 15 16 17 19 21 23 24 25 27]\n",
      "\n",
      "Test set: Average loss: 2.3048 \n",
      "Accuracy: 9304/10000 (93.04%)\n",
      "\n",
      "Round   9, Average loss 2.305 Test accuracy 93.040\n",
      "selected users: [ 0  2  3 10 11 14 17 18 19 26 29 30]\n",
      "\n",
      "Test set: Average loss: 14.4693 \n",
      "Accuracy: 9317/10000 (93.17%)\n",
      "\n",
      "Round  10, Average loss 14.469 Test accuracy 93.170\n",
      "selected users: [ 1  2  7 10 12 15 16 19 21 23 26 27]\n",
      "\n",
      "Test set: Average loss: 0.7379 \n",
      "Accuracy: 9254/10000 (92.54%)\n",
      "\n",
      "Round  11, Average loss 0.738 Test accuracy 92.540\n",
      "selected users: [ 1  2  3  4  6 14 16 18 19 23 26 29]\n",
      "\n",
      "Test set: Average loss: 5.6403 \n",
      "Accuracy: 9387/10000 (93.87%)\n",
      "\n",
      "Round  12, Average loss 5.640 Test accuracy 93.870\n",
      "selected users: [ 0  1  3  4  5  6 10 13 14 23 27 29]\n",
      "\n",
      "Test set: Average loss: 7.1328 \n",
      "Accuracy: 9370/10000 (93.70%)\n",
      "\n",
      "Round  13, Average loss 7.133 Test accuracy 93.700\n",
      "selected users: [ 0  2  3  4  5 10 16 20 22 24 25 26]\n",
      "\n",
      "Test set: Average loss: 92.9132 \n",
      "Accuracy: 9129/10000 (91.29%)\n",
      "\n",
      "Round  14, Average loss 92.913 Test accuracy 91.290\n",
      "selected users: [ 0  3  5  6  9 11 19 20 21 22 26 30]\n",
      "\n",
      "Test set: Average loss: 75.4904 \n",
      "Accuracy: 9236/10000 (92.36%)\n",
      "\n",
      "Round  15, Average loss 75.490 Test accuracy 92.360\n",
      "selected users: [ 2  4  5 15 21 22 24 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 146.8270 \n",
      "Accuracy: 9178/10000 (91.78%)\n",
      "\n",
      "Round  16, Average loss 146.827 Test accuracy 91.780\n",
      "selected users: [ 2 10 11 14 17 20 21 23 24 25 27 30]\n",
      "\n",
      "Test set: Average loss: 63.3631 \n",
      "Accuracy: 9312/10000 (93.12%)\n",
      "\n",
      "Round  17, Average loss 63.363 Test accuracy 93.120\n",
      "selected users: [ 0  3  7  9 13 17 19 23 24 25 26 28]\n",
      "\n",
      "Test set: Average loss: 10.8447 \n",
      "Accuracy: 9202/10000 (92.02%)\n",
      "\n",
      "Round  18, Average loss 10.845 Test accuracy 92.020\n",
      "selected users: [ 1  5  6  7  8 10 12 13 17 18 25 27]\n",
      "\n",
      "Test set: Average loss: 289.6034 \n",
      "Accuracy: 9064/10000 (90.64%)\n",
      "\n",
      "Round  19, Average loss 289.603 Test accuracy 90.640\n",
      "selected users: [ 0  3  5 13 15 16 22 23 24 28 29 30]\n",
      "\n",
      "Test set: Average loss: 243.1798 \n",
      "Accuracy: 9223/10000 (92.23%)\n",
      "\n",
      "Round  20, Average loss 243.180 Test accuracy 92.230\n",
      "selected users: [ 1  3  4  8 14 16 17 20 21 22 23 25]\n",
      "\n",
      "Test set: Average loss: 134.4790 \n",
      "Accuracy: 9324/10000 (93.24%)\n",
      "\n",
      "Round  21, Average loss 134.479 Test accuracy 93.240\n",
      "selected users: [ 6  7  8 12 17 20 21 22 24 25 26 28]\n",
      "\n",
      "Test set: Average loss: 132.5327 \n",
      "Accuracy: 9348/10000 (93.48%)\n",
      "\n",
      "Round  22, Average loss 132.533 Test accuracy 93.480\n",
      "selected users: [ 0  2  3  4  6 10 12 15 20 25 26 28]\n",
      "\n",
      "Test set: Average loss: 366.5020 \n",
      "Accuracy: 9243/10000 (92.43%)\n",
      "\n",
      "Round  23, Average loss 366.502 Test accuracy 92.430\n",
      "selected users: [ 1  2  3  4  5  7 13 14 20 22 24 28]\n",
      "\n",
      "Test set: Average loss: 161.8785 \n",
      "Accuracy: 9367/10000 (93.67%)\n",
      "\n",
      "Round  24, Average loss 161.878 Test accuracy 93.670\n",
      "selected users: [ 0  5  6  7  9 10 11 22 24 25 27 29]\n",
      "\n",
      "Test set: Average loss: 156.7772 \n",
      "Accuracy: 9368/10000 (93.68%)\n",
      "\n",
      "Round  25, Average loss 156.777 Test accuracy 93.680\n",
      "selected users: [ 1  2  6 12 14 15 16 18 23 24 28 30]\n",
      "\n",
      "Test set: Average loss: 136.7052 \n",
      "Accuracy: 9373/10000 (93.73%)\n",
      "\n",
      "Round  26, Average loss 136.705 Test accuracy 93.730\n",
      "selected users: [ 1  3  7  9 11 12 16 21 22 25 26 30]\n",
      "\n",
      "Test set: Average loss: 296.1935 \n",
      "Accuracy: 9327/10000 (93.27%)\n",
      "\n",
      "Round  27, Average loss 296.193 Test accuracy 93.270\n",
      "selected users: [ 2  7  9 10 11 13 14 17 19 23 28 30]\n",
      "\n",
      "Test set: Average loss: 69.8046 \n",
      "Accuracy: 9376/10000 (93.76%)\n",
      "\n",
      "Round  28, Average loss 69.805 Test accuracy 93.760\n",
      "selected users: [ 1  5  6  7  8  9 13 15 21 28 29 30]\n",
      "\n",
      "Test set: Average loss: 438.8214 \n",
      "Accuracy: 9283/10000 (92.83%)\n",
      "\n",
      "Round  29, Average loss 438.821 Test accuracy 92.830\n",
      "(m= 12 )  2 -th Trial!!\n",
      "selected users: [ 0  3 14 15 17 20 21 22 23 25 26 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  5  7  8 10 12 13 14 16 17 22 26]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   1, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  2  3  4  8 14 19 21 23 26 27 28]\n",
      "\n",
      "Test set: Average loss: 2.3004 \n",
      "Accuracy: 2619/10000 (26.19%)\n",
      "\n",
      "Round   2, Average loss 2.300 Test accuracy 26.190\n",
      "selected users: [ 0  5  7  8  9 11 12 15 17 19 20 24]\n",
      "\n",
      "Test set: Average loss: 0.3294 \n",
      "Accuracy: 8995/10000 (89.95%)\n",
      "\n",
      "Round   3, Average loss 0.329 Test accuracy 89.950\n",
      "selected users: [ 9 11 13 14 15 16 18 21 23 26 28 29]\n",
      "\n",
      "Test set: Average loss: 0.3256 \n",
      "Accuracy: 9311/10000 (93.11%)\n",
      "\n",
      "Round   4, Average loss 0.326 Test accuracy 93.110\n",
      "selected users: [ 0  5  7 11 12 13 19 21 22 23 27 30]\n",
      "\n",
      "Test set: Average loss: 0.8206 \n",
      "Accuracy: 9370/10000 (93.70%)\n",
      "\n",
      "Round   5, Average loss 0.821 Test accuracy 93.700\n",
      "selected users: [ 0  4  7  8  9 13 14 16 20 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.4410 \n",
      "Accuracy: 9437/10000 (94.37%)\n",
      "\n",
      "Round   6, Average loss 1.441 Test accuracy 94.370\n",
      "selected users: [ 5  9 10 14 15 16 22 23 24 27 29 30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 3.3156 \n",
      "Accuracy: 9390/10000 (93.90%)\n",
      "\n",
      "Round   7, Average loss 3.316 Test accuracy 93.900\n",
      "selected users: [ 1  3  8  9 17 18 19 21 22 23 26 28]\n",
      "\n",
      "Test set: Average loss: 1.7806 \n",
      "Accuracy: 6055/10000 (60.55%)\n",
      "\n",
      "Round   8, Average loss 1.781 Test accuracy 60.550\n",
      "selected users: [ 1  4  5 10 17 19 20 21 24 26 27 29]\n",
      "\n",
      "Test set: Average loss: 0.8460 \n",
      "Accuracy: 9403/10000 (94.03%)\n",
      "\n",
      "Round   9, Average loss 0.846 Test accuracy 94.030\n",
      "selected users: [ 0  2  3  7 10 13 15 19 20 22 25 26]\n",
      "\n",
      "Test set: Average loss: 2.5993 \n",
      "Accuracy: 9346/10000 (93.46%)\n",
      "\n",
      "Round  10, Average loss 2.599 Test accuracy 93.460\n",
      "selected users: [ 0  2  3  9 10 11 13 14 17 21 23 29]\n",
      "\n",
      "Test set: Average loss: 0.3361 \n",
      "Accuracy: 9509/10000 (95.09%)\n",
      "\n",
      "Round  11, Average loss 0.336 Test accuracy 95.090\n",
      "selected users: [ 1  6  7 10 14 16 20 21 22 23 24 30]\n",
      "\n",
      "Test set: Average loss: 4.3850 \n",
      "Accuracy: 9395/10000 (93.95%)\n",
      "\n",
      "Round  12, Average loss 4.385 Test accuracy 93.950\n",
      "selected users: [ 8 10 12 13 14 16 17 20 21 23 24 30]\n",
      "\n",
      "Test set: Average loss: 1.9234 \n",
      "Accuracy: 4237/10000 (42.37%)\n",
      "\n",
      "Round  13, Average loss 1.923 Test accuracy 42.370\n",
      "selected users: [ 0  3  4  9 10 11 14 18 23 24 29 30]\n",
      "\n",
      "Test set: Average loss: 0.1975 \n",
      "Accuracy: 9501/10000 (95.01%)\n",
      "\n",
      "Round  14, Average loss 0.197 Test accuracy 95.010\n",
      "selected users: [ 0  1  3  9 15 17 19 21 23 24 28 30]\n",
      "\n",
      "Test set: Average loss: 0.3845 \n",
      "Accuracy: 9559/10000 (95.59%)\n",
      "\n",
      "Round  15, Average loss 0.385 Test accuracy 95.590\n",
      "selected users: [ 5  7  8  9 12 14 15 19 20 22 24 26]\n",
      "\n",
      "Test set: Average loss: 10.7179 \n",
      "Accuracy: 9204/10000 (92.04%)\n",
      "\n",
      "Round  16, Average loss 10.718 Test accuracy 92.040\n",
      "selected users: [ 3 11 12 13 14 16 17 20 21 24 27 29]\n",
      "\n",
      "Test set: Average loss: 0.5252 \n",
      "Accuracy: 9448/10000 (94.48%)\n",
      "\n",
      "Round  17, Average loss 0.525 Test accuracy 94.480\n",
      "selected users: [ 2  3  5  9 10 13 17 18 23 26 28 29]\n",
      "\n",
      "Test set: Average loss: 2.4145 \n",
      "Accuracy: 9333/10000 (93.33%)\n",
      "\n",
      "Round  18, Average loss 2.415 Test accuracy 93.330\n",
      "selected users: [ 2  5  8  9 12 14 15 16 18 19 27 30]\n",
      "\n",
      "Test set: Average loss: 4.9110 \n",
      "Accuracy: 9397/10000 (93.97%)\n",
      "\n",
      "Round  19, Average loss 4.911 Test accuracy 93.970\n",
      "selected users: [ 0  2  3  7  8 11 15 18 20 23 26 28]\n",
      "\n",
      "Test set: Average loss: 14.5891 \n",
      "Accuracy: 9278/10000 (92.78%)\n",
      "\n",
      "Round  20, Average loss 14.589 Test accuracy 92.780\n",
      "selected users: [ 4  8  9 13 15 21 22 23 24 25 27 30]\n",
      "\n",
      "Test set: Average loss: 35.8797 \n",
      "Accuracy: 9372/10000 (93.72%)\n",
      "\n",
      "Round  21, Average loss 35.880 Test accuracy 93.720\n",
      "selected users: [ 0  2  6  7 10 11 13 15 16 18 23 25]\n",
      "\n",
      "Test set: Average loss: 25.6425 \n",
      "Accuracy: 9287/10000 (92.87%)\n",
      "\n",
      "Round  22, Average loss 25.642 Test accuracy 92.870\n",
      "selected users: [ 2  3  5  7 12 13 16 17 20 22 23 30]\n",
      "\n",
      "Test set: Average loss: 36.1584 \n",
      "Accuracy: 9330/10000 (93.30%)\n",
      "\n",
      "Round  23, Average loss 36.158 Test accuracy 93.300\n",
      "selected users: [ 4  5  7 10 12 17 19 20 24 26 27 29]\n",
      "\n",
      "Test set: Average loss: 15.0913 \n",
      "Accuracy: 9464/10000 (94.64%)\n",
      "\n",
      "Round  24, Average loss 15.091 Test accuracy 94.640\n",
      "selected users: [ 1  3  5  7  8 10 13 14 23 25 27 28]\n",
      "\n",
      "Test set: Average loss: 11.6382 \n",
      "Accuracy: 9441/10000 (94.41%)\n",
      "\n",
      "Round  25, Average loss 11.638 Test accuracy 94.410\n",
      "selected users: [ 3  4  7  8 11 15 18 21 22 23 24 25]\n",
      "\n",
      "Test set: Average loss: 144.7915 \n",
      "Accuracy: 9230/10000 (92.30%)\n",
      "\n",
      "Round  26, Average loss 144.791 Test accuracy 92.300\n",
      "selected users: [ 1  3  7 10 11 18 21 23 25 26 28 30]\n",
      "\n",
      "Test set: Average loss: 83.7958 \n",
      "Accuracy: 9320/10000 (93.20%)\n",
      "\n",
      "Round  27, Average loss 83.796 Test accuracy 93.200\n",
      "selected users: [ 1  3  5  9 10 12 14 20 23 27 28 30]\n",
      "\n",
      "Test set: Average loss: 176.2974 \n",
      "Accuracy: 9298/10000 (92.98%)\n",
      "\n",
      "Round  28, Average loss 176.297 Test accuracy 92.980\n",
      "selected users: [ 1  4  5  7 10 12 14 15 17 18 27 30]\n",
      "\n",
      "Test set: Average loss: 246.4534 \n",
      "Accuracy: 9293/10000 (92.93%)\n",
      "\n",
      "Round  29, Average loss 246.453 Test accuracy 92.930\n",
      "(m= 12 )  3 -th Trial!!\n",
      "selected users: [ 0  2  6  8 10 12 16 18 22 26 27 28]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 2  5  6  7  8 11 15 21 24 27 28 29]\n",
      "\n",
      "Test set: Average loss: 2.1914 \n",
      "Accuracy: 5414/10000 (54.14%)\n",
      "\n",
      "Round   1, Average loss 2.191 Test accuracy 54.140\n",
      "selected users: [ 0  2  4  5 10 11 15 17 20 21 24 27]\n",
      "\n",
      "Test set: Average loss: 0.5800 \n",
      "Accuracy: 8973/10000 (89.73%)\n",
      "\n",
      "Round   2, Average loss 0.580 Test accuracy 89.730\n",
      "selected users: [ 2  4  5 10 11 13 20 21 22 25 26 28]\n",
      "\n",
      "Test set: Average loss: 0.4889 \n",
      "Accuracy: 9133/10000 (91.33%)\n",
      "\n",
      "Round   3, Average loss 0.489 Test accuracy 91.330\n",
      "selected users: [ 4  8 11 12 13 14 17 18 19 24 26 29]\n",
      "\n",
      "Test set: Average loss: 0.2606 \n",
      "Accuracy: 9433/10000 (94.33%)\n",
      "\n",
      "Round   4, Average loss 0.261 Test accuracy 94.330\n",
      "selected users: [ 1  6 10 12 14 17 18 19 20 25 27 28]\n",
      "\n",
      "Test set: Average loss: 1.2311 \n",
      "Accuracy: 9370/10000 (93.70%)\n",
      "\n",
      "Round   5, Average loss 1.231 Test accuracy 93.700\n",
      "selected users: [ 7  8 11 14 15 17 18 19 21 23 26 28]\n",
      "\n",
      "Test set: Average loss: 4.5130 \n",
      "Accuracy: 9149/10000 (91.49%)\n",
      "\n",
      "Round   6, Average loss 4.513 Test accuracy 91.490\n",
      "selected users: [ 1  4  6  7 12 13 18 21 24 25 26 29]\n",
      "\n",
      "Test set: Average loss: 0.9618 \n",
      "Accuracy: 9361/10000 (93.61%)\n",
      "\n",
      "Round   7, Average loss 0.962 Test accuracy 93.610\n",
      "selected users: [ 1  3  4  6  8 17 19 22 24 26 27 30]\n",
      "\n",
      "Test set: Average loss: 2.6369 \n",
      "Accuracy: 9473/10000 (94.73%)\n",
      "\n",
      "Round   8, Average loss 2.637 Test accuracy 94.730\n",
      "selected users: [ 1  5  8 12 14 17 18 21 24 25 28 29]\n",
      "\n",
      "Test set: Average loss: 6.4347 \n",
      "Accuracy: 9250/10000 (92.50%)\n",
      "\n",
      "Round   9, Average loss 6.435 Test accuracy 92.500\n",
      "selected users: [ 0  3 11 13 14 16 17 18 20 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.4421 \n",
      "Accuracy: 9324/10000 (93.24%)\n",
      "\n",
      "Round  10, Average loss 1.442 Test accuracy 93.240\n",
      "selected users: [ 3  6  7 12 13 14 18 19 21 23 27 28]\n",
      "\n",
      "Test set: Average loss: 0.4418 \n",
      "Accuracy: 9259/10000 (92.59%)\n",
      "\n",
      "Round  11, Average loss 0.442 Test accuracy 92.590\n",
      "selected users: [ 0  4  9 11 12 17 18 22 23 25 28 29]\n",
      "\n",
      "Test set: Average loss: 1.6301 \n",
      "Accuracy: 9468/10000 (94.68%)\n",
      "\n",
      "Round  12, Average loss 1.630 Test accuracy 94.680\n",
      "selected users: [ 0  2  6  9 12 14 15 19 20 22 24 30]\n",
      "\n",
      "Test set: Average loss: 3.2296 \n",
      "Accuracy: 9506/10000 (95.06%)\n",
      "\n",
      "Round  13, Average loss 3.230 Test accuracy 95.060\n",
      "selected users: [ 2  3  6  7  9 11 15 20 21 24 26 28]\n",
      "\n",
      "Test set: Average loss: 7.9287 \n",
      "Accuracy: 9299/10000 (92.99%)\n",
      "\n",
      "Round  14, Average loss 7.929 Test accuracy 92.990\n",
      "selected users: [ 5  6  7  8 10 12 16 20 22 24 27 30]\n",
      "\n",
      "Test set: Average loss: 64.5779 \n",
      "Accuracy: 9179/10000 (91.79%)\n",
      "\n",
      "Round  15, Average loss 64.578 Test accuracy 91.790\n",
      "selected users: [ 1  2  6  8 10 13 15 16 20 21 26 29]\n",
      "\n",
      "Test set: Average loss: 121.6321 \n",
      "Accuracy: 9106/10000 (91.06%)\n",
      "\n",
      "Round  16, Average loss 121.632 Test accuracy 91.060\n",
      "selected users: [ 0  7 10 11 14 17 18 19 21 22 24 27]\n",
      "\n",
      "Test set: Average loss: 51.5886 \n",
      "Accuracy: 9376/10000 (93.76%)\n",
      "\n",
      "Round  17, Average loss 51.589 Test accuracy 93.760\n",
      "selected users: [ 5  6  7  9 15 20 21 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 203.6839 \n",
      "Accuracy: 9241/10000 (92.41%)\n",
      "\n",
      "Round  18, Average loss 203.684 Test accuracy 92.410\n",
      "selected users: [ 0  4 10 11 17 19 21 23 24 25 29 30]\n",
      "\n",
      "Test set: Average loss: 126.2902 \n",
      "Accuracy: 9364/10000 (93.64%)\n",
      "\n",
      "Round  19, Average loss 126.290 Test accuracy 93.640\n",
      "selected users: [ 0  1  2 10 12 13 14 17 23 24 28 30]\n",
      "\n",
      "Test set: Average loss: 48.5062 \n",
      "Accuracy: 9347/10000 (93.47%)\n",
      "\n",
      "Round  20, Average loss 48.506 Test accuracy 93.470\n",
      "selected users: [ 1 10 13 17 18 19 20 21 22 24 27 29]\n",
      "\n",
      "Test set: Average loss: 89.2523 \n",
      "Accuracy: 9386/10000 (93.86%)\n",
      "\n",
      "Round  21, Average loss 89.252 Test accuracy 93.860\n",
      "selected users: [ 4  5  7 10 11 14 15 17 21 25 26 28]\n",
      "\n",
      "Test set: Average loss: 76.2682 \n",
      "Accuracy: 9464/10000 (94.64%)\n",
      "\n",
      "Round  22, Average loss 76.268 Test accuracy 94.640\n",
      "selected users: [ 0  1  5  7  8 11 13 14 15 16 26 29]\n",
      "\n",
      "Test set: Average loss: 590.0791 \n",
      "Accuracy: 9120/10000 (91.20%)\n",
      "\n",
      "Round  23, Average loss 590.079 Test accuracy 91.200\n",
      "selected users: [ 0  5  6  9 12 14 16 24 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 443.6588 \n",
      "Accuracy: 9281/10000 (92.81%)\n",
      "\n",
      "Round  24, Average loss 443.659 Test accuracy 92.810\n",
      "selected users: [ 1  3  5  7  8 12 14 18 21 22 24 28]\n",
      "\n",
      "Test set: Average loss: 243.4837 \n",
      "Accuracy: 9357/10000 (93.57%)\n",
      "\n",
      "Round  25, Average loss 243.484 Test accuracy 93.570\n",
      "selected users: [ 0  3  4  5 10 11 14 18 20 21 29 30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 114.9624 \n",
      "Accuracy: 9034/10000 (90.34%)\n",
      "\n",
      "Round  26, Average loss 114.962 Test accuracy 90.340\n",
      "selected users: [ 1  3  7  8 10 11 13 15 19 25 28 29]\n",
      "\n",
      "Test set: Average loss: 157.8692 \n",
      "Accuracy: 9297/10000 (92.97%)\n",
      "\n",
      "Round  27, Average loss 157.869 Test accuracy 92.970\n",
      "selected users: [ 5  6  7 10 12 13 16 20 21 22 24 26]\n",
      "\n",
      "Test set: Average loss: 835.4856 \n",
      "Accuracy: 9104/10000 (91.04%)\n",
      "\n",
      "Round  28, Average loss 835.486 Test accuracy 91.040\n",
      "selected users: [ 0  1  7 11 13 17 21 22 23 26 27 29]\n",
      "\n",
      "Test set: Average loss: 320.4820 \n",
      "Accuracy: 9330/10000 (93.30%)\n",
      "\n",
      "Round  29, Average loss 320.482 Test accuracy 93.300\n",
      "(m= 12 )  4 -th Trial!!\n",
      "selected users: [ 2  3  7 13 14 15 17 18 21 27 28 29]\n",
      "\n",
      "Test set: Average loss: 2.3025 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.302 Test accuracy 9.800\n",
      "selected users: [ 3  4  8 13 14 15 18 19 21 23 26 29]\n",
      "\n",
      "Test set: Average loss: 1.8036 \n",
      "Accuracy: 6948/10000 (69.48%)\n",
      "\n",
      "Round   1, Average loss 1.804 Test accuracy 69.480\n",
      "selected users: [ 1  2  3  9 11 14 16 18 22 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.3311 \n",
      "Accuracy: 9238/10000 (92.38%)\n",
      "\n",
      "Round   2, Average loss 0.331 Test accuracy 92.380\n",
      "selected users: [ 3  4  5  7  9 10 12 15 19 20 22 27]\n",
      "\n",
      "Test set: Average loss: 0.2321 \n",
      "Accuracy: 9400/10000 (94.00%)\n",
      "\n",
      "Round   3, Average loss 0.232 Test accuracy 94.000\n",
      "selected users: [ 1  7  9 10 11 14 22 24 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.3233 \n",
      "Accuracy: 9463/10000 (94.63%)\n",
      "\n",
      "Round   4, Average loss 0.323 Test accuracy 94.630\n",
      "selected users: [ 2  8  9 12 16 17 19 20 23 26 27 29]\n",
      "\n",
      "Test set: Average loss: 0.1634 \n",
      "Accuracy: 9538/10000 (95.38%)\n",
      "\n",
      "Round   5, Average loss 0.163 Test accuracy 95.380\n",
      "selected users: [ 2  6 11 14 17 19 21 22 24 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.3963 \n",
      "Accuracy: 9549/10000 (95.49%)\n",
      "\n",
      "Round   6, Average loss 0.396 Test accuracy 95.490\n",
      "selected users: [ 1  7  9 15 16 22 23 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.8271 \n",
      "Accuracy: 9599/10000 (95.99%)\n",
      "\n",
      "Round   7, Average loss 0.827 Test accuracy 95.990\n",
      "selected users: [ 3  4  7  9 10 12 15 18 22 24 25 28]\n",
      "\n",
      "Test set: Average loss: 0.1745 \n",
      "Accuracy: 9541/10000 (95.41%)\n",
      "\n",
      "Round   8, Average loss 0.175 Test accuracy 95.410\n",
      "selected users: [ 2  5  7 10 12 14 15 18 20 22 25 27]\n",
      "\n",
      "Test set: Average loss: 1.3368 \n",
      "Accuracy: 9516/10000 (95.16%)\n",
      "\n",
      "Round   9, Average loss 1.337 Test accuracy 95.160\n",
      "selected users: [ 0  3  6  9 11 13 17 19 20 23 28 29]\n",
      "\n",
      "Test set: Average loss: 0.2536 \n",
      "Accuracy: 9481/10000 (94.81%)\n",
      "\n",
      "Round  10, Average loss 0.254 Test accuracy 94.810\n",
      "selected users: [ 2  5  6 10 14 15 16 19 20 23 26 28]\n",
      "\n",
      "Test set: Average loss: 2.5142 \n",
      "Accuracy: 9320/10000 (93.20%)\n",
      "\n",
      "Round  11, Average loss 2.514 Test accuracy 93.200\n",
      "selected users: [ 2  5  7  8 10 11 12 13 19 26 27 29]\n",
      "\n",
      "Test set: Average loss: 3.6122 \n",
      "Accuracy: 9285/10000 (92.85%)\n",
      "\n",
      "Round  12, Average loss 3.612 Test accuracy 92.850\n",
      "selected users: [ 1  3  5  6  9 12 13 17 20 22 24 30]\n",
      "\n",
      "Test set: Average loss: 1.8963 \n",
      "Accuracy: 9500/10000 (95.00%)\n",
      "\n",
      "Round  13, Average loss 1.896 Test accuracy 95.000\n",
      "selected users: [ 5  6  7  8  9 10 11 14 15 22 23 29]\n",
      "\n",
      "Test set: Average loss: 3.8921 \n",
      "Accuracy: 9409/10000 (94.09%)\n",
      "\n",
      "Round  14, Average loss 3.892 Test accuracy 94.090\n",
      "selected users: [ 7  8  9 12 13 16 17 19 22 24 28 30]\n",
      "\n",
      "Test set: Average loss: 0.2921 \n",
      "Accuracy: 9170/10000 (91.70%)\n",
      "\n",
      "Round  15, Average loss 0.292 Test accuracy 91.700\n",
      "selected users: [ 3  4  5 10 14 15 17 20 24 25 26 27]\n",
      "\n",
      "Test set: Average loss: 13.1180 \n",
      "Accuracy: 9275/10000 (92.75%)\n",
      "\n",
      "Round  16, Average loss 13.118 Test accuracy 92.750\n",
      "selected users: [ 1  2  5  8  9 10 12 16 17 25 27 28]\n",
      "\n",
      "Test set: Average loss: 5.3090 \n",
      "Accuracy: 9305/10000 (93.05%)\n",
      "\n",
      "Round  17, Average loss 5.309 Test accuracy 93.050\n",
      "selected users: [ 0  1  4 10 12 16 17 18 22 24 28 29]\n",
      "\n",
      "Test set: Average loss: 17.0511 \n",
      "Accuracy: 9377/10000 (93.77%)\n",
      "\n",
      "Round  18, Average loss 17.051 Test accuracy 93.770\n",
      "selected users: [ 1  6  8 11 12 14 15 17 22 23 28 29]\n",
      "\n",
      "Test set: Average loss: 4.5181 \n",
      "Accuracy: 9456/10000 (94.56%)\n",
      "\n",
      "Round  19, Average loss 4.518 Test accuracy 94.560\n",
      "selected users: [ 0  9 10 12 14 15 18 20 21 22 27 28]\n",
      "\n",
      "Test set: Average loss: 8.9277 \n",
      "Accuracy: 9499/10000 (94.99%)\n",
      "\n",
      "Round  20, Average loss 8.928 Test accuracy 94.990\n",
      "selected users: [ 4  5  6  8 11 13 14 15 16 20 24 27]\n",
      "\n",
      "Test set: Average loss: 44.7050 \n",
      "Accuracy: 9430/10000 (94.30%)\n",
      "\n",
      "Round  21, Average loss 44.705 Test accuracy 94.300\n",
      "selected users: [ 2  4  7  8 11 12 16 19 21 22 24 27]\n",
      "\n",
      "Test set: Average loss: 45.2557 \n",
      "Accuracy: 9461/10000 (94.61%)\n",
      "\n",
      "Round  22, Average loss 45.256 Test accuracy 94.610\n",
      "selected users: [ 3  6  8 12 14 15 17 18 21 23 26 28]\n",
      "\n",
      "Test set: Average loss: 33.5803 \n",
      "Accuracy: 9400/10000 (94.00%)\n",
      "\n",
      "Round  23, Average loss 33.580 Test accuracy 94.000\n",
      "selected users: [ 0  1 11 17 18 19 22 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 28.7216 \n",
      "Accuracy: 9177/10000 (91.77%)\n",
      "\n",
      "Round  24, Average loss 28.722 Test accuracy 91.770\n",
      "selected users: [ 2  3  4  7  8  9 10 12 14 19 21 30]\n",
      "\n",
      "Test set: Average loss: 52.8747 \n",
      "Accuracy: 9482/10000 (94.82%)\n",
      "\n",
      "Round  25, Average loss 52.875 Test accuracy 94.820\n",
      "selected users: [ 0  5 11 12 16 21 22 23 24 25 26 28]\n",
      "\n",
      "Test set: Average loss: 195.8588 \n",
      "Accuracy: 9292/10000 (92.92%)\n",
      "\n",
      "Round  26, Average loss 195.859 Test accuracy 92.920\n",
      "selected users: [ 2  3  6  8 10 15 16 17 23 25 26 28]\n",
      "\n",
      "Test set: Average loss: 81.3436 \n",
      "Accuracy: 9408/10000 (94.08%)\n",
      "\n",
      "Round  27, Average loss 81.344 Test accuracy 94.080\n",
      "selected users: [ 3  6  9 14 18 19 20 21 22 25 28 30]\n",
      "\n",
      "Test set: Average loss: 23.4336 \n",
      "Accuracy: 9470/10000 (94.70%)\n",
      "\n",
      "Round  28, Average loss 23.434 Test accuracy 94.700\n",
      "selected users: [ 2  3  4  8  9 13 15 24 25 26 28 30]\n",
      "\n",
      "Test set: Average loss: 123.4920 \n",
      "Accuracy: 9412/10000 (94.12%)\n",
      "\n",
      "Round  29, Average loss 123.492 Test accuracy 94.120\n",
      "(m= 12 )  5 -th Trial!!\n",
      "selected users: [ 0  1  3  6  7 10 15 17 18 19 26 28]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 6  7  8 11 15 16 17 23 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   1, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 2  3  5  6  7  8 14 15 18 23 27 28]\n",
      "\n",
      "Test set: Average loss: 1.5398 \n",
      "Accuracy: 7256/10000 (72.56%)\n",
      "\n",
      "Round   2, Average loss 1.540 Test accuracy 72.560\n",
      "selected users: [ 0  1  6  7 11 20 22 23 24 26 27 28]\n",
      "\n",
      "Test set: Average loss: 0.3754 \n",
      "Accuracy: 9157/10000 (91.57%)\n",
      "\n",
      "Round   3, Average loss 0.375 Test accuracy 91.570\n",
      "selected users: [ 0  1  3 11 16 17 18 20 21 24 25 30]\n",
      "\n",
      "Test set: Average loss: 1.2004 \n",
      "Accuracy: 9258/10000 (92.58%)\n",
      "\n",
      "Round   4, Average loss 1.200 Test accuracy 92.580\n",
      "selected users: [ 0  1  5  6 10 16 17 18 20 22 27 30]\n",
      "\n",
      "Test set: Average loss: 3.6669 \n",
      "Accuracy: 9261/10000 (92.61%)\n",
      "\n",
      "Round   5, Average loss 3.667 Test accuracy 92.610\n",
      "selected users: [ 3  5  6  8 10 11 12 17 21 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.8087 \n",
      "Accuracy: 9359/10000 (93.59%)\n",
      "\n",
      "Round   6, Average loss 1.809 Test accuracy 93.590\n",
      "selected users: [ 1  3  7  8 17 18 21 22 24 26 27 29]\n",
      "\n",
      "Test set: Average loss: 0.6303 \n",
      "Accuracy: 8004/10000 (80.04%)\n",
      "\n",
      "Round   7, Average loss 0.630 Test accuracy 80.040\n",
      "selected users: [ 2  4  5  8  9 12 14 15 17 21 22 25]\n",
      "\n",
      "Test set: Average loss: 5.9501 \n",
      "Accuracy: 9050/10000 (90.50%)\n",
      "\n",
      "Round   8, Average loss 5.950 Test accuracy 90.500\n",
      "selected users: [ 6  7  8 11 13 18 20 21 22 23 27 28]\n",
      "\n",
      "Test set: Average loss: 6.9052 \n",
      "Accuracy: 9235/10000 (92.35%)\n",
      "\n",
      "Round   9, Average loss 6.905 Test accuracy 92.350\n",
      "selected users: [ 0  4  6  7  8  9 12 20 21 23 26 27]\n",
      "\n",
      "Test set: Average loss: 4.2288 \n",
      "Accuracy: 9438/10000 (94.38%)\n",
      "\n",
      "Round  10, Average loss 4.229 Test accuracy 94.380\n",
      "selected users: [ 2  3  6  7 13 16 18 20 22 23 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3375 \n",
      "Accuracy: 5828/10000 (58.28%)\n",
      "\n",
      "Round  11, Average loss 1.337 Test accuracy 58.280\n",
      "selected users: [ 0  1  5  9 11 12 14 15 16 17 19 22]\n",
      "\n",
      "Test set: Average loss: 0.5257 \n",
      "Accuracy: 8802/10000 (88.02%)\n",
      "\n",
      "Round  12, Average loss 0.526 Test accuracy 88.020\n",
      "selected users: [ 2  3  6  9 12 15 18 20 24 25 29 30]\n",
      "\n",
      "Test set: Average loss: 0.6156 \n",
      "Accuracy: 8947/10000 (89.47%)\n",
      "\n",
      "Round  13, Average loss 0.616 Test accuracy 89.470\n",
      "selected users: [ 2  5  9 13 14 15 16 18 21 24 28 29]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.7005 \n",
      "Accuracy: 9229/10000 (92.29%)\n",
      "\n",
      "Round  14, Average loss 0.700 Test accuracy 92.290\n",
      "selected users: [ 2  5  6  7  8 13 14 24 25 26 27 30]\n",
      "\n",
      "Test set: Average loss: 0.2707 \n",
      "Accuracy: 9326/10000 (93.26%)\n",
      "\n",
      "Round  15, Average loss 0.271 Test accuracy 93.260\n",
      "selected users: [ 1  2  3  4  8 11 16 17 20 23 24 30]\n",
      "\n",
      "Test set: Average loss: 4.4076 \n",
      "Accuracy: 9294/10000 (92.94%)\n",
      "\n",
      "Round  16, Average loss 4.408 Test accuracy 92.940\n",
      "selected users: [ 0  1  2  8  9 11 19 21 23 24 28 29]\n",
      "\n",
      "Test set: Average loss: 1.4549 \n",
      "Accuracy: 9249/10000 (92.49%)\n",
      "\n",
      "Round  17, Average loss 1.455 Test accuracy 92.490\n",
      "selected users: [ 2  4  5  8 10 12 14 15 19 20 23 28]\n",
      "\n",
      "Test set: Average loss: 1.8794 \n",
      "Accuracy: 9204/10000 (92.04%)\n",
      "\n",
      "Round  18, Average loss 1.879 Test accuracy 92.040\n",
      "selected users: [ 0  1  8 11 12 14 18 21 22 24 25 29]\n",
      "\n",
      "Test set: Average loss: 0.4171 \n",
      "Accuracy: 9399/10000 (93.99%)\n",
      "\n",
      "Round  19, Average loss 0.417 Test accuracy 93.990\n",
      "selected users: [ 1  6  9 11 12 14 15 16 24 28 29 30]\n",
      "\n",
      "Test set: Average loss: 6.2152 \n",
      "Accuracy: 9277/10000 (92.77%)\n",
      "\n",
      "Round  20, Average loss 6.215 Test accuracy 92.770\n",
      "selected users: [ 2  7 11 14 15 20 21 22 23 24 26 28]\n",
      "\n",
      "Test set: Average loss: 1.2921 \n",
      "Accuracy: 9352/10000 (93.52%)\n",
      "\n",
      "Round  21, Average loss 1.292 Test accuracy 93.520\n",
      "selected users: [ 0  3  8 12 13 14 16 20 22 23 26 29]\n",
      "\n",
      "Test set: Average loss: 6.3207 \n",
      "Accuracy: 9247/10000 (92.47%)\n",
      "\n",
      "Round  22, Average loss 6.321 Test accuracy 92.470\n",
      "selected users: [ 0  1  2  3  9 17 18 20 25 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6155 \n",
      "Accuracy: 9513/10000 (95.13%)\n",
      "\n",
      "Round  23, Average loss 1.616 Test accuracy 95.130\n",
      "selected users: [ 0  3  4  5  6  8  9 12 14 17 18 29]\n",
      "\n",
      "Test set: Average loss: 2.2801 \n",
      "Accuracy: 9360/10000 (93.60%)\n",
      "\n",
      "Round  24, Average loss 2.280 Test accuracy 93.600\n",
      "selected users: [ 0  1  4  8  9 14 17 19 20 21 25 26]\n",
      "\n",
      "Test set: Average loss: 26.0567 \n",
      "Accuracy: 9274/10000 (92.74%)\n",
      "\n",
      "Round  25, Average loss 26.057 Test accuracy 92.740\n",
      "selected users: [ 4  9 11 14 15 20 21 22 25 27 29 30]\n",
      "\n",
      "Test set: Average loss: 26.3632 \n",
      "Accuracy: 9427/10000 (94.27%)\n",
      "\n",
      "Round  26, Average loss 26.363 Test accuracy 94.270\n",
      "selected users: [ 1  4  5  6  7  9 11 17 19 22 23 28]\n",
      "\n",
      "Test set: Average loss: 9.2624 \n",
      "Accuracy: 9513/10000 (95.13%)\n",
      "\n",
      "Round  27, Average loss 9.262 Test accuracy 95.130\n",
      "selected users: [ 0  1  2  8 11 15 17 21 23 24 26 28]\n",
      "\n",
      "Test set: Average loss: 27.8245 \n",
      "Accuracy: 9358/10000 (93.58%)\n",
      "\n",
      "Round  28, Average loss 27.825 Test accuracy 93.580\n",
      "selected users: [ 7  9 10 11 12 16 19 21 23 25 27 29]\n",
      "\n",
      "Test set: Average loss: 37.1849 \n",
      "Accuracy: 9365/10000 (93.65%)\n",
      "\n",
      "Round  29, Average loss 37.185 Test accuracy 93.650\n",
      "(m= 12 )  6 -th Trial!!\n",
      "selected users: [ 1  2  3  4  5  6  8 14 16 17 25 27]\n",
      "\n",
      "Test set: Average loss: 2.2740 \n",
      "Accuracy: 1054/10000 (10.54%)\n",
      "\n",
      "Round   0, Average loss 2.274 Test accuracy 10.540\n",
      "selected users: [ 1  4  5  7  8 12 13 17 21 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.6156 \n",
      "Accuracy: 8349/10000 (83.49%)\n",
      "\n",
      "Round   1, Average loss 0.616 Test accuracy 83.490\n",
      "selected users: [ 0  4  7  9 10 11 13 14 15 22 24 26]\n",
      "\n",
      "Test set: Average loss: 0.8661 \n",
      "Accuracy: 9244/10000 (92.44%)\n",
      "\n",
      "Round   2, Average loss 0.866 Test accuracy 92.440\n",
      "selected users: [ 1  4  5  6 11 14 15 16 21 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.6504 \n",
      "Accuracy: 9427/10000 (94.27%)\n",
      "\n",
      "Round   3, Average loss 0.650 Test accuracy 94.270\n",
      "selected users: [ 0  3 10 12 19 20 21 24 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4903 \n",
      "Accuracy: 8692/10000 (86.92%)\n",
      "\n",
      "Round   4, Average loss 0.490 Test accuracy 86.920\n",
      "selected users: [ 2  3  4  7 14 17 21 22 23 25 26 27]\n",
      "\n",
      "Test set: Average loss: 0.2272 \n",
      "Accuracy: 9534/10000 (95.34%)\n",
      "\n",
      "Round   5, Average loss 0.227 Test accuracy 95.340\n",
      "selected users: [ 1  2  4  7  8 11 15 17 20 24 25 27]\n",
      "\n",
      "Test set: Average loss: 1.6800 \n",
      "Accuracy: 9437/10000 (94.37%)\n",
      "\n",
      "Round   6, Average loss 1.680 Test accuracy 94.370\n",
      "selected users: [ 1  6  7  8 10 13 14 17 18 22 25 28]\n",
      "\n",
      "Test set: Average loss: 0.7250 \n",
      "Accuracy: 9485/10000 (94.85%)\n",
      "\n",
      "Round   7, Average loss 0.725 Test accuracy 94.850\n",
      "selected users: [ 4  8 12 13 15 16 17 22 23 24 26 29]\n",
      "\n",
      "Test set: Average loss: 0.6870 \n",
      "Accuracy: 9462/10000 (94.62%)\n",
      "\n",
      "Round   8, Average loss 0.687 Test accuracy 94.620\n",
      "selected users: [ 1  6  9 12 14 21 22 23 25 26 27 30]\n",
      "\n",
      "Test set: Average loss: 3.9332 \n",
      "Accuracy: 9461/10000 (94.61%)\n",
      "\n",
      "Round   9, Average loss 3.933 Test accuracy 94.610\n",
      "selected users: [ 5  7  9 10 12 17 20 22 24 26 29 30]\n",
      "\n",
      "Test set: Average loss: 7.3906 \n",
      "Accuracy: 9332/10000 (93.32%)\n",
      "\n",
      "Round  10, Average loss 7.391 Test accuracy 93.320\n",
      "selected users: [ 2  3  4  6 12 13 15 16 20 23 26 29]\n",
      "\n",
      "Test set: Average loss: 24.2309 \n",
      "Accuracy: 9232/10000 (92.32%)\n",
      "\n",
      "Round  11, Average loss 24.231 Test accuracy 92.320\n",
      "selected users: [ 7  8 12 17 19 20 21 22 23 26 27 30]\n",
      "\n",
      "Test set: Average loss: 3.5750 \n",
      "Accuracy: 9460/10000 (94.60%)\n",
      "\n",
      "Round  12, Average loss 3.575 Test accuracy 94.600\n",
      "selected users: [ 6  8  9 12 13 14 16 19 20 22 29 30]\n",
      "\n",
      "Test set: Average loss: 1.0143 \n",
      "Accuracy: 7390/10000 (73.90%)\n",
      "\n",
      "Round  13, Average loss 1.014 Test accuracy 73.900\n",
      "selected users: [ 1  3  6  8  9 11 14 19 21 25 27 29]\n",
      "\n",
      "Test set: Average loss: 2.9590 \n",
      "Accuracy: 9363/10000 (93.63%)\n",
      "\n",
      "Round  14, Average loss 2.959 Test accuracy 93.630\n",
      "selected users: [ 0  1  2  4  8 11 16 17 20 22 23 26]\n",
      "\n",
      "Test set: Average loss: 37.7964 \n",
      "Accuracy: 9213/10000 (92.13%)\n",
      "\n",
      "Round  15, Average loss 37.796 Test accuracy 92.130\n",
      "selected users: [ 5  9 12 14 15 16 17 20 21 24 29 30]\n",
      "\n",
      "Test set: Average loss: 18.2412 \n",
      "Accuracy: 9238/10000 (92.38%)\n",
      "\n",
      "Round  16, Average loss 18.241 Test accuracy 92.380\n",
      "selected users: [ 0  4  6 12 14 16 19 21 22 23 28 30]\n",
      "\n",
      "Test set: Average loss: 30.3943 \n",
      "Accuracy: 9287/10000 (92.87%)\n",
      "\n",
      "Round  17, Average loss 30.394 Test accuracy 92.870\n",
      "selected users: [ 1  4 11 13 14 18 22 24 25 26 27 28]\n",
      "\n",
      "Test set: Average loss: 28.5995 \n",
      "Accuracy: 9401/10000 (94.01%)\n",
      "\n",
      "Round  18, Average loss 28.599 Test accuracy 94.010\n",
      "selected users: [ 0  4  5 10 13 18 19 21 24 25 27 28]\n",
      "\n",
      "Test set: Average loss: 124.9928 \n",
      "Accuracy: 9222/10000 (92.22%)\n",
      "\n",
      "Round  19, Average loss 124.993 Test accuracy 92.220\n",
      "selected users: [ 1  3  9 10 13 14 17 18 20 22 23 30]\n",
      "\n",
      "Test set: Average loss: 20.9332 \n",
      "Accuracy: 9373/10000 (93.73%)\n",
      "\n",
      "Round  20, Average loss 20.933 Test accuracy 93.730\n",
      "selected users: [ 0  4 14 15 16 19 22 23 24 28 29 30]\n",
      "\n",
      "Test set: Average loss: 9.3606 \n",
      "Accuracy: 9451/10000 (94.51%)\n",
      "\n",
      "Round  21, Average loss 9.361 Test accuracy 94.510\n",
      "selected users: [ 2  3  5  8  9 15 17 24 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 27.6871 \n",
      "Accuracy: 9470/10000 (94.70%)\n",
      "\n",
      "Round  22, Average loss 27.687 Test accuracy 94.700\n",
      "selected users: [ 1  2  3  6 10 11 12 13 19 22 23 30]\n",
      "\n",
      "Test set: Average loss: 87.9180 \n",
      "Accuracy: 9343/10000 (93.43%)\n",
      "\n",
      "Round  23, Average loss 87.918 Test accuracy 93.430\n",
      "selected users: [ 4  8  9 12 14 19 20 21 23 24 25 29]\n",
      "\n",
      "Test set: Average loss: 41.0598 \n",
      "Accuracy: 9518/10000 (95.18%)\n",
      "\n",
      "Round  24, Average loss 41.060 Test accuracy 95.180\n",
      "selected users: [ 2  3  4  5  9 16 19 22 23 24 27 30]\n",
      "\n",
      "Test set: Average loss: 48.6147 \n",
      "Accuracy: 9510/10000 (95.10%)\n",
      "\n",
      "Round  25, Average loss 48.615 Test accuracy 95.100\n",
      "selected users: [ 1  6  8  9 13 14 15 20 22 23 29 30]\n",
      "\n",
      "Test set: Average loss: 57.7170 \n",
      "Accuracy: 9428/10000 (94.28%)\n",
      "\n",
      "Round  26, Average loss 57.717 Test accuracy 94.280\n",
      "selected users: [ 0  4  6  8 10 16 19 22 23 25 26 28]\n",
      "\n",
      "Test set: Average loss: 179.7313 \n",
      "Accuracy: 9350/10000 (93.50%)\n",
      "\n",
      "Round  27, Average loss 179.731 Test accuracy 93.500\n",
      "selected users: [ 1  4  8  9 11 12 15 17 20 23 25 28]\n",
      "\n",
      "Test set: Average loss: 188.7550 \n",
      "Accuracy: 9399/10000 (93.99%)\n",
      "\n",
      "Round  28, Average loss 188.755 Test accuracy 93.990\n",
      "selected users: [ 1  4  9 10 17 18 20 21 22 24 25 29]\n",
      "\n",
      "Test set: Average loss: 230.8085 \n",
      "Accuracy: 9375/10000 (93.75%)\n",
      "\n",
      "Round  29, Average loss 230.808 Test accuracy 93.750\n",
      "(m= 12 )  7 -th Trial!!\n",
      "selected users: [ 3  4  6  7  8 10 12 16 17 19 21 24]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 1  3  8  9 11 14 16 17 20 22 26 27]\n",
      "\n",
      "Test set: Average loss: 2.3024 \n",
      "Accuracy: 1064/10000 (10.64%)\n",
      "\n",
      "Round   1, Average loss 2.302 Test accuracy 10.640\n",
      "selected users: [ 3  4  6 12 15 16 18 19 21 23 28 30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.7932 \n",
      "Accuracy: 8571/10000 (85.71%)\n",
      "\n",
      "Round   2, Average loss 0.793 Test accuracy 85.710\n",
      "selected users: [ 1  8  9 13 15 16 17 21 23 25 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2783 \n",
      "Accuracy: 9299/10000 (92.99%)\n",
      "\n",
      "Round   3, Average loss 0.278 Test accuracy 92.990\n",
      "selected users: [ 1  3  6 10 11 12 13 18 21 24 26 27]\n",
      "\n",
      "Test set: Average loss: 0.1903 \n",
      "Accuracy: 9468/10000 (94.68%)\n",
      "\n",
      "Round   4, Average loss 0.190 Test accuracy 94.680\n",
      "selected users: [ 2  6  8 11 12 15 16 17 19 22 29 30]\n",
      "\n",
      "Test set: Average loss: 0.1552 \n",
      "Accuracy: 9533/10000 (95.33%)\n",
      "\n",
      "Round   5, Average loss 0.155 Test accuracy 95.330\n",
      "selected users: [ 5  7 11 12 15 16 17 19 21 23 24 25]\n",
      "\n",
      "Test set: Average loss: 2.1324 \n",
      "Accuracy: 9314/10000 (93.14%)\n",
      "\n",
      "Round   6, Average loss 2.132 Test accuracy 93.140\n",
      "selected users: [ 2  4  6 12 13 14 15 16 19 20 21 22]\n",
      "\n",
      "Test set: Average loss: 1.5515 \n",
      "Accuracy: 5178/10000 (51.78%)\n",
      "\n",
      "Round   7, Average loss 1.552 Test accuracy 51.780\n",
      "selected users: [ 1  5  9 10 14 18 21 22 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7981 \n",
      "Accuracy: 9354/10000 (93.54%)\n",
      "\n",
      "Round   8, Average loss 1.798 Test accuracy 93.540\n",
      "selected users: [ 0  1  2  5  6 10 16 21 24 26 28 29]\n",
      "\n",
      "Test set: Average loss: 9.5679 \n",
      "Accuracy: 9176/10000 (91.76%)\n",
      "\n",
      "Round   9, Average loss 9.568 Test accuracy 91.760\n",
      "selected users: [10 11 12 13 15 16 18 20 23 24 26 27]\n",
      "\n",
      "Test set: Average loss: 19.3797 \n",
      "Accuracy: 9040/10000 (90.40%)\n",
      "\n",
      "Round  10, Average loss 19.380 Test accuracy 90.400\n",
      "selected users: [ 1  2  8 11 12 14 16 21 22 23 27 28]\n",
      "\n",
      "Test set: Average loss: 2.4688 \n",
      "Accuracy: 9002/10000 (90.02%)\n",
      "\n",
      "Round  11, Average loss 2.469 Test accuracy 90.020\n",
      "selected users: [ 1  3  6  7 10 11 14 20 21 22 24 27]\n",
      "\n",
      "Test set: Average loss: 3.9719 \n",
      "Accuracy: 9367/10000 (93.67%)\n",
      "\n",
      "Round  12, Average loss 3.972 Test accuracy 93.670\n",
      "selected users: [ 2  5  6  7  8 11 18 20 21 24 25 30]\n",
      "\n",
      "Test set: Average loss: 32.0711 \n",
      "Accuracy: 9217/10000 (92.17%)\n",
      "\n",
      "Round  13, Average loss 32.071 Test accuracy 92.170\n",
      "selected users: [ 1  4  8 11 12 14 15 18 23 28 29 30]\n",
      "\n",
      "Test set: Average loss: 19.3266 \n",
      "Accuracy: 9372/10000 (93.72%)\n",
      "\n",
      "Round  14, Average loss 19.327 Test accuracy 93.720\n",
      "selected users: [ 0  4  6  7  8  9 10 13 17 19 21 27]\n",
      "\n",
      "Test set: Average loss: 43.8042 \n",
      "Accuracy: 9373/10000 (93.73%)\n",
      "\n",
      "Round  15, Average loss 43.804 Test accuracy 93.730\n",
      "selected users: [ 7 12 13 15 16 17 19 20 22 25 26 30]\n",
      "\n",
      "Test set: Average loss: 27.1701 \n",
      "Accuracy: 9436/10000 (94.36%)\n",
      "\n",
      "Round  16, Average loss 27.170 Test accuracy 94.360\n",
      "selected users: [ 1  3  8 12 13 14 15 16 17 18 23 25]\n",
      "\n",
      "Test set: Average loss: 27.7953 \n",
      "Accuracy: 9377/10000 (93.77%)\n",
      "\n",
      "Round  17, Average loss 27.795 Test accuracy 93.770\n",
      "selected users: [ 1  2  4  6  7  8 10 19 22 24 27 30]\n",
      "\n",
      "Test set: Average loss: 77.2926 \n",
      "Accuracy: 9363/10000 (93.63%)\n",
      "\n",
      "Round  18, Average loss 77.293 Test accuracy 93.630\n",
      "selected users: [ 3  4  8 14 15 16 17 20 21 22 26 27]\n",
      "\n",
      "Test set: Average loss: 16.7245 \n",
      "Accuracy: 9517/10000 (95.17%)\n",
      "\n",
      "Round  19, Average loss 16.724 Test accuracy 95.170\n",
      "selected users: [ 4  7 10 12 13 16 18 20 21 24 25 30]\n",
      "\n",
      "Test set: Average loss: 178.0065 \n",
      "Accuracy: 9301/10000 (93.01%)\n",
      "\n",
      "Round  20, Average loss 178.006 Test accuracy 93.010\n",
      "selected users: [ 2  4  7 10 17 19 20 21 24 25 26 30]\n",
      "\n",
      "Test set: Average loss: 202.5069 \n",
      "Accuracy: 9309/10000 (93.09%)\n",
      "\n",
      "Round  21, Average loss 202.507 Test accuracy 93.090\n",
      "selected users: [ 4  5 11 13 15 18 20 21 24 27 28 29]\n",
      "\n",
      "Test set: Average loss: 401.0982 \n",
      "Accuracy: 9232/10000 (92.32%)\n",
      "\n",
      "Round  22, Average loss 401.098 Test accuracy 92.320\n",
      "selected users: [ 4  6  8  9 10 11 12 14 17 21 23 27]\n",
      "\n",
      "Test set: Average loss: 269.4326 \n",
      "Accuracy: 9390/10000 (93.90%)\n",
      "\n",
      "Round  23, Average loss 269.433 Test accuracy 93.900\n",
      "selected users: [ 0  1  2  5  6 11 12 15 16 20 22 23]\n",
      "\n",
      "Test set: Average loss: 809.1681 \n",
      "Accuracy: 9139/10000 (91.39%)\n",
      "\n",
      "Round  24, Average loss 809.168 Test accuracy 91.390\n",
      "selected users: [ 3  4  7  8 10 11 14 16 20 21 28 29]\n",
      "\n",
      "Test set: Average loss: 751.4724 \n",
      "Accuracy: 9213/10000 (92.13%)\n",
      "\n",
      "Round  25, Average loss 751.472 Test accuracy 92.130\n",
      "selected users: [ 2  3  4  5  6  7 11 16 21 22 24 26]\n",
      "\n",
      "Test set: Average loss: 1358.8276 \n",
      "Accuracy: 9137/10000 (91.37%)\n",
      "\n",
      "Round  26, Average loss 1358.828 Test accuracy 91.370\n",
      "selected users: [ 1  4  9 10 12 13 19 24 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 805.5264 \n",
      "Accuracy: 9319/10000 (93.19%)\n",
      "\n",
      "Round  27, Average loss 805.526 Test accuracy 93.190\n",
      "selected users: [ 2  4  5  8  9 10 11 12 13 17 20 22]\n",
      "\n",
      "Test set: Average loss: 131.2494 \n",
      "Accuracy: 9437/10000 (94.37%)\n",
      "\n",
      "Round  28, Average loss 131.249 Test accuracy 94.370\n",
      "selected users: [ 5  7 11 12 15 16 19 20 21 23 27 29]\n",
      "\n",
      "Test set: Average loss: 689.3413 \n",
      "Accuracy: 9314/10000 (93.14%)\n",
      "\n",
      "Round  29, Average loss 689.341 Test accuracy 93.140\n",
      "(m= 12 )  8 -th Trial!!\n",
      "selected users: [ 4  8 11 12 13 16 18 19 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 2  3  4 10 11 19 20 21 25 27 28 30]\n",
      "\n",
      "Test set: Average loss: 2.1269 \n",
      "Accuracy: 5412/10000 (54.12%)\n",
      "\n",
      "Round   1, Average loss 2.127 Test accuracy 54.120\n",
      "selected users: [ 5  6 10 12 14 15 16 17 22 23 24 30]\n",
      "\n",
      "Test set: Average loss: 0.6557 \n",
      "Accuracy: 9105/10000 (91.05%)\n",
      "\n",
      "Round   2, Average loss 0.656 Test accuracy 91.050\n",
      "selected users: [ 2  5  6 10 13 14 18 20 21 22 25 27]\n",
      "\n",
      "Test set: Average loss: 0.4078 \n",
      "Accuracy: 9345/10000 (93.45%)\n",
      "\n",
      "Round   3, Average loss 0.408 Test accuracy 93.450\n",
      "selected users: [ 1  2  5  8  9 12 14 18 21 24 25 28]\n",
      "\n",
      "Test set: Average loss: 0.9419 \n",
      "Accuracy: 9234/10000 (92.34%)\n",
      "\n",
      "Round   4, Average loss 0.942 Test accuracy 92.340\n",
      "selected users: [ 2  4  9 10 11 13 15 17 23 24 25 27]\n",
      "\n",
      "Test set: Average loss: 0.5479 \n",
      "Accuracy: 9533/10000 (95.33%)\n",
      "\n",
      "Round   5, Average loss 0.548 Test accuracy 95.330\n",
      "selected users: [ 0  1  2  3  6 10 18 20 21 22 23 27]\n",
      "\n",
      "Test set: Average loss: 3.4045 \n",
      "Accuracy: 9230/10000 (92.30%)\n",
      "\n",
      "Round   6, Average loss 3.405 Test accuracy 92.300\n",
      "selected users: [ 0  1  2  3  6  9 12 14 16 20 21 27]\n",
      "\n",
      "Test set: Average loss: 4.0595 \n",
      "Accuracy: 9428/10000 (94.28%)\n",
      "\n",
      "Round   7, Average loss 4.059 Test accuracy 94.280\n",
      "selected users: [ 3  4  7  8 13 16 17 20 21 23 25 26]\n",
      "\n",
      "Test set: Average loss: 7.0161 \n",
      "Accuracy: 9275/10000 (92.75%)\n",
      "\n",
      "Round   8, Average loss 7.016 Test accuracy 92.750\n",
      "selected users: [ 4  5 10 11 12 13 17 18 19 21 28 29]\n",
      "\n",
      "Test set: Average loss: 20.6331 \n",
      "Accuracy: 9206/10000 (92.06%)\n",
      "\n",
      "Round   9, Average loss 20.633 Test accuracy 92.060\n",
      "selected users: [ 0  1  2  6  7 11 13 18 21 22 25 27]\n",
      "\n",
      "Test set: Average loss: 13.3530 \n",
      "Accuracy: 9393/10000 (93.93%)\n",
      "\n",
      "Round  10, Average loss 13.353 Test accuracy 93.930\n",
      "selected users: [ 2  4  7 15 16 17 18 21 25 28 29 30]\n",
      "\n",
      "Test set: Average loss: 16.0674 \n",
      "Accuracy: 9316/10000 (93.16%)\n",
      "\n",
      "Round  11, Average loss 16.067 Test accuracy 93.160\n",
      "selected users: [ 0  2  5  7 12 13 15 17 20 22 25 27]\n",
      "\n",
      "Test set: Average loss: 10.6415 \n",
      "Accuracy: 9417/10000 (94.17%)\n",
      "\n",
      "Round  12, Average loss 10.642 Test accuracy 94.170\n",
      "selected users: [ 0  2  6  7  9 10 11 12 15 18 23 29]\n",
      "\n",
      "Test set: Average loss: 5.6137 \n",
      "Accuracy: 9078/10000 (90.78%)\n",
      "\n",
      "Round  13, Average loss 5.614 Test accuracy 90.780\n",
      "selected users: [ 0  1  2  3  7 10 16 18 22 25 28 29]\n",
      "\n",
      "Test set: Average loss: 22.8958 \n",
      "Accuracy: 9354/10000 (93.54%)\n",
      "\n",
      "Round  14, Average loss 22.896 Test accuracy 93.540\n",
      "selected users: [ 3  5  7  9 10 14 16 18 20 21 23 30]\n",
      "\n",
      "Test set: Average loss: 36.1623 \n",
      "Accuracy: 9362/10000 (93.62%)\n",
      "\n",
      "Round  15, Average loss 36.162 Test accuracy 93.620\n",
      "selected users: [ 0  1  3  7  9 10 11 13 17 22 23 29]\n",
      "\n",
      "Test set: Average loss: 3.1489 \n",
      "Accuracy: 9198/10000 (91.98%)\n",
      "\n",
      "Round  16, Average loss 3.149 Test accuracy 91.980\n",
      "selected users: [ 0  4  5  8 12 14 19 20 22 26 29 30]\n",
      "\n",
      "Test set: Average loss: 18.3956 \n",
      "Accuracy: 9411/10000 (94.11%)\n",
      "\n",
      "Round  17, Average loss 18.396 Test accuracy 94.110\n",
      "selected users: [ 1  3  6  8  9 16 17 21 24 26 27 29]\n",
      "\n",
      "Test set: Average loss: 75.1318 \n",
      "Accuracy: 9319/10000 (93.19%)\n",
      "\n",
      "Round  18, Average loss 75.132 Test accuracy 93.190\n",
      "selected users: [ 3  4  5 12 14 18 19 20 21 24 27 28]\n",
      "\n",
      "Test set: Average loss: 26.2337 \n",
      "Accuracy: 9353/10000 (93.53%)\n",
      "\n",
      "Round  19, Average loss 26.234 Test accuracy 93.530\n",
      "selected users: [ 3  7  8 10 12 13 15 16 22 24 25 29]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 71.2994 \n",
      "Accuracy: 9271/10000 (92.71%)\n",
      "\n",
      "Round  20, Average loss 71.299 Test accuracy 92.710\n",
      "selected users: [ 3  4  6 12 13 18 21 23 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 107.5425 \n",
      "Accuracy: 9276/10000 (92.76%)\n",
      "\n",
      "Round  21, Average loss 107.543 Test accuracy 92.760\n",
      "selected users: [ 0  2  4  9 11 13 15 16 18 23 26 27]\n",
      "\n",
      "Test set: Average loss: 171.5210 \n",
      "Accuracy: 9325/10000 (93.25%)\n",
      "\n",
      "Round  22, Average loss 171.521 Test accuracy 93.250\n",
      "selected users: [ 0  2  5  6  9 12 16 18 19 20 24 29]\n",
      "\n",
      "Test set: Average loss: 85.5265 \n",
      "Accuracy: 9354/10000 (93.54%)\n",
      "\n",
      "Round  23, Average loss 85.526 Test accuracy 93.540\n",
      "selected users: [ 0  2  3  4  6  7  8 17 18 22 24 29]\n",
      "\n",
      "Test set: Average loss: 73.4620 \n",
      "Accuracy: 9304/10000 (93.04%)\n",
      "\n",
      "Round  24, Average loss 73.462 Test accuracy 93.040\n",
      "selected users: [ 1  3 12 15 16 17 18 23 24 27 28 30]\n",
      "\n",
      "Test set: Average loss: 45.9971 \n",
      "Accuracy: 9286/10000 (92.86%)\n",
      "\n",
      "Round  25, Average loss 45.997 Test accuracy 92.860\n",
      "selected users: [ 0  1  3  8  9 11 12 14 20 26 27 28]\n",
      "\n",
      "Test set: Average loss: 180.0969 \n",
      "Accuracy: 9291/10000 (92.91%)\n",
      "\n",
      "Round  26, Average loss 180.097 Test accuracy 92.910\n",
      "selected users: [ 0  9 10 11 12 14 16 21 23 24 26 30]\n",
      "\n",
      "Test set: Average loss: 512.5117 \n",
      "Accuracy: 9207/10000 (92.07%)\n",
      "\n",
      "Round  27, Average loss 512.512 Test accuracy 92.070\n",
      "selected users: [ 1  2  7  8  9 14 17 18 19 22 23 27]\n",
      "\n",
      "Test set: Average loss: 49.1503 \n",
      "Accuracy: 9461/10000 (94.61%)\n",
      "\n",
      "Round  28, Average loss 49.150 Test accuracy 94.610\n",
      "selected users: [ 0  1  2  4  7 10 12 16 25 26 29 30]\n",
      "\n",
      "Test set: Average loss: 279.8680 \n",
      "Accuracy: 9336/10000 (93.36%)\n",
      "\n",
      "Round  29, Average loss 279.868 Test accuracy 93.360\n",
      "(m= 12 )  9 -th Trial!!\n",
      "selected users: [ 3  8 10 13 14 15 21 22 23 27 28 30]\n",
      "\n",
      "Test set: Average loss: 2.2998 \n",
      "Accuracy: 1796/10000 (17.96%)\n",
      "\n",
      "Round   0, Average loss 2.300 Test accuracy 17.960\n",
      "selected users: [ 3  4  9 13 14 15 18 19 22 23 24 25]\n",
      "\n",
      "Test set: Average loss: 1.3831 \n",
      "Accuracy: 7509/10000 (75.09%)\n",
      "\n",
      "Round   1, Average loss 1.383 Test accuracy 75.090\n",
      "selected users: [ 1  4  6  8 12 15 17 18 25 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.2601 \n",
      "Accuracy: 9253/10000 (92.53%)\n",
      "\n",
      "Round   2, Average loss 0.260 Test accuracy 92.530\n",
      "selected users: [ 2  3  5  7  9 10 11 14 21 23 25 26]\n",
      "\n",
      "Test set: Average loss: 3.0003 \n",
      "Accuracy: 9061/10000 (90.61%)\n",
      "\n",
      "Round   3, Average loss 3.000 Test accuracy 90.610\n",
      "selected users: [ 4  7  8 12 14 15 16 20 21 23 25 26]\n",
      "\n",
      "Test set: Average loss: 3.4106 \n",
      "Accuracy: 9190/10000 (91.90%)\n",
      "\n",
      "Round   4, Average loss 3.411 Test accuracy 91.900\n",
      "selected users: [ 5  6 10 11 12 13 17 21 22 24 26 30]\n",
      "\n",
      "Test set: Average loss: 13.8146 \n",
      "Accuracy: 9024/10000 (90.24%)\n",
      "\n",
      "Round   5, Average loss 13.815 Test accuracy 90.240\n",
      "selected users: [ 0  1  7 11 12 13 15 19 22 26 27 28]\n",
      "\n",
      "Test set: Average loss: 0.5796 \n",
      "Accuracy: 9173/10000 (91.73%)\n",
      "\n",
      "Round   6, Average loss 0.580 Test accuracy 91.730\n",
      "selected users: [ 4  5  6 11 14 16 18 19 21 24 25 29]\n",
      "\n",
      "Test set: Average loss: 6.5534 \n",
      "Accuracy: 9277/10000 (92.77%)\n",
      "\n",
      "Round   7, Average loss 6.553 Test accuracy 92.770\n",
      "selected users: [ 2  4  5  7 10 15 21 23 25 26 27 29]\n",
      "\n",
      "Test set: Average loss: 19.8310 \n",
      "Accuracy: 9230/10000 (92.30%)\n",
      "\n",
      "Round   8, Average loss 19.831 Test accuracy 92.300\n",
      "selected users: [ 1  4  6  8 12 13 14 15 17 23 25 30]\n",
      "\n",
      "Test set: Average loss: 3.9797 \n",
      "Accuracy: 9455/10000 (94.55%)\n",
      "\n",
      "Round   9, Average loss 3.980 Test accuracy 94.550\n",
      "selected users: [ 0  2  4  6  8  9 16 17 18 23 26 27]\n",
      "\n",
      "Test set: Average loss: 13.3996 \n",
      "Accuracy: 9377/10000 (93.77%)\n",
      "\n",
      "Round  10, Average loss 13.400 Test accuracy 93.770\n",
      "selected users: [ 0  1  2  3  4  9 14 16 20 21 27 30]\n",
      "\n",
      "Test set: Average loss: 22.5797 \n",
      "Accuracy: 9433/10000 (94.33%)\n",
      "\n",
      "Round  11, Average loss 22.580 Test accuracy 94.330\n",
      "selected users: [ 8 12 14 15 16 19 20 22 23 24 26 28]\n",
      "\n",
      "Test set: Average loss: 7.7382 \n",
      "Accuracy: 9358/10000 (93.58%)\n",
      "\n",
      "Round  12, Average loss 7.738 Test accuracy 93.580\n",
      "selected users: [ 0  3  8  9 11 12 13 16 20 21 24 27]\n",
      "\n",
      "Test set: Average loss: 24.4465 \n",
      "Accuracy: 9315/10000 (93.15%)\n",
      "\n",
      "Round  13, Average loss 24.446 Test accuracy 93.150\n",
      "selected users: [ 0  2  4  5  7 11 16 17 19 20 27 30]\n",
      "\n",
      "Test set: Average loss: 99.0040 \n",
      "Accuracy: 9234/10000 (92.34%)\n",
      "\n",
      "Round  14, Average loss 99.004 Test accuracy 92.340\n",
      "selected users: [ 2  8  9 11 12 13 15 21 23 25 27 30]\n",
      "\n",
      "Test set: Average loss: 95.5322 \n",
      "Accuracy: 9272/10000 (92.72%)\n",
      "\n",
      "Round  15, Average loss 95.532 Test accuracy 92.720\n",
      "selected users: [ 0  2  5 10 14 15 19 21 23 24 26 29]\n",
      "\n",
      "Test set: Average loss: 192.1438 \n",
      "Accuracy: 9154/10000 (91.54%)\n",
      "\n",
      "Round  16, Average loss 192.144 Test accuracy 91.540\n",
      "selected users: [ 2  3  4  7 10 20 21 22 24 25 26 27]\n",
      "\n",
      "Test set: Average loss: 156.5422 \n",
      "Accuracy: 9350/10000 (93.50%)\n",
      "\n",
      "Round  17, Average loss 156.542 Test accuracy 93.500\n",
      "selected users: [ 1  3  5  6  8 11 12 15 19 24 25 30]\n",
      "\n",
      "Test set: Average loss: 154.9837 \n",
      "Accuracy: 9335/10000 (93.35%)\n",
      "\n",
      "Round  18, Average loss 154.984 Test accuracy 93.350\n",
      "selected users: [ 0  4  7  8 15 17 18 21 24 25 26 27]\n",
      "\n",
      "Test set: Average loss: 355.1011 \n",
      "Accuracy: 9276/10000 (92.76%)\n",
      "\n",
      "Round  19, Average loss 355.101 Test accuracy 92.760\n",
      "selected users: [ 2  3  9 12 13 14 16 18 20 23 27 30]\n",
      "\n",
      "Test set: Average loss: 155.3507 \n",
      "Accuracy: 9414/10000 (94.14%)\n",
      "\n",
      "Round  20, Average loss 155.351 Test accuracy 94.140\n",
      "selected users: [ 2  6  9 10 11 12 16 19 21 24 25 27]\n",
      "\n",
      "Test set: Average loss: 212.7282 \n",
      "Accuracy: 9393/10000 (93.93%)\n",
      "\n",
      "Round  21, Average loss 212.728 Test accuracy 93.930\n",
      "selected users: [ 4  5  7  8 10 12 14 17 19 20 25 26]\n",
      "\n",
      "Test set: Average loss: 566.9883 \n",
      "Accuracy: 9262/10000 (92.62%)\n",
      "\n",
      "Round  22, Average loss 566.988 Test accuracy 92.620\n",
      "selected users: [ 0  3  6  7  8 10 12 15 21 22 25 27]\n",
      "\n",
      "Test set: Average loss: 896.0383 \n",
      "Accuracy: 9208/10000 (92.08%)\n",
      "\n",
      "Round  23, Average loss 896.038 Test accuracy 92.080\n",
      "selected users: [ 0  8 10 11 12 14 15 17 18 19 25 27]\n",
      "\n",
      "Test set: Average loss: 525.7918 \n",
      "Accuracy: 9327/10000 (93.27%)\n",
      "\n",
      "Round  24, Average loss 525.792 Test accuracy 93.270\n",
      "selected users: [ 4 11 13 14 15 18 19 20 21 25 26 30]\n",
      "\n",
      "Test set: Average loss: 967.7948 \n",
      "Accuracy: 9281/10000 (92.81%)\n",
      "\n",
      "Round  25, Average loss 967.795 Test accuracy 92.810\n",
      "selected users: [ 0  7  8 11 13 15 19 21 24 25 27 30]\n",
      "\n",
      "Test set: Average loss: 1315.4978 \n",
      "Accuracy: 9254/10000 (92.54%)\n",
      "\n",
      "Round  26, Average loss 1315.498 Test accuracy 92.540\n",
      "selected users: [ 1  3  5  6  8 10 11 15 17 20 22 25]\n",
      "\n",
      "Test set: Average loss: 1882.7563 \n",
      "Accuracy: 9198/10000 (91.98%)\n",
      "\n",
      "Round  27, Average loss 1882.756 Test accuracy 91.980\n",
      "selected users: [ 3  9 10 11 12 17 18 20 21 22 26 30]\n",
      "\n",
      "Test set: Average loss: 820.8301 \n",
      "Accuracy: 9359/10000 (93.59%)\n",
      "\n",
      "Round  28, Average loss 820.830 Test accuracy 93.590\n",
      "selected users: [ 2  3  4  5  6  7 11 12 13 17 18 27]\n",
      "\n",
      "Test set: Average loss: 1185.8748 \n",
      "Accuracy: 9326/10000 (93.26%)\n",
      "\n",
      "Round  29, Average loss 1185.875 Test accuracy 93.260\n",
      "number of results: 15\n",
      "(m= 15 )  0 -th Trial!!\n",
      "selected users: [ 1  2  3  4  5  7  9 12 16 17 19 21 24 25 28]\n",
      "\n",
      "Test set: Average loss: 2.3027 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  2  3 11 14 15 16 20 21 22 23 25 26 28 30]\n",
      "\n",
      "Test set: Average loss: 2.1821 \n",
      "Accuracy: 6051/10000 (60.51%)\n",
      "\n",
      "Round   1, Average loss 2.182 Test accuracy 60.510\n",
      "selected users: [ 3  4  7  9 13 15 16 17 18 20 21 22 25 26 27]\n",
      "\n",
      "Test set: Average loss: 0.6352 \n",
      "Accuracy: 9368/10000 (93.68%)\n",
      "\n",
      "Round   2, Average loss 0.635 Test accuracy 93.680\n",
      "selected users: [ 3  4  6  9 10 11 19 21 22 23 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2199 \n",
      "Accuracy: 9470/10000 (94.70%)\n",
      "\n",
      "Round   3, Average loss 0.220 Test accuracy 94.700\n",
      "selected users: [ 2  4  5  8 10 11 12 13 15 18 21 25 26 27 30]\n",
      "\n",
      "Test set: Average loss: 0.2292 \n",
      "Accuracy: 9417/10000 (94.17%)\n",
      "\n",
      "Round   4, Average loss 0.229 Test accuracy 94.170\n",
      "selected users: [ 1  3  4  6  7  8 10 11 12 16 22 23 24 25 26]\n",
      "\n",
      "Test set: Average loss: 0.8808 \n",
      "Accuracy: 9183/10000 (91.83%)\n",
      "\n",
      "Round   5, Average loss 0.881 Test accuracy 91.830\n",
      "selected users: [ 0  1  3  4  5  7  8 10 12 14 17 19 21 25 26]\n",
      "\n",
      "Test set: Average loss: 1.7307 \n",
      "Accuracy: 9202/10000 (92.02%)\n",
      "\n",
      "Round   6, Average loss 1.731 Test accuracy 92.020\n",
      "selected users: [ 2  4  5 10 13 17 18 19 20 22 24 25 26 28 30]\n",
      "\n",
      "Test set: Average loss: 0.5030 \n",
      "Accuracy: 8824/10000 (88.24%)\n",
      "\n",
      "Round   7, Average loss 0.503 Test accuracy 88.240\n",
      "selected users: [ 1  3  5  7 10 12 16 17 19 21 22 24 25 28 30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.5208 \n",
      "Accuracy: 9176/10000 (91.76%)\n",
      "\n",
      "Round   8, Average loss 0.521 Test accuracy 91.760\n",
      "selected users: [ 8  9 10 11 12 15 17 20 22 23 24 25 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.3293 \n",
      "Accuracy: 9178/10000 (91.78%)\n",
      "\n",
      "Round   9, Average loss 0.329 Test accuracy 91.780\n",
      "selected users: [ 4  7  8 10 11 12 13 15 16 17 19 20 22 24 26]\n",
      "\n",
      "Test set: Average loss: 1.0389 \n",
      "Accuracy: 9262/10000 (92.62%)\n",
      "\n",
      "Round  10, Average loss 1.039 Test accuracy 92.620\n",
      "selected users: [ 0  1  2  4  5  6  8 14 16 18 19 22 23 24 30]\n",
      "\n",
      "Test set: Average loss: 0.1898 \n",
      "Accuracy: 9444/10000 (94.44%)\n",
      "\n",
      "Round  11, Average loss 0.190 Test accuracy 94.440\n",
      "selected users: [ 0  1  2  3  4  5 12 13 15 17 19 20 23 27 30]\n",
      "\n",
      "Test set: Average loss: 0.6649 \n",
      "Accuracy: 9194/10000 (91.94%)\n",
      "\n",
      "Round  12, Average loss 0.665 Test accuracy 91.940\n",
      "selected users: [ 1  3  4  5  6  8  9 14 16 18 19 20 24 26 29]\n",
      "\n",
      "Test set: Average loss: 0.2043 \n",
      "Accuracy: 9462/10000 (94.62%)\n",
      "\n",
      "Round  13, Average loss 0.204 Test accuracy 94.620\n",
      "selected users: [ 2  5  6  7 11 13 14 16 17 18 21 22 23 28 30]\n",
      "\n",
      "Test set: Average loss: 0.4949 \n",
      "Accuracy: 9208/10000 (92.08%)\n",
      "\n",
      "Round  14, Average loss 0.495 Test accuracy 92.080\n",
      "selected users: [ 0  1  2  3  6  7  8  9 12 13 19 21 22 28 29]\n",
      "\n",
      "Test set: Average loss: 2.2403 \n",
      "Accuracy: 1869/10000 (18.69%)\n",
      "\n",
      "Round  15, Average loss 2.240 Test accuracy 18.690\n",
      "selected users: [ 2  6  8 10 14 18 19 20 21 23 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.2037 \n",
      "Accuracy: 9452/10000 (94.52%)\n",
      "\n",
      "Round  16, Average loss 0.204 Test accuracy 94.520\n",
      "selected users: [ 1  2  6  8 10 14 15 16 17 18 19 22 24 25 29]\n",
      "\n",
      "Test set: Average loss: 0.1682 \n",
      "Accuracy: 9491/10000 (94.91%)\n",
      "\n",
      "Round  17, Average loss 0.168 Test accuracy 94.910\n",
      "selected users: [ 0  2  3  5  8  9 11 15 19 20 22 25 26 27 30]\n",
      "\n",
      "Test set: Average loss: 0.3828 \n",
      "Accuracy: 9449/10000 (94.49%)\n",
      "\n",
      "Round  18, Average loss 0.383 Test accuracy 94.490\n",
      "selected users: [ 1  3  6  9 13 15 17 18 21 22 23 24 25 27 29]\n",
      "\n",
      "Test set: Average loss: 0.1837 \n",
      "Accuracy: 9477/10000 (94.77%)\n",
      "\n",
      "Round  19, Average loss 0.184 Test accuracy 94.770\n",
      "selected users: [ 0  2  3  5  9 11 13 14 16 17 18 20 25 27 29]\n",
      "\n",
      "Test set: Average loss: 0.8369 \n",
      "Accuracy: 9345/10000 (93.45%)\n",
      "\n",
      "Round  20, Average loss 0.837 Test accuracy 93.450\n",
      "selected users: [ 0  1  2  3  7  8  9 10 11 12 15 17 18 25 29]\n",
      "\n",
      "Test set: Average loss: 0.1920 \n",
      "Accuracy: 9396/10000 (93.96%)\n",
      "\n",
      "Round  21, Average loss 0.192 Test accuracy 93.960\n",
      "selected users: [ 0  2  4  6  7  9 10 12 13 16 17 20 23 27 29]\n",
      "\n",
      "Test set: Average loss: 0.4336 \n",
      "Accuracy: 9432/10000 (94.32%)\n",
      "\n",
      "Round  22, Average loss 0.434 Test accuracy 94.320\n",
      "selected users: [ 1  4  7  8  9 11 13 15 18 19 24 25 26 29 30]\n",
      "\n",
      "Test set: Average loss: 0.5406 \n",
      "Accuracy: 9401/10000 (94.01%)\n",
      "\n",
      "Round  23, Average loss 0.541 Test accuracy 94.010\n",
      "selected users: [ 3  4  6  7  9 12 13 17 20 21 22 23 24 26 29]\n",
      "\n",
      "Test set: Average loss: 0.1904 \n",
      "Accuracy: 9433/10000 (94.33%)\n",
      "\n",
      "Round  24, Average loss 0.190 Test accuracy 94.330\n",
      "selected users: [ 0  4  5  6  7 10 12 15 16 18 19 22 23 25 26]\n",
      "\n",
      "Test set: Average loss: 1.6931 \n",
      "Accuracy: 9212/10000 (92.12%)\n",
      "\n",
      "Round  25, Average loss 1.693 Test accuracy 92.120\n",
      "selected users: [ 0  1  2  5  6  7  8  9 11 12 13 18 26 27 29]\n",
      "\n",
      "Test set: Average loss: 0.5306 \n",
      "Accuracy: 9050/10000 (90.50%)\n",
      "\n",
      "Round  26, Average loss 0.531 Test accuracy 90.500\n",
      "selected users: [ 2  3  4  6  7  8 10 15 16 17 20 21 23 24 30]\n",
      "\n",
      "Test set: Average loss: 0.7953 \n",
      "Accuracy: 9340/10000 (93.40%)\n",
      "\n",
      "Round  27, Average loss 0.795 Test accuracy 93.400\n",
      "selected users: [ 2  6  8 11 12 13 14 17 18 20 21 24 25 27 29]\n",
      "\n",
      "Test set: Average loss: 0.1957 \n",
      "Accuracy: 9434/10000 (94.34%)\n",
      "\n",
      "Round  28, Average loss 0.196 Test accuracy 94.340\n",
      "selected users: [ 0  1  3  4  5  9 11 14 15 17 18 21 22 24 26]\n",
      "\n",
      "Test set: Average loss: 2.5351 \n",
      "Accuracy: 9158/10000 (91.58%)\n",
      "\n",
      "Round  29, Average loss 2.535 Test accuracy 91.580\n",
      "(m= 15 )  1 -th Trial!!\n",
      "selected users: [ 2  8  9 10 12 14 17 19 20 24 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3017 \n",
      "Accuracy: 1979/10000 (19.79%)\n",
      "\n",
      "Round   0, Average loss 2.302 Test accuracy 19.790\n",
      "selected users: [ 2  5  7  8  9 10 12 14 15 16 22 25 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.2473 \n",
      "Accuracy: 6245/10000 (62.45%)\n",
      "\n",
      "Round   1, Average loss 2.247 Test accuracy 62.450\n",
      "selected users: [ 2  3  8 10 13 14 15 17 18 19 25 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.4993 \n",
      "Accuracy: 9316/10000 (93.16%)\n",
      "\n",
      "Round   2, Average loss 0.499 Test accuracy 93.160\n",
      "selected users: [ 0  1  3  5  8  9 11 12 15 16 21 25 26 29 30]\n",
      "\n",
      "Test set: Average loss: 0.3654 \n",
      "Accuracy: 9227/10000 (92.27%)\n",
      "\n",
      "Round   3, Average loss 0.365 Test accuracy 92.270\n",
      "selected users: [ 2  3  6  7 12 13 14 15 19 21 23 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4221 \n",
      "Accuracy: 9440/10000 (94.40%)\n",
      "\n",
      "Round   4, Average loss 0.422 Test accuracy 94.400\n",
      "selected users: [ 1  3  6 10 12 15 16 17 18 19 22 23 24 25 28]\n",
      "\n",
      "Test set: Average loss: 0.1657 \n",
      "Accuracy: 9498/10000 (94.98%)\n",
      "\n",
      "Round   5, Average loss 0.166 Test accuracy 94.980\n",
      "selected users: [ 0  1  2  4  7  8 15 18 20 21 23 24 25 27 30]\n",
      "\n",
      "Test set: Average loss: 0.1807 \n",
      "Accuracy: 9501/10000 (95.01%)\n",
      "\n",
      "Round   6, Average loss 0.181 Test accuracy 95.010\n",
      "selected users: [ 0  1  2  4 10 12 17 19 21 22 24 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2501 \n",
      "Accuracy: 9506/10000 (95.06%)\n",
      "\n",
      "Round   7, Average loss 0.250 Test accuracy 95.060\n",
      "selected users: [ 2  3  5  6  7 11 12 19 21 22 23 25 26 28 30]\n",
      "\n",
      "Test set: Average loss: 0.2099 \n",
      "Accuracy: 9508/10000 (95.08%)\n",
      "\n",
      "Round   8, Average loss 0.210 Test accuracy 95.080\n",
      "selected users: [ 1  2  7  8 11 12 15 16 19 20 21 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.5605 \n",
      "Accuracy: 9167/10000 (91.67%)\n",
      "\n",
      "Round   9, Average loss 0.560 Test accuracy 91.670\n",
      "selected users: [ 1  3  4  5  9 11 14 15 16 17 18 19 23 26 30]\n",
      "\n",
      "Test set: Average loss: 0.4896 \n",
      "Accuracy: 9286/10000 (92.86%)\n",
      "\n",
      "Round  10, Average loss 0.490 Test accuracy 92.860\n",
      "selected users: [ 0  3  4  5 11 12 13 16 17 18 20 21 22 24 27]\n",
      "\n",
      "Test set: Average loss: 0.8501 \n",
      "Accuracy: 9260/10000 (92.60%)\n",
      "\n",
      "Round  11, Average loss 0.850 Test accuracy 92.600\n",
      "selected users: [ 4  8 11 12 13 14 15 16 17 19 20 24 25 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2890 \n",
      "Accuracy: 9416/10000 (94.16%)\n",
      "\n",
      "Round  12, Average loss 0.289 Test accuracy 94.160\n",
      "selected users: [ 1  2  4  6  7  9 11 13 15 18 21 22 23 26 29]\n",
      "\n",
      "Test set: Average loss: 0.8386 \n",
      "Accuracy: 9288/10000 (92.88%)\n",
      "\n",
      "Round  13, Average loss 0.839 Test accuracy 92.880\n",
      "selected users: [ 0  1  2  4  5  8 11 14 16 19 20 21 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4142 \n",
      "Accuracy: 9365/10000 (93.65%)\n",
      "\n",
      "Round  14, Average loss 0.414 Test accuracy 93.650\n",
      "selected users: [ 0  1  7  8 10 11 12 15 18 19 21 23 25 26 30]\n",
      "\n",
      "Test set: Average loss: 0.2874 \n",
      "Accuracy: 9377/10000 (93.77%)\n",
      "\n",
      "Round  15, Average loss 0.287 Test accuracy 93.770\n",
      "selected users: [ 0  1  2  3  5  8 10 12 15 16 22 23 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.2048 \n",
      "Accuracy: 9520/10000 (95.20%)\n",
      "\n",
      "Round  16, Average loss 0.205 Test accuracy 95.200\n",
      "selected users: [ 0  1  4  5 11 12 13 15 17 18 20 21 22 26 30]\n",
      "\n",
      "Test set: Average loss: 1.4901 \n",
      "Accuracy: 9249/10000 (92.49%)\n",
      "\n",
      "Round  17, Average loss 1.490 Test accuracy 92.490\n",
      "selected users: [ 0  1  5  7  8 14 15 16 18 19 21 23 25 26 27]\n",
      "\n",
      "Test set: Average loss: 0.6389 \n",
      "Accuracy: 9388/10000 (93.88%)\n",
      "\n",
      "Round  18, Average loss 0.639 Test accuracy 93.880\n",
      "selected users: [ 1  2  4  5  8 12 14 17 20 21 24 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.2294 \n",
      "Accuracy: 9445/10000 (94.45%)\n",
      "\n",
      "Round  19, Average loss 0.229 Test accuracy 94.450\n",
      "selected users: [ 4  9 10 11 13 14 16 17 20 22 23 25 26 27 30]\n",
      "\n",
      "Test set: Average loss: 0.2648 \n",
      "Accuracy: 9563/10000 (95.63%)\n",
      "\n",
      "Round  20, Average loss 0.265 Test accuracy 95.630\n",
      "selected users: [ 3  4  5  7  8 10 12 16 17 18 19 20 25 28 29]\n",
      "\n",
      "Test set: Average loss: 0.8737 \n",
      "Accuracy: 9157/10000 (91.57%)\n",
      "\n",
      "Round  21, Average loss 0.874 Test accuracy 91.570\n",
      "selected users: [ 1  2  3  6  7  9 10 11 18 20 21 22 26 27 28]\n",
      "\n",
      "Test set: Average loss: 0.4775 \n",
      "Accuracy: 9263/10000 (92.63%)\n",
      "\n",
      "Round  22, Average loss 0.477 Test accuracy 92.630\n",
      "selected users: [ 0  2  3  7 10 12 14 15 18 19 20 23 25 26 28]\n",
      "\n",
      "Test set: Average loss: 0.6224 \n",
      "Accuracy: 9261/10000 (92.61%)\n",
      "\n",
      "Round  23, Average loss 0.622 Test accuracy 92.610\n",
      "selected users: [ 1  2  4  5  6  7  8  9 10 12 13 15 23 27 28]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.2668 \n",
      "Accuracy: 9391/10000 (93.91%)\n",
      "\n",
      "Round  24, Average loss 0.267 Test accuracy 93.910\n",
      "selected users: [ 0  2  3  4  6 10 11 13 14 16 19 20 23 27 28]\n",
      "\n",
      "Test set: Average loss: 1.0439 \n",
      "Accuracy: 9274/10000 (92.74%)\n",
      "\n",
      "Round  25, Average loss 1.044 Test accuracy 92.740\n",
      "selected users: [ 2  3  4  8  9 10 11 19 20 22 23 24 25 26 28]\n",
      "\n",
      "Test set: Average loss: 0.2200 \n",
      "Accuracy: 9515/10000 (95.15%)\n",
      "\n",
      "Round  26, Average loss 0.220 Test accuracy 95.150\n",
      "selected users: [ 8 12 13 14 15 16 18 19 20 23 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4296 \n",
      "Accuracy: 9330/10000 (93.30%)\n",
      "\n",
      "Round  27, Average loss 0.430 Test accuracy 93.300\n",
      "selected users: [ 0  5 10 11 13 15 16 17 21 22 23 24 25 27 28]\n",
      "\n",
      "Test set: Average loss: 3.3994 \n",
      "Accuracy: 8958/10000 (89.58%)\n",
      "\n",
      "Round  28, Average loss 3.399 Test accuracy 89.580\n",
      "selected users: [ 0  2  3  7 10 11 12 16 19 20 21 24 26 28 29]\n",
      "\n",
      "Test set: Average loss: 1.1438 \n",
      "Accuracy: 9027/10000 (90.27%)\n",
      "\n",
      "Round  29, Average loss 1.144 Test accuracy 90.270\n",
      "(m= 15 )  2 -th Trial!!\n",
      "selected users: [ 1  2  3  5  6 10 12 13 14 16 21 25 26 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 1  3  5  8  9 10 12 16 18 21 22 24 26 28 30]\n",
      "\n",
      "Test set: Average loss: 2.2400 \n",
      "Accuracy: 6874/10000 (68.74%)\n",
      "\n",
      "Round   1, Average loss 2.240 Test accuracy 68.740\n",
      "selected users: [ 1  4  6  7  9 11 12 13 14 20 24 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.3344 \n",
      "Accuracy: 9310/10000 (93.10%)\n",
      "\n",
      "Round   2, Average loss 0.334 Test accuracy 93.100\n",
      "selected users: [ 0  4  6  7  8  9 11 12 14 16 24 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.1762 \n",
      "Accuracy: 9512/10000 (95.12%)\n",
      "\n",
      "Round   3, Average loss 0.176 Test accuracy 95.120\n",
      "selected users: [ 2  4  6  7  8  9 11 13 14 16 17 20 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.1736 \n",
      "Accuracy: 9497/10000 (94.97%)\n",
      "\n",
      "Round   4, Average loss 0.174 Test accuracy 94.970\n",
      "selected users: [ 2  9 12 13 15 16 19 20 21 22 23 24 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.1347 \n",
      "Accuracy: 9588/10000 (95.88%)\n",
      "\n",
      "Round   5, Average loss 0.135 Test accuracy 95.880\n",
      "selected users: [ 5  7 11 12 13 15 17 19 21 23 24 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.9277 \n",
      "Accuracy: 9335/10000 (93.35%)\n",
      "\n",
      "Round   6, Average loss 0.928 Test accuracy 93.350\n",
      "selected users: [ 0  8  9 11 12 13 14 15 16 19 20 22 23 25 28]\n",
      "\n",
      "Test set: Average loss: 0.3888 \n",
      "Accuracy: 9440/10000 (94.40%)\n",
      "\n",
      "Round   7, Average loss 0.389 Test accuracy 94.400\n",
      "selected users: [ 0  1  3  4  6  7 10 13 17 19 20 25 26 27 28]\n",
      "\n",
      "Test set: Average loss: 0.4388 \n",
      "Accuracy: 9393/10000 (93.93%)\n",
      "\n",
      "Round   8, Average loss 0.439 Test accuracy 93.930\n",
      "selected users: [ 4  5  6  7  8 16 17 21 22 23 24 25 26 29 30]\n",
      "\n",
      "Test set: Average loss: 0.3246 \n",
      "Accuracy: 9436/10000 (94.36%)\n",
      "\n",
      "Round   9, Average loss 0.325 Test accuracy 94.360\n",
      "selected users: [ 1  3  4  5 10 11 12 14 15 18 21 22 23 28 29]\n",
      "\n",
      "Test set: Average loss: 1.1597 \n",
      "Accuracy: 9200/10000 (92.00%)\n",
      "\n",
      "Round  10, Average loss 1.160 Test accuracy 92.000\n",
      "selected users: [ 0  2  3  4  5  8 10 13 17 21 23 25 26 29 30]\n",
      "\n",
      "Test set: Average loss: 1.2706 \n",
      "Accuracy: 9112/10000 (91.12%)\n",
      "\n",
      "Round  11, Average loss 1.271 Test accuracy 91.120\n",
      "selected users: [ 0  1  2  4  5  7  9 13 14 15 17 21 23 25 30]\n",
      "\n",
      "Test set: Average loss: 0.9314 \n",
      "Accuracy: 9371/10000 (93.71%)\n",
      "\n",
      "Round  12, Average loss 0.931 Test accuracy 93.710\n",
      "selected users: [ 1  4  6  7  8  9 11 12 13 20 22 23 24 26 30]\n",
      "\n",
      "Test set: Average loss: 3.0883 \n",
      "Accuracy: 9273/10000 (92.73%)\n",
      "\n",
      "Round  13, Average loss 3.088 Test accuracy 92.730\n",
      "selected users: [ 0  1  3  4  5  8 10 12 17 18 20 22 23 27 30]\n",
      "\n",
      "Test set: Average loss: 0.3657 \n",
      "Accuracy: 9469/10000 (94.69%)\n",
      "\n",
      "Round  14, Average loss 0.366 Test accuracy 94.690\n",
      "selected users: [ 2  3  7  9 10 11 15 17 19 23 24 25 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.5150 \n",
      "Accuracy: 9280/10000 (92.80%)\n",
      "\n",
      "Round  15, Average loss 0.515 Test accuracy 92.800\n",
      "selected users: [ 2  3  6  7  9 10 13 14 18 22 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.3270 \n",
      "Accuracy: 9412/10000 (94.12%)\n",
      "\n",
      "Round  16, Average loss 0.327 Test accuracy 94.120\n",
      "selected users: [ 0  1  3  4  6  7 10 11 13 14 15 19 20 23 29]\n",
      "\n",
      "Test set: Average loss: 0.3476 \n",
      "Accuracy: 9413/10000 (94.13%)\n",
      "\n",
      "Round  17, Average loss 0.348 Test accuracy 94.130\n",
      "selected users: [ 0  1  3  4 10 11 14 15 19 21 23 24 26 28 30]\n",
      "\n",
      "Test set: Average loss: 1.2389 \n",
      "Accuracy: 9360/10000 (93.60%)\n",
      "\n",
      "Round  18, Average loss 1.239 Test accuracy 93.600\n",
      "selected users: [ 2  3  4  5  9 10 12 15 17 18 19 20 21 27 28]\n",
      "\n",
      "Test set: Average loss: 0.3763 \n",
      "Accuracy: 9222/10000 (92.22%)\n",
      "\n",
      "Round  19, Average loss 0.376 Test accuracy 92.220\n",
      "selected users: [ 1  3  5  7 10 11 14 15 20 21 24 25 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.6950 \n",
      "Accuracy: 9343/10000 (93.43%)\n",
      "\n",
      "Round  20, Average loss 0.695 Test accuracy 93.430\n",
      "selected users: [ 0  2  4  9 10 12 15 16 17 18 23 25 26 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4429 \n",
      "Accuracy: 9427/10000 (94.27%)\n",
      "\n",
      "Round  21, Average loss 0.443 Test accuracy 94.270\n",
      "selected users: [ 0  3  4  5  8 11 12 13 14 15 17 18 20 24 28]\n",
      "\n",
      "Test set: Average loss: 0.5132 \n",
      "Accuracy: 9231/10000 (92.31%)\n",
      "\n",
      "Round  22, Average loss 0.513 Test accuracy 92.310\n",
      "selected users: [ 1  3  4  7  8 10 14 16 17 20 21 22 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4851 \n",
      "Accuracy: 9519/10000 (95.19%)\n",
      "\n",
      "Round  23, Average loss 0.485 Test accuracy 95.190\n",
      "selected users: [ 0  1  3  5  8 12 14 16 18 19 21 22 23 26 30]\n",
      "\n",
      "Test set: Average loss: 1.0037 \n",
      "Accuracy: 9319/10000 (93.19%)\n",
      "\n",
      "Round  24, Average loss 1.004 Test accuracy 93.190\n",
      "selected users: [ 1  4  6  8  9 10 12 17 18 19 20 21 23 27 29]\n",
      "\n",
      "Test set: Average loss: 0.1985 \n",
      "Accuracy: 9574/10000 (95.74%)\n",
      "\n",
      "Round  25, Average loss 0.199 Test accuracy 95.740\n",
      "selected users: [ 1  2  3  5  6  8  9 13 14 20 21 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.3379 \n",
      "Accuracy: 9288/10000 (92.88%)\n",
      "\n",
      "Round  26, Average loss 1.338 Test accuracy 92.880\n",
      "selected users: [ 1  3  5  6  7  9 11 12 13 16 17 22 24 25 27]\n",
      "\n",
      "Test set: Average loss: 0.3263 \n",
      "Accuracy: 9504/10000 (95.04%)\n",
      "\n",
      "Round  27, Average loss 0.326 Test accuracy 95.040\n",
      "selected users: [ 0  1  2  3  4  6  7  8  9 10 14 15 16 17 24]\n",
      "\n",
      "Test set: Average loss: 0.1886 \n",
      "Accuracy: 9495/10000 (94.95%)\n",
      "\n",
      "Round  28, Average loss 0.189 Test accuracy 94.950\n",
      "selected users: [ 1  4  5  9 10 15 16 17 18 19 22 23 24 27 29]\n",
      "\n",
      "Test set: Average loss: 0.8324 \n",
      "Accuracy: 9415/10000 (94.15%)\n",
      "\n",
      "Round  29, Average loss 0.832 Test accuracy 94.150\n",
      "(m= 15 )  3 -th Trial!!\n",
      "selected users: [ 3  4  6  7  8  9 10 11 12 14 19 20 22 25 26]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  1  2  3  6  8 10 14 15 17 19 23 24 25 28]\n",
      "\n",
      "Test set: Average loss: 2.2524 \n",
      "Accuracy: 4538/10000 (45.38%)\n",
      "\n",
      "Round   1, Average loss 2.252 Test accuracy 45.380\n",
      "selected users: [ 3  6  9 11 12 14 16 19 23 24 25 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.9000 \n",
      "Accuracy: 8581/10000 (85.81%)\n",
      "\n",
      "Round   2, Average loss 0.900 Test accuracy 85.810\n",
      "selected users: [ 3  4  7 10 11 14 16 18 19 20 22 24 26 27 28]\n",
      "\n",
      "Test set: Average loss: 0.3245 \n",
      "Accuracy: 9379/10000 (93.79%)\n",
      "\n",
      "Round   3, Average loss 0.324 Test accuracy 93.790\n",
      "selected users: [ 0  1  2  4  6  7  9 11 15 18 21 22 25 26 28]\n",
      "\n",
      "Test set: Average loss: 0.2362 \n",
      "Accuracy: 9463/10000 (94.63%)\n",
      "\n",
      "Round   4, Average loss 0.236 Test accuracy 94.630\n",
      "selected users: [ 2  4  6  9 10 13 14 15 16 18 21 23 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.1991 \n",
      "Accuracy: 9442/10000 (94.42%)\n",
      "\n",
      "Round   5, Average loss 0.199 Test accuracy 94.420\n",
      "selected users: [ 1  2  5  6  7  8  9 10 15 16 18 19 25 26 27]\n",
      "\n",
      "Test set: Average loss: 0.1629 \n",
      "Accuracy: 9494/10000 (94.94%)\n",
      "\n",
      "Round   6, Average loss 0.163 Test accuracy 94.940\n",
      "selected users: [ 1  3  4  5  7  8  9 12 15 17 18 19 23 27 29]\n",
      "\n",
      "Test set: Average loss: 0.1523 \n",
      "Accuracy: 9546/10000 (95.46%)\n",
      "\n",
      "Round   7, Average loss 0.152 Test accuracy 95.460\n",
      "selected users: [ 0  9 10 11 15 16 18 20 21 23 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.3183 \n",
      "Accuracy: 9518/10000 (95.18%)\n",
      "\n",
      "Round   8, Average loss 0.318 Test accuracy 95.180\n",
      "selected users: [ 0  2  3  4  7  8  9 11 12 14 15 16 23 25 26]\n",
      "\n",
      "Test set: Average loss: 0.7960 \n",
      "Accuracy: 9346/10000 (93.46%)\n",
      "\n",
      "Round   9, Average loss 0.796 Test accuracy 93.460\n",
      "selected users: [ 1  2  6  7 12 13 14 16 19 20 25 26 27 28 30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1808 \n",
      "Accuracy: 9445/10000 (94.45%)\n",
      "\n",
      "Round  10, Average loss 0.181 Test accuracy 94.450\n",
      "selected users: [ 2  4  7  9 13 15 18 20 21 24 25 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.2030 \n",
      "Accuracy: 9510/10000 (95.10%)\n",
      "\n",
      "Round  11, Average loss 0.203 Test accuracy 95.100\n",
      "selected users: [ 0  3  5  6  7 10 11 14 16 19 21 25 26 28 29]\n",
      "\n",
      "Test set: Average loss: 0.7711 \n",
      "Accuracy: 9349/10000 (93.49%)\n",
      "\n",
      "Round  12, Average loss 0.771 Test accuracy 93.490\n",
      "selected users: [ 0  6  7  9 11 12 13 16 18 21 22 23 24 29 30]\n",
      "\n",
      "Test set: Average loss: 0.3446 \n",
      "Accuracy: 9368/10000 (93.68%)\n",
      "\n",
      "Round  13, Average loss 0.345 Test accuracy 93.680\n",
      "selected users: [ 1  3  5  6  7 10 11 12 16 17 20 23 24 28 29]\n",
      "\n",
      "Test set: Average loss: 0.4464 \n",
      "Accuracy: 9359/10000 (93.59%)\n",
      "\n",
      "Round  14, Average loss 0.446 Test accuracy 93.590\n",
      "selected users: [ 2  4  6  7  9 11 12 13 14 15 18 19 25 27 30]\n",
      "\n",
      "Test set: Average loss: 0.4213 \n",
      "Accuracy: 9462/10000 (94.62%)\n",
      "\n",
      "Round  15, Average loss 0.421 Test accuracy 94.620\n",
      "selected users: [ 0  5  7 11 12 13 15 16 18 19 21 22 24 26 29]\n",
      "\n",
      "Test set: Average loss: 1.4039 \n",
      "Accuracy: 9257/10000 (92.57%)\n",
      "\n",
      "Round  16, Average loss 1.404 Test accuracy 92.570\n",
      "selected users: [ 0  2  5 10 11 12 13 14 15 17 20 21 22 23 24]\n",
      "\n",
      "Test set: Average loss: 1.0170 \n",
      "Accuracy: 9246/10000 (92.46%)\n",
      "\n",
      "Round  17, Average loss 1.017 Test accuracy 92.460\n",
      "selected users: [ 2  4  7  9 10 14 17 20 21 22 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2505 \n",
      "Accuracy: 9387/10000 (93.87%)\n",
      "\n",
      "Round  18, Average loss 0.251 Test accuracy 93.870\n",
      "selected users: [ 1  3  6  7  9 10 13 14 16 18 22 23 24 25 29]\n",
      "\n",
      "Test set: Average loss: 0.2808 \n",
      "Accuracy: 9374/10000 (93.74%)\n",
      "\n",
      "Round  19, Average loss 0.281 Test accuracy 93.740\n",
      "selected users: [ 0  5  9 10 11 12 14 16 18 20 22 24 25 26 30]\n",
      "\n",
      "Test set: Average loss: 1.2010 \n",
      "Accuracy: 9320/10000 (93.20%)\n",
      "\n",
      "Round  20, Average loss 1.201 Test accuracy 93.200\n",
      "selected users: [ 0  1  3  4  5  6  8 13 17 19 20 22 25 27 30]\n",
      "\n",
      "Test set: Average loss: 0.1705 \n",
      "Accuracy: 9543/10000 (95.43%)\n",
      "\n",
      "Round  21, Average loss 0.171 Test accuracy 95.430\n",
      "selected users: [ 0  1  2  3  5 10 11 12 13 14 16 18 21 26 29]\n",
      "\n",
      "Test set: Average loss: 3.6882 \n",
      "Accuracy: 9113/10000 (91.13%)\n",
      "\n",
      "Round  22, Average loss 3.688 Test accuracy 91.130\n",
      "selected users: [ 0  2  3  4  6  9 10 17 18 20 22 23 24 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3441 \n",
      "Accuracy: 8746/10000 (87.46%)\n",
      "\n",
      "Round  23, Average loss 1.344 Test accuracy 87.460\n",
      "selected users: [ 2  3  6  7  8 12 14 16 21 22 23 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.2788 \n",
      "Accuracy: 9199/10000 (91.99%)\n",
      "\n",
      "Round  24, Average loss 1.279 Test accuracy 91.990\n",
      "selected users: [ 1  4  8 11 13 14 16 18 20 23 24 25 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.1942 \n",
      "Accuracy: 9332/10000 (93.32%)\n",
      "\n",
      "Round  25, Average loss 1.194 Test accuracy 93.320\n",
      "selected users: [ 0  1  2  6  8 12 13 14 17 21 22 24 26 27 30]\n",
      "\n",
      "Test set: Average loss: 0.3077 \n",
      "Accuracy: 9308/10000 (93.08%)\n",
      "\n",
      "Round  26, Average loss 0.308 Test accuracy 93.080\n",
      "selected users: [ 2  3  5  6  9 10 11 12 14 15 17 18 25 27 29]\n",
      "\n",
      "Test set: Average loss: 0.4643 \n",
      "Accuracy: 9312/10000 (93.12%)\n",
      "\n",
      "Round  27, Average loss 0.464 Test accuracy 93.120\n",
      "selected users: [ 0  1  2  3  4  7  8 10 11 12 14 19 21 24 27]\n",
      "\n",
      "Test set: Average loss: 0.3044 \n",
      "Accuracy: 9443/10000 (94.43%)\n",
      "\n",
      "Round  28, Average loss 0.304 Test accuracy 94.430\n",
      "selected users: [ 0  1  7  9 10 11 12 13 15 17 18 21 22 26 30]\n",
      "\n",
      "Test set: Average loss: 0.3733 \n",
      "Accuracy: 9328/10000 (93.28%)\n",
      "\n",
      "Round  29, Average loss 0.373 Test accuracy 93.280\n",
      "(m= 15 )  4 -th Trial!!\n",
      "selected users: [ 0  1  3  4  7  9 10 12 14 21 22 25 27 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 1  4  8  9 11 12 13 17 20 22 23 24 26 28 29]\n",
      "\n",
      "Test set: Average loss: 2.0568 \n",
      "Accuracy: 7797/10000 (77.97%)\n",
      "\n",
      "Round   1, Average loss 2.057 Test accuracy 77.970\n",
      "selected users: [ 1  2  3  4  7 11 16 17 18 20 24 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2402 \n",
      "Accuracy: 9316/10000 (93.16%)\n",
      "\n",
      "Round   2, Average loss 0.240 Test accuracy 93.160\n",
      "selected users: [ 3  5  6  7  8  9 18 19 20 23 24 25 26 27 30]\n",
      "\n",
      "Test set: Average loss: 0.2900 \n",
      "Accuracy: 9382/10000 (93.82%)\n",
      "\n",
      "Round   3, Average loss 0.290 Test accuracy 93.820\n",
      "selected users: [ 1  2  3  6  7  9 14 16 17 18 20 21 23 26 28]\n",
      "\n",
      "Test set: Average loss: 0.1949 \n",
      "Accuracy: 9405/10000 (94.05%)\n",
      "\n",
      "Round   4, Average loss 0.195 Test accuracy 94.050\n",
      "selected users: [ 1  3  4  6  7 12 15 17 19 21 22 24 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.1666 \n",
      "Accuracy: 9550/10000 (95.50%)\n",
      "\n",
      "Round   5, Average loss 0.167 Test accuracy 95.500\n",
      "selected users: [ 2  6 10 14 15 16 17 19 20 23 24 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.3469 \n",
      "Accuracy: 9467/10000 (94.67%)\n",
      "\n",
      "Round   6, Average loss 0.347 Test accuracy 94.670\n",
      "selected users: [ 0  2  4  7 10 13 14 16 17 19 23 24 25 26 29]\n",
      "\n",
      "Test set: Average loss: 0.2893 \n",
      "Accuracy: 9457/10000 (94.57%)\n",
      "\n",
      "Round   7, Average loss 0.289 Test accuracy 94.570\n",
      "selected users: [ 0  1  2  4  6  7  8 12 13 14 18 21 23 25 26]\n",
      "\n",
      "Test set: Average loss: 0.8084 \n",
      "Accuracy: 9237/10000 (92.37%)\n",
      "\n",
      "Round   8, Average loss 0.808 Test accuracy 92.370\n",
      "selected users: [ 3  5  9 10 13 14 15 17 18 19 23 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.4232 \n",
      "Accuracy: 9343/10000 (93.43%)\n",
      "\n",
      "Round   9, Average loss 0.423 Test accuracy 93.430\n",
      "selected users: [ 0  1  2  3  4  7  8 10 18 21 22 25 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.5352 \n",
      "Accuracy: 9339/10000 (93.39%)\n",
      "\n",
      "Round  10, Average loss 0.535 Test accuracy 93.390\n",
      "selected users: [ 0  2  4  6  7  8 11 12 13 15 22 24 26 28 30]\n",
      "\n",
      "Test set: Average loss: 1.1007 \n",
      "Accuracy: 9344/10000 (93.44%)\n",
      "\n",
      "Round  11, Average loss 1.101 Test accuracy 93.440\n",
      "selected users: [ 0  3  6  8  9 10 13 14 15 16 17 18 22 25 29]\n",
      "\n",
      "Test set: Average loss: 0.2391 \n",
      "Accuracy: 9324/10000 (93.24%)\n",
      "\n",
      "Round  12, Average loss 0.239 Test accuracy 93.240\n",
      "selected users: [ 1  3  5  6  7  9 10 16 17 19 20 21 22 26 30]\n",
      "\n",
      "Test set: Average loss: 3.6395 \n",
      "Accuracy: 9098/10000 (90.98%)\n",
      "\n",
      "Round  13, Average loss 3.639 Test accuracy 90.980\n",
      "selected users: [ 0  2 10 11 12 13 15 17 18 20 21 23 24 26 27]\n",
      "\n",
      "Test set: Average loss: 0.9423 \n",
      "Accuracy: 7796/10000 (77.96%)\n",
      "\n",
      "Round  14, Average loss 0.942 Test accuracy 77.960\n",
      "selected users: [ 0  1  2  3  4  5  7 14 15 18 23 24 25 28 30]\n",
      "\n",
      "Test set: Average loss: 0.1960 \n",
      "Accuracy: 9523/10000 (95.23%)\n",
      "\n",
      "Round  15, Average loss 0.196 Test accuracy 95.230\n",
      "selected users: [ 0  1  4 10 11 15 17 18 19 20 21 22 25 28 29]\n",
      "\n",
      "Test set: Average loss: 0.5482 \n",
      "Accuracy: 9421/10000 (94.21%)\n",
      "\n",
      "Round  16, Average loss 0.548 Test accuracy 94.210\n",
      "selected users: [ 4  7 11 12 13 15 17 21 22 23 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.4113 \n",
      "Accuracy: 9483/10000 (94.83%)\n",
      "\n",
      "Round  17, Average loss 0.411 Test accuracy 94.830\n",
      "selected users: [ 0  4  5  6  7  8  9 10 12 13 15 16 21 25 28]\n",
      "\n",
      "Test set: Average loss: 1.1198 \n",
      "Accuracy: 9327/10000 (93.27%)\n",
      "\n",
      "Round  18, Average loss 1.120 Test accuracy 93.270\n",
      "selected users: [ 1  2  4  5  6  8  9 10 11 12 14 17 23 24 27]\n",
      "\n",
      "Test set: Average loss: 0.2675 \n",
      "Accuracy: 9508/10000 (95.08%)\n",
      "\n",
      "Round  19, Average loss 0.267 Test accuracy 95.080\n",
      "selected users: [ 1  8  9 12 15 16 17 18 19 20 21 23 25 28 30]\n",
      "\n",
      "Test set: Average loss: 0.2635 \n",
      "Accuracy: 9504/10000 (95.04%)\n",
      "\n",
      "Round  20, Average loss 0.263 Test accuracy 95.040\n",
      "selected users: [ 3  6  7  8  9 13 15 16 20 21 23 24 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4264 \n",
      "Accuracy: 9416/10000 (94.16%)\n",
      "\n",
      "Round  21, Average loss 0.426 Test accuracy 94.160\n",
      "selected users: [ 0  1  2  3  5  7  8 10 14 18 22 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4370 \n",
      "Accuracy: 9444/10000 (94.44%)\n",
      "\n",
      "Round  22, Average loss 0.437 Test accuracy 94.440\n",
      "selected users: [ 2  3  4  5  7  8 11 13 14 18 19 21 24 26 28]\n",
      "\n",
      "Test set: Average loss: 0.4546 \n",
      "Accuracy: 9352/10000 (93.52%)\n",
      "\n",
      "Round  23, Average loss 0.455 Test accuracy 93.520\n",
      "selected users: [ 3  4  5  6  9 12 13 15 17 18 21 24 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4697 \n",
      "Accuracy: 9328/10000 (93.28%)\n",
      "\n",
      "Round  24, Average loss 0.470 Test accuracy 93.280\n",
      "selected users: [ 0  1  2  3  5  6  8 12 13 14 15 16 17 23 28]\n",
      "\n",
      "Test set: Average loss: 0.2120 \n",
      "Accuracy: 9407/10000 (94.07%)\n",
      "\n",
      "Round  25, Average loss 0.212 Test accuracy 94.070\n",
      "selected users: [ 0  2  3  4  5  6  7  9 11 15 17 18 21 23 24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.5198 \n",
      "Accuracy: 9207/10000 (92.07%)\n",
      "\n",
      "Round  26, Average loss 1.520 Test accuracy 92.070\n",
      "selected users: [ 2  3  4  6 10 14 15 19 20 21 23 24 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.0833 \n",
      "Accuracy: 9306/10000 (93.06%)\n",
      "\n",
      "Round  27, Average loss 1.083 Test accuracy 93.060\n",
      "selected users: [ 0  1  3  4  5  7  8 10 15 16 18 21 24 26 28]\n",
      "\n",
      "Test set: Average loss: 1.2098 \n",
      "Accuracy: 9079/10000 (90.79%)\n",
      "\n",
      "Round  28, Average loss 1.210 Test accuracy 90.790\n",
      "selected users: [ 0  2  4  7  8 10 13 14 15 16 21 24 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.4040 \n",
      "Accuracy: 9187/10000 (91.87%)\n",
      "\n",
      "Round  29, Average loss 2.404 Test accuracy 91.870\n",
      "(m= 15 )  5 -th Trial!!\n",
      "selected users: [ 1  3  6  9 10 12 15 17 20 22 24 25 26 28 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  1  2  5  7  9 11 12 15 16 17 20 21 22 24]\n",
      "\n",
      "Test set: Average loss: 2.1517 \n",
      "Accuracy: 6197/10000 (61.97%)\n",
      "\n",
      "Round   1, Average loss 2.152 Test accuracy 61.970\n",
      "selected users: [ 0  4  5  6  8  9 10 16 20 21 23 24 25 26 30]\n",
      "\n",
      "Test set: Average loss: 0.3312 \n",
      "Accuracy: 9220/10000 (92.20%)\n",
      "\n",
      "Round   2, Average loss 0.331 Test accuracy 92.200\n",
      "selected users: [ 1  4 12 13 14 15 16 19 22 23 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2438 \n",
      "Accuracy: 9397/10000 (93.97%)\n",
      "\n",
      "Round   3, Average loss 0.244 Test accuracy 93.970\n",
      "selected users: [ 0  1  2  4  6  7  8  9 14 18 19 24 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2896 \n",
      "Accuracy: 9552/10000 (95.52%)\n",
      "\n",
      "Round   4, Average loss 0.290 Test accuracy 95.520\n",
      "selected users: [ 2  3  4  8 11 12 13 18 19 21 23 24 26 28 30]\n",
      "\n",
      "Test set: Average loss: 0.3332 \n",
      "Accuracy: 9395/10000 (93.95%)\n",
      "\n",
      "Round   5, Average loss 0.333 Test accuracy 93.950\n",
      "selected users: [ 1  3  5  7  8  9 10 13 14 16 17 18 19 25 27]\n",
      "\n",
      "Test set: Average loss: 0.5440 \n",
      "Accuracy: 9411/10000 (94.11%)\n",
      "\n",
      "Round   6, Average loss 0.544 Test accuracy 94.110\n",
      "selected users: [ 0  2  3  4  6 12 14 15 16 18 19 20 23 28 29]\n",
      "\n",
      "Test set: Average loss: 0.3452 \n",
      "Accuracy: 9362/10000 (93.62%)\n",
      "\n",
      "Round   7, Average loss 0.345 Test accuracy 93.620\n",
      "selected users: [ 3  5  6  7  9 11 12 15 19 24 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.1694 \n",
      "Accuracy: 9491/10000 (94.91%)\n",
      "\n",
      "Round   8, Average loss 0.169 Test accuracy 94.910\n",
      "selected users: [ 0  1  3  4  6  7 11 13 14 15 17 20 24 27 30]\n",
      "\n",
      "Test set: Average loss: 0.2485 \n",
      "Accuracy: 9530/10000 (95.30%)\n",
      "\n",
      "Round   9, Average loss 0.249 Test accuracy 95.300\n",
      "selected users: [ 0  2  5  6  8  9 10 11 13 15 19 20 21 25 26]\n",
      "\n",
      "Test set: Average loss: 2.4144 \n",
      "Accuracy: 9115/10000 (91.15%)\n",
      "\n",
      "Round  10, Average loss 2.414 Test accuracy 91.150\n",
      "selected users: [ 0  4  8  9 10 11 12 14 16 18 20 21 25 26 30]\n",
      "\n",
      "Test set: Average loss: 0.9045 \n",
      "Accuracy: 9400/10000 (94.00%)\n",
      "\n",
      "Round  11, Average loss 0.904 Test accuracy 94.000\n",
      "selected users: [ 2  3  5  6  8 10 12 14 16 18 20 21 23 24 28]\n",
      "\n",
      "Test set: Average loss: 0.3746 \n",
      "Accuracy: 8983/10000 (89.83%)\n",
      "\n",
      "Round  12, Average loss 0.375 Test accuracy 89.830\n",
      "selected users: [ 0  4  5  6 10 12 13 14 15 16 17 19 23 25 26]\n",
      "\n",
      "Test set: Average loss: 2.4721 \n",
      "Accuracy: 9175/10000 (91.75%)\n",
      "\n",
      "Round  13, Average loss 2.472 Test accuracy 91.750\n",
      "selected users: [ 0  5  6  8 10 12 13 14 18 23 24 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5628 \n",
      "Accuracy: 9139/10000 (91.39%)\n",
      "\n",
      "Round  14, Average loss 1.563 Test accuracy 91.390\n",
      "selected users: [ 3  4  5  6  8 15 16 17 19 20 22 24 25 26 29]\n",
      "\n",
      "Test set: Average loss: 0.5716 \n",
      "Accuracy: 8859/10000 (88.59%)\n",
      "\n",
      "Round  15, Average loss 0.572 Test accuracy 88.590\n",
      "selected users: [ 0  2  4  5  6  9 17 21 22 23 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2516 \n",
      "Accuracy: 9486/10000 (94.86%)\n",
      "\n",
      "Round  16, Average loss 0.252 Test accuracy 94.860\n",
      "selected users: [ 0  3  4  5  7 13 15 16 18 19 21 24 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.7410 \n",
      "Accuracy: 9327/10000 (93.27%)\n",
      "\n",
      "Round  17, Average loss 0.741 Test accuracy 93.270\n",
      "selected users: [ 0  1  4  6 10 12 17 19 20 21 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4246 \n",
      "Accuracy: 9534/10000 (95.34%)\n",
      "\n",
      "Round  18, Average loss 0.425 Test accuracy 95.340\n",
      "selected users: [ 1  2  6  7  9 11 12 15 18 19 21 25 26 27 28]\n",
      "\n",
      "Test set: Average loss: 0.6264 \n",
      "Accuracy: 9302/10000 (93.02%)\n",
      "\n",
      "Round  19, Average loss 0.626 Test accuracy 93.020\n",
      "selected users: [ 1  3  4 10 11 14 20 21 22 23 24 25 26 28 29]\n",
      "\n",
      "Test set: Average loss: 0.6378 \n",
      "Accuracy: 9456/10000 (94.56%)\n",
      "\n",
      "Round  20, Average loss 0.638 Test accuracy 94.560\n",
      "selected users: [ 2  6  8 10 12 13 14 21 22 23 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.3514 \n",
      "Accuracy: 9355/10000 (93.55%)\n",
      "\n",
      "Round  21, Average loss 0.351 Test accuracy 93.550\n",
      "selected users: [ 1  5  9 10 12 15 18 19 20 23 24 25 26 27 30]\n",
      "\n",
      "Test set: Average loss: 1.1649 \n",
      "Accuracy: 9391/10000 (93.91%)\n",
      "\n",
      "Round  22, Average loss 1.165 Test accuracy 93.910\n",
      "selected users: [ 0  2  5  6  7  9 10 17 20 23 24 25 26 28 29]\n",
      "\n",
      "Test set: Average loss: 2.0561 \n",
      "Accuracy: 8724/10000 (87.24%)\n",
      "\n",
      "Round  23, Average loss 2.056 Test accuracy 87.240\n",
      "selected users: [ 2  5  8  9 13 17 18 19 20 22 24 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4476 \n",
      "Accuracy: 9377/10000 (93.77%)\n",
      "\n",
      "Round  24, Average loss 0.448 Test accuracy 93.770\n",
      "selected users: [ 1  3 13 14 17 18 19 20 21 23 24 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4622 \n",
      "Accuracy: 8711/10000 (87.11%)\n",
      "\n",
      "Round  25, Average loss 0.462 Test accuracy 87.110\n",
      "selected users: [ 1  3  5  7  8 10 14 15 16 22 23 25 26 28 30]\n",
      "\n",
      "Test set: Average loss: 0.6948 \n",
      "Accuracy: 9254/10000 (92.54%)\n",
      "\n",
      "Round  26, Average loss 0.695 Test accuracy 92.540\n",
      "selected users: [ 2  4  8 11 12 14 16 17 18 19 21 24 25 28 29]\n",
      "\n",
      "Test set: Average loss: 0.3273 \n",
      "Accuracy: 9346/10000 (93.46%)\n",
      "\n",
      "Round  27, Average loss 0.327 Test accuracy 93.460\n",
      "selected users: [ 1  2  5  7  9 10 12 13 16 22 24 25 26 28 30]\n",
      "\n",
      "Test set: Average loss: 0.8750 \n",
      "Accuracy: 9286/10000 (92.86%)\n",
      "\n",
      "Round  28, Average loss 0.875 Test accuracy 92.860\n",
      "selected users: [ 0  2  3  8  9 10 12 16 17 18 19 22 24 28 29]\n",
      "\n",
      "Test set: Average loss: 0.2379 \n",
      "Accuracy: 9373/10000 (93.73%)\n",
      "\n",
      "Round  29, Average loss 0.238 Test accuracy 93.730\n",
      "(m= 15 )  6 -th Trial!!\n",
      "selected users: [ 1  2  6 10 11 12 15 16 18 19 21 22 24 27 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  6  7  8 13 15 21 23 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.1945 \n",
      "Accuracy: 6369/10000 (63.69%)\n",
      "\n",
      "Round   1, Average loss 2.194 Test accuracy 63.690\n",
      "selected users: [ 0  1  2  5  6 10 14 20 21 22 23 24 26 27 28]\n",
      "\n",
      "Test set: Average loss: 0.4196 \n",
      "Accuracy: 9296/10000 (92.96%)\n",
      "\n",
      "Round   2, Average loss 0.420 Test accuracy 92.960\n",
      "selected users: [ 0  1  3  4  6  8  9 15 16 18 23 24 25 26 30]\n",
      "\n",
      "Test set: Average loss: 0.2339 \n",
      "Accuracy: 9384/10000 (93.84%)\n",
      "\n",
      "Round   3, Average loss 0.234 Test accuracy 93.840\n",
      "selected users: [ 3  6  7 11 12 14 15 17 19 21 23 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.2296 \n",
      "Accuracy: 9317/10000 (93.17%)\n",
      "\n",
      "Round   4, Average loss 0.230 Test accuracy 93.170\n",
      "selected users: [ 0  7  9 10 11 12 13 15 16 18 21 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.0204 \n",
      "Accuracy: 9158/10000 (91.58%)\n",
      "\n",
      "Round   5, Average loss 1.020 Test accuracy 91.580\n",
      "selected users: [ 0  1  2  7  8 10 15 18 20 21 23 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.5102 \n",
      "Accuracy: 9278/10000 (92.78%)\n",
      "\n",
      "Round   6, Average loss 0.510 Test accuracy 92.780\n",
      "selected users: [ 3  4  5  6  7  8 12 16 18 22 23 24 25 26 30]\n",
      "\n",
      "Test set: Average loss: 0.2224 \n",
      "Accuracy: 9386/10000 (93.86%)\n",
      "\n",
      "Round   7, Average loss 0.222 Test accuracy 93.860\n",
      "selected users: [ 0  2  6  8 10 11 15 18 19 20 24 25 26 28 30]\n",
      "\n",
      "Test set: Average loss: 0.5011 \n",
      "Accuracy: 9193/10000 (91.93%)\n",
      "\n",
      "Round   8, Average loss 0.501 Test accuracy 91.930\n",
      "selected users: [ 1  2  4  5  6  9 13 14 15 18 21 22 24 26 29]\n",
      "\n",
      "Test set: Average loss: 0.2774 \n",
      "Accuracy: 9376/10000 (93.76%)\n",
      "\n",
      "Round   9, Average loss 0.277 Test accuracy 93.760\n",
      "selected users: [ 1  4  5  6  7  9 10 13 14 15 22 24 26 27 30]\n",
      "\n",
      "Test set: Average loss: 0.3724 \n",
      "Accuracy: 9428/10000 (94.28%)\n",
      "\n",
      "Round  10, Average loss 0.372 Test accuracy 94.280\n",
      "selected users: [ 2  6  7  8 10 12 14 16 17 18 21 22 25 27 30]\n",
      "\n",
      "Test set: Average loss: 0.3494 \n",
      "Accuracy: 9418/10000 (94.18%)\n",
      "\n",
      "Round  11, Average loss 0.349 Test accuracy 94.180\n",
      "selected users: [ 4  5  6  8 11 13 14 16 19 21 22 23 25 26 27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1544 \n",
      "Accuracy: 9555/10000 (95.55%)\n",
      "\n",
      "Round  12, Average loss 0.154 Test accuracy 95.550\n",
      "selected users: [ 0  2  3  5  8  9 13 14 17 19 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2063 \n",
      "Accuracy: 9545/10000 (95.45%)\n",
      "\n",
      "Round  13, Average loss 0.206 Test accuracy 95.450\n",
      "selected users: [ 3  8 11 13 14 15 16 17 18 19 20 22 23 24 25]\n",
      "\n",
      "Test set: Average loss: 0.2634 \n",
      "Accuracy: 9363/10000 (93.63%)\n",
      "\n",
      "Round  14, Average loss 0.263 Test accuracy 93.630\n",
      "selected users: [ 2  3  4  5  7  9 11 12 15 17 19 20 24 27 30]\n",
      "\n",
      "Test set: Average loss: 0.1951 \n",
      "Accuracy: 9503/10000 (95.03%)\n",
      "\n",
      "Round  15, Average loss 0.195 Test accuracy 95.030\n",
      "selected users: [ 1  2  3  8 10 12 14 16 18 20 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.5912 \n",
      "Accuracy: 9342/10000 (93.42%)\n",
      "\n",
      "Round  16, Average loss 0.591 Test accuracy 93.420\n",
      "selected users: [ 0  1  2  4  8  9 14 15 21 22 24 25 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.3875 \n",
      "Accuracy: 9471/10000 (94.71%)\n",
      "\n",
      "Round  17, Average loss 0.387 Test accuracy 94.710\n",
      "selected users: [ 0  2  4  8  9 11 12 13 14 15 22 23 24 28 30]\n",
      "\n",
      "Test set: Average loss: 0.2614 \n",
      "Accuracy: 9499/10000 (94.99%)\n",
      "\n",
      "Round  18, Average loss 0.261 Test accuracy 94.990\n",
      "selected users: [ 0  1  3  4  8 10 13 16 19 21 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.3472 \n",
      "Accuracy: 9515/10000 (95.15%)\n",
      "\n",
      "Round  19, Average loss 0.347 Test accuracy 95.150\n",
      "selected users: [ 0  1  2 10 13 15 17 18 19 20 21 22 24 25 30]\n",
      "\n",
      "Test set: Average loss: 0.5424 \n",
      "Accuracy: 9422/10000 (94.22%)\n",
      "\n",
      "Round  20, Average loss 0.542 Test accuracy 94.220\n",
      "selected users: [ 1  2  4  5  8 11 12 13 14 16 20 21 23 24 25]\n",
      "\n",
      "Test set: Average loss: 0.7032 \n",
      "Accuracy: 9377/10000 (93.77%)\n",
      "\n",
      "Round  21, Average loss 0.703 Test accuracy 93.770\n",
      "selected users: [ 1  3 10 12 13 14 15 17 19 23 24 25 26 28 29]\n",
      "\n",
      "Test set: Average loss: 0.5790 \n",
      "Accuracy: 8509/10000 (85.09%)\n",
      "\n",
      "Round  22, Average loss 0.579 Test accuracy 85.090\n",
      "selected users: [ 0  2  4  5  7  9 10 15 16 18 19 20 21 27 28]\n",
      "\n",
      "Test set: Average loss: 1.5378 \n",
      "Accuracy: 9266/10000 (92.66%)\n",
      "\n",
      "Round  23, Average loss 1.538 Test accuracy 92.660\n",
      "selected users: [ 1  3  5  7  8  9 10 12 16 18 19 22 23 29 30]\n",
      "\n",
      "Test set: Average loss: 0.6172 \n",
      "Accuracy: 7990/10000 (79.90%)\n",
      "\n",
      "Round  24, Average loss 0.617 Test accuracy 79.900\n",
      "selected users: [ 3  6  7  8  9 10 14 17 18 20 21 22 26 28 30]\n",
      "\n",
      "Test set: Average loss: 0.4372 \n",
      "Accuracy: 9323/10000 (93.23%)\n",
      "\n",
      "Round  25, Average loss 0.437 Test accuracy 93.230\n",
      "selected users: [ 1  2  3  6  9 11 12 13 17 19 21 22 24 26 27]\n",
      "\n",
      "Test set: Average loss: 0.1643 \n",
      "Accuracy: 9539/10000 (95.39%)\n",
      "\n",
      "Round  26, Average loss 0.164 Test accuracy 95.390\n",
      "selected users: [ 0  2  3  4  7  9 11 15 18 19 23 24 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.2797 \n",
      "Accuracy: 9494/10000 (94.94%)\n",
      "\n",
      "Round  27, Average loss 0.280 Test accuracy 94.940\n",
      "selected users: [ 1  5  8  9 10 11 15 16 17 20 23 24 25 26 27]\n",
      "\n",
      "Test set: Average loss: 0.3421 \n",
      "Accuracy: 9505/10000 (95.05%)\n",
      "\n",
      "Round  28, Average loss 0.342 Test accuracy 95.050\n",
      "selected users: [ 1  2  4  6  7 10 11 12 17 19 22 24 26 28 29]\n",
      "\n",
      "Test set: Average loss: 0.2903 \n",
      "Accuracy: 9445/10000 (94.45%)\n",
      "\n",
      "Round  29, Average loss 0.290 Test accuracy 94.450\n",
      "(m= 15 )  7 -th Trial!!\n",
      "selected users: [ 1  2  3  4  6  7 11 12 14 17 20 24 25 27 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  1  2  3  7  9 11 12 14 15 17 19 21 26 29]\n",
      "\n",
      "Test set: Average loss: 2.2570 \n",
      "Accuracy: 5387/10000 (53.87%)\n",
      "\n",
      "Round   1, Average loss 2.257 Test accuracy 53.870\n",
      "selected users: [ 0  3  6  7 10 13 15 17 19 21 22 25 26 27 29]\n",
      "\n",
      "Test set: Average loss: 0.4403 \n",
      "Accuracy: 8933/10000 (89.33%)\n",
      "\n",
      "Round   2, Average loss 0.440 Test accuracy 89.330\n",
      "selected users: [ 2  3  4  9 10 11 15 16 17 18 20 21 24 25 28]\n",
      "\n",
      "Test set: Average loss: 0.2625 \n",
      "Accuracy: 9436/10000 (94.36%)\n",
      "\n",
      "Round   3, Average loss 0.262 Test accuracy 94.360\n",
      "selected users: [ 0  6  8 10 11 12 13 14 15 16 20 22 25 26 27]\n",
      "\n",
      "Test set: Average loss: 0.1932 \n",
      "Accuracy: 9480/10000 (94.80%)\n",
      "\n",
      "Round   4, Average loss 0.193 Test accuracy 94.800\n",
      "selected users: [ 1  2  6  7  8  9 10 11 13 17 20 21 22 27 30]\n",
      "\n",
      "Test set: Average loss: 0.1577 \n",
      "Accuracy: 9572/10000 (95.72%)\n",
      "\n",
      "Round   5, Average loss 0.158 Test accuracy 95.720\n",
      "selected users: [ 1  2  3  7  9 10 14 15 20 22 23 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.3241 \n",
      "Accuracy: 9417/10000 (94.17%)\n",
      "\n",
      "Round   6, Average loss 0.324 Test accuracy 94.170\n",
      "selected users: [ 0  1  4  8 10 11 14 15 17 19 20 22 24 25 29]\n",
      "\n",
      "Test set: Average loss: 0.1524 \n",
      "Accuracy: 9548/10000 (95.48%)\n",
      "\n",
      "Round   7, Average loss 0.152 Test accuracy 95.480\n",
      "selected users: [ 0  2  4  5  8  9 11 12 15 17 19 20 21 23 30]\n",
      "\n",
      "Test set: Average loss: 0.3081 \n",
      "Accuracy: 9438/10000 (94.38%)\n",
      "\n",
      "Round   8, Average loss 0.308 Test accuracy 94.380\n",
      "selected users: [ 0  2  4  6  8 10 11 13 17 20 23 25 26 27 30]\n",
      "\n",
      "Test set: Average loss: 0.2501 \n",
      "Accuracy: 9478/10000 (94.78%)\n",
      "\n",
      "Round   9, Average loss 0.250 Test accuracy 94.780\n",
      "selected users: [ 0  4  6  8  9 11 12 14 16 18 19 20 23 25 30]\n",
      "\n",
      "Test set: Average loss: 0.4558 \n",
      "Accuracy: 9449/10000 (94.49%)\n",
      "\n",
      "Round  10, Average loss 0.456 Test accuracy 94.490\n",
      "selected users: [ 1  2  3  4  5  7  8  9 16 18 24 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.1503 \n",
      "Accuracy: 9554/10000 (95.54%)\n",
      "\n",
      "Round  11, Average loss 0.150 Test accuracy 95.540\n",
      "selected users: [ 1  5  8  9 10 11 14 16 19 20 21 24 25 26 30]\n",
      "\n",
      "Test set: Average loss: 0.6687 \n",
      "Accuracy: 9411/10000 (94.11%)\n",
      "\n",
      "Round  12, Average loss 0.669 Test accuracy 94.110\n",
      "selected users: [ 0  4  6  8  9 10 12 18 20 21 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.7431 \n",
      "Accuracy: 9315/10000 (93.15%)\n",
      "\n",
      "Round  13, Average loss 0.743 Test accuracy 93.150\n",
      "selected users: [ 0  3  5  7  8 12 13 15 16 17 18 21 23 26 30]\n",
      "\n",
      "Test set: Average loss: 2.4194 \n",
      "Accuracy: 9181/10000 (91.81%)\n",
      "\n",
      "Round  14, Average loss 2.419 Test accuracy 91.810\n",
      "selected users: [ 0  1  2  3  5  7 10 11 12 15 18 24 25 27 30]\n",
      "\n",
      "Test set: Average loss: 0.3196 \n",
      "Accuracy: 9260/10000 (92.60%)\n",
      "\n",
      "Round  15, Average loss 0.320 Test accuracy 92.600\n",
      "selected users: [ 6  7  8  9 13 15 17 18 19 20 22 23 26 27 29]\n",
      "\n",
      "Test set: Average loss: 0.5320 \n",
      "Accuracy: 9324/10000 (93.24%)\n",
      "\n",
      "Round  16, Average loss 0.532 Test accuracy 93.240\n",
      "selected users: [ 0  1  2  3  5  7  8 10 16 18 19 22 23 25 28]\n",
      "\n",
      "Test set: Average loss: 0.7665 \n",
      "Accuracy: 9245/10000 (92.45%)\n",
      "\n",
      "Round  17, Average loss 0.767 Test accuracy 92.450\n",
      "selected users: [ 0  6 10 13 14 17 18 20 21 23 24 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.6261 \n",
      "Accuracy: 9250/10000 (92.50%)\n",
      "\n",
      "Round  18, Average loss 1.626 Test accuracy 92.500\n",
      "selected users: [ 0  1  3  7  9 10 13 18 23 24 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.3203 \n",
      "Accuracy: 9214/10000 (92.14%)\n",
      "\n",
      "Round  19, Average loss 0.320 Test accuracy 92.140\n",
      "selected users: [ 4  8 12 13 14 18 19 20 21 22 23 25 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.2374 \n",
      "Accuracy: 9480/10000 (94.80%)\n",
      "\n",
      "Round  20, Average loss 0.237 Test accuracy 94.800\n",
      "selected users: [ 4  6  7  8 10 12 17 18 20 22 23 24 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.2966 \n",
      "Accuracy: 9488/10000 (94.88%)\n",
      "\n",
      "Round  21, Average loss 0.297 Test accuracy 94.880\n",
      "selected users: [ 0  1  4  5  6  8 10 12 13 19 21 23 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.2126 \n",
      "Accuracy: 9514/10000 (95.14%)\n",
      "\n",
      "Round  22, Average loss 0.213 Test accuracy 95.140\n",
      "selected users: [ 1  3  4  6  7  8 12 14 16 20 22 24 26 28 29]\n",
      "\n",
      "Test set: Average loss: 0.5087 \n",
      "Accuracy: 9357/10000 (93.57%)\n",
      "\n",
      "Round  23, Average loss 0.509 Test accuracy 93.570\n",
      "selected users: [ 0  1  2  3  8  9 11 12 13 15 16 21 25 27 28]\n",
      "\n",
      "Test set: Average loss: 0.5336 \n",
      "Accuracy: 9130/10000 (91.30%)\n",
      "\n",
      "Round  24, Average loss 0.534 Test accuracy 91.300\n",
      "selected users: [ 0  1  4  6  7 13 14 15 19 20 21 23 25 28 30]\n",
      "\n",
      "Test set: Average loss: 0.3439 \n",
      "Accuracy: 9348/10000 (93.48%)\n",
      "\n",
      "Round  25, Average loss 0.344 Test accuracy 93.480\n",
      "selected users: [ 0  1  5  8  9 10 11 12 15 18 20 21 22 26 27]\n",
      "\n",
      "Test set: Average loss: 0.6641 \n",
      "Accuracy: 9358/10000 (93.58%)\n",
      "\n",
      "Round  26, Average loss 0.664 Test accuracy 93.580\n",
      "selected users: [ 5  7  8  9 10 11 14 16 18 19 20 21 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.5714 \n",
      "Accuracy: 9201/10000 (92.01%)\n",
      "\n",
      "Round  27, Average loss 1.571 Test accuracy 92.010\n",
      "selected users: [ 1  4  5  6  8 10 11 15 16 17 19 21 24 25 26]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.5214 \n",
      "Accuracy: 9209/10000 (92.09%)\n",
      "\n",
      "Round  28, Average loss 1.521 Test accuracy 92.090\n",
      "selected users: [ 3  5  6  7  9 12 13 17 18 19 20 22 24 25 28]\n",
      "\n",
      "Test set: Average loss: 1.1110 \n",
      "Accuracy: 9208/10000 (92.08%)\n",
      "\n",
      "Round  29, Average loss 1.111 Test accuracy 92.080\n",
      "(m= 15 )  8 -th Trial!!\n",
      "selected users: [ 2  3  4  6  7  9 12 13 18 20 21 23 24 25 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  1  4  7  8 11 12 14 15 16 17 21 24 25 27]\n",
      "\n",
      "Test set: Average loss: 2.1563 \n",
      "Accuracy: 6966/10000 (69.66%)\n",
      "\n",
      "Round   1, Average loss 2.156 Test accuracy 69.660\n",
      "selected users: [ 1  2  5  6 10 11 13 14 19 21 23 24 25 28 29]\n",
      "\n",
      "Test set: Average loss: 0.3756 \n",
      "Accuracy: 9276/10000 (92.76%)\n",
      "\n",
      "Round   2, Average loss 0.376 Test accuracy 92.760\n",
      "selected users: [ 0  4  5  7  9 10 12 13 17 18 20 21 22 25 29]\n",
      "\n",
      "Test set: Average loss: 0.1934 \n",
      "Accuracy: 9410/10000 (94.10%)\n",
      "\n",
      "Round   3, Average loss 0.193 Test accuracy 94.100\n",
      "selected users: [ 2  3  4  6  9 11 12 15 18 23 24 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.1853 \n",
      "Accuracy: 9480/10000 (94.80%)\n",
      "\n",
      "Round   4, Average loss 0.185 Test accuracy 94.800\n",
      "selected users: [ 0  6  8  9 11 12 13 15 17 20 21 25 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.3378 \n",
      "Accuracy: 9419/10000 (94.19%)\n",
      "\n",
      "Round   5, Average loss 0.338 Test accuracy 94.190\n",
      "selected users: [ 1  2  4  5  7  8  9 11 12 14 16 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.3506 \n",
      "Accuracy: 9468/10000 (94.68%)\n",
      "\n",
      "Round   6, Average loss 0.351 Test accuracy 94.680\n",
      "selected users: [ 1  2  3  4  6  9 11 12 13 14 19 20 22 24 30]\n",
      "\n",
      "Test set: Average loss: 0.1323 \n",
      "Accuracy: 9610/10000 (96.10%)\n",
      "\n",
      "Round   7, Average loss 0.132 Test accuracy 96.100\n",
      "selected users: [ 0  5  6  7 13 14 16 17 19 20 21 23 24 25 28]\n",
      "\n",
      "Test set: Average loss: 0.4457 \n",
      "Accuracy: 9344/10000 (93.44%)\n",
      "\n",
      "Round   8, Average loss 0.446 Test accuracy 93.440\n",
      "selected users: [ 0  2  3  4 11 12 13 15 16 18 20 21 23 26 30]\n",
      "\n",
      "Test set: Average loss: 0.5581 \n",
      "Accuracy: 9417/10000 (94.17%)\n",
      "\n",
      "Round   9, Average loss 0.558 Test accuracy 94.170\n",
      "selected users: [ 2  4  5  6 10 11 16 17 18 20 23 24 26 27 30]\n",
      "\n",
      "Test set: Average loss: 0.7629 \n",
      "Accuracy: 9410/10000 (94.10%)\n",
      "\n",
      "Round  10, Average loss 0.763 Test accuracy 94.100\n",
      "selected users: [ 0  3  5  7  9 12 13 14 22 23 24 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2026 \n",
      "Accuracy: 9522/10000 (95.22%)\n",
      "\n",
      "Round  11, Average loss 0.203 Test accuracy 95.220\n",
      "selected users: [ 0  2  4  5  7 11 14 16 17 19 22 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.3059 \n",
      "Accuracy: 9489/10000 (94.89%)\n",
      "\n",
      "Round  12, Average loss 0.306 Test accuracy 94.890\n",
      "selected users: [ 0  1  3  4  6  8 10 15 19 20 21 23 26 28 30]\n",
      "\n",
      "Test set: Average loss: 0.6869 \n",
      "Accuracy: 9362/10000 (93.62%)\n",
      "\n",
      "Round  13, Average loss 0.687 Test accuracy 93.620\n",
      "selected users: [ 2  4  5  6  7  8 12 13 15 16 20 23 24 28 29]\n",
      "\n",
      "Test set: Average loss: 0.2805 \n",
      "Accuracy: 9374/10000 (93.74%)\n",
      "\n",
      "Round  14, Average loss 0.281 Test accuracy 93.740\n",
      "selected users: [ 0  3  6  8 11 12 14 15 16 18 22 23 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2124 \n",
      "Accuracy: 9497/10000 (94.97%)\n",
      "\n",
      "Round  15, Average loss 0.212 Test accuracy 94.970\n",
      "selected users: [ 2  3  4  6  7 11 12 18 20 21 22 25 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.1830 \n",
      "Accuracy: 9520/10000 (95.20%)\n",
      "\n",
      "Round  16, Average loss 0.183 Test accuracy 95.200\n",
      "selected users: [ 0  3  6  7  8  9 11 12 14 18 20 21 26 28 30]\n",
      "\n",
      "Test set: Average loss: 0.8763 \n",
      "Accuracy: 9332/10000 (93.32%)\n",
      "\n",
      "Round  17, Average loss 0.876 Test accuracy 93.320\n",
      "selected users: [ 0  3  5  7  8  9 10 11 12 16 17 21 26 29 30]\n",
      "\n",
      "Test set: Average loss: 2.1871 \n",
      "Accuracy: 9244/10000 (92.44%)\n",
      "\n",
      "Round  18, Average loss 2.187 Test accuracy 92.440\n",
      "selected users: [ 0  2  5  7 10 12 13 15 20 21 22 25 26 27 30]\n",
      "\n",
      "Test set: Average loss: 0.9653 \n",
      "Accuracy: 9258/10000 (92.58%)\n",
      "\n",
      "Round  19, Average loss 0.965 Test accuracy 92.580\n",
      "selected users: [ 1  2  3  5  7  9 10 13 19 21 23 24 25 27 30]\n",
      "\n",
      "Test set: Average loss: 0.6398 \n",
      "Accuracy: 9371/10000 (93.71%)\n",
      "\n",
      "Round  20, Average loss 0.640 Test accuracy 93.710\n",
      "selected users: [ 1  2  4  5  6  7  9 12 15 18 21 22 25 26 28]\n",
      "\n",
      "Test set: Average loss: 0.2778 \n",
      "Accuracy: 9394/10000 (93.94%)\n",
      "\n",
      "Round  21, Average loss 0.278 Test accuracy 93.940\n",
      "selected users: [ 4  7  8  9 10 12 16 17 19 20 21 22 24 27 29]\n",
      "\n",
      "Test set: Average loss: 0.2700 \n",
      "Accuracy: 9510/10000 (95.10%)\n",
      "\n",
      "Round  22, Average loss 0.270 Test accuracy 95.100\n",
      "selected users: [ 1  2  3  4  6  9 12 14 15 21 22 23 24 26 29]\n",
      "\n",
      "Test set: Average loss: 1.5900 \n",
      "Accuracy: 9277/10000 (92.77%)\n",
      "\n",
      "Round  23, Average loss 1.590 Test accuracy 92.770\n",
      "selected users: [ 3  4  5  6  7  8  9 10 13 17 19 21 22 23 29]\n",
      "\n",
      "Test set: Average loss: 0.3315 \n",
      "Accuracy: 9248/10000 (92.48%)\n",
      "\n",
      "Round  24, Average loss 0.331 Test accuracy 92.480\n",
      "selected users: [ 2  5  6  7  8  9 10 12 15 17 21 22 25 26 27]\n",
      "\n",
      "Test set: Average loss: 0.2191 \n",
      "Accuracy: 9538/10000 (95.38%)\n",
      "\n",
      "Round  25, Average loss 0.219 Test accuracy 95.380\n",
      "selected users: [ 1  4  5  6  8 11 14 15 20 21 22 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.3345 \n",
      "Accuracy: 9508/10000 (95.08%)\n",
      "\n",
      "Round  26, Average loss 0.335 Test accuracy 95.080\n",
      "selected users: [ 1  4  9 11 12 15 16 17 18 19 21 22 24 27 28]\n",
      "\n",
      "Test set: Average loss: 0.1378 \n",
      "Accuracy: 9594/10000 (95.94%)\n",
      "\n",
      "Round  27, Average loss 0.138 Test accuracy 95.940\n",
      "selected users: [ 3  5  8 16 18 20 21 23 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.1815 \n",
      "Accuracy: 9480/10000 (94.80%)\n",
      "\n",
      "Round  28, Average loss 0.182 Test accuracy 94.800\n",
      "selected users: [ 0  1  2  6  8  9 12 14 17 18 22 23 25 26 30]\n",
      "\n",
      "Test set: Average loss: 0.1480 \n",
      "Accuracy: 9612/10000 (96.12%)\n",
      "\n",
      "Round  29, Average loss 0.148 Test accuracy 96.120\n",
      "(m= 15 )  9 -th Trial!!\n",
      "selected users: [ 0  2  5  8 10 11 16 18 20 21 22 23 24 27 28]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 2  5  6 10 13 15 21 22 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.2683 \n",
      "Accuracy: 5233/10000 (52.33%)\n",
      "\n",
      "Round   1, Average loss 2.268 Test accuracy 52.330\n",
      "selected users: [ 0  4  5  6  8 13 16 18 19 21 22 25 26 28 30]\n",
      "\n",
      "Test set: Average loss: 0.3617 \n",
      "Accuracy: 9055/10000 (90.55%)\n",
      "\n",
      "Round   2, Average loss 0.362 Test accuracy 90.550\n",
      "selected users: [ 0  1  2 10 11 12 14 15 17 21 23 25 26 27 29]\n",
      "\n",
      "Test set: Average loss: 0.2897 \n",
      "Accuracy: 9231/10000 (92.31%)\n",
      "\n",
      "Round   3, Average loss 0.290 Test accuracy 92.310\n",
      "selected users: [ 0  4  5  8 14 16 17 18 19 21 22 23 24 25 29]\n",
      "\n",
      "Test set: Average loss: 0.2324 \n",
      "Accuracy: 9484/10000 (94.84%)\n",
      "\n",
      "Round   4, Average loss 0.232 Test accuracy 94.840\n",
      "selected users: [ 0  2  4  5  6  7 12 15 16 20 22 24 25 26 30]\n",
      "\n",
      "Test set: Average loss: 0.2377 \n",
      "Accuracy: 9374/10000 (93.74%)\n",
      "\n",
      "Round   5, Average loss 0.238 Test accuracy 93.740\n",
      "selected users: [ 0  4  6  7 10 12 14 15 17 22 23 25 26 27 28]\n",
      "\n",
      "Test set: Average loss: 0.1859 \n",
      "Accuracy: 9479/10000 (94.79%)\n",
      "\n",
      "Round   6, Average loss 0.186 Test accuracy 94.790\n",
      "selected users: [ 2  3  6  7 11 13 15 16 17 18 22 24 26 29 30]\n",
      "\n",
      "Test set: Average loss: 0.3123 \n",
      "Accuracy: 9366/10000 (93.66%)\n",
      "\n",
      "Round   7, Average loss 0.312 Test accuracy 93.660\n",
      "selected users: [ 0  1  7  8 11 13 14 16 18 23 24 25 26 27 28]\n",
      "\n",
      "Test set: Average loss: 2.0299 \n",
      "Accuracy: 6316/10000 (63.16%)\n",
      "\n",
      "Round   8, Average loss 2.030 Test accuracy 63.160\n",
      "selected users: [ 0  1  2  3  4  6  7 12 14 16 19 21 22 23 30]\n",
      "\n",
      "Test set: Average loss: 0.1690 \n",
      "Accuracy: 9472/10000 (94.72%)\n",
      "\n",
      "Round   9, Average loss 0.169 Test accuracy 94.720\n",
      "selected users: [ 2  3  4  6  7 12 13 15 17 18 20 21 24 25 26]\n",
      "\n",
      "Test set: Average loss: 0.5100 \n",
      "Accuracy: 9320/10000 (93.20%)\n",
      "\n",
      "Round  10, Average loss 0.510 Test accuracy 93.200\n",
      "selected users: [ 0  1  6  9 10 14 15 16 17 18 19 20 24 25 28]\n",
      "\n",
      "Test set: Average loss: 0.1728 \n",
      "Accuracy: 9488/10000 (94.88%)\n",
      "\n",
      "Round  11, Average loss 0.173 Test accuracy 94.880\n",
      "selected users: [ 0  1  2  4  5  8 12 16 18 19 23 24 26 27 30]\n",
      "\n",
      "Test set: Average loss: 0.2591 \n",
      "Accuracy: 9473/10000 (94.73%)\n",
      "\n",
      "Round  12, Average loss 0.259 Test accuracy 94.730\n",
      "selected users: [ 0  1  3  5  8 10 11 14 17 18 19 22 23 25 26]\n",
      "\n",
      "Test set: Average loss: 0.9775 \n",
      "Accuracy: 9159/10000 (91.59%)\n",
      "\n",
      "Round  13, Average loss 0.978 Test accuracy 91.590\n",
      "selected users: [ 0  2  5  6  8  9 10 12 18 21 24 25 27 28 29]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1845 \n",
      "Accuracy: 9491/10000 (94.91%)\n",
      "\n",
      "Round  14, Average loss 0.184 Test accuracy 94.910\n",
      "selected users: [ 2  5  6  7 10 11 13 15 17 18 19 21 24 27 28]\n",
      "\n",
      "Test set: Average loss: 0.3702 \n",
      "Accuracy: 9279/10000 (92.79%)\n",
      "\n",
      "Round  15, Average loss 0.370 Test accuracy 92.790\n",
      "selected users: [ 2  3  4  5  6  8  9 13 15 18 20 21 23 28 29]\n",
      "\n",
      "Test set: Average loss: 0.1619 \n",
      "Accuracy: 9523/10000 (95.23%)\n",
      "\n",
      "Round  16, Average loss 0.162 Test accuracy 95.230\n",
      "selected users: [ 3  4  5  7  8  9 11 12 16 18 21 24 25 26 30]\n",
      "\n",
      "Test set: Average loss: 0.5926 \n",
      "Accuracy: 9354/10000 (93.54%)\n",
      "\n",
      "Round  17, Average loss 0.593 Test accuracy 93.540\n",
      "selected users: [ 0  1  2  5  6  7  8 10 13 21 22 23 25 27 30]\n",
      "\n",
      "Test set: Average loss: 0.5908 \n",
      "Accuracy: 9325/10000 (93.25%)\n",
      "\n",
      "Round  18, Average loss 0.591 Test accuracy 93.250\n",
      "selected users: [ 1  4  5 10 11 13 14 20 21 22 23 25 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.9994 \n",
      "Accuracy: 9370/10000 (93.70%)\n",
      "\n",
      "Round  19, Average loss 0.999 Test accuracy 93.700\n",
      "selected users: [ 1  3  4  5  7 11 12 14 15 18 21 23 24 28 29]\n",
      "\n",
      "Test set: Average loss: 0.2547 \n",
      "Accuracy: 9448/10000 (94.48%)\n",
      "\n",
      "Round  20, Average loss 0.255 Test accuracy 94.480\n",
      "selected users: [ 4  6  7  9 10 11 12 13 14 17 18 22 24 27 28]\n",
      "\n",
      "Test set: Average loss: 0.1589 \n",
      "Accuracy: 9524/10000 (95.24%)\n",
      "\n",
      "Round  21, Average loss 0.159 Test accuracy 95.240\n",
      "selected users: [ 1  3  4  5  7 11 12 15 16 17 19 20 21 24 27]\n",
      "\n",
      "Test set: Average loss: 0.2212 \n",
      "Accuracy: 9559/10000 (95.59%)\n",
      "\n",
      "Round  22, Average loss 0.221 Test accuracy 95.590\n",
      "selected users: [ 1  2  5  6  7  8 10 12 13 16 17 18 19 21 30]\n",
      "\n",
      "Test set: Average loss: 1.1285 \n",
      "Accuracy: 9283/10000 (92.83%)\n",
      "\n",
      "Round  23, Average loss 1.129 Test accuracy 92.830\n",
      "selected users: [ 1  5  7  9 10 11 13 16 18 21 22 23 24 26 29]\n",
      "\n",
      "Test set: Average loss: 0.9185 \n",
      "Accuracy: 9269/10000 (92.69%)\n",
      "\n",
      "Round  24, Average loss 0.919 Test accuracy 92.690\n",
      "selected users: [ 0  1  2  3  4  6  9 13 20 22 23 25 26 27 28]\n",
      "\n",
      "Test set: Average loss: 0.3914 \n",
      "Accuracy: 9372/10000 (93.72%)\n",
      "\n",
      "Round  25, Average loss 0.391 Test accuracy 93.720\n",
      "selected users: [ 2  3  5  6  8 11 13 15 17 18 19 23 24 26 30]\n",
      "\n",
      "Test set: Average loss: 1.0451 \n",
      "Accuracy: 9197/10000 (91.97%)\n",
      "\n",
      "Round  26, Average loss 1.045 Test accuracy 91.970\n",
      "selected users: [ 0  2  3  5  6  7  8 11 13 14 15 18 19 21 28]\n",
      "\n",
      "Test set: Average loss: 1.2054 \n",
      "Accuracy: 8984/10000 (89.84%)\n",
      "\n",
      "Round  27, Average loss 1.205 Test accuracy 89.840\n",
      "selected users: [ 1  4  5  6  7 12 14 17 18 20 22 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.6494 \n",
      "Accuracy: 9210/10000 (92.10%)\n",
      "\n",
      "Round  28, Average loss 0.649 Test accuracy 92.100\n",
      "selected users: [ 2  6  8  9 13 16 18 20 21 23 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.6000 \n",
      "Accuracy: 9349/10000 (93.49%)\n",
      "\n",
      "Round  29, Average loss 0.600 Test accuracy 93.490\n",
      "number of results: 18\n",
      "(m= 18 )  0 -th Trial!!\n",
      "selected users: [ 0  1  4  5  8 10 13 14 15 16 19 22 23 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  3  5  6  8  9 11 15 17 19 20 21 22 24 25 26 28 30]\n",
      "\n",
      "Test set: Average loss: 2.2994 \n",
      "Accuracy: 3256/10000 (32.56%)\n",
      "\n",
      "Round   1, Average loss 2.299 Test accuracy 32.560\n",
      "selected users: [ 0  3  4  5  6  7  8 10 14 15 16 18 22 24 25 26 27 30]\n",
      "\n",
      "Test set: Average loss: 1.2188 \n",
      "Accuracy: 8809/10000 (88.09%)\n",
      "\n",
      "Round   2, Average loss 1.219 Test accuracy 88.090\n",
      "selected users: [ 0  1  2  3  7  8  9 10 13 14 16 17 18 19 21 22 24 29]\n",
      "\n",
      "Test set: Average loss: 0.8958 \n",
      "Accuracy: 9250/10000 (92.50%)\n",
      "\n",
      "Round   3, Average loss 0.896 Test accuracy 92.500\n",
      "selected users: [ 0  1  2  5  7  9 10 13 14 16 19 21 22 23 25 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.2359 \n",
      "Accuracy: 9333/10000 (93.33%)\n",
      "\n",
      "Round   4, Average loss 0.236 Test accuracy 93.330\n",
      "selected users: [ 0  1  2  3  4  7  8  9 10 11 15 16 17 18 21 22 24 29]\n",
      "\n",
      "Test set: Average loss: 0.3898 \n",
      "Accuracy: 9438/10000 (94.38%)\n",
      "\n",
      "Round   5, Average loss 0.390 Test accuracy 94.380\n",
      "selected users: [ 0  1  2  4  5 11 12 13 14 15 17 18 19 23 25 26 28 29]\n",
      "\n",
      "Test set: Average loss: 0.2682 \n",
      "Accuracy: 9298/10000 (92.98%)\n",
      "\n",
      "Round   6, Average loss 0.268 Test accuracy 92.980\n",
      "selected users: [ 0  1  4  5  6  8  9 11 12 14 15 17 19 24 25 26 28 30]\n",
      "\n",
      "Test set: Average loss: 0.3239 \n",
      "Accuracy: 9495/10000 (94.95%)\n",
      "\n",
      "Round   7, Average loss 0.324 Test accuracy 94.950\n",
      "selected users: [ 0  1  2  4  5  6  7  8  9 10 11 12 15 16 21 22 23 28]\n",
      "\n",
      "Test set: Average loss: 0.2294 \n",
      "Accuracy: 9388/10000 (93.88%)\n",
      "\n",
      "Round   8, Average loss 0.229 Test accuracy 93.880\n",
      "selected users: [ 0  1  3  5  6  7  8  9 10 11 13 15 16 22 23 24 25 27]\n",
      "\n",
      "Test set: Average loss: 0.2217 \n",
      "Accuracy: 9534/10000 (95.34%)\n",
      "\n",
      "Round   9, Average loss 0.222 Test accuracy 95.340\n",
      "selected users: [ 0  2  3  5  6 12 13 14 16 17 18 19 21 22 25 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.1926 \n",
      "Accuracy: 9598/10000 (95.98%)\n",
      "\n",
      "Round  10, Average loss 0.193 Test accuracy 95.980\n",
      "selected users: [ 0  4  5  6  7  9 10 11 12 13 15 17 20 21 23 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.1938 \n",
      "Accuracy: 9583/10000 (95.83%)\n",
      "\n",
      "Round  11, Average loss 0.194 Test accuracy 95.830\n",
      "selected users: [ 0  1  2  3  4  6  7  8 10 12 15 17 20 22 24 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.2031 \n",
      "Accuracy: 9554/10000 (95.54%)\n",
      "\n",
      "Round  12, Average loss 0.203 Test accuracy 95.540\n",
      "selected users: [ 1  3  4  6  7 12 14 17 18 19 21 22 23 24 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.1983 \n",
      "Accuracy: 9583/10000 (95.83%)\n",
      "\n",
      "Round  13, Average loss 0.198 Test accuracy 95.830\n",
      "selected users: [ 0  3  4  5  6  7  8 10 11 12 14 15 18 19 23 26 29 30]\n",
      "\n",
      "Test set: Average loss: 0.1846 \n",
      "Accuracy: 9437/10000 (94.37%)\n",
      "\n",
      "Round  14, Average loss 0.185 Test accuracy 94.370\n",
      "selected users: [ 0  1  2  5  7  8 10 11 13 14 20 21 22 24 25 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.1714 \n",
      "Accuracy: 9484/10000 (94.84%)\n",
      "\n",
      "Round  15, Average loss 0.171 Test accuracy 94.840\n",
      "selected users: [ 0  1  3  4  5  7  8  9 11 14 15 17 22 24 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.2725 \n",
      "Accuracy: 9620/10000 (96.20%)\n",
      "\n",
      "Round  16, Average loss 0.272 Test accuracy 96.200\n",
      "selected users: [ 0  1  3  5  6  9 10 11 12 13 15 16 21 22 23 24 25 29]\n",
      "\n",
      "Test set: Average loss: 0.1777 \n",
      "Accuracy: 9471/10000 (94.71%)\n",
      "\n",
      "Round  17, Average loss 0.178 Test accuracy 94.710\n",
      "selected users: [ 1  2  5  6  7  8 12 13 15 16 17 18 19 20 22 25 29 30]\n",
      "\n",
      "Test set: Average loss: 0.3190 \n",
      "Accuracy: 9557/10000 (95.57%)\n",
      "\n",
      "Round  18, Average loss 0.319 Test accuracy 95.570\n",
      "selected users: [ 3  6  7 10 11 12 16 19 20 21 23 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2012 \n",
      "Accuracy: 9517/10000 (95.17%)\n",
      "\n",
      "Round  19, Average loss 0.201 Test accuracy 95.170\n",
      "selected users: [ 0  1  3  7  8  9 10 11 12 14 17 18 21 22 25 26 27 29]\n",
      "\n",
      "Test set: Average loss: 0.4865 \n",
      "Accuracy: 9613/10000 (96.13%)\n",
      "\n",
      "Round  20, Average loss 0.487 Test accuracy 96.130\n",
      "selected users: [ 5  6  7  8 10 11 12 14 15 16 17 18 20 22 24 26 28 30]\n",
      "\n",
      "Test set: Average loss: 0.3304 \n",
      "Accuracy: 9322/10000 (93.22%)\n",
      "\n",
      "Round  21, Average loss 0.330 Test accuracy 93.220\n",
      "selected users: [ 0  2  3  5  7  8  9 10 11 15 19 20 22 23 24 25 27 30]\n",
      "\n",
      "Test set: Average loss: 0.1919 \n",
      "Accuracy: 9460/10000 (94.60%)\n",
      "\n",
      "Round  22, Average loss 0.192 Test accuracy 94.600\n",
      "selected users: [ 0  2  3  4  5  8 12 14 15 16 17 18 19 21 22 23 24 28]\n",
      "\n",
      "Test set: Average loss: 0.3055 \n",
      "Accuracy: 9509/10000 (95.09%)\n",
      "\n",
      "Round  23, Average loss 0.306 Test accuracy 95.090\n",
      "selected users: [ 2  4  5  6  8 10 12 13 14 15 19 21 22 23 24 26 28 29]\n",
      "\n",
      "Test set: Average loss: 0.1906 \n",
      "Accuracy: 9466/10000 (94.66%)\n",
      "\n",
      "Round  24, Average loss 0.191 Test accuracy 94.660\n",
      "selected users: [ 3  4  5  6  9 10 11 12 13 14 16 17 18 19 22 26 27 28]\n",
      "\n",
      "Test set: Average loss: 0.2223 \n",
      "Accuracy: 9540/10000 (95.40%)\n",
      "\n",
      "Round  25, Average loss 0.222 Test accuracy 95.400\n",
      "selected users: [ 1  3  4  7  9 10 13 14 15 17 18 20 21 22 24 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.1853 \n",
      "Accuracy: 9563/10000 (95.63%)\n",
      "\n",
      "Round  26, Average loss 0.185 Test accuracy 95.630\n",
      "selected users: [ 0  2  5  7  8 11 12 13 15 16 19 20 22 23 24 26 27 28]\n",
      "\n",
      "Test set: Average loss: 0.2151 \n",
      "Accuracy: 9397/10000 (93.97%)\n",
      "\n",
      "Round  27, Average loss 0.215 Test accuracy 93.970\n",
      "selected users: [ 0  2  5  6  9 10 11 12 13 15 17 18 19 23 25 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.1780 \n",
      "Accuracy: 9546/10000 (95.46%)\n",
      "\n",
      "Round  28, Average loss 0.178 Test accuracy 95.460\n",
      "selected users: [ 4  6  8  9 11 12 13 16 17 18 19 20 21 23 24 26 28 29]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.2078 \n",
      "Accuracy: 9413/10000 (94.13%)\n",
      "\n",
      "Round  29, Average loss 0.208 Test accuracy 94.130\n",
      "(m= 18 )  1 -th Trial!!\n",
      "selected users: [ 0  1  2  4  5  8  9 10 13 14 15 16 17 19 24 25 28 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 2  5  7  9 11 13 14 15 16 17 18 20 23 24 25 26 29 30]\n",
      "\n",
      "Test set: Average loss: 2.2937 \n",
      "Accuracy: 4245/10000 (42.45%)\n",
      "\n",
      "Round   1, Average loss 2.294 Test accuracy 42.450\n",
      "selected users: [ 0  2  3  5  6  8 11 12 14 15 17 19 20 21 25 26 27 29]\n",
      "\n",
      "Test set: Average loss: 1.0971 \n",
      "Accuracy: 8967/10000 (89.67%)\n",
      "\n",
      "Round   2, Average loss 1.097 Test accuracy 89.670\n",
      "selected users: [ 4  7  8  9 11 12 15 16 17 18 19 20 23 24 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4038 \n",
      "Accuracy: 9277/10000 (92.77%)\n",
      "\n",
      "Round   3, Average loss 0.404 Test accuracy 92.770\n",
      "selected users: [ 0  1  7  8 10 11 13 14 15 18 22 23 24 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.1422 \n",
      "Accuracy: 7893/10000 (78.93%)\n",
      "\n",
      "Round   4, Average loss 2.142 Test accuracy 78.930\n",
      "selected users: [ 1  3  5  6  7 10 13 15 16 17 19 21 22 24 25 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.3231 \n",
      "Accuracy: 9369/10000 (93.69%)\n",
      "\n",
      "Round   5, Average loss 0.323 Test accuracy 93.690\n",
      "selected users: [ 0  1  2  3  4  7 11 12 14 15 18 20 21 24 25 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.3848 \n",
      "Accuracy: 9425/10000 (94.25%)\n",
      "\n",
      "Round   6, Average loss 0.385 Test accuracy 94.250\n",
      "selected users: [ 1  2  5  7  8  9 13 15 16 18 19 20 22 23 24 25 28 29]\n",
      "\n",
      "Test set: Average loss: 0.2625 \n",
      "Accuracy: 9307/10000 (93.07%)\n",
      "\n",
      "Round   7, Average loss 0.262 Test accuracy 93.070\n",
      "selected users: [ 1  4  5  7  8 12 13 14 15 16 17 19 20 21 22 24 26 29]\n",
      "\n",
      "Test set: Average loss: 0.2141 \n",
      "Accuracy: 9380/10000 (93.80%)\n",
      "\n",
      "Round   8, Average loss 0.214 Test accuracy 93.800\n",
      "selected users: [ 0  1  2  3  4  8  9 12 13 16 17 19 20 22 24 25 26 30]\n",
      "\n",
      "Test set: Average loss: 0.2006 \n",
      "Accuracy: 9492/10000 (94.92%)\n",
      "\n",
      "Round   9, Average loss 0.201 Test accuracy 94.920\n",
      "selected users: [ 1  3  4  7  9 10 11 12 14 15 18 19 20 22 24 25 28 30]\n",
      "\n",
      "Test set: Average loss: 0.2998 \n",
      "Accuracy: 9488/10000 (94.88%)\n",
      "\n",
      "Round  10, Average loss 0.300 Test accuracy 94.880\n",
      "selected users: [ 0  1  4  7  8  9 10 12 14 15 16 17 18 22 23 26 27 28]\n",
      "\n",
      "Test set: Average loss: 0.2296 \n",
      "Accuracy: 9431/10000 (94.31%)\n",
      "\n",
      "Round  11, Average loss 0.230 Test accuracy 94.310\n",
      "selected users: [ 0  3  4  5  6  9 10 15 16 17 18 22 23 24 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.1722 \n",
      "Accuracy: 9519/10000 (95.19%)\n",
      "\n",
      "Round  12, Average loss 0.172 Test accuracy 95.190\n",
      "selected users: [ 4  5  6  9 10 11 12 13 14 16 17 18 19 23 24 26 27 29]\n",
      "\n",
      "Test set: Average loss: 0.6256 \n",
      "Accuracy: 9583/10000 (95.83%)\n",
      "\n",
      "Round  13, Average loss 0.626 Test accuracy 95.830\n",
      "selected users: [ 2  4  5  6  9 10 11 12 14 15 18 20 22 24 25 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.3399 \n",
      "Accuracy: 9574/10000 (95.74%)\n",
      "\n",
      "Round  14, Average loss 0.340 Test accuracy 95.740\n",
      "selected users: [ 0  1  2  3  6  8  9 12 13 15 17 18 20 22 23 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.3926 \n",
      "Accuracy: 9563/10000 (95.63%)\n",
      "\n",
      "Round  15, Average loss 0.393 Test accuracy 95.630\n",
      "selected users: [ 0  1  2  4  6  7 11 14 15 16 17 18 19 24 25 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2759 \n",
      "Accuracy: 9586/10000 (95.86%)\n",
      "\n",
      "Round  16, Average loss 0.276 Test accuracy 95.860\n",
      "selected users: [ 0  1  2  3  4  5  6  8  9 10 11 12 13 14 17 18 20 30]\n",
      "\n",
      "Test set: Average loss: 0.2683 \n",
      "Accuracy: 9628/10000 (96.28%)\n",
      "\n",
      "Round  17, Average loss 0.268 Test accuracy 96.280\n",
      "selected users: [ 1  3  4  6  7 11 12 13 14 16 17 19 21 23 24 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.1733 \n",
      "Accuracy: 9590/10000 (95.90%)\n",
      "\n",
      "Round  18, Average loss 0.173 Test accuracy 95.900\n",
      "selected users: [ 0  3  5  6  8 11 13 17 18 20 22 23 24 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2104 \n",
      "Accuracy: 9540/10000 (95.40%)\n",
      "\n",
      "Round  19, Average loss 0.210 Test accuracy 95.400\n",
      "selected users: [ 0  2  3  4  8  9 10 12 14 15 16 20 21 22 24 26 29 30]\n",
      "\n",
      "Test set: Average loss: 0.1775 \n",
      "Accuracy: 9462/10000 (94.62%)\n",
      "\n",
      "Round  20, Average loss 0.178 Test accuracy 94.620\n",
      "selected users: [ 2  3  4  7  9 10 11 12 13 14 15 16 18 19 21 23 25 29]\n",
      "\n",
      "Test set: Average loss: 0.2146 \n",
      "Accuracy: 9549/10000 (95.49%)\n",
      "\n",
      "Round  21, Average loss 0.215 Test accuracy 95.490\n",
      "selected users: [ 0  1  2  3  6  7  8 11 14 16 17 20 24 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.3978 \n",
      "Accuracy: 9543/10000 (95.43%)\n",
      "\n",
      "Round  22, Average loss 0.398 Test accuracy 95.430\n",
      "selected users: [ 0  3  4  6  7  9 10 11 12 13 15 16 17 18 21 22 25 30]\n",
      "\n",
      "Test set: Average loss: 0.1708 \n",
      "Accuracy: 9494/10000 (94.94%)\n",
      "\n",
      "Round  23, Average loss 0.171 Test accuracy 94.940\n",
      "selected users: [ 1  7  8  9 10 11 12 13 14 15 16 18 19 22 23 24 29 30]\n",
      "\n",
      "Test set: Average loss: 0.6662 \n",
      "Accuracy: 9630/10000 (96.30%)\n",
      "\n",
      "Round  24, Average loss 0.666 Test accuracy 96.300\n",
      "selected users: [ 0  1  2  3  4  6 10 11 12 17 21 23 24 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.2210 \n",
      "Accuracy: 9530/10000 (95.30%)\n",
      "\n",
      "Round  25, Average loss 0.221 Test accuracy 95.300\n",
      "selected users: [ 0  2  3  4  6  8  9 12 13 15 16 18 20 21 22 24 26 28]\n",
      "\n",
      "Test set: Average loss: 0.1922 \n",
      "Accuracy: 9500/10000 (95.00%)\n",
      "\n",
      "Round  26, Average loss 0.192 Test accuracy 95.000\n",
      "selected users: [ 2  3  5  7  9 14 15 17 19 20 22 23 24 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.1624 \n",
      "Accuracy: 9527/10000 (95.27%)\n",
      "\n",
      "Round  27, Average loss 0.162 Test accuracy 95.270\n",
      "selected users: [ 0  2  4  5  6  7  9 10 11 12 13 14 15 18 22 24 27 29]\n",
      "\n",
      "Test set: Average loss: 0.2881 \n",
      "Accuracy: 9594/10000 (95.94%)\n",
      "\n",
      "Round  28, Average loss 0.288 Test accuracy 95.940\n",
      "selected users: [ 0  1  3  5  6  7  8  9 11 12 13 17 18 19 20 22 24 29]\n",
      "\n",
      "Test set: Average loss: 0.5852 \n",
      "Accuracy: 9612/10000 (96.12%)\n",
      "\n",
      "Round  29, Average loss 0.585 Test accuracy 96.120\n",
      "(m= 18 )  2 -th Trial!!\n",
      "selected users: [ 0  3  5  6  7  8  9 10 12 13 16 18 21 22 23 24 26 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  1  2  6  7  8  9 13 14 16 17 18 20 21 25 26 28 30]\n",
      "\n",
      "Test set: Average loss: 2.1984 \n",
      "Accuracy: 7654/10000 (76.54%)\n",
      "\n",
      "Round   1, Average loss 2.198 Test accuracy 76.540\n",
      "selected users: [ 3  5  7  8 11 12 13 18 20 21 22 23 24 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.5328 \n",
      "Accuracy: 9164/10000 (91.64%)\n",
      "\n",
      "Round   2, Average loss 0.533 Test accuracy 91.640\n",
      "selected users: [ 0  2  3  4 12 13 15 16 17 19 20 21 22 23 25 27 28 30]\n",
      "\n",
      "Test set: Average loss: 1.6341 \n",
      "Accuracy: 9296/10000 (92.96%)\n",
      "\n",
      "Round   3, Average loss 1.634 Test accuracy 92.960\n",
      "selected users: [ 0  1  2  3  4  6  8  9 11 15 17 20 21 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.1840 \n",
      "Accuracy: 9448/10000 (94.48%)\n",
      "\n",
      "Round   4, Average loss 0.184 Test accuracy 94.480\n",
      "selected users: [ 2  3  6  7  8  9 12 13 14 15 16 17 18 20 22 23 26 29]\n",
      "\n",
      "Test set: Average loss: 0.2061 \n",
      "Accuracy: 9354/10000 (93.54%)\n",
      "\n",
      "Round   5, Average loss 0.206 Test accuracy 93.540\n",
      "selected users: [ 0  1  4  7  8 10 11 12 13 14 16 17 18 21 23 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.1892 \n",
      "Accuracy: 9423/10000 (94.23%)\n",
      "\n",
      "Round   6, Average loss 0.189 Test accuracy 94.230\n",
      "selected users: [ 1  4  5  7  9 10 12 14 16 17 20 22 23 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.1750 \n",
      "Accuracy: 9540/10000 (95.40%)\n",
      "\n",
      "Round   7, Average loss 0.175 Test accuracy 95.400\n",
      "selected users: [ 2  7  8  9 10 11 14 17 18 19 20 23 24 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.7503 \n",
      "Accuracy: 9641/10000 (96.41%)\n",
      "\n",
      "Round   8, Average loss 0.750 Test accuracy 96.410\n",
      "selected users: [ 2  5  7  9 10 11 13 14 15 18 19 20 21 22 25 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.1674 \n",
      "Accuracy: 9533/10000 (95.33%)\n",
      "\n",
      "Round   9, Average loss 0.167 Test accuracy 95.330\n",
      "selected users: [ 4  5  6  7 10 11 12 14 17 20 22 23 24 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.2745 \n",
      "Accuracy: 9632/10000 (96.32%)\n",
      "\n",
      "Round  10, Average loss 0.275 Test accuracy 96.320\n",
      "selected users: [ 0  2  4  6  9 10 12 13 15 16 17 19 20 21 22 23 26 27]\n",
      "\n",
      "Test set: Average loss: 0.1613 \n",
      "Accuracy: 9549/10000 (95.49%)\n",
      "\n",
      "Round  11, Average loss 0.161 Test accuracy 95.490\n",
      "selected users: [ 1  2  4  6  7  8  9 12 15 16 19 20 21 23 24 25 26 29]\n",
      "\n",
      "Test set: Average loss: 0.1556 \n",
      "Accuracy: 9533/10000 (95.33%)\n",
      "\n",
      "Round  12, Average loss 0.156 Test accuracy 95.330\n",
      "selected users: [ 0  2  3  4  5  7  9 11 15 16 18 19 20 23 24 28 29 30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1750 \n",
      "Accuracy: 9476/10000 (94.76%)\n",
      "\n",
      "Round  13, Average loss 0.175 Test accuracy 94.760\n",
      "selected users: [ 0  1  2  4  5  7  9 12 13 14 15 16 20 21 22 23 27 29]\n",
      "\n",
      "Test set: Average loss: 0.1843 \n",
      "Accuracy: 9610/10000 (96.10%)\n",
      "\n",
      "Round  14, Average loss 0.184 Test accuracy 96.100\n",
      "selected users: [ 2  3  4  6  9 12 13 14 15 17 18 19 21 22 23 26 27 30]\n",
      "\n",
      "Test set: Average loss: 0.1400 \n",
      "Accuracy: 9582/10000 (95.82%)\n",
      "\n",
      "Round  15, Average loss 0.140 Test accuracy 95.820\n",
      "selected users: [ 1  2  3  4  5  6  7  8  9 11 15 20 21 23 24 25 27 30]\n",
      "\n",
      "Test set: Average loss: 0.1572 \n",
      "Accuracy: 9546/10000 (95.46%)\n",
      "\n",
      "Round  16, Average loss 0.157 Test accuracy 95.460\n",
      "selected users: [ 1  2  3  4  5  7  8  9 10 13 15 16 17 20 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.3531 \n",
      "Accuracy: 9354/10000 (93.54%)\n",
      "\n",
      "Round  17, Average loss 0.353 Test accuracy 93.540\n",
      "selected users: [ 0  1  2  7 11 12 13 15 16 17 18 20 22 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.1709 \n",
      "Accuracy: 9496/10000 (94.96%)\n",
      "\n",
      "Round  18, Average loss 0.171 Test accuracy 94.960\n",
      "selected users: [ 0  1  3  4  5  6  8 10 11 12 17 19 22 23 24 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.5219 \n",
      "Accuracy: 9591/10000 (95.91%)\n",
      "\n",
      "Round  19, Average loss 0.522 Test accuracy 95.910\n",
      "selected users: [ 1  7  9 11 12 13 16 17 18 19 20 22 23 24 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.1710 \n",
      "Accuracy: 9578/10000 (95.78%)\n",
      "\n",
      "Round  20, Average loss 0.171 Test accuracy 95.780\n",
      "selected users: [ 0  4  8 11 12 13 15 16 19 20 21 22 23 24 25 26 29 30]\n",
      "\n",
      "Test set: Average loss: 0.1727 \n",
      "Accuracy: 9486/10000 (94.86%)\n",
      "\n",
      "Round  21, Average loss 0.173 Test accuracy 94.860\n",
      "selected users: [ 1  2  3  4  6  7 10 14 18 19 20 21 22 23 24 26 29 30]\n",
      "\n",
      "Test set: Average loss: 0.1723 \n",
      "Accuracy: 9484/10000 (94.84%)\n",
      "\n",
      "Round  22, Average loss 0.172 Test accuracy 94.840\n",
      "selected users: [ 1  2  3  4  5  7  8  9 10 11 14 15 16 21 23 24 27 30]\n",
      "\n",
      "Test set: Average loss: 0.1797 \n",
      "Accuracy: 9507/10000 (95.07%)\n",
      "\n",
      "Round  23, Average loss 0.180 Test accuracy 95.070\n",
      "selected users: [ 0  3  4  7  8  9 10 11 12 14 16 19 21 23 24 25 28 29]\n",
      "\n",
      "Test set: Average loss: 0.1534 \n",
      "Accuracy: 9533/10000 (95.33%)\n",
      "\n",
      "Round  24, Average loss 0.153 Test accuracy 95.330\n",
      "selected users: [ 5  6  7  8  9 11 12 14 16 18 20 21 22 23 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.5939 \n",
      "Accuracy: 9220/10000 (92.20%)\n",
      "\n",
      "Round  25, Average loss 0.594 Test accuracy 92.200\n",
      "selected users: [ 1  2  3  4  5  8 10 11 13 14 17 18 19 20 23 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.3218 \n",
      "Accuracy: 9476/10000 (94.76%)\n",
      "\n",
      "Round  26, Average loss 0.322 Test accuracy 94.760\n",
      "selected users: [ 0  1  2  7  9 13 15 18 19 20 22 23 24 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.1885 \n",
      "Accuracy: 9584/10000 (95.84%)\n",
      "\n",
      "Round  27, Average loss 0.189 Test accuracy 95.840\n",
      "selected users: [ 2  3  4  5  6  7  8  9 10 12 13 14 16 18 20 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.1334 \n",
      "Accuracy: 9595/10000 (95.95%)\n",
      "\n",
      "Round  28, Average loss 0.133 Test accuracy 95.950\n",
      "selected users: [ 0  1  3  4  8  9 10 13 15 17 18 19 21 22 24 25 28 29]\n",
      "\n",
      "Test set: Average loss: 0.2149 \n",
      "Accuracy: 9578/10000 (95.78%)\n",
      "\n",
      "Round  29, Average loss 0.215 Test accuracy 95.780\n",
      "(m= 18 )  3 -th Trial!!\n",
      "selected users: [ 1  2  4  5  6  7  8 11 14 15 16 17 19 24 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  1  3  4  5  6  9 10 14 15 16 20 22 24 25 27 28 30]\n",
      "\n",
      "Test set: Average loss: 2.0039 \n",
      "Accuracy: 8312/10000 (83.12%)\n",
      "\n",
      "Round   1, Average loss 2.004 Test accuracy 83.120\n",
      "selected users: [ 1  3  5  6  8  9 10 13 15 16 18 19 20 24 25 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.5084 \n",
      "Accuracy: 9233/10000 (92.33%)\n",
      "\n",
      "Round   2, Average loss 0.508 Test accuracy 92.330\n",
      "selected users: [ 3  4  5  6  7 11 12 14 16 19 20 22 23 24 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2631 \n",
      "Accuracy: 9323/10000 (93.23%)\n",
      "\n",
      "Round   3, Average loss 0.263 Test accuracy 93.230\n",
      "selected users: [ 2  5  6  7  8 10 12 13 15 16 18 19 20 21 22 24 26 29]\n",
      "\n",
      "Test set: Average loss: 0.2286 \n",
      "Accuracy: 9301/10000 (93.01%)\n",
      "\n",
      "Round   4, Average loss 0.229 Test accuracy 93.010\n",
      "selected users: [ 1  2  3  4  5  7 10 13 17 19 21 22 24 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.3366 \n",
      "Accuracy: 9468/10000 (94.68%)\n",
      "\n",
      "Round   5, Average loss 0.337 Test accuracy 94.680\n",
      "selected users: [ 0  1  5  6  8 14 15 16 18 19 20 22 24 25 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.2510 \n",
      "Accuracy: 9403/10000 (94.03%)\n",
      "\n",
      "Round   6, Average loss 0.251 Test accuracy 94.030\n",
      "selected users: [ 1  2  5  6  8  9 10 11 14 15 16 17 18 22 23 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2235 \n",
      "Accuracy: 9432/10000 (94.32%)\n",
      "\n",
      "Round   7, Average loss 0.223 Test accuracy 94.320\n",
      "selected users: [ 0  1  5  7  8  9 12 13 14 16 18 20 22 23 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.2008 \n",
      "Accuracy: 9390/10000 (93.90%)\n",
      "\n",
      "Round   8, Average loss 0.201 Test accuracy 93.900\n",
      "selected users: [ 4  5  6  7  8 10 11 12 14 15 17 19 20 21 25 26 28 29]\n",
      "\n",
      "Test set: Average loss: 0.2701 \n",
      "Accuracy: 9474/10000 (94.74%)\n",
      "\n",
      "Round   9, Average loss 0.270 Test accuracy 94.740\n",
      "selected users: [ 1  2  3  4  5  8  9 10 15 17 19 23 24 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.1916 \n",
      "Accuracy: 9460/10000 (94.60%)\n",
      "\n",
      "Round  10, Average loss 0.192 Test accuracy 94.600\n",
      "selected users: [ 1  2  3  4  8  9 14 15 16 17 18 19 22 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.4038 \n",
      "Accuracy: 9566/10000 (95.66%)\n",
      "\n",
      "Round  11, Average loss 0.404 Test accuracy 95.660\n",
      "selected users: [ 0  1  2  4 10 12 14 15 16 18 20 22 24 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.1927 \n",
      "Accuracy: 9474/10000 (94.74%)\n",
      "\n",
      "Round  12, Average loss 0.193 Test accuracy 94.740\n",
      "selected users: [ 0  2  3  5  6  7 10 11 12 14 16 18 19 21 22 23 25 28]\n",
      "\n",
      "Test set: Average loss: 0.2046 \n",
      "Accuracy: 9377/10000 (93.77%)\n",
      "\n",
      "Round  13, Average loss 0.205 Test accuracy 93.770\n",
      "selected users: [ 0  1  3  6  8  9 10 11 12 14 16 17 18 21 22 23 24 28]\n",
      "\n",
      "Test set: Average loss: 0.3560 \n",
      "Accuracy: 9472/10000 (94.72%)\n",
      "\n",
      "Round  14, Average loss 0.356 Test accuracy 94.720\n",
      "selected users: [ 0  2  4  6  8  9 10 12 15 17 18 20 21 22 23 25 28 30]\n",
      "\n",
      "Test set: Average loss: 0.2157 \n",
      "Accuracy: 9498/10000 (94.98%)\n",
      "\n",
      "Round  15, Average loss 0.216 Test accuracy 94.980\n",
      "selected users: [ 3  5  6  8 10 14 17 18 19 20 21 22 24 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2624 \n",
      "Accuracy: 9491/10000 (94.91%)\n",
      "\n",
      "Round  16, Average loss 0.262 Test accuracy 94.910\n",
      "selected users: [ 0  1  3  4  5  6  9 12 14 16 18 19 21 22 23 24 26 29]\n",
      "\n",
      "Test set: Average loss: 0.2361 \n",
      "Accuracy: 9529/10000 (95.29%)\n",
      "\n",
      "Round  17, Average loss 0.236 Test accuracy 95.290\n",
      "selected users: [ 0  4  6  7 10 11 13 14 15 17 18 19 22 23 24 25 27 28]\n",
      "\n",
      "Test set: Average loss: 0.2088 \n",
      "Accuracy: 9520/10000 (95.20%)\n",
      "\n",
      "Round  18, Average loss 0.209 Test accuracy 95.200\n",
      "selected users: [ 0  1  2  4  6  8 12 14 15 16 17 20 22 23 25 26 28 29]\n",
      "\n",
      "Test set: Average loss: 0.3101 \n",
      "Accuracy: 9513/10000 (95.13%)\n",
      "\n",
      "Round  19, Average loss 0.310 Test accuracy 95.130\n",
      "selected users: [ 0  1  3  7  8  9 10 12 13 14 15 17 18 20 23 26 27 30]\n",
      "\n",
      "Test set: Average loss: 0.2321 \n",
      "Accuracy: 9534/10000 (95.34%)\n",
      "\n",
      "Round  20, Average loss 0.232 Test accuracy 95.340\n",
      "selected users: [ 0  1  2  6  7  8  9 10 11 12 14 15 16 18 21 23 28 29]\n",
      "\n",
      "Test set: Average loss: 0.4498 \n",
      "Accuracy: 9443/10000 (94.43%)\n",
      "\n",
      "Round  21, Average loss 0.450 Test accuracy 94.430\n",
      "selected users: [ 0  1  2  3  5  6  7  8 10 12 14 16 17 18 20 24 28 29]\n",
      "\n",
      "Test set: Average loss: 0.3966 \n",
      "Accuracy: 9481/10000 (94.81%)\n",
      "\n",
      "Round  22, Average loss 0.397 Test accuracy 94.810\n",
      "selected users: [ 2  3  4  5  6  7  8  9 11 12 14 18 21 23 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.3649 \n",
      "Accuracy: 9515/10000 (95.15%)\n",
      "\n",
      "Round  23, Average loss 0.365 Test accuracy 95.150\n",
      "selected users: [ 0  1  2  4  6  7 11 12 13 14 15 16 17 19 22 24 26 29]\n",
      "\n",
      "Test set: Average loss: 0.2273 \n",
      "Accuracy: 9534/10000 (95.34%)\n",
      "\n",
      "Round  24, Average loss 0.227 Test accuracy 95.340\n",
      "selected users: [ 0  1  2  5  6  7  9 10 11 12 13 15 21 23 24 26 28 30]\n",
      "\n",
      "Test set: Average loss: 0.2297 \n",
      "Accuracy: 9375/10000 (93.75%)\n",
      "\n",
      "Round  25, Average loss 0.230 Test accuracy 93.750\n",
      "selected users: [ 1  2  4  7  8 10 11 14 16 17 18 21 22 23 24 25 26 29]\n",
      "\n",
      "Test set: Average loss: 0.1963 \n",
      "Accuracy: 9500/10000 (95.00%)\n",
      "\n",
      "Round  26, Average loss 0.196 Test accuracy 95.000\n",
      "selected users: [ 0  1  3  4  5  6  8  9 10 14 15 22 24 25 26 27 28 29]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.2190 \n",
      "Accuracy: 9523/10000 (95.23%)\n",
      "\n",
      "Round  27, Average loss 0.219 Test accuracy 95.230\n",
      "selected users: [ 0  1  3  5  6  7 12 14 15 16 18 20 21 22 25 26 27 29]\n",
      "\n",
      "Test set: Average loss: 0.3210 \n",
      "Accuracy: 9551/10000 (95.51%)\n",
      "\n",
      "Round  28, Average loss 0.321 Test accuracy 95.510\n",
      "selected users: [ 0  1  5  6  7  8  9 11 12 15 16 18 19 20 22 23 28 29]\n",
      "\n",
      "Test set: Average loss: 0.2898 \n",
      "Accuracy: 9442/10000 (94.42%)\n",
      "\n",
      "Round  29, Average loss 0.290 Test accuracy 94.420\n",
      "(m= 18 )  4 -th Trial!!\n",
      "selected users: [ 0  1  3  5  6  7  8 10 12 13 14 17 18 20 23 24 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 1176/10000 (11.76%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 11.760\n",
      "selected users: [ 0  1  2  4  5  6  7 10 12 14 15 18 20 21 22 24 26 28]\n",
      "\n",
      "Test set: Average loss: 2.1569 \n",
      "Accuracy: 7495/10000 (74.95%)\n",
      "\n",
      "Round   1, Average loss 2.157 Test accuracy 74.950\n",
      "selected users: [ 0  1  2  3  6  8  9 12 13 14 17 18 19 20 22 26 29 30]\n",
      "\n",
      "Test set: Average loss: 0.5871 \n",
      "Accuracy: 9307/10000 (93.07%)\n",
      "\n",
      "Round   2, Average loss 0.587 Test accuracy 93.070\n",
      "selected users: [ 0  1  2  3  4  5  7  8  9 14 15 17 18 19 20 23 25 27]\n",
      "\n",
      "Test set: Average loss: 0.7708 \n",
      "Accuracy: 9504/10000 (95.04%)\n",
      "\n",
      "Round   3, Average loss 0.771 Test accuracy 95.040\n",
      "selected users: [ 0  1  3  5  6  8 10 11 13 15 18 19 22 23 24 25 26 29]\n",
      "\n",
      "Test set: Average loss: 0.3658 \n",
      "Accuracy: 9414/10000 (94.14%)\n",
      "\n",
      "Round   4, Average loss 0.366 Test accuracy 94.140\n",
      "selected users: [ 0  1  3  5  6  7 10 12 13 15 17 19 20 21 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.3051 \n",
      "Accuracy: 9267/10000 (92.67%)\n",
      "\n",
      "Round   5, Average loss 0.305 Test accuracy 92.670\n",
      "selected users: [ 0  3  5  6  7  8  9 10 11 12 13 14 17 18 19 22 25 28]\n",
      "\n",
      "Test set: Average loss: 0.4823 \n",
      "Accuracy: 9451/10000 (94.51%)\n",
      "\n",
      "Round   6, Average loss 0.482 Test accuracy 94.510\n",
      "selected users: [ 3  4  7  8  9 11 12 13 14 15 18 20 21 23 24 26 27 28]\n",
      "\n",
      "Test set: Average loss: 0.3373 \n",
      "Accuracy: 9397/10000 (93.97%)\n",
      "\n",
      "Round   7, Average loss 0.337 Test accuracy 93.970\n",
      "selected users: [ 0  1  2  3  4  5  7  8 10 11 16 17 18 20 21 26 27 29]\n",
      "\n",
      "Test set: Average loss: 0.2922 \n",
      "Accuracy: 9204/10000 (92.04%)\n",
      "\n",
      "Round   8, Average loss 0.292 Test accuracy 92.040\n",
      "selected users: [ 0  3  8  9 10 11 13 14 16 17 18 20 21 22 24 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.4904 \n",
      "Accuracy: 9399/10000 (93.99%)\n",
      "\n",
      "Round   9, Average loss 0.490 Test accuracy 93.990\n",
      "selected users: [ 1  3  4  5  6  8  9 10 11 13 14 17 18 21 23 25 28 30]\n",
      "\n",
      "Test set: Average loss: 0.3021 \n",
      "Accuracy: 9493/10000 (94.93%)\n",
      "\n",
      "Round  10, Average loss 0.302 Test accuracy 94.930\n",
      "selected users: [ 0  2  4  6 10 11 12 13 14 17 18 19 20 21 23 25 26 27]\n",
      "\n",
      "Test set: Average loss: 0.3093 \n",
      "Accuracy: 9532/10000 (95.32%)\n",
      "\n",
      "Round  11, Average loss 0.309 Test accuracy 95.320\n",
      "selected users: [ 1  4  7  8  9 12 13 14 16 18 20 22 23 24 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2963 \n",
      "Accuracy: 9575/10000 (95.75%)\n",
      "\n",
      "Round  12, Average loss 0.296 Test accuracy 95.750\n",
      "selected users: [ 0  2  4  8 12 13 14 17 19 20 21 22 23 24 25 26 28 29]\n",
      "\n",
      "Test set: Average loss: 0.2865 \n",
      "Accuracy: 9568/10000 (95.68%)\n",
      "\n",
      "Round  13, Average loss 0.286 Test accuracy 95.680\n",
      "selected users: [ 1  3  4  5  6  7  8  9 10 11 16 19 20 24 25 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.2397 \n",
      "Accuracy: 9541/10000 (95.41%)\n",
      "\n",
      "Round  14, Average loss 0.240 Test accuracy 95.410\n",
      "selected users: [ 0  2  3  5  7  9 10 12 14 15 18 19 21 22 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.1631 \n",
      "Accuracy: 9515/10000 (95.15%)\n",
      "\n",
      "Round  15, Average loss 0.163 Test accuracy 95.150\n",
      "selected users: [ 1  2  3  4  5  6  7  9 10 12 15 16 18 21 22 25 26 28]\n",
      "\n",
      "Test set: Average loss: 0.4835 \n",
      "Accuracy: 9539/10000 (95.39%)\n",
      "\n",
      "Round  16, Average loss 0.484 Test accuracy 95.390\n",
      "selected users: [ 0  1  3  6  7  8  9 11 12 14 15 16 18 22 23 25 27 28]\n",
      "\n",
      "Test set: Average loss: 0.5795 \n",
      "Accuracy: 9529/10000 (95.29%)\n",
      "\n",
      "Round  17, Average loss 0.579 Test accuracy 95.290\n",
      "selected users: [ 2  3  4  5  7  9 12 14 16 17 19 21 22 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.4384 \n",
      "Accuracy: 9585/10000 (95.85%)\n",
      "\n",
      "Round  18, Average loss 0.438 Test accuracy 95.850\n",
      "selected users: [ 0  2  6 10 12 14 15 17 19 20 22 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.3344 \n",
      "Accuracy: 9527/10000 (95.27%)\n",
      "\n",
      "Round  19, Average loss 0.334 Test accuracy 95.270\n",
      "selected users: [ 0  3  5  8 10 11 12 13 15 16 18 20 21 22 23 24 26 29]\n",
      "\n",
      "Test set: Average loss: 0.2048 \n",
      "Accuracy: 9394/10000 (93.94%)\n",
      "\n",
      "Round  20, Average loss 0.205 Test accuracy 93.940\n",
      "selected users: [ 0  1  4  6  7 12 13 14 16 17 19 20 23 24 25 26 29 30]\n",
      "\n",
      "Test set: Average loss: 0.3230 \n",
      "Accuracy: 9528/10000 (95.28%)\n",
      "\n",
      "Round  21, Average loss 0.323 Test accuracy 95.280\n",
      "selected users: [ 0  2  7  8  9 10 11 14 17 19 20 21 22 23 25 26 27 30]\n",
      "\n",
      "Test set: Average loss: 1.5788 \n",
      "Accuracy: 9581/10000 (95.81%)\n",
      "\n",
      "Round  22, Average loss 1.579 Test accuracy 95.810\n",
      "selected users: [ 0  1  2  4  5  7  8  9 12 14 15 16 17 21 24 26 27 30]\n",
      "\n",
      "Test set: Average loss: 0.2338 \n",
      "Accuracy: 9524/10000 (95.24%)\n",
      "\n",
      "Round  23, Average loss 0.234 Test accuracy 95.240\n",
      "selected users: [ 0  2  4  5  6  7  8 10 12 13 15 18 23 24 25 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2373 \n",
      "Accuracy: 9472/10000 (94.72%)\n",
      "\n",
      "Round  24, Average loss 0.237 Test accuracy 94.720\n",
      "selected users: [ 3  5  9 10 11 12 13 15 16 19 20 22 23 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.1857 \n",
      "Accuracy: 9463/10000 (94.63%)\n",
      "\n",
      "Round  25, Average loss 0.186 Test accuracy 94.630\n",
      "selected users: [ 1  2  3  7  8  9 12 14 15 16 17 19 20 21 22 24 25 27]\n",
      "\n",
      "Test set: Average loss: 0.3178 \n",
      "Accuracy: 9580/10000 (95.80%)\n",
      "\n",
      "Round  26, Average loss 0.318 Test accuracy 95.800\n",
      "selected users: [ 0  2  3  5  8 11 12 14 15 17 19 20 23 24 25 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.1739 \n",
      "Accuracy: 9477/10000 (94.77%)\n",
      "\n",
      "Round  27, Average loss 0.174 Test accuracy 94.770\n",
      "selected users: [ 0  2  3  4  5  6  7  8  9 10 11 12 16 17 18 22 23 29]\n",
      "\n",
      "Test set: Average loss: 0.3136 \n",
      "Accuracy: 9508/10000 (95.08%)\n",
      "\n",
      "Round  28, Average loss 0.314 Test accuracy 95.080\n",
      "selected users: [ 0  5  6  7  8  9 11 12 15 18 20 22 24 25 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.1597 \n",
      "Accuracy: 9534/10000 (95.34%)\n",
      "\n",
      "Round  29, Average loss 0.160 Test accuracy 95.340\n",
      "(m= 18 )  5 -th Trial!!\n",
      "selected users: [ 2  4  5  6  8 12 13 14 15 16 20 21 23 24 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 1  2  6  7  8  9 10 12 15 16 19 20 21 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 2.2966 \n",
      "Accuracy: 1911/10000 (19.11%)\n",
      "\n",
      "Round   1, Average loss 2.297 Test accuracy 19.110\n",
      "selected users: [ 0  2  3  4  7  8  9 14 15 16 18 19 22 23 24 25 28 30]\n",
      "\n",
      "Test set: Average loss: 1.1150 \n",
      "Accuracy: 8867/10000 (88.67%)\n",
      "\n",
      "Round   2, Average loss 1.115 Test accuracy 88.670\n",
      "selected users: [ 0  2  3  4  5  6  7  9 11 12 15 16 19 20 22 24 28 29]\n",
      "\n",
      "Test set: Average loss: 0.8014 \n",
      "Accuracy: 9434/10000 (94.34%)\n",
      "\n",
      "Round   3, Average loss 0.801 Test accuracy 94.340\n",
      "selected users: [ 2  3  5  8  9 11 12 15 16 17 18 19 21 23 24 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2027 \n",
      "Accuracy: 9416/10000 (94.16%)\n",
      "\n",
      "Round   4, Average loss 0.203 Test accuracy 94.160\n",
      "selected users: [ 1  5  6  7  8 10 13 14 15 18 20 22 23 24 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.2011 \n",
      "Accuracy: 9407/10000 (94.07%)\n",
      "\n",
      "Round   5, Average loss 0.201 Test accuracy 94.070\n",
      "selected users: [ 0  1  3  6  7  9 10 12 16 17 18 19 20 21 23 25 27 29]\n",
      "\n",
      "Test set: Average loss: 0.4379 \n",
      "Accuracy: 9511/10000 (95.11%)\n",
      "\n",
      "Round   6, Average loss 0.438 Test accuracy 95.110\n",
      "selected users: [ 1  2  4  6 10 11 12 16 17 18 21 22 24 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2104 \n",
      "Accuracy: 9430/10000 (94.30%)\n",
      "\n",
      "Round   7, Average loss 0.210 Test accuracy 94.300\n",
      "selected users: [ 1  2  3  7  9 10 11 13 14 15 19 20 24 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2363 \n",
      "Accuracy: 9415/10000 (94.15%)\n",
      "\n",
      "Round   8, Average loss 0.236 Test accuracy 94.150\n",
      "selected users: [ 1  3  8 10 11 12 13 14 15 16 19 23 24 25 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.5739 \n",
      "Accuracy: 9498/10000 (94.98%)\n",
      "\n",
      "Round   9, Average loss 0.574 Test accuracy 94.980\n",
      "selected users: [ 1  2  5  6  9 12 13 14 15 16 17 19 20 21 23 25 27 30]\n",
      "\n",
      "Test set: Average loss: 0.2239 \n",
      "Accuracy: 9512/10000 (95.12%)\n",
      "\n",
      "Round  10, Average loss 0.224 Test accuracy 95.120\n",
      "selected users: [ 1  4  5  6  8 10 12 13 14 15 16 17 23 24 25 28 29 30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.3992 \n",
      "Accuracy: 9513/10000 (95.13%)\n",
      "\n",
      "Round  11, Average loss 0.399 Test accuracy 95.130\n",
      "selected users: [ 0  4  5 10 11 12 13 14 16 17 18 19 21 23 25 26 27 28]\n",
      "\n",
      "Test set: Average loss: 0.2032 \n",
      "Accuracy: 9433/10000 (94.33%)\n",
      "\n",
      "Round  12, Average loss 0.203 Test accuracy 94.330\n",
      "selected users: [ 1  3  4  9 12 13 14 15 16 17 18 21 22 23 24 26 29 30]\n",
      "\n",
      "Test set: Average loss: 0.1779 \n",
      "Accuracy: 9471/10000 (94.71%)\n",
      "\n",
      "Round  13, Average loss 0.178 Test accuracy 94.710\n",
      "selected users: [ 1  2  3  6  8  9 10 11 12 13 15 19 21 22 23 24 25 28]\n",
      "\n",
      "Test set: Average loss: 0.3570 \n",
      "Accuracy: 9515/10000 (95.15%)\n",
      "\n",
      "Round  14, Average loss 0.357 Test accuracy 95.150\n",
      "selected users: [ 0  2  6  8  9 10 14 15 17 18 19 20 23 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4754 \n",
      "Accuracy: 9547/10000 (95.47%)\n",
      "\n",
      "Round  15, Average loss 0.475 Test accuracy 95.470\n",
      "selected users: [ 2  3  4  5  7 11 12 13 14 15 16 17 18 19 22 23 25 30]\n",
      "\n",
      "Test set: Average loss: 0.3829 \n",
      "Accuracy: 9575/10000 (95.75%)\n",
      "\n",
      "Round  16, Average loss 0.383 Test accuracy 95.750\n",
      "selected users: [ 0  2  3  4  5  6  9 10 11 12 13 14 16 20 24 25 27 28]\n",
      "\n",
      "Test set: Average loss: 0.2959 \n",
      "Accuracy: 9559/10000 (95.59%)\n",
      "\n",
      "Round  17, Average loss 0.296 Test accuracy 95.590\n",
      "selected users: [ 1  4  5  8  9 11 12 13 14 17 18 19 20 21 22 24 25 26]\n",
      "\n",
      "Test set: Average loss: 0.1985 \n",
      "Accuracy: 9436/10000 (94.36%)\n",
      "\n",
      "Round  18, Average loss 0.198 Test accuracy 94.360\n",
      "selected users: [ 0  1  2  4  5  7  8  9 12 13 14 16 17 21 22 23 24 25]\n",
      "\n",
      "Test set: Average loss: 0.1867 \n",
      "Accuracy: 9467/10000 (94.67%)\n",
      "\n",
      "Round  19, Average loss 0.187 Test accuracy 94.670\n",
      "selected users: [ 1  2  3  4  6  7  8 12 13 14 15 16 18 21 23 24 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2579 \n",
      "Accuracy: 9469/10000 (94.69%)\n",
      "\n",
      "Round  20, Average loss 0.258 Test accuracy 94.690\n",
      "selected users: [ 1  2  3  4  5  7  8 10 13 14 16 17 19 23 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.1893 \n",
      "Accuracy: 9422/10000 (94.22%)\n",
      "\n",
      "Round  21, Average loss 0.189 Test accuracy 94.220\n",
      "selected users: [ 0  3  4  5  6  8  9 10 12 14 15 17 20 21 23 25 26 27]\n",
      "\n",
      "Test set: Average loss: 0.2802 \n",
      "Accuracy: 9522/10000 (95.22%)\n",
      "\n",
      "Round  22, Average loss 0.280 Test accuracy 95.220\n",
      "selected users: [ 2  3  6  7  8 11 14 15 16 17 18 19 23 24 25 26 27 29]\n",
      "\n",
      "Test set: Average loss: 0.5786 \n",
      "Accuracy: 9555/10000 (95.55%)\n",
      "\n",
      "Round  23, Average loss 0.579 Test accuracy 95.550\n",
      "selected users: [ 0  1  2  3  5  6  7  8  9 11 16 20 21 22 24 26 29 30]\n",
      "\n",
      "Test set: Average loss: 0.1979 \n",
      "Accuracy: 9444/10000 (94.44%)\n",
      "\n",
      "Round  24, Average loss 0.198 Test accuracy 94.440\n",
      "selected users: [ 0  2  3  4  7  8 11 13 14 15 18 20 21 22 23 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.2009 \n",
      "Accuracy: 9504/10000 (95.04%)\n",
      "\n",
      "Round  25, Average loss 0.201 Test accuracy 95.040\n",
      "selected users: [ 0  2  3  4  5  6  9 12 13 14 17 19 20 21 23 25 29 30]\n",
      "\n",
      "Test set: Average loss: 0.3833 \n",
      "Accuracy: 9579/10000 (95.79%)\n",
      "\n",
      "Round  26, Average loss 0.383 Test accuracy 95.790\n",
      "selected users: [ 3  5  7  8  9 10 11 12 13 14 17 18 20 22 24 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.3911 \n",
      "Accuracy: 9556/10000 (95.56%)\n",
      "\n",
      "Round  27, Average loss 0.391 Test accuracy 95.560\n",
      "selected users: [ 3  4  8  9 10 11 12 13 16 19 21 22 23 24 25 26 27 30]\n",
      "\n",
      "Test set: Average loss: 0.3358 \n",
      "Accuracy: 9513/10000 (95.13%)\n",
      "\n",
      "Round  28, Average loss 0.336 Test accuracy 95.130\n",
      "selected users: [ 0  2  3  5  7 10 12 14 17 19 21 22 23 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.1647 \n",
      "Accuracy: 9501/10000 (95.01%)\n",
      "\n",
      "Round  29, Average loss 0.165 Test accuracy 95.010\n",
      "(m= 18 )  6 -th Trial!!\n",
      "selected users: [ 0  1  2  3  4  6  7  8 13 15 17 18 19 20 21 23 24 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 1  3  4  5  7 10 11 13 14 16 17 18 20 21 22 25 26 27]\n",
      "\n",
      "Test set: Average loss: 2.2868 \n",
      "Accuracy: 4465/10000 (44.65%)\n",
      "\n",
      "Round   1, Average loss 2.287 Test accuracy 44.650\n",
      "selected users: [ 0  1  3  4  6  8 10 11 12 17 18 20 21 24 25 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.9234 \n",
      "Accuracy: 9225/10000 (92.25%)\n",
      "\n",
      "Round   2, Average loss 0.923 Test accuracy 92.250\n",
      "selected users: [ 0  1  3  4  8  9 12 17 18 19 22 23 24 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.8457 \n",
      "Accuracy: 9479/10000 (94.79%)\n",
      "\n",
      "Round   3, Average loss 0.846 Test accuracy 94.790\n",
      "selected users: [ 0  4  5  7  8 11 12 15 16 17 19 20 23 24 25 26 27 30]\n",
      "\n",
      "Test set: Average loss: 0.1799 \n",
      "Accuracy: 9467/10000 (94.67%)\n",
      "\n",
      "Round   4, Average loss 0.180 Test accuracy 94.670\n",
      "selected users: [ 0  1  2  3  4  5  6  8 11 13 14 16 20 22 23 24 26 30]\n",
      "\n",
      "Test set: Average loss: 0.1958 \n",
      "Accuracy: 9417/10000 (94.17%)\n",
      "\n",
      "Round   5, Average loss 0.196 Test accuracy 94.170\n",
      "selected users: [ 1  2  5  6  7  9 10 11 13 14 17 19 21 22 23 24 25 30]\n",
      "\n",
      "Test set: Average loss: 0.1848 \n",
      "Accuracy: 9481/10000 (94.81%)\n",
      "\n",
      "Round   6, Average loss 0.185 Test accuracy 94.810\n",
      "selected users: [ 1  2  4  5  6  7  8 10 12 14 15 16 18 20 23 24 25 27]\n",
      "\n",
      "Test set: Average loss: 0.1758 \n",
      "Accuracy: 9477/10000 (94.77%)\n",
      "\n",
      "Round   7, Average loss 0.176 Test accuracy 94.770\n",
      "selected users: [ 1  2  4  5  7  8  9 10 13 14 20 22 23 24 25 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2808 \n",
      "Accuracy: 9371/10000 (93.71%)\n",
      "\n",
      "Round   8, Average loss 0.281 Test accuracy 93.710\n",
      "selected users: [ 0  1  4  5  6  7  8 10 11 12 13 14 16 18 23 25 28 29]\n",
      "\n",
      "Test set: Average loss: 0.6099 \n",
      "Accuracy: 9453/10000 (94.53%)\n",
      "\n",
      "Round   9, Average loss 0.610 Test accuracy 94.530\n",
      "selected users: [ 1  2  5  7  8 10 11 12 13 14 16 17 21 24 25 26 27 30]\n",
      "\n",
      "Test set: Average loss: 0.2005 \n",
      "Accuracy: 9450/10000 (94.50%)\n",
      "\n",
      "Round  10, Average loss 0.201 Test accuracy 94.500\n",
      "selected users: [ 0  1  2  4  7  8 10 11 15 16 17 19 20 23 24 25 28 29]\n",
      "\n",
      "Test set: Average loss: 0.2650 \n",
      "Accuracy: 9477/10000 (94.77%)\n",
      "\n",
      "Round  11, Average loss 0.265 Test accuracy 94.770\n",
      "selected users: [ 0  1  2  3  5  6  8  9 10 15 18 21 22 23 25 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2353 \n",
      "Accuracy: 9372/10000 (93.72%)\n",
      "\n",
      "Round  12, Average loss 0.235 Test accuracy 93.720\n",
      "selected users: [ 0  2  4  5  6  8 11 13 14 15 18 19 20 23 25 26 28 29]\n",
      "\n",
      "Test set: Average loss: 0.2205 \n",
      "Accuracy: 9363/10000 (93.63%)\n",
      "\n",
      "Round  13, Average loss 0.221 Test accuracy 93.630\n",
      "selected users: [ 0  1  2  4  5  7  8 13 15 16 17 18 19 24 25 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.2204 \n",
      "Accuracy: 9442/10000 (94.42%)\n",
      "\n",
      "Round  14, Average loss 0.220 Test accuracy 94.420\n",
      "selected users: [ 0  2  3  4  5 10 12 13 15 16 17 18 20 21 22 25 26 28]\n",
      "\n",
      "Test set: Average loss: 0.2713 \n",
      "Accuracy: 9370/10000 (93.70%)\n",
      "\n",
      "Round  15, Average loss 0.271 Test accuracy 93.700\n",
      "selected users: [ 2  5  6  9 10 12 13 14 17 18 20 22 23 25 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.2669 \n",
      "Accuracy: 9548/10000 (95.48%)\n",
      "\n",
      "Round  16, Average loss 0.267 Test accuracy 95.480\n",
      "selected users: [ 0  2  4  9 11 12 15 16 18 20 21 22 23 24 25 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.1657 \n",
      "Accuracy: 9573/10000 (95.73%)\n",
      "\n",
      "Round  17, Average loss 0.166 Test accuracy 95.730\n",
      "selected users: [ 0  1  5  6  8  9 12 13 16 17 18 20 21 23 25 26 27 28]\n",
      "\n",
      "Test set: Average loss: 0.2640 \n",
      "Accuracy: 9452/10000 (94.52%)\n",
      "\n",
      "Round  18, Average loss 0.264 Test accuracy 94.520\n",
      "selected users: [ 0  1  3  4  7  9 11 13 14 15 16 19 20 22 24 25 27 28]\n",
      "\n",
      "Test set: Average loss: 0.3079 \n",
      "Accuracy: 9538/10000 (95.38%)\n",
      "\n",
      "Round  19, Average loss 0.308 Test accuracy 95.380\n",
      "selected users: [ 1  2  6  7  8 14 15 16 17 18 19 20 21 22 24 25 29 30]\n",
      "\n",
      "Test set: Average loss: 0.3692 \n",
      "Accuracy: 9577/10000 (95.77%)\n",
      "\n",
      "Round  20, Average loss 0.369 Test accuracy 95.770\n",
      "selected users: [ 1  3  4  5  7 11 12 13 14 16 17 18 20 21 24 26 28 29]\n",
      "\n",
      "Test set: Average loss: 0.1809 \n",
      "Accuracy: 9453/10000 (94.53%)\n",
      "\n",
      "Round  21, Average loss 0.181 Test accuracy 94.530\n",
      "selected users: [ 0  2  3  4  5  6  8  9 10 13 15 16 18 19 23 26 27 29]\n",
      "\n",
      "Test set: Average loss: 0.2307 \n",
      "Accuracy: 9528/10000 (95.28%)\n",
      "\n",
      "Round  22, Average loss 0.231 Test accuracy 95.280\n",
      "selected users: [ 2  5  6  8  9 10 11 12 16 17 22 23 24 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.1717 \n",
      "Accuracy: 9536/10000 (95.36%)\n",
      "\n",
      "Round  23, Average loss 0.172 Test accuracy 95.360\n",
      "selected users: [ 0  3  4  5  6  8  9 11 12 16 18 19 20 21 24 25 27 29]\n",
      "\n",
      "Test set: Average loss: 0.1592 \n",
      "Accuracy: 9514/10000 (95.14%)\n",
      "\n",
      "Round  24, Average loss 0.159 Test accuracy 95.140\n",
      "selected users: [ 0  1  3  4  7  9 10 13 14 15 16 17 21 22 23 24 27 29]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.2098 \n",
      "Accuracy: 9573/10000 (95.73%)\n",
      "\n",
      "Round  25, Average loss 0.210 Test accuracy 95.730\n",
      "selected users: [ 1  3  4  6  7 11 12 15 16 17 19 21 22 23 24 26 28 29]\n",
      "\n",
      "Test set: Average loss: 0.1633 \n",
      "Accuracy: 9504/10000 (95.04%)\n",
      "\n",
      "Round  26, Average loss 0.163 Test accuracy 95.040\n",
      "selected users: [ 0  1  3  4  5  6 12 14 15 16 19 21 22 23 24 26 28 30]\n",
      "\n",
      "Test set: Average loss: 1.4768 \n",
      "Accuracy: 9154/10000 (91.54%)\n",
      "\n",
      "Round  27, Average loss 1.477 Test accuracy 91.540\n",
      "selected users: [ 1  3  6  7  9 12 13 16 17 18 20 22 23 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2419 \n",
      "Accuracy: 9581/10000 (95.81%)\n",
      "\n",
      "Round  28, Average loss 0.242 Test accuracy 95.810\n",
      "selected users: [ 0  2  4  5  7 10 12 13 14 15 17 18 19 20 23 25 29 30]\n",
      "\n",
      "Test set: Average loss: 0.1762 \n",
      "Accuracy: 9525/10000 (95.25%)\n",
      "\n",
      "Round  29, Average loss 0.176 Test accuracy 95.250\n",
      "(m= 18 )  7 -th Trial!!\n",
      "selected users: [ 2  6  9 14 15 16 17 18 19 21 22 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 1  3  4  6  8  9 12 14 15 16 20 21 22 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.2970 \n",
      "Accuracy: 4143/10000 (41.43%)\n",
      "\n",
      "Round   1, Average loss 2.297 Test accuracy 41.430\n",
      "selected users: [ 2  4  5  7  9 10 11 14 16 17 18 22 23 24 25 26 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4846 \n",
      "Accuracy: 9237/10000 (92.37%)\n",
      "\n",
      "Round   2, Average loss 0.485 Test accuracy 92.370\n",
      "selected users: [ 0  1  2  3  8  9 10 11 14 15 16 18 19 24 25 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.2598 \n",
      "Accuracy: 9386/10000 (93.86%)\n",
      "\n",
      "Round   3, Average loss 1.260 Test accuracy 93.860\n",
      "selected users: [ 1  3  4  6 10 12 13 15 16 17 20 21 23 24 25 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.2385 \n",
      "Accuracy: 9359/10000 (93.59%)\n",
      "\n",
      "Round   4, Average loss 0.239 Test accuracy 93.590\n",
      "selected users: [ 2  3  4  5  7  8 10 11 12 18 20 22 23 24 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2688 \n",
      "Accuracy: 9402/10000 (94.02%)\n",
      "\n",
      "Round   5, Average loss 0.269 Test accuracy 94.020\n",
      "selected users: [ 0  3  4  5  6  8 10 11 12 17 18 19 20 21 22 25 28 29]\n",
      "\n",
      "Test set: Average loss: 0.2078 \n",
      "Accuracy: 9481/10000 (94.81%)\n",
      "\n",
      "Round   6, Average loss 0.208 Test accuracy 94.810\n",
      "selected users: [ 0  1  2  4  6  7  9 11 13 16 17 21 22 23 24 26 27 28]\n",
      "\n",
      "Test set: Average loss: 0.2046 \n",
      "Accuracy: 9406/10000 (94.06%)\n",
      "\n",
      "Round   7, Average loss 0.205 Test accuracy 94.060\n",
      "selected users: [ 0  1  2  6  7  8  9 11 15 17 20 21 23 24 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2201 \n",
      "Accuracy: 9345/10000 (93.45%)\n",
      "\n",
      "Round   8, Average loss 0.220 Test accuracy 93.450\n",
      "selected users: [ 1  2  3  6  7  8 11 13 14 15 19 20 21 23 24 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.1946 \n",
      "Accuracy: 9401/10000 (94.01%)\n",
      "\n",
      "Round   9, Average loss 0.195 Test accuracy 94.010\n",
      "selected users: [ 0  1  3  5  6  7  9 11 12 17 19 20 21 24 25 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.3498 \n",
      "Accuracy: 9551/10000 (95.51%)\n",
      "\n",
      "Round  10, Average loss 0.350 Test accuracy 95.510\n",
      "selected users: [ 1  3  4  8 10 11 12 13 15 16 18 19 20 24 25 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2001 \n",
      "Accuracy: 9414/10000 (94.14%)\n",
      "\n",
      "Round  11, Average loss 0.200 Test accuracy 94.140\n",
      "selected users: [ 1  4  5  6  7 12 13 14 16 18 19 21 23 24 25 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.1769 \n",
      "Accuracy: 9460/10000 (94.60%)\n",
      "\n",
      "Round  12, Average loss 0.177 Test accuracy 94.600\n",
      "selected users: [ 1  2  3  4  5  7  8  9 11 12 16 17 18 21 23 25 26 28]\n",
      "\n",
      "Test set: Average loss: 0.1807 \n",
      "Accuracy: 9468/10000 (94.68%)\n",
      "\n",
      "Round  13, Average loss 0.181 Test accuracy 94.680\n",
      "selected users: [ 0  1  3  4  5  6  9 13 14 15 17 20 21 22 23 25 26 30]\n",
      "\n",
      "Test set: Average loss: 0.2033 \n",
      "Accuracy: 9573/10000 (95.73%)\n",
      "\n",
      "Round  14, Average loss 0.203 Test accuracy 95.730\n",
      "selected users: [ 2  3  4  6  7 11 12 14 19 20 21 22 24 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.1717 \n",
      "Accuracy: 9524/10000 (95.24%)\n",
      "\n",
      "Round  15, Average loss 0.172 Test accuracy 95.240\n",
      "selected users: [ 0  1  2  3  4  6  7  9 12 14 15 17 18 19 23 24 26 27]\n",
      "\n",
      "Test set: Average loss: 0.7829 \n",
      "Accuracy: 9630/10000 (96.30%)\n",
      "\n",
      "Round  16, Average loss 0.783 Test accuracy 96.300\n",
      "selected users: [ 0  1  3  4  6  9 10 11 14 16 17 19 22 24 25 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.3260 \n",
      "Accuracy: 9636/10000 (96.36%)\n",
      "\n",
      "Round  17, Average loss 0.326 Test accuracy 96.360\n",
      "selected users: [ 2  3  6  8  9 10 11 12 13 15 16 20 21 22 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.1788 \n",
      "Accuracy: 9582/10000 (95.82%)\n",
      "\n",
      "Round  18, Average loss 0.179 Test accuracy 95.820\n",
      "selected users: [ 2  3  4  5  6  7  8  9 12 14 15 18 20 23 24 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2142 \n",
      "Accuracy: 9612/10000 (96.12%)\n",
      "\n",
      "Round  19, Average loss 0.214 Test accuracy 96.120\n",
      "selected users: [ 0  2  4  5  6  8 12 13 16 17 18 21 22 23 25 26 27 29]\n",
      "\n",
      "Test set: Average loss: 0.1980 \n",
      "Accuracy: 9520/10000 (95.20%)\n",
      "\n",
      "Round  20, Average loss 0.198 Test accuracy 95.200\n",
      "selected users: [ 0  1  2  3  5  7  8 10 12 13 14 15 18 19 25 26 28 30]\n",
      "\n",
      "Test set: Average loss: 0.2930 \n",
      "Accuracy: 9475/10000 (94.75%)\n",
      "\n",
      "Round  21, Average loss 0.293 Test accuracy 94.750\n",
      "selected users: [ 3  4  6  8  9 11 12 13 14 15 19 20 21 22 25 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.1898 \n",
      "Accuracy: 9517/10000 (95.17%)\n",
      "\n",
      "Round  22, Average loss 0.190 Test accuracy 95.170\n",
      "selected users: [ 0  3  5  9 10 11 12 14 16 17 19 20 21 22 23 26 28 29]\n",
      "\n",
      "Test set: Average loss: 0.2637 \n",
      "Accuracy: 9409/10000 (94.09%)\n",
      "\n",
      "Round  23, Average loss 0.264 Test accuracy 94.090\n",
      "selected users: [ 0  1  4  5  7  8  9 12 13 14 15 20 23 24 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.1954 \n",
      "Accuracy: 9532/10000 (95.32%)\n",
      "\n",
      "Round  24, Average loss 0.195 Test accuracy 95.320\n",
      "selected users: [ 0  1  4  6  8  9 10 13 15 16 17 18 19 21 24 26 27 29]\n",
      "\n",
      "Test set: Average loss: 0.1914 \n",
      "Accuracy: 9524/10000 (95.24%)\n",
      "\n",
      "Round  25, Average loss 0.191 Test accuracy 95.240\n",
      "selected users: [ 0  1  3  4  6  7  8 10 13 15 16 17 19 20 21 26 27 29]\n",
      "\n",
      "Test set: Average loss: 0.2403 \n",
      "Accuracy: 9363/10000 (93.63%)\n",
      "\n",
      "Round  26, Average loss 0.240 Test accuracy 93.630\n",
      "selected users: [ 0  4  5  6  8  9 10 14 15 17 18 20 21 22 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.1693 \n",
      "Accuracy: 9571/10000 (95.71%)\n",
      "\n",
      "Round  27, Average loss 0.169 Test accuracy 95.710\n",
      "selected users: [ 0  2  3  4  5  7  8  9 10 13 15 17 18 19 21 24 26 30]\n",
      "\n",
      "Test set: Average loss: 0.2202 \n",
      "Accuracy: 9478/10000 (94.78%)\n",
      "\n",
      "Round  28, Average loss 0.220 Test accuracy 94.780\n",
      "selected users: [ 3  6  9 10 11 12 13 14 16 19 21 22 23 24 25 26 27 28]\n",
      "\n",
      "Test set: Average loss: 0.3151 \n",
      "Accuracy: 9563/10000 (95.63%)\n",
      "\n",
      "Round  29, Average loss 0.315 Test accuracy 95.630\n",
      "(m= 18 )  8 -th Trial!!\n",
      "selected users: [ 3  4  5  6  7  9 10 13 14 15 16 18 20 21 23 26 27 28]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  1  2  4  6  7  8 10 11 12 14 15 20 21 24 25 28 29]\n",
      "\n",
      "Test set: Average loss: 2.1918 \n",
      "Accuracy: 7715/10000 (77.15%)\n",
      "\n",
      "Round   1, Average loss 2.192 Test accuracy 77.150\n",
      "selected users: [ 0  1  2  3  5  6  7  8  9 11 13 15 16 17 21 23 28 30]\n",
      "\n",
      "Test set: Average loss: 0.9187 \n",
      "Accuracy: 9200/10000 (92.00%)\n",
      "\n",
      "Round   2, Average loss 0.919 Test accuracy 92.000\n",
      "selected users: [ 0  1  2  3  4  7  8  9 10 11 12 13 14 16 22 24 28 29]\n",
      "\n",
      "Test set: Average loss: 0.5098 \n",
      "Accuracy: 9390/10000 (93.90%)\n",
      "\n",
      "Round   3, Average loss 0.510 Test accuracy 93.900\n",
      "selected users: [ 0  1  2  3  4  8  9 13 15 17 18 20 22 24 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.1853 \n",
      "Accuracy: 9465/10000 (94.65%)\n",
      "\n",
      "Round   4, Average loss 0.185 Test accuracy 94.650\n",
      "selected users: [ 1  2  4  6  8 11 12 14 16 17 18 21 22 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.1742 \n",
      "Accuracy: 9484/10000 (94.84%)\n",
      "\n",
      "Round   5, Average loss 0.174 Test accuracy 94.840\n",
      "selected users: [ 0  1  2  3  4  9 15 18 19 20 21 22 23 24 25 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.1547 \n",
      "Accuracy: 9535/10000 (95.35%)\n",
      "\n",
      "Round   6, Average loss 0.155 Test accuracy 95.350\n",
      "selected users: [ 0  2  3  4  5  8  9 11 13 14 15 17 20 21 22 23 24 30]\n",
      "\n",
      "Test set: Average loss: 0.1705 \n",
      "Accuracy: 9505/10000 (95.05%)\n",
      "\n",
      "Round   7, Average loss 0.170 Test accuracy 95.050\n",
      "selected users: [ 0  1  4  6  8  9 11 12 13 14 17 18 20 22 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.1560 \n",
      "Accuracy: 9526/10000 (95.26%)\n",
      "\n",
      "Round   8, Average loss 0.156 Test accuracy 95.260\n",
      "selected users: [ 0  3  4  6  8 10 11 12 14 16 17 18 21 22 25 26 27 29]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1772 \n",
      "Accuracy: 9498/10000 (94.98%)\n",
      "\n",
      "Round   9, Average loss 0.177 Test accuracy 94.980\n",
      "selected users: [ 0  1  2  5  9 11 13 14 16 17 19 20 22 23 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.3820 \n",
      "Accuracy: 9353/10000 (93.53%)\n",
      "\n",
      "Round  10, Average loss 0.382 Test accuracy 93.530\n",
      "selected users: [ 1  2  3  4  7  8 10 11 12 13 15 19 20 21 24 26 28 29]\n",
      "\n",
      "Test set: Average loss: 0.2877 \n",
      "Accuracy: 9368/10000 (93.68%)\n",
      "\n",
      "Round  11, Average loss 0.288 Test accuracy 93.680\n",
      "selected users: [ 1  5  6  7  9 10 15 16 17 19 20 21 22 23 24 25 28 30]\n",
      "\n",
      "Test set: Average loss: 0.3651 \n",
      "Accuracy: 9337/10000 (93.37%)\n",
      "\n",
      "Round  12, Average loss 0.365 Test accuracy 93.370\n",
      "selected users: [ 1  2  4  5  6  7  9 10 11 14 16 20 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.1683 \n",
      "Accuracy: 9472/10000 (94.72%)\n",
      "\n",
      "Round  13, Average loss 0.168 Test accuracy 94.720\n",
      "selected users: [ 0  3  5  6  7  8  9 10 12 14 16 18 20 23 24 25 28 30]\n",
      "\n",
      "Test set: Average loss: 0.1925 \n",
      "Accuracy: 9461/10000 (94.61%)\n",
      "\n",
      "Round  14, Average loss 0.193 Test accuracy 94.610\n",
      "selected users: [ 0  5  7  8  9 10 11 14 16 17 18 19 20 21 22 25 26 30]\n",
      "\n",
      "Test set: Average loss: 0.2730 \n",
      "Accuracy: 9482/10000 (94.82%)\n",
      "\n",
      "Round  15, Average loss 0.273 Test accuracy 94.820\n",
      "selected users: [ 0  5  8 12 13 14 15 17 18 20 23 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2512 \n",
      "Accuracy: 9456/10000 (94.56%)\n",
      "\n",
      "Round  16, Average loss 0.251 Test accuracy 94.560\n",
      "selected users: [ 0  1  2  3  9 10 13 16 17 18 19 21 23 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.1703 \n",
      "Accuracy: 4671/10000 (46.71%)\n",
      "\n",
      "Round  17, Average loss 2.170 Test accuracy 46.710\n",
      "selected users: [ 0  1  2  3  5  6  7  8 14 17 20 22 23 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.7415 \n",
      "Accuracy: 9524/10000 (95.24%)\n",
      "\n",
      "Round  18, Average loss 0.742 Test accuracy 95.240\n",
      "selected users: [ 1  3  4  5  6  7  9 10 11 12 13 14 16 17 24 25 29 30]\n",
      "\n",
      "Test set: Average loss: 1.8435 \n",
      "Accuracy: 9486/10000 (94.86%)\n",
      "\n",
      "Round  19, Average loss 1.843 Test accuracy 94.860\n",
      "selected users: [ 0  1  3  5  6  8  9 12 14 15 17 18 19 22 24 26 29 30]\n",
      "\n",
      "Test set: Average loss: 0.1690 \n",
      "Accuracy: 9554/10000 (95.54%)\n",
      "\n",
      "Round  20, Average loss 0.169 Test accuracy 95.540\n",
      "selected users: [ 1  2  3  4  6  7  8  9 12 13 14 15 17 22 23 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.2353 \n",
      "Accuracy: 9660/10000 (96.60%)\n",
      "\n",
      "Round  21, Average loss 0.235 Test accuracy 96.600\n",
      "selected users: [ 2  3  4  5  8  9 10 11 14 15 16 18 21 22 24 26 27 30]\n",
      "\n",
      "Test set: Average loss: 0.1542 \n",
      "Accuracy: 9519/10000 (95.19%)\n",
      "\n",
      "Round  22, Average loss 0.154 Test accuracy 95.190\n",
      "selected users: [ 0  1  2  4  5  7  8 10 15 16 17 20 21 22 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2336 \n",
      "Accuracy: 9445/10000 (94.45%)\n",
      "\n",
      "Round  23, Average loss 0.234 Test accuracy 94.450\n",
      "selected users: [ 0  3  4  5  7  8  9 10 11 14 15 18 19 20 24 26 28 30]\n",
      "\n",
      "Test set: Average loss: 0.1747 \n",
      "Accuracy: 9472/10000 (94.72%)\n",
      "\n",
      "Round  24, Average loss 0.175 Test accuracy 94.720\n",
      "selected users: [ 0  1  2  4  5 10 14 15 16 17 18 20 22 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2829 \n",
      "Accuracy: 9443/10000 (94.43%)\n",
      "\n",
      "Round  25, Average loss 0.283 Test accuracy 94.430\n",
      "selected users: [ 2  3  6  7  8 10 11 13 14 15 17 19 21 23 24 26 27 28]\n",
      "\n",
      "Test set: Average loss: 0.2322 \n",
      "Accuracy: 9329/10000 (93.29%)\n",
      "\n",
      "Round  26, Average loss 0.232 Test accuracy 93.290\n",
      "selected users: [ 0  1  3  4  5  8  9 14 17 19 20 21 22 23 24 25 26 30]\n",
      "\n",
      "Test set: Average loss: 0.1654 \n",
      "Accuracy: 9512/10000 (95.12%)\n",
      "\n",
      "Round  27, Average loss 0.165 Test accuracy 95.120\n",
      "selected users: [ 0  3  4  5  6  8 11 12 16 18 20 21 22 23 25 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.1734 \n",
      "Accuracy: 9483/10000 (94.83%)\n",
      "\n",
      "Round  28, Average loss 0.173 Test accuracy 94.830\n",
      "selected users: [ 2  4  5  7  8 10 12 13 14 15 16 18 21 22 24 26 29 30]\n",
      "\n",
      "Test set: Average loss: 0.3262 \n",
      "Accuracy: 9324/10000 (93.24%)\n",
      "\n",
      "Round  29, Average loss 0.326 Test accuracy 93.240\n",
      "(m= 18 )  9 -th Trial!!\n",
      "selected users: [ 2  3  5  6  9 10 11 13 14 16 18 19 22 23 24 26 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  3  4  6  9 11 13 14 15 16 17 18 19 20 22 24 27 29]\n",
      "\n",
      "Test set: Average loss: 2.2324 \n",
      "Accuracy: 7830/10000 (78.30%)\n",
      "\n",
      "Round   1, Average loss 2.232 Test accuracy 78.300\n",
      "selected users: [ 0  1  4  6  7  9 10 11 12 14 18 23 24 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.1296 \n",
      "Accuracy: 9345/10000 (93.45%)\n",
      "\n",
      "Round   2, Average loss 1.130 Test accuracy 93.450\n",
      "selected users: [ 1  3  4  6  8 10 11 12 13 15 18 19 20 22 24 25 28 30]\n",
      "\n",
      "Test set: Average loss: 0.2293 \n",
      "Accuracy: 9391/10000 (93.91%)\n",
      "\n",
      "Round   3, Average loss 0.229 Test accuracy 93.910\n",
      "selected users: [ 0  2  3  4  6  7  9 10 11 12 13 15 18 21 22 24 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2155 \n",
      "Accuracy: 9337/10000 (93.37%)\n",
      "\n",
      "Round   4, Average loss 0.216 Test accuracy 93.370\n",
      "selected users: [ 1  3  4  5  7 11 14 15 16 20 22 23 24 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.1850 \n",
      "Accuracy: 9441/10000 (94.41%)\n",
      "\n",
      "Round   5, Average loss 0.185 Test accuracy 94.410\n",
      "selected users: [ 2  3  4  5  6  7  9 10 11 12 14 17 18 20 21 23 27 28]\n",
      "\n",
      "Test set: Average loss: 0.4532 \n",
      "Accuracy: 9529/10000 (95.29%)\n",
      "\n",
      "Round   6, Average loss 0.453 Test accuracy 95.290\n",
      "selected users: [ 0  2  3  7 11 13 15 16 18 19 21 22 23 25 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.1884 \n",
      "Accuracy: 9433/10000 (94.33%)\n",
      "\n",
      "Round   7, Average loss 0.188 Test accuracy 94.330\n",
      "selected users: [ 0  1  3  5  6  7  8 10 13 14 15 16 17 21 24 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.2503 \n",
      "Accuracy: 9380/10000 (93.80%)\n",
      "\n",
      "Round   8, Average loss 0.250 Test accuracy 93.800\n",
      "selected users: [ 2  3  5  6  8 10 11 12 13 14 16 19 21 22 24 26 27 29]\n",
      "\n",
      "Test set: Average loss: 0.2180 \n",
      "Accuracy: 9368/10000 (93.68%)\n",
      "\n",
      "Round   9, Average loss 0.218 Test accuracy 93.680\n",
      "selected users: [ 0  1  2  4  5  7  8 11 13 16 17 19 21 22 24 25 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2007 \n",
      "Accuracy: 9418/10000 (94.18%)\n",
      "\n",
      "Round  10, Average loss 0.201 Test accuracy 94.180\n",
      "selected users: [ 0  1  3  4  5  6  8 10 13 14 15 16 17 18 21 24 27 28]\n",
      "\n",
      "Test set: Average loss: 0.1886 \n",
      "Accuracy: 9429/10000 (94.29%)\n",
      "\n",
      "Round  11, Average loss 0.189 Test accuracy 94.290\n",
      "selected users: [ 0  1  3  5  7  8  9 10 12 13 14 16 22 23 25 26 28 29]\n",
      "\n",
      "Test set: Average loss: 0.2840 \n",
      "Accuracy: 9391/10000 (93.91%)\n",
      "\n",
      "Round  12, Average loss 0.284 Test accuracy 93.910\n",
      "selected users: [ 0  1  4  5  6  7  8  9 11 15 17 18 20 22 23 26 27 29]\n",
      "\n",
      "Test set: Average loss: 0.1939 \n",
      "Accuracy: 9433/10000 (94.33%)\n",
      "\n",
      "Round  13, Average loss 0.194 Test accuracy 94.330\n",
      "selected users: [ 2  3  4  5  6  7  9 10 14 15 16 17 18 20 21 23 27 29]\n",
      "\n",
      "Test set: Average loss: 0.1703 \n",
      "Accuracy: 9489/10000 (94.89%)\n",
      "\n",
      "Round  14, Average loss 0.170 Test accuracy 94.890\n",
      "selected users: [ 0  1  6  7  9 10 11 12 13 14 17 18 20 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.3094 \n",
      "Accuracy: 9603/10000 (96.03%)\n",
      "\n",
      "Round  15, Average loss 0.309 Test accuracy 96.030\n",
      "selected users: [ 0  2  6  7 10 11 12 13 17 18 20 21 22 23 24 25 27 30]\n",
      "\n",
      "Test set: Average loss: 0.6945 \n",
      "Accuracy: 9611/10000 (96.11%)\n",
      "\n",
      "Round  16, Average loss 0.694 Test accuracy 96.110\n",
      "selected users: [ 1  2  3  6  7  8  9 11 14 15 16 17 18 23 24 26 29 30]\n",
      "\n",
      "Test set: Average loss: 0.1993 \n",
      "Accuracy: 9465/10000 (94.65%)\n",
      "\n",
      "Round  17, Average loss 0.199 Test accuracy 94.650\n",
      "selected users: [ 0  1  2  4  5  7  9 10 11 12 13 14 16 17 19 21 25 30]\n",
      "\n",
      "Test set: Average loss: 0.1771 \n",
      "Accuracy: 9503/10000 (95.03%)\n",
      "\n",
      "Round  18, Average loss 0.177 Test accuracy 95.030\n",
      "selected users: [ 2  3  5  6  9 10 12 13 15 16 18 19 22 23 24 25 27 29]\n",
      "\n",
      "Test set: Average loss: 0.1629 \n",
      "Accuracy: 9494/10000 (94.94%)\n",
      "\n",
      "Round  19, Average loss 0.163 Test accuracy 94.940\n",
      "selected users: [ 0  2  3  4  5  6  7  8  9 11 13 20 21 23 25 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.1569 \n",
      "Accuracy: 9537/10000 (95.37%)\n",
      "\n",
      "Round  20, Average loss 0.157 Test accuracy 95.370\n",
      "selected users: [ 0  1  2  6  7  8 10 14 15 16 19 21 22 23 24 26 27 28]\n",
      "\n",
      "Test set: Average loss: 0.1916 \n",
      "Accuracy: 9476/10000 (94.76%)\n",
      "\n",
      "Round  21, Average loss 0.192 Test accuracy 94.760\n",
      "selected users: [ 5  6  7  8  9 10 11 13 15 16 18 21 22 23 24 26 27 28]\n",
      "\n",
      "Test set: Average loss: 0.4515 \n",
      "Accuracy: 9306/10000 (93.06%)\n",
      "\n",
      "Round  22, Average loss 0.451 Test accuracy 93.060\n",
      "selected users: [ 2  4  5  6  8 10 12 14 15 16 18 19 21 22 24 27 28 29]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1752 \n",
      "Accuracy: 9466/10000 (94.66%)\n",
      "\n",
      "Round  23, Average loss 0.175 Test accuracy 94.660\n",
      "selected users: [ 0  1  3  6  8  9 10 11 14 15 16 17 18 19 23 26 27 30]\n",
      "\n",
      "Test set: Average loss: 0.1732 \n",
      "Accuracy: 9585/10000 (95.85%)\n",
      "\n",
      "Round  24, Average loss 0.173 Test accuracy 95.850\n",
      "selected users: [ 2  3  6  8 10 13 14 15 16 17 18 20 22 23 24 25 26 27]\n",
      "\n",
      "Test set: Average loss: 0.1472 \n",
      "Accuracy: 9568/10000 (95.68%)\n",
      "\n",
      "Round  25, Average loss 0.147 Test accuracy 95.680\n",
      "selected users: [ 1  2  3  4  6  7  8 11 12 13 17 18 19 20 21 26 29 30]\n",
      "\n",
      "Test set: Average loss: 0.3459 \n",
      "Accuracy: 9422/10000 (94.22%)\n",
      "\n",
      "Round  26, Average loss 0.346 Test accuracy 94.220\n",
      "selected users: [ 2  3  7 10 12 13 15 16 17 18 19 20 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.1743 \n",
      "Accuracy: 9520/10000 (95.20%)\n",
      "\n",
      "Round  27, Average loss 0.174 Test accuracy 95.200\n",
      "selected users: [ 0  1  5  6  7  8  9 12 15 16 18 19 20 21 22 23 24 26]\n",
      "\n",
      "Test set: Average loss: 0.4410 \n",
      "Accuracy: 9320/10000 (93.20%)\n",
      "\n",
      "Round  28, Average loss 0.441 Test accuracy 93.200\n",
      "selected users: [ 0  4  6  7  9 10 12 13 14 15 16 17 18 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2002 \n",
      "Accuracy: 9482/10000 (94.82%)\n",
      "\n",
      "Round  29, Average loss 0.200 Test accuracy 94.820\n",
      "number of results: 21\n",
      "(m= 21 )  0 -th Trial!!\n",
      "selected users: [ 0  1  2  5  6  7  9 10 13 14 16 18 19 20 22 23 24 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  1  2  5  6  7  8  9 10 12 13 14 17 18 19 21 25 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 2.1931 \n",
      "Accuracy: 7293/10000 (72.93%)\n",
      "\n",
      "Round   1, Average loss 2.193 Test accuracy 72.930\n",
      "selected users: [ 2  3  4  5  6  9 10 12 13 14 15 16 18 19 21 22 23 24 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.2917 \n",
      "Accuracy: 9241/10000 (92.41%)\n",
      "\n",
      "Round   2, Average loss 1.292 Test accuracy 92.410\n",
      "selected users: [ 0  2  3  5  6  9 11 12 13 15 16 19 20 21 22 24 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.7179 \n",
      "Accuracy: 9313/10000 (93.13%)\n",
      "\n",
      "Round   3, Average loss 0.718 Test accuracy 93.130\n",
      "selected users: [ 1  2  3  5  6  7  8  9 10 11 12 14 17 18 22 23 24 25 26 27 30]\n",
      "\n",
      "Test set: Average loss: 0.8146 \n",
      "Accuracy: 9458/10000 (94.58%)\n",
      "\n",
      "Round   4, Average loss 0.815 Test accuracy 94.580\n",
      "selected users: [ 0  2  3  5  6  8  9 10 12 13 15 17 18 19 20 22 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.6609 \n",
      "Accuracy: 9423/10000 (94.23%)\n",
      "\n",
      "Round   5, Average loss 0.661 Test accuracy 94.230\n",
      "selected users: [ 1  2  7  9 11 12 13 14 15 16 17 18 19 20 21 22 24 25 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.1945 \n",
      "Accuracy: 9425/10000 (94.25%)\n",
      "\n",
      "Round   6, Average loss 1.194 Test accuracy 94.250\n",
      "selected users: [ 1  2  4  5  6  7  9 12 14 15 16 17 18 19 20 21 22 24 25 28 29]\n",
      "\n",
      "Test set: Average loss: 1.0270 \n",
      "Accuracy: 9476/10000 (94.76%)\n",
      "\n",
      "Round   7, Average loss 1.027 Test accuracy 94.760\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 11 12 13 14 18 20 21 23 26 28 29]\n",
      "\n",
      "Test set: Average loss: 0.8110 \n",
      "Accuracy: 9410/10000 (94.10%)\n",
      "\n",
      "Round   8, Average loss 0.811 Test accuracy 94.100\n",
      "selected users: [ 0  1  3  5  6  8  9 10 12 14 17 18 19 20 21 22 23 24 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.8233 \n",
      "Accuracy: 9442/10000 (94.42%)\n",
      "\n",
      "Round   9, Average loss 0.823 Test accuracy 94.420\n",
      "selected users: [ 0  1  3  4  5  6  7 10 11 12 13 14 16 17 19 20 21 23 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.7824 \n",
      "Accuracy: 9424/10000 (94.24%)\n",
      "\n",
      "Round  10, Average loss 0.782 Test accuracy 94.240\n",
      "selected users: [ 0  1  2  3  5  6  8  9 10 12 13 14 15 16 17 20 21 22 25 26 29]\n",
      "\n",
      "Test set: Average loss: 0.9751 \n",
      "Accuracy: 9430/10000 (94.30%)\n",
      "\n",
      "Round  11, Average loss 0.975 Test accuracy 94.300\n",
      "selected users: [ 0  1  4  6  7  9 10 11 12 13 14 15 16 17 18 20 21 23 26 28 29]\n",
      "\n",
      "Test set: Average loss: 0.7585 \n",
      "Accuracy: 9400/10000 (94.00%)\n",
      "\n",
      "Round  12, Average loss 0.758 Test accuracy 94.000\n",
      "selected users: [ 1  2  4  5  6  7  8  9 10 13 15 16 17 18 20 21 22 25 26 27 30]\n",
      "\n",
      "Test set: Average loss: 0.8683 \n",
      "Accuracy: 9455/10000 (94.55%)\n",
      "\n",
      "Round  13, Average loss 0.868 Test accuracy 94.550\n",
      "selected users: [ 0  2  3  5  6  7  8  9 11 15 16 17 18 19 20 22 23 24 26 28 30]\n",
      "\n",
      "Test set: Average loss: 0.4645 \n",
      "Accuracy: 9410/10000 (94.10%)\n",
      "\n",
      "Round  14, Average loss 0.464 Test accuracy 94.100\n",
      "selected users: [ 2  3  4  5  7  8  9 11 13 14 15 16 17 18 19 21 22 23 25 29 30]\n",
      "\n",
      "Test set: Average loss: 0.8653 \n",
      "Accuracy: 9462/10000 (94.62%)\n",
      "\n",
      "Round  15, Average loss 0.865 Test accuracy 94.620\n",
      "selected users: [ 0  1  2  3  5  6  7  9 10 13 14 15 16 18 19 20 21 24 25 26 29]\n",
      "\n",
      "Test set: Average loss: 0.5484 \n",
      "Accuracy: 9323/10000 (93.23%)\n",
      "\n",
      "Round  16, Average loss 0.548 Test accuracy 93.230\n",
      "selected users: [ 0  1  3  4  5  6  8  9 11 12 15 16 17 18 19 21 22 23 25 26 29]\n",
      "\n",
      "Test set: Average loss: 1.0804 \n",
      "Accuracy: 9452/10000 (94.52%)\n",
      "\n",
      "Round  17, Average loss 1.080 Test accuracy 94.520\n",
      "selected users: [ 0  1  2  4  5  6  7 12 14 15 16 20 21 22 23 24 25 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.7921 \n",
      "Accuracy: 9446/10000 (94.46%)\n",
      "\n",
      "Round  18, Average loss 0.792 Test accuracy 94.460\n",
      "selected users: [ 0  1  4  5  6  7  8  9 12 13 14 16 18 19 20 21 22 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.6071 \n",
      "Accuracy: 9473/10000 (94.73%)\n",
      "\n",
      "Round  19, Average loss 0.607 Test accuracy 94.730\n",
      "selected users: [ 0  1  2  3  4  6  7  8 13 15 17 19 20 21 22 23 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.0543 \n",
      "Accuracy: 9424/10000 (94.24%)\n",
      "\n",
      "Round  20, Average loss 1.054 Test accuracy 94.240\n",
      "selected users: [ 0  1  2  3  4  5  6  9 10 12 13 14 16 17 18 19 21 22 23 27 28]\n",
      "\n",
      "Test set: Average loss: 1.0850 \n",
      "Accuracy: 9426/10000 (94.26%)\n",
      "\n",
      "Round  21, Average loss 1.085 Test accuracy 94.260\n",
      "selected users: [ 1  3  4  5  7  8 10 11 12 13 15 17 18 19 20 21 22 23 24 26 28]\n",
      "\n",
      "Test set: Average loss: 0.6428 \n",
      "Accuracy: 9362/10000 (93.62%)\n",
      "\n",
      "Round  22, Average loss 0.643 Test accuracy 93.620\n",
      "selected users: [ 0  3  4  5  8  9 10 12 13 15 16 17 19 20 21 23 24 25 26 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4586 \n",
      "Accuracy: 9308/10000 (93.08%)\n",
      "\n",
      "Round  23, Average loss 0.459 Test accuracy 93.080\n",
      "selected users: [ 0  2  3  4  7  9 10 11 12 13 15 16 18 20 22 23 24 25 26 28 30]\n",
      "\n",
      "Test set: Average loss: 0.7032 \n",
      "Accuracy: 9399/10000 (93.99%)\n",
      "\n",
      "Round  24, Average loss 0.703 Test accuracy 93.990\n",
      "selected users: [ 0  1  2  3  4  8 10 11 12 14 15 16 17 18 21 22 24 25 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.7914 \n",
      "Accuracy: 9387/10000 (93.87%)\n",
      "\n",
      "Round  25, Average loss 0.791 Test accuracy 93.870\n",
      "selected users: [ 0  3  4  5  8  9 10 11 13 15 16 17 18 19 21 22 23 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.4135 \n",
      "Accuracy: 9295/10000 (92.95%)\n",
      "\n",
      "Round  26, Average loss 0.413 Test accuracy 92.950\n",
      "selected users: [ 0  1  2  3  4  6  7  8  9 10 11 13 14 15 16 17 18 21 22 23 28]\n",
      "\n",
      "Test set: Average loss: 0.9807 \n",
      "Accuracy: 9357/10000 (93.57%)\n",
      "\n",
      "Round  27, Average loss 0.981 Test accuracy 93.570\n",
      "selected users: [ 2  4  5  6  7  8 10 11 13 14 16 17 19 20 21 22 24 25 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.8904 \n",
      "Accuracy: 9410/10000 (94.10%)\n",
      "\n",
      "Round  28, Average loss 0.890 Test accuracy 94.100\n",
      "selected users: [ 1  2  3  4  7  8  9 10 11 12 13 16 17 18 21 22 23 24 25 26 30]\n",
      "\n",
      "Test set: Average loss: 0.6119 \n",
      "Accuracy: 9434/10000 (94.34%)\n",
      "\n",
      "Round  29, Average loss 0.612 Test accuracy 94.340\n",
      "(m= 21 )  1 -th Trial!!\n",
      "selected users: [ 0  2  4  5  6  7  9 10 11 13 14 15 16 20 21 22 24 25 26 27 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  1  2  4  5  6  7  8  9 11 12 13 14 15 16 17 18 19 22 23 28]\n",
      "\n",
      "Test set: Average loss: 2.2640 \n",
      "Accuracy: 6130/10000 (61.30%)\n",
      "\n",
      "Round   1, Average loss 2.264 Test accuracy 61.300\n",
      "selected users: [ 2  3  4  5  6  7  8  9 10 11 12 14 17 18 21 23 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.9342 \n",
      "Accuracy: 9194/10000 (91.94%)\n",
      "\n",
      "Round   2, Average loss 0.934 Test accuracy 91.940\n",
      "selected users: [ 0  1  2  6  8  9 10 12 13 14 15 16 17 18 19 20 22 23 25 27 30]\n",
      "\n",
      "Test set: Average loss: 0.7745 \n",
      "Accuracy: 9401/10000 (94.01%)\n",
      "\n",
      "Round   3, Average loss 0.775 Test accuracy 94.010\n",
      "selected users: [ 0  1  2  3  4  5  6  7 11 12 13 14 15 16 17 18 19 22 24 28 29]\n",
      "\n",
      "Test set: Average loss: 1.1367 \n",
      "Accuracy: 9460/10000 (94.60%)\n",
      "\n",
      "Round   4, Average loss 1.137 Test accuracy 94.600\n",
      "selected users: [ 1  2  3  5  7  8  9 10 11 13 14 15 16 17 18 21 22 23 25 26 29]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.3062 \n",
      "Accuracy: 9371/10000 (93.71%)\n",
      "\n",
      "Round   5, Average loss 0.306 Test accuracy 93.710\n",
      "selected users: [ 0  1  2  3  4  5  6  7  9 10 11 18 20 21 22 23 24 25 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.0633 \n",
      "Accuracy: 9489/10000 (94.89%)\n",
      "\n",
      "Round   6, Average loss 1.063 Test accuracy 94.890\n",
      "selected users: [ 0  2  4  5  6  7 11 12 13 14 15 17 18 20 21 24 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.3036 \n",
      "Accuracy: 9410/10000 (94.10%)\n",
      "\n",
      "Round   7, Average loss 0.304 Test accuracy 94.100\n",
      "selected users: [ 0  1  2  4  5  6  7  8 11 12 14 15 17 20 21 22 23 25 26 27 28]\n",
      "\n",
      "Test set: Average loss: 1.1011 \n",
      "Accuracy: 9499/10000 (94.99%)\n",
      "\n",
      "Round   8, Average loss 1.101 Test accuracy 94.990\n",
      "selected users: [ 2  3  4  6  7 10 13 14 15 16 17 18 20 21 22 23 24 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.3509 \n",
      "Accuracy: 9470/10000 (94.70%)\n",
      "\n",
      "Round   9, Average loss 0.351 Test accuracy 94.700\n",
      "selected users: [ 1  3  4  5  6  8  9 10 11 13 14 15 16 17 18 20 21 24 25 29 30]\n",
      "\n",
      "Test set: Average loss: 0.6409 \n",
      "Accuracy: 9536/10000 (95.36%)\n",
      "\n",
      "Round  10, Average loss 0.641 Test accuracy 95.360\n",
      "selected users: [ 1  2  4  6  7  9 10 11 13 14 16 17 18 19 21 22 23 24 25 28 29]\n",
      "\n",
      "Test set: Average loss: 0.4602 \n",
      "Accuracy: 9511/10000 (95.11%)\n",
      "\n",
      "Round  11, Average loss 0.460 Test accuracy 95.110\n",
      "selected users: [ 0  1  5  7  8  9 10 11 13 15 16 17 18 20 21 22 23 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.2477 \n",
      "Accuracy: 9308/10000 (93.08%)\n",
      "\n",
      "Round  12, Average loss 0.248 Test accuracy 93.080\n",
      "selected users: [ 0  1  3  5  8  9 10 11 12 14 15 17 18 19 20 22 23 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2229 \n",
      "Accuracy: 9318/10000 (93.18%)\n",
      "\n",
      "Round  13, Average loss 0.223 Test accuracy 93.180\n",
      "selected users: [ 2  3  4  6  7  8 11 12 13 15 17 18 20 21 22 23 24 25 26 27 29]\n",
      "\n",
      "Test set: Average loss: 0.4617 \n",
      "Accuracy: 9464/10000 (94.64%)\n",
      "\n",
      "Round  14, Average loss 0.462 Test accuracy 94.640\n",
      "selected users: [ 0  3  5  6  7  8  9 10 12 13 14 16 17 20 22 24 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4108 \n",
      "Accuracy: 9433/10000 (94.33%)\n",
      "\n",
      "Round  15, Average loss 0.411 Test accuracy 94.330\n",
      "selected users: [ 0  2  5  6  7 10 11 12 13 14 15 16 17 18 19 22 23 25 26 27 28]\n",
      "\n",
      "Test set: Average loss: 0.3195 \n",
      "Accuracy: 9431/10000 (94.31%)\n",
      "\n",
      "Round  16, Average loss 0.319 Test accuracy 94.310\n",
      "selected users: [ 0  1  2  3  4  6  7 12 14 15 17 19 20 22 23 24 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.7013 \n",
      "Accuracy: 9542/10000 (95.42%)\n",
      "\n",
      "Round  17, Average loss 0.701 Test accuracy 95.420\n",
      "selected users: [ 0  1  4  5  6  7 10 11 12 14 16 17 19 20 21 22 23 25 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.5996 \n",
      "Accuracy: 9532/10000 (95.32%)\n",
      "\n",
      "Round  18, Average loss 0.600 Test accuracy 95.320\n",
      "selected users: [ 0  1  2  3  5  6  7  9 10 11 13 14 16 19 22 23 24 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.6364 \n",
      "Accuracy: 9487/10000 (94.87%)\n",
      "\n",
      "Round  19, Average loss 0.636 Test accuracy 94.870\n",
      "selected users: [ 0  1  2  4  6  7 11 12 13 14 15 16 18 19 20 22 23 25 26 29 30]\n",
      "\n",
      "Test set: Average loss: 0.3802 \n",
      "Accuracy: 9453/10000 (94.53%)\n",
      "\n",
      "Round  20, Average loss 0.380 Test accuracy 94.530\n",
      "selected users: [ 0  1  4  7  8  9 10 13 14 16 18 19 21 22 23 24 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.5037 \n",
      "Accuracy: 9480/10000 (94.80%)\n",
      "\n",
      "Round  21, Average loss 0.504 Test accuracy 94.800\n",
      "selected users: [ 0  1  4  6  7  8  9 10 12 14 18 19 20 21 22 23 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.1261 \n",
      "Accuracy: 9526/10000 (95.26%)\n",
      "\n",
      "Round  22, Average loss 1.126 Test accuracy 95.260\n",
      "selected users: [ 1  2  3  6  7 11 12 13 14 16 17 18 19 20 22 23 24 25 26 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4666 \n",
      "Accuracy: 9487/10000 (94.87%)\n",
      "\n",
      "Round  23, Average loss 0.467 Test accuracy 94.870\n",
      "selected users: [ 1  2  3  4  5  7  8  9 10 14 16 18 19 20 21 23 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2840 \n",
      "Accuracy: 9438/10000 (94.38%)\n",
      "\n",
      "Round  24, Average loss 0.284 Test accuracy 94.380\n",
      "selected users: [ 2  4  5  9 10 11 12 14 15 16 18 20 21 22 23 24 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.5328 \n",
      "Accuracy: 9519/10000 (95.19%)\n",
      "\n",
      "Round  25, Average loss 0.533 Test accuracy 95.190\n",
      "selected users: [ 0  1  5  6  7  8  9 10 11 12 13 14 16 17 18 19 21 23 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.7010 \n",
      "Accuracy: 9463/10000 (94.63%)\n",
      "\n",
      "Round  26, Average loss 0.701 Test accuracy 94.630\n",
      "selected users: [ 0  1  3  6  8 10 11 12 13 14 15 16 17 18 19 20 22 24 26 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2999 \n",
      "Accuracy: 9440/10000 (94.40%)\n",
      "\n",
      "Round  27, Average loss 0.300 Test accuracy 94.400\n",
      "selected users: [ 0  1  2  4  5  6  8 11 12 13 15 16 17 19 20 21 22 24 26 27 28]\n",
      "\n",
      "Test set: Average loss: 0.7748 \n",
      "Accuracy: 9462/10000 (94.62%)\n",
      "\n",
      "Round  28, Average loss 0.775 Test accuracy 94.620\n",
      "selected users: [ 0  1  2  3  4  5  6  9 10 12 14 15 16 17 18 20 21 22 23 28 29]\n",
      "\n",
      "Test set: Average loss: 0.8739 \n",
      "Accuracy: 9490/10000 (94.90%)\n",
      "\n",
      "Round  29, Average loss 0.874 Test accuracy 94.900\n",
      "(m= 21 )  2 -th Trial!!\n",
      "selected users: [ 4  5  6  7  8 10 11 12 14 15 16 17 18 19 21 22 24 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  1  2  3  4  8 10 11 14 15 17 19 20 21 22 23 24 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.2771 \n",
      "Accuracy: 5167/10000 (51.67%)\n",
      "\n",
      "Round   1, Average loss 2.277 Test accuracy 51.670\n",
      "selected users: [ 1  2  3  6  8  9 14 15 16 17 18 19 20 21 22 23 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.2526 \n",
      "Accuracy: 8936/10000 (89.36%)\n",
      "\n",
      "Round   2, Average loss 1.253 Test accuracy 89.360\n",
      "selected users: [ 0  2  3  4  5  6  7  8 10 14 15 16 17 19 23 24 25 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.8160 \n",
      "Accuracy: 9392/10000 (93.92%)\n",
      "\n",
      "Round   3, Average loss 0.816 Test accuracy 93.920\n",
      "selected users: [ 1  2  3  4  5  7  8  9 11 12 15 17 18 20 21 22 23 24 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.6705 \n",
      "Accuracy: 9467/10000 (94.67%)\n",
      "\n",
      "Round   4, Average loss 0.671 Test accuracy 94.670\n",
      "selected users: [ 2  6  7  8  9 10 11 12 13 14 17 19 20 21 22 23 25 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 1.0785 \n",
      "Accuracy: 9531/10000 (95.31%)\n",
      "\n",
      "Round   5, Average loss 1.079 Test accuracy 95.310\n",
      "selected users: [ 0  2  3  5  6  7  9 10 11 12 13 14 15 18 20 22 24 25 26 29 30]\n",
      "\n",
      "Test set: Average loss: 0.3695 \n",
      "Accuracy: 9408/10000 (94.08%)\n",
      "\n",
      "Round   6, Average loss 0.369 Test accuracy 94.080\n",
      "selected users: [ 0  1  3  5  6  7  8  9 11 12 13 17 18 20 22 23 24 25 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.8268 \n",
      "Accuracy: 9450/10000 (94.50%)\n",
      "\n",
      "Round   7, Average loss 0.827 Test accuracy 94.500\n",
      "selected users: [ 1  3  5  7  8  9 10 11 12 13 17 19 20 21 23 24 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.8062 \n",
      "Accuracy: 9468/10000 (94.68%)\n",
      "\n",
      "Round   8, Average loss 0.806 Test accuracy 94.680\n",
      "selected users: [ 0  1  3  5  6  7  8 10 13 14 15 17 19 20 21 22 23 25 26 27 28]\n",
      "\n",
      "Test set: Average loss: 0.6495 \n",
      "Accuracy: 9387/10000 (93.87%)\n",
      "\n",
      "Round   9, Average loss 0.649 Test accuracy 93.870\n",
      "selected users: [ 1  2  3  4  6  8 10 11 12 14 15 16 18 19 20 22 23 24 25 26 28]\n",
      "\n",
      "Test set: Average loss: 0.8136 \n",
      "Accuracy: 9430/10000 (94.30%)\n",
      "\n",
      "Round  10, Average loss 0.814 Test accuracy 94.300\n",
      "selected users: [ 0  1  5  7  8  9 10 12 13 14 15 16 17 18 21 22 23 25 26 27 28]\n",
      "\n",
      "Test set: Average loss: 0.3966 \n",
      "Accuracy: 9340/10000 (93.40%)\n",
      "\n",
      "Round  11, Average loss 0.397 Test accuracy 93.400\n",
      "selected users: [ 0  1  3  5  6  7  8  9 10 12 13 15 17 18 19 21 23 25 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.5762 \n",
      "Accuracy: 9401/10000 (94.01%)\n",
      "\n",
      "Round  12, Average loss 0.576 Test accuracy 94.010\n",
      "selected users: [ 2  3  4  7  8 11 12 14 15 17 18 19 20 21 22 23 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.7097 \n",
      "Accuracy: 9467/10000 (94.67%)\n",
      "\n",
      "Round  13, Average loss 0.710 Test accuracy 94.670\n",
      "selected users: [ 0  1  2  3  4  6  7  8  9 10 11 12 13 15 16 17 18 20 23 27 29]\n",
      "\n",
      "Test set: Average loss: 0.8535 \n",
      "Accuracy: 9475/10000 (94.75%)\n",
      "\n",
      "Round  14, Average loss 0.854 Test accuracy 94.750\n",
      "selected users: [ 0  2  3  5  6  8  9 11 13 15 16 17 18 20 21 22 23 25 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4115 \n",
      "Accuracy: 9429/10000 (94.29%)\n",
      "\n",
      "Round  15, Average loss 0.412 Test accuracy 94.290\n",
      "selected users: [ 0  2  4  5  7  8  9 10 11 12 13 14 15 17 18 20 24 25 26 27 28]\n",
      "\n",
      "Test set: Average loss: 0.5769 \n",
      "Accuracy: 9427/10000 (94.27%)\n",
      "\n",
      "Round  16, Average loss 0.577 Test accuracy 94.270\n",
      "selected users: [ 0  1  2  3  6  7  8  9 11 13 14 15 17 18 20 21 22 23 27 28 29]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.0889 \n",
      "Accuracy: 9437/10000 (94.37%)\n",
      "\n",
      "Round  17, Average loss 1.089 Test accuracy 94.370\n",
      "selected users: [ 0  1  3  4  6  9 10 12 14 15 16 17 19 21 22 23 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.5879 \n",
      "Accuracy: 9462/10000 (94.62%)\n",
      "\n",
      "Round  18, Average loss 0.588 Test accuracy 94.620\n",
      "selected users: [ 3  5  6  7  8  9 10 11 14 15 16 17 18 19 20 23 24 25 26 28 29]\n",
      "\n",
      "Test set: Average loss: 0.9047 \n",
      "Accuracy: 9426/10000 (94.26%)\n",
      "\n",
      "Round  19, Average loss 0.905 Test accuracy 94.260\n",
      "selected users: [ 0  2  3  6  7  8 10 11 14 15 16 17 18 19 20 22 23 25 26 27 29]\n",
      "\n",
      "Test set: Average loss: 0.5649 \n",
      "Accuracy: 9443/10000 (94.43%)\n",
      "\n",
      "Round  20, Average loss 0.565 Test accuracy 94.430\n",
      "selected users: [ 0  1  3  4  5  7  8  9 10 12 17 18 19 22 23 24 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.0586 \n",
      "Accuracy: 9509/10000 (95.09%)\n",
      "\n",
      "Round  21, Average loss 1.059 Test accuracy 95.090\n",
      "selected users: [ 0  1  2  3  5  6  8  9 10 11 14 15 17 19 20 22 23 25 26 27 29]\n",
      "\n",
      "Test set: Average loss: 1.1026 \n",
      "Accuracy: 9448/10000 (94.48%)\n",
      "\n",
      "Round  22, Average loss 1.103 Test accuracy 94.480\n",
      "selected users: [ 0  1  2  3  5  6  7  8  9 10 14 16 20 21 22 24 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.9400 \n",
      "Accuracy: 9430/10000 (94.30%)\n",
      "\n",
      "Round  23, Average loss 0.940 Test accuracy 94.300\n",
      "selected users: [ 1  2  3  4  5  6  7  8 10 15 17 18 20 21 22 23 24 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.4227 \n",
      "Accuracy: 9438/10000 (94.38%)\n",
      "\n",
      "Round  24, Average loss 0.423 Test accuracy 94.380\n",
      "selected users: [ 0  2  4  5  7  8  9 10 11 12 14 15 18 21 23 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4756 \n",
      "Accuracy: 9437/10000 (94.37%)\n",
      "\n",
      "Round  25, Average loss 0.476 Test accuracy 94.370\n",
      "selected users: [ 0  2  3  5  6  7  8 10 12 15 16 19 20 21 23 24 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.3489 \n",
      "Accuracy: 9397/10000 (93.97%)\n",
      "\n",
      "Round  26, Average loss 0.349 Test accuracy 93.970\n",
      "selected users: [ 0  1  3  6  7  8  9 10 11 12 13 16 21 22 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.5605 \n",
      "Accuracy: 9438/10000 (94.38%)\n",
      "\n",
      "Round  27, Average loss 0.561 Test accuracy 94.380\n",
      "selected users: [ 0  1  2  4  5  7  9 10 12 13 14 15 17 18 19 20 22 23 24 28 30]\n",
      "\n",
      "Test set: Average loss: 0.8474 \n",
      "Accuracy: 9491/10000 (94.91%)\n",
      "\n",
      "Round  28, Average loss 0.847 Test accuracy 94.910\n",
      "selected users: [ 0  1  3  4  5  7 11 12 13 14 15 16 17 18 19 20 21 23 25 27 28]\n",
      "\n",
      "Test set: Average loss: 0.9039 \n",
      "Accuracy: 9474/10000 (94.74%)\n",
      "\n",
      "Round  29, Average loss 0.904 Test accuracy 94.740\n",
      "(m= 21 )  3 -th Trial!!\n",
      "selected users: [ 0  1  2  3  6  7  9 10 12 14 15 16 19 20 21 22 23 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  3  4  5  6  7  8  9 10 11 12 13 15 17 19 20 22 23 25 29 30]\n",
      "\n",
      "Test set: Average loss: 2.2816 \n",
      "Accuracy: 5344/10000 (53.44%)\n",
      "\n",
      "Round   1, Average loss 2.282 Test accuracy 53.440\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 14 16 17 18 19 20 21 24 27 29]\n",
      "\n",
      "Test set: Average loss: 1.4520 \n",
      "Accuracy: 9180/10000 (91.80%)\n",
      "\n",
      "Round   2, Average loss 1.452 Test accuracy 91.800\n",
      "selected users: [ 0  1  5  6  7  9 11 12 14 15 17 18 19 20 22 23 24 25 26 29 30]\n",
      "\n",
      "Test set: Average loss: 0.5397 \n",
      "Accuracy: 9369/10000 (93.69%)\n",
      "\n",
      "Round   3, Average loss 0.540 Test accuracy 93.690\n",
      "selected users: [ 0  2  4  5  6  7  8 11 12 13 14 15 16 17 19 20 21 24 25 27 29]\n",
      "\n",
      "Test set: Average loss: 0.5246 \n",
      "Accuracy: 9455/10000 (94.55%)\n",
      "\n",
      "Round   4, Average loss 0.525 Test accuracy 94.550\n",
      "selected users: [ 0  1  2  3  4  5  6  8 11 12 17 19 20 21 23 24 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.5519 \n",
      "Accuracy: 9423/10000 (94.23%)\n",
      "\n",
      "Round   5, Average loss 0.552 Test accuracy 94.230\n",
      "selected users: [ 0  2  3  4  5  7  8  9 10 11 13 14 15 17 19 21 22 23 26 28 29]\n",
      "\n",
      "Test set: Average loss: 0.2345 \n",
      "Accuracy: 9413/10000 (94.13%)\n",
      "\n",
      "Round   6, Average loss 0.234 Test accuracy 94.130\n",
      "selected users: [ 1  3  4  6  7  8  9 10 12 13 14 16 17 19 20 22 23 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.4101 \n",
      "Accuracy: 9466/10000 (94.66%)\n",
      "\n",
      "Round   7, Average loss 0.410 Test accuracy 94.660\n",
      "selected users: [ 0  2  3  6  7  8  9 10 11 12 13 15 16 17 18 19 24 25 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4881 \n",
      "Accuracy: 9499/10000 (94.99%)\n",
      "\n",
      "Round   8, Average loss 0.488 Test accuracy 94.990\n",
      "selected users: [ 0  1  3  4  5  7  8  9 10 12 13 14 16 18 21 22 23 25 26 29 30]\n",
      "\n",
      "Test set: Average loss: 0.3015 \n",
      "Accuracy: 9436/10000 (94.36%)\n",
      "\n",
      "Round   9, Average loss 0.301 Test accuracy 94.360\n",
      "selected users: [ 1  2  5  6  8  9 11 13 14 15 16 17 18 19 20 21 22 23 24 25 30]\n",
      "\n",
      "Test set: Average loss: 0.3102 \n",
      "Accuracy: 9465/10000 (94.65%)\n",
      "\n",
      "Round  10, Average loss 0.310 Test accuracy 94.650\n",
      "selected users: [ 0  1  4  5  6  7  8 10 11 13 15 16 17 18 21 22 23 24 25 27 29]\n",
      "\n",
      "Test set: Average loss: 0.5545 \n",
      "Accuracy: 9481/10000 (94.81%)\n",
      "\n",
      "Round  11, Average loss 0.554 Test accuracy 94.810\n",
      "selected users: [ 0  1  2  3  4  5  7  8  9 10 11 13 14 16 18 21 23 24 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4726 \n",
      "Accuracy: 9455/10000 (94.55%)\n",
      "\n",
      "Round  12, Average loss 0.473 Test accuracy 94.550\n",
      "selected users: [ 0  2  3  5  7  8 10 12 13 14 15 16 17 19 21 23 24 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.2129 \n",
      "Accuracy: 9337/10000 (93.37%)\n",
      "\n",
      "Round  13, Average loss 0.213 Test accuracy 93.370\n",
      "selected users: [ 0  1  2  6  7  8 11 12 13 14 15 16 17 19 20 21 22 23 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.6075 \n",
      "Accuracy: 9513/10000 (95.13%)\n",
      "\n",
      "Round  14, Average loss 0.608 Test accuracy 95.130\n",
      "selected users: [ 0  1  2  4  8 10 11 12 13 14 15 16 17 18 19 21 22 24 26 27 30]\n",
      "\n",
      "Test set: Average loss: 0.5731 \n",
      "Accuracy: 9543/10000 (95.43%)\n",
      "\n",
      "Round  15, Average loss 0.573 Test accuracy 95.430\n",
      "selected users: [ 2  3  4  5  6  7  8 12 13 16 17 19 20 22 23 24 25 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 1.0156 \n",
      "Accuracy: 9532/10000 (95.32%)\n",
      "\n",
      "Round  16, Average loss 1.016 Test accuracy 95.320\n",
      "selected users: [ 0  1  4  5  6  8  9 10 11 13 14 16 17 18 19 22 24 25 26 27 28]\n",
      "\n",
      "Test set: Average loss: 1.2420 \n",
      "Accuracy: 9538/10000 (95.38%)\n",
      "\n",
      "Round  17, Average loss 1.242 Test accuracy 95.380\n",
      "selected users: [ 0  2  4  6  7  8 10 11 12 13 14 15 16 18 19 20 22 24 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.5041 \n",
      "Accuracy: 9486/10000 (94.86%)\n",
      "\n",
      "Round  18, Average loss 0.504 Test accuracy 94.860\n",
      "selected users: [ 0  1  2  3  4  8  9 10 11 12 13 14 15 16 17 18 20 22 23 25 28]\n",
      "\n",
      "Test set: Average loss: 0.7281 \n",
      "Accuracy: 9492/10000 (94.92%)\n",
      "\n",
      "Round  19, Average loss 0.728 Test accuracy 94.920\n",
      "selected users: [ 0  3  5  6  7  8  9 10 11 13 14 15 16 18 19 20 23 24 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4959 \n",
      "Accuracy: 9431/10000 (94.31%)\n",
      "\n",
      "Round  20, Average loss 0.496 Test accuracy 94.310\n",
      "selected users: [ 0  1  2  4  6  7  8  9 10 12 13 16 17 18 21 23 24 25 26 27 30]\n",
      "\n",
      "Test set: Average loss: 0.4281 \n",
      "Accuracy: 9473/10000 (94.73%)\n",
      "\n",
      "Round  21, Average loss 0.428 Test accuracy 94.730\n",
      "selected users: [ 0  2  4  6  7  8 11 12 13 14 15 17 18 19 20 22 24 25 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.4894 \n",
      "Accuracy: 9485/10000 (94.85%)\n",
      "\n",
      "Round  22, Average loss 0.489 Test accuracy 94.850\n",
      "selected users: [ 2  3  4  5  7 11 13 14 15 17 18 19 20 21 22 23 24 25 26 29 30]\n",
      "\n",
      "Test set: Average loss: 0.5493 \n",
      "Accuracy: 9485/10000 (94.85%)\n",
      "\n",
      "Round  23, Average loss 0.549 Test accuracy 94.850\n",
      "selected users: [ 5  7  8  9 11 12 13 14 15 16 18 19 20 21 22 23 24 25 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2001 \n",
      "Accuracy: 9377/10000 (93.77%)\n",
      "\n",
      "Round  24, Average loss 0.200 Test accuracy 93.770\n",
      "selected users: [ 1  2  3  4  6  7  8 11 13 15 17 18 19 20 21 22 23 24 25 28 30]\n",
      "\n",
      "Test set: Average loss: 0.5993 \n",
      "Accuracy: 9450/10000 (94.50%)\n",
      "\n",
      "Round  25, Average loss 0.599 Test accuracy 94.500\n",
      "selected users: [ 0  1  4  6  7  8  9 11 12 15 16 18 19 21 22 23 24 25 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.6798 \n",
      "Accuracy: 9497/10000 (94.97%)\n",
      "\n",
      "Round  26, Average loss 0.680 Test accuracy 94.970\n",
      "selected users: [ 0  1  2  3  4  5  7  9 10 11 12 13 15 16 20 22 23 25 26 29 30]\n",
      "\n",
      "Test set: Average loss: 0.3699 \n",
      "Accuracy: 9405/10000 (94.05%)\n",
      "\n",
      "Round  27, Average loss 0.370 Test accuracy 94.050\n",
      "selected users: [ 0  2  6  7  8 10 11 15 16 17 18 19 20 21 22 23 24 25 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.5208 \n",
      "Accuracy: 9457/10000 (94.57%)\n",
      "\n",
      "Round  28, Average loss 0.521 Test accuracy 94.570\n",
      "selected users: [ 2  3  4  5  6  7  8 10 14 15 16 17 18 19 21 23 24 25 26 29 30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.3409 \n",
      "Accuracy: 9405/10000 (94.05%)\n",
      "\n",
      "Round  29, Average loss 0.341 Test accuracy 94.050\n",
      "(m= 21 )  4 -th Trial!!\n",
      "selected users: [ 0  1  2  3  4  5  6  8  9 10 13 15 17 18 19 21 22 23 24 27 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 2  4  5  7  9 10 11 12 15 16 18 19 20 21 22 24 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3000 \n",
      "Accuracy: 2291/10000 (22.91%)\n",
      "\n",
      "Round   1, Average loss 2.300 Test accuracy 22.910\n",
      "selected users: [ 0  1  2  3  5  6  7  8  9 11 12 13 14 20 21 22 23 24 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5915 \n",
      "Accuracy: 8586/10000 (85.86%)\n",
      "\n",
      "Round   2, Average loss 1.591 Test accuracy 85.860\n",
      "selected users: [ 0  1  4  5  6  7  9 10 11 12 14 15 16 17 18 20 21 22 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3351 \n",
      "Accuracy: 9439/10000 (94.39%)\n",
      "\n",
      "Round   3, Average loss 1.335 Test accuracy 94.390\n",
      "selected users: [ 0  1  2  3  4  5  7  8  9 10 11 13 14 15 16 17 18 21 25 27 30]\n",
      "\n",
      "Test set: Average loss: 0.3741 \n",
      "Accuracy: 9437/10000 (94.37%)\n",
      "\n",
      "Round   4, Average loss 0.374 Test accuracy 94.370\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8 13 14 15 18 20 21 22 23 24 25 27 28]\n",
      "\n",
      "Test set: Average loss: 1.0579 \n",
      "Accuracy: 9451/10000 (94.51%)\n",
      "\n",
      "Round   5, Average loss 1.058 Test accuracy 94.510\n",
      "selected users: [ 2  3  5  6  7  8  9 10 13 14 15 18 19 21 23 24 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2558 \n",
      "Accuracy: 9371/10000 (93.71%)\n",
      "\n",
      "Round   6, Average loss 0.256 Test accuracy 93.710\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 13 14 15 16 17 22 24 25 29 30]\n",
      "\n",
      "Test set: Average loss: 0.7898 \n",
      "Accuracy: 9494/10000 (94.94%)\n",
      "\n",
      "Round   7, Average loss 0.790 Test accuracy 94.940\n",
      "selected users: [ 2  3  6  7  9 10 13 14 15 17 19 20 21 22 23 24 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4447 \n",
      "Accuracy: 9440/10000 (94.40%)\n",
      "\n",
      "Round   8, Average loss 0.445 Test accuracy 94.400\n",
      "selected users: [ 0  1  3  5  7  8 11 12 14 15 16 18 19 20 21 22 24 25 26 27 30]\n",
      "\n",
      "Test set: Average loss: 0.3117 \n",
      "Accuracy: 9465/10000 (94.65%)\n",
      "\n",
      "Round   9, Average loss 0.312 Test accuracy 94.650\n",
      "selected users: [ 0  4  6  7  8  9 12 13 14 15 16 17 18 22 23 24 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.3096 \n",
      "Accuracy: 9519/10000 (95.19%)\n",
      "\n",
      "Round  10, Average loss 0.310 Test accuracy 95.190\n",
      "selected users: [ 0  1  2  4  6  7  9 11 12 13 16 17 18 20 21 23 24 25 26 27 30]\n",
      "\n",
      "Test set: Average loss: 0.5281 \n",
      "Accuracy: 9520/10000 (95.20%)\n",
      "\n",
      "Round  11, Average loss 0.528 Test accuracy 95.200\n",
      "selected users: [ 0  1  2  3  6  7  8  9 10 11 12 13 14 16 19 22 23 24 25 28 30]\n",
      "\n",
      "Test set: Average loss: 1.0374 \n",
      "Accuracy: 9524/10000 (95.24%)\n",
      "\n",
      "Round  12, Average loss 1.037 Test accuracy 95.240\n",
      "selected users: [ 0  1  3  4  5  8  9 10 12 13 14 15 16 17 20 21 24 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2407 \n",
      "Accuracy: 9450/10000 (94.50%)\n",
      "\n",
      "Round  13, Average loss 0.241 Test accuracy 94.500\n",
      "selected users: [ 0  1  2  4  5  7  9 11 13 15 16 18 20 22 23 24 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.3859 \n",
      "Accuracy: 9483/10000 (94.83%)\n",
      "\n",
      "Round  14, Average loss 0.386 Test accuracy 94.830\n",
      "selected users: [ 1  3  4  5  6  7  8 10 12 14 15 17 18 20 21 23 24 25 26 27 30]\n",
      "\n",
      "Test set: Average loss: 0.6206 \n",
      "Accuracy: 9485/10000 (94.85%)\n",
      "\n",
      "Round  15, Average loss 0.621 Test accuracy 94.850\n",
      "selected users: [ 0  1  3  6  7  8 10 11 13 14 15 16 17 22 23 24 25 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.6537 \n",
      "Accuracy: 9497/10000 (94.97%)\n",
      "\n",
      "Round  16, Average loss 0.654 Test accuracy 94.970\n",
      "selected users: [ 0  1  2  3  4  5  6  7 11 12 13 15 16 19 22 23 25 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.4724 \n",
      "Accuracy: 9523/10000 (95.23%)\n",
      "\n",
      "Round  17, Average loss 0.472 Test accuracy 95.230\n",
      "selected users: [ 0  3  5  7  8  9 11 13 14 16 17 18 21 22 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2231 \n",
      "Accuracy: 9445/10000 (94.45%)\n",
      "\n",
      "Round  18, Average loss 0.223 Test accuracy 94.450\n",
      "selected users: [ 0  1  2  4  6  7  8  9 10 11 15 16 17 18 22 24 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.1230 \n",
      "Accuracy: 9569/10000 (95.69%)\n",
      "\n",
      "Round  19, Average loss 1.123 Test accuracy 95.690\n",
      "selected users: [ 0  1  2  4  5  7  9 10 13 15 16 17 19 20 21 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4087 \n",
      "Accuracy: 9519/10000 (95.19%)\n",
      "\n",
      "Round  20, Average loss 0.409 Test accuracy 95.190\n",
      "selected users: [ 1  2  4  5  8  9 11 12 15 16 17 18 19 20 21 22 23 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2026 \n",
      "Accuracy: 9456/10000 (94.56%)\n",
      "\n",
      "Round  21, Average loss 0.203 Test accuracy 94.560\n",
      "selected users: [ 0  2  3  5  6  7  9 11 13 14 15 16 17 18 19 20 21 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2191 \n",
      "Accuracy: 9384/10000 (93.84%)\n",
      "\n",
      "Round  22, Average loss 0.219 Test accuracy 93.840\n",
      "selected users: [ 1  2  5  7  8  9 10 11 12 15 18 19 20 21 22 24 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.3604 \n",
      "Accuracy: 9486/10000 (94.86%)\n",
      "\n",
      "Round  23, Average loss 0.360 Test accuracy 94.860\n",
      "selected users: [ 0  1  2  3  5  6  7  8 11 12 13 14 16 17 20 23 24 25 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.7962 \n",
      "Accuracy: 9459/10000 (94.59%)\n",
      "\n",
      "Round  24, Average loss 0.796 Test accuracy 94.590\n",
      "selected users: [ 0  2  3  5  6  7 10 11 14 15 16 17 19 20 22 24 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2237 \n",
      "Accuracy: 9450/10000 (94.50%)\n",
      "\n",
      "Round  25, Average loss 0.224 Test accuracy 94.500\n",
      "selected users: [ 0  3  4  5  7  8  9 10 11 12 15 16 17 19 20 21 22 23 24 28 30]\n",
      "\n",
      "Test set: Average loss: 0.5685 \n",
      "Accuracy: 9503/10000 (95.03%)\n",
      "\n",
      "Round  26, Average loss 0.568 Test accuracy 95.030\n",
      "selected users: [ 0  1  2  3  5  6  7  8  9 10 12 13 15 18 20 21 22 23 26 27 29]\n",
      "\n",
      "Test set: Average loss: 0.5168 \n",
      "Accuracy: 9419/10000 (94.19%)\n",
      "\n",
      "Round  27, Average loss 0.517 Test accuracy 94.190\n",
      "selected users: [ 1  3  4  6  9 10 11 12 14 15 16 17 18 21 23 24 25 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.3243 \n",
      "Accuracy: 9490/10000 (94.90%)\n",
      "\n",
      "Round  28, Average loss 0.324 Test accuracy 94.900\n",
      "selected users: [ 1  3  4  5 11 12 13 14 15 16 18 19 21 23 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2624 \n",
      "Accuracy: 9282/10000 (92.82%)\n",
      "\n",
      "Round  29, Average loss 0.262 Test accuracy 92.820\n",
      "(m= 21 )  5 -th Trial!!\n",
      "selected users: [ 0  1  3  6  7  8  9 10 11 13 15 17 19 20 21 22 23 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  2  3  5  6  8  9 11 12 13 14 15 19 21 22 23 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.2832 \n",
      "Accuracy: 6282/10000 (62.82%)\n",
      "\n",
      "Round   1, Average loss 2.283 Test accuracy 62.820\n",
      "selected users: [ 2  3  4  6  7  8  9 10 11 12 13 14 16 18 19 21 23 24 25 27 30]\n",
      "\n",
      "Test set: Average loss: 0.9856 \n",
      "Accuracy: 9250/10000 (92.50%)\n",
      "\n",
      "Round   2, Average loss 0.986 Test accuracy 92.500\n",
      "selected users: [ 0  1  2  3  5  6  7  8  9 11 12 14 15 16 20 21 22 24 25 27 28]\n",
      "\n",
      "Test set: Average loss: 1.1453 \n",
      "Accuracy: 9360/10000 (93.60%)\n",
      "\n",
      "Round   3, Average loss 1.145 Test accuracy 93.600\n",
      "selected users: [ 2  3  4  5  8 10 12 13 14 16 18 19 20 21 23 24 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.3904 \n",
      "Accuracy: 9362/10000 (93.62%)\n",
      "\n",
      "Round   4, Average loss 0.390 Test accuracy 93.620\n",
      "selected users: [ 0  1  2  3  6  8 10 11 12 15 16 17 18 19 21 23 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.7265 \n",
      "Accuracy: 9402/10000 (94.02%)\n",
      "\n",
      "Round   5, Average loss 0.727 Test accuracy 94.020\n",
      "selected users: [ 0  1  3  4  6  7  9 10 13 15 17 18 19 20 21 22 23 24 26 28 30]\n",
      "\n",
      "Test set: Average loss: 0.4875 \n",
      "Accuracy: 9407/10000 (94.07%)\n",
      "\n",
      "Round   6, Average loss 0.487 Test accuracy 94.070\n",
      "selected users: [ 1  2  5  6  7  8  9 10 11 12 13 14 16 17 18 19 21 23 24 25 29]\n",
      "\n",
      "Test set: Average loss: 0.3464 \n",
      "Accuracy: 9400/10000 (94.00%)\n",
      "\n",
      "Round   7, Average loss 0.346 Test accuracy 94.000\n",
      "selected users: [ 0  2  7  8 11 12 13 14 15 16 17 19 20 21 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.4097 \n",
      "Accuracy: 9481/10000 (94.81%)\n",
      "\n",
      "Round   8, Average loss 1.410 Test accuracy 94.810\n",
      "selected users: [ 1  2  5  7  8  9 10 11 13 16 18 19 20 21 22 23 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.3943 \n",
      "Accuracy: 9431/10000 (94.31%)\n",
      "\n",
      "Round   9, Average loss 0.394 Test accuracy 94.310\n",
      "selected users: [ 2  4  5  6  7  8  9 10 11 12 14 15 16 17 18 19 22 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.8945 \n",
      "Accuracy: 9529/10000 (95.29%)\n",
      "\n",
      "Round  10, Average loss 0.895 Test accuracy 95.290\n",
      "selected users: [ 0  1  2  3  4  5  7  9 10 11 12 13 14 15 17 20 22 23 24 27 29]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.7755 \n",
      "Accuracy: 9548/10000 (95.48%)\n",
      "\n",
      "Round  11, Average loss 0.776 Test accuracy 95.480\n",
      "selected users: [ 0  1  2  3  5  6  7 11 12 13 15 16 17 18 19 20 22 25 26 28 30]\n",
      "\n",
      "Test set: Average loss: 0.5590 \n",
      "Accuracy: 9499/10000 (94.99%)\n",
      "\n",
      "Round  12, Average loss 0.559 Test accuracy 94.990\n",
      "selected users: [ 0  1  2  3  4  5  7  8 11 12 15 17 18 21 22 24 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.3535 \n",
      "Accuracy: 9401/10000 (94.01%)\n",
      "\n",
      "Round  13, Average loss 0.354 Test accuracy 94.010\n",
      "selected users: [ 0  1  2  4  5  8 10 11 12 15 17 18 21 22 23 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.3782 \n",
      "Accuracy: 9424/10000 (94.24%)\n",
      "\n",
      "Round  14, Average loss 0.378 Test accuracy 94.240\n",
      "selected users: [ 1  2  4  5  7  9 10 11 14 15 16 17 18 19 22 23 24 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.6455 \n",
      "Accuracy: 9508/10000 (95.08%)\n",
      "\n",
      "Round  15, Average loss 0.646 Test accuracy 95.080\n",
      "selected users: [ 0  1  2  6  8  9 10 12 15 16 17 18 19 20 21 22 23 24 26 28 29]\n",
      "\n",
      "Test set: Average loss: 0.4743 \n",
      "Accuracy: 9455/10000 (94.55%)\n",
      "\n",
      "Round  16, Average loss 0.474 Test accuracy 94.550\n",
      "selected users: [ 0  1  3  4  6  7 10 11 12 15 16 17 18 19 20 21 22 23 25 27 28]\n",
      "\n",
      "Test set: Average loss: 0.7765 \n",
      "Accuracy: 9454/10000 (94.54%)\n",
      "\n",
      "Round  17, Average loss 0.776 Test accuracy 94.540\n",
      "selected users: [ 0  1  3  6  8 10 12 13 14 15 16 17 18 22 23 24 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.7557 \n",
      "Accuracy: 9491/10000 (94.91%)\n",
      "\n",
      "Round  18, Average loss 0.756 Test accuracy 94.910\n",
      "selected users: [ 1  3  4  5  6  8  9 10 11 12 14 15 16 19 22 23 24 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.5505 \n",
      "Accuracy: 9530/10000 (95.30%)\n",
      "\n",
      "Round  19, Average loss 0.551 Test accuracy 95.300\n",
      "selected users: [ 0  1  2  3  4  8  9 10 11 12 13 14 15 18 19 20 23 25 26 27 28]\n",
      "\n",
      "Test set: Average loss: 0.5689 \n",
      "Accuracy: 9456/10000 (94.56%)\n",
      "\n",
      "Round  20, Average loss 0.569 Test accuracy 94.560\n",
      "selected users: [ 0  1  2  3  6  7  8 10 11 13 17 19 20 21 22 23 24 25 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.8852 \n",
      "Accuracy: 9436/10000 (94.36%)\n",
      "\n",
      "Round  21, Average loss 0.885 Test accuracy 94.360\n",
      "selected users: [ 0  2  4  5  6  7  8  9 10 12 13 15 16 18 20 22 24 25 26 28 29]\n",
      "\n",
      "Test set: Average loss: 0.3689 \n",
      "Accuracy: 9439/10000 (94.39%)\n",
      "\n",
      "Round  22, Average loss 0.369 Test accuracy 94.390\n",
      "selected users: [ 0  2  3  4  7  8  9 10 13 14 15 16 17 18 20 21 23 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4626 \n",
      "Accuracy: 9455/10000 (94.55%)\n",
      "\n",
      "Round  23, Average loss 0.463 Test accuracy 94.550\n",
      "selected users: [ 0  1  6  7  8  9 10 13 14 15 16 17 18 19 20 24 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.5191 \n",
      "Accuracy: 9395/10000 (93.95%)\n",
      "\n",
      "Round  24, Average loss 0.519 Test accuracy 93.950\n",
      "selected users: [ 0  1  3  4  6  7  8 10 11 12 15 16 17 18 19 21 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4441 \n",
      "Accuracy: 9495/10000 (94.95%)\n",
      "\n",
      "Round  25, Average loss 0.444 Test accuracy 94.950\n",
      "selected users: [ 0  1  2  6  8  9 10 11 13 14 15 16 19 21 22 23 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.6184 \n",
      "Accuracy: 9522/10000 (95.22%)\n",
      "\n",
      "Round  26, Average loss 0.618 Test accuracy 95.220\n",
      "selected users: [ 1  2  3  4  5  6  7  9 10 13 14 15 16 17 18 19 20 21 25 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4402 \n",
      "Accuracy: 9475/10000 (94.75%)\n",
      "\n",
      "Round  27, Average loss 0.440 Test accuracy 94.750\n",
      "selected users: [ 0  2  3  4  5  7  8  9 12 15 16 17 18 20 21 22 23 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2047 \n",
      "Accuracy: 9440/10000 (94.40%)\n",
      "\n",
      "Round  28, Average loss 0.205 Test accuracy 94.400\n",
      "selected users: [ 0  2  4  5  7  8 10 11 13 14 15 17 18 20 21 22 23 24 25 28 29]\n",
      "\n",
      "Test set: Average loss: 0.3966 \n",
      "Accuracy: 9407/10000 (94.07%)\n",
      "\n",
      "Round  29, Average loss 0.397 Test accuracy 94.070\n",
      "(m= 21 )  6 -th Trial!!\n",
      "selected users: [ 1  2  3  7  9 10 11 12 14 15 16 17 18 19 20 22 23 24 27 28 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  1  2  3  4  6  7  8  9 10 11 17 18 19 22 23 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.2528 \n",
      "Accuracy: 5926/10000 (59.26%)\n",
      "\n",
      "Round   1, Average loss 2.253 Test accuracy 59.260\n",
      "selected users: [ 0  1  3  5  7  9 11 13 14 16 17 19 20 21 22 23 24 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.8752 \n",
      "Accuracy: 9301/10000 (93.01%)\n",
      "\n",
      "Round   2, Average loss 0.875 Test accuracy 93.010\n",
      "selected users: [ 2  4  5  6  7  8  9 10 11 12 13 16 17 18 20 21 22 24 25 27 30]\n",
      "\n",
      "Test set: Average loss: 0.7026 \n",
      "Accuracy: 9437/10000 (94.37%)\n",
      "\n",
      "Round   3, Average loss 0.703 Test accuracy 94.370\n",
      "selected users: [ 0  1  2  4  5  6 10 11 14 15 17 18 20 21 23 24 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.6106 \n",
      "Accuracy: 9394/10000 (93.94%)\n",
      "\n",
      "Round   4, Average loss 1.611 Test accuracy 93.940\n",
      "selected users: [ 0  3  4  5  6  7  8  9 13 15 17 20 21 22 23 24 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4168 \n",
      "Accuracy: 9474/10000 (94.74%)\n",
      "\n",
      "Round   5, Average loss 0.417 Test accuracy 94.740\n",
      "selected users: [ 0  1  4  5  6  8 10 11 13 14 16 17 18 19 23 24 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.9683 \n",
      "Accuracy: 9506/10000 (95.06%)\n",
      "\n",
      "Round   6, Average loss 0.968 Test accuracy 95.060\n",
      "selected users: [ 1  3  4  5  6  7  9 10 12 14 15 16 17 19 20 21 22 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.3047 \n",
      "Accuracy: 9477/10000 (94.77%)\n",
      "\n",
      "Round   7, Average loss 0.305 Test accuracy 94.770\n",
      "selected users: [ 0  1  5  6  7  9 11 13 14 16 17 18 19 20 21 24 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.3887 \n",
      "Accuracy: 9440/10000 (94.40%)\n",
      "\n",
      "Round   8, Average loss 0.389 Test accuracy 94.400\n",
      "selected users: [ 1  3  4  5  6  7  8 10 12 13 15 16 18 19 20 21 23 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2889 \n",
      "Accuracy: 9365/10000 (93.65%)\n",
      "\n",
      "Round   9, Average loss 0.289 Test accuracy 93.650\n",
      "selected users: [ 0  1  3  4  5  6 10 11 12 14 16 18 19 20 21 22 24 25 26 27 30]\n",
      "\n",
      "Test set: Average loss: 0.7204 \n",
      "Accuracy: 9511/10000 (95.11%)\n",
      "\n",
      "Round  10, Average loss 0.720 Test accuracy 95.110\n",
      "selected users: [ 0  1  2  3  4  7  9 10 11 16 17 18 19 21 22 23 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2852 \n",
      "Accuracy: 9488/10000 (94.88%)\n",
      "\n",
      "Round  11, Average loss 0.285 Test accuracy 94.880\n",
      "selected users: [ 1  2  3  5  6  7  9 11 12 14 15 16 17 19 20 22 24 25 26 28 29]\n",
      "\n",
      "Test set: Average loss: 0.4072 \n",
      "Accuracy: 9473/10000 (94.73%)\n",
      "\n",
      "Round  12, Average loss 0.407 Test accuracy 94.730\n",
      "selected users: [ 0  2  3  4  8  9 10 12 13 14 15 16 17 18 19 20 23 24 25 26 27]\n",
      "\n",
      "Test set: Average loss: 0.7703 \n",
      "Accuracy: 9559/10000 (95.59%)\n",
      "\n",
      "Round  13, Average loss 0.770 Test accuracy 95.590\n",
      "selected users: [ 1  2  3  4  5  7  8 10 12 13 14 15 17 18 20 21 22 23 25 26 28]\n",
      "\n",
      "Test set: Average loss: 0.6142 \n",
      "Accuracy: 9474/10000 (94.74%)\n",
      "\n",
      "Round  14, Average loss 0.614 Test accuracy 94.740\n",
      "selected users: [ 0  1  2  5  7  8  9 10 11 12 14 15 17 20 21 22 24 25 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.4802 \n",
      "Accuracy: 9509/10000 (95.09%)\n",
      "\n",
      "Round  15, Average loss 0.480 Test accuracy 95.090\n",
      "selected users: [ 0  1  2  5  6  8  9 10 12 13 14 16 20 21 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.3024 \n",
      "Accuracy: 9399/10000 (93.99%)\n",
      "\n",
      "Round  16, Average loss 0.302 Test accuracy 93.990\n",
      "selected users: [ 0  2  3  4  5  6  7  8  9 11 12 15 17 19 21 22 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.5022 \n",
      "Accuracy: 9508/10000 (95.08%)\n",
      "\n",
      "Round  17, Average loss 0.502 Test accuracy 95.080\n",
      "selected users: [ 0  1  2  6  8  9 10 11 15 16 17 18 19 20 22 23 24 25 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.7080 \n",
      "Accuracy: 9503/10000 (95.03%)\n",
      "\n",
      "Round  18, Average loss 0.708 Test accuracy 95.030\n",
      "selected users: [ 0  2  3  4  6  7 10 11 12 13 14 15 17 18 19 21 24 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2912 \n",
      "Accuracy: 9501/10000 (95.01%)\n",
      "\n",
      "Round  19, Average loss 0.291 Test accuracy 95.010\n",
      "selected users: [ 0  1  2  5  7 12 13 14 15 16 17 18 19 21 22 23 24 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.3089 \n",
      "Accuracy: 9492/10000 (94.92%)\n",
      "\n",
      "Round  20, Average loss 0.309 Test accuracy 94.920\n",
      "selected users: [ 0  1  2  3  4  5  6  8  9 11 14 15 17 20 21 22 23 24 26 27 29]\n",
      "\n",
      "Test set: Average loss: 1.0310 \n",
      "Accuracy: 9521/10000 (95.21%)\n",
      "\n",
      "Round  21, Average loss 1.031 Test accuracy 95.210\n",
      "selected users: [ 1  2  4  5  6  7  9 11 12 13 16 18 21 23 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2927 \n",
      "Accuracy: 9452/10000 (94.52%)\n",
      "\n",
      "Round  22, Average loss 0.293 Test accuracy 94.520\n",
      "selected users: [ 0  1  3  4  5  7 10 11 12 13 18 19 20 21 22 23 24 25 26 28 29]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.0575 \n",
      "Accuracy: 9486/10000 (94.86%)\n",
      "\n",
      "Round  23, Average loss 1.058 Test accuracy 94.860\n",
      "selected users: [ 0  2  3  4  6  7  9 12 13 14 15 16 17 18 19 22 23 25 26 28 29]\n",
      "\n",
      "Test set: Average loss: 0.4524 \n",
      "Accuracy: 9501/10000 (95.01%)\n",
      "\n",
      "Round  24, Average loss 0.452 Test accuracy 95.010\n",
      "selected users: [ 0  1  3  4  7  8  9 10 11 13 14 15 16 22 23 24 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.6765 \n",
      "Accuracy: 9568/10000 (95.68%)\n",
      "\n",
      "Round  25, Average loss 0.676 Test accuracy 95.680\n",
      "selected users: [ 2  3  4  5  6  7  8  9 10 13 14 16 17 18 19 21 22 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.3610 \n",
      "Accuracy: 9501/10000 (95.01%)\n",
      "\n",
      "Round  26, Average loss 0.361 Test accuracy 95.010\n",
      "selected users: [ 0  1  2  5  6  9 10 12 15 16 17 18 20 21 22 23 24 25 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.5614 \n",
      "Accuracy: 9517/10000 (95.17%)\n",
      "\n",
      "Round  27, Average loss 0.561 Test accuracy 95.170\n",
      "selected users: [ 0  2  4  6  8 10 12 14 16 17 18 19 20 21 22 23 24 25 26 27 28]\n",
      "\n",
      "Test set: Average loss: 0.5381 \n",
      "Accuracy: 9483/10000 (94.83%)\n",
      "\n",
      "Round  28, Average loss 0.538 Test accuracy 94.830\n",
      "selected users: [ 0  3  5  6  7  8  9 10 11 12 14 16 17 18 20 21 23 24 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4367 \n",
      "Accuracy: 9451/10000 (94.51%)\n",
      "\n",
      "Round  29, Average loss 0.437 Test accuracy 94.510\n",
      "(m= 21 )  7 -th Trial!!\n",
      "selected users: [ 1  2  3  4  6  7  8  9 10 11 13 16 18 19 22 23 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3024 \n",
      "Accuracy: 1598/10000 (15.98%)\n",
      "\n",
      "Round   0, Average loss 2.302 Test accuracy 15.980\n",
      "selected users: [ 0  1  4  5  6  7  8 10 11 12 13 14 15 18 20 22 23 24 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.1669 \n",
      "Accuracy: 6677/10000 (66.77%)\n",
      "\n",
      "Round   1, Average loss 2.167 Test accuracy 66.770\n",
      "selected users: [ 0  3  4  5  6  7  8 10 11 13 14 15 16 17 19 20 22 23 24 29 30]\n",
      "\n",
      "Test set: Average loss: 1.1542 \n",
      "Accuracy: 9303/10000 (93.03%)\n",
      "\n",
      "Round   2, Average loss 1.154 Test accuracy 93.030\n",
      "selected users: [ 0  1  2  3  4  8  9 10 12 14 15 20 21 22 23 24 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.6038 \n",
      "Accuracy: 9413/10000 (94.13%)\n",
      "\n",
      "Round   3, Average loss 0.604 Test accuracy 94.130\n",
      "selected users: [ 1  2  3  7  8  9 10 13 14 15 17 18 20 21 22 24 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.6767 \n",
      "Accuracy: 9454/10000 (94.54%)\n",
      "\n",
      "Round   4, Average loss 0.677 Test accuracy 94.540\n",
      "selected users: [ 0  1  2  4  7  8 10 11 12 15 17 18 19 21 23 24 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.5489 \n",
      "Accuracy: 9417/10000 (94.17%)\n",
      "\n",
      "Round   5, Average loss 0.549 Test accuracy 94.170\n",
      "selected users: [ 2  3  4  5  7 10 11 12 13 14 16 17 18 20 21 23 24 25 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.7504 \n",
      "Accuracy: 9446/10000 (94.46%)\n",
      "\n",
      "Round   6, Average loss 0.750 Test accuracy 94.460\n",
      "selected users: [ 0  1  2  4  5  6 13 14 15 16 17 19 20 21 22 23 24 25 27 29 30]\n",
      "\n",
      "Test set: Average loss: 2.2328 \n",
      "Accuracy: 4866/10000 (48.66%)\n",
      "\n",
      "Round   7, Average loss 2.233 Test accuracy 48.660\n",
      "selected users: [ 1  2  3  4  5  6  7  9 10 11 12 13 14 16 20 21 22 24 25 29 30]\n",
      "\n",
      "Test set: Average loss: 0.8844 \n",
      "Accuracy: 9527/10000 (95.27%)\n",
      "\n",
      "Round   8, Average loss 0.884 Test accuracy 95.270\n",
      "selected users: [ 0  1  2  5  6  7  8 10 11 13 14 15 17 18 21 22 24 25 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.5056 \n",
      "Accuracy: 9460/10000 (94.60%)\n",
      "\n",
      "Round   9, Average loss 0.506 Test accuracy 94.600\n",
      "selected users: [ 0  2  3  5  7  8 10 12 13 14 16 18 19 21 22 23 24 25 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2922 \n",
      "Accuracy: 9388/10000 (93.88%)\n",
      "\n",
      "Round  10, Average loss 0.292 Test accuracy 93.880\n",
      "selected users: [ 0  3  4  5  6  7  8  9 11 13 14 15 16 17 18 19 20 22 23 25 29]\n",
      "\n",
      "Test set: Average loss: 0.6732 \n",
      "Accuracy: 9509/10000 (95.09%)\n",
      "\n",
      "Round  11, Average loss 0.673 Test accuracy 95.090\n",
      "selected users: [ 3  5  8 10 11 12 13 14 15 16 17 18 19 21 22 24 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.6387 \n",
      "Accuracy: 9429/10000 (94.29%)\n",
      "\n",
      "Round  12, Average loss 0.639 Test accuracy 94.290\n",
      "selected users: [ 1  2  3  4  5  6  8 10 12 13 15 16 17 18 19 23 24 25 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.7067 \n",
      "Accuracy: 9484/10000 (94.84%)\n",
      "\n",
      "Round  13, Average loss 0.707 Test accuracy 94.840\n",
      "selected users: [ 0  1  2  3  5  6  8 10 11 12 16 17 18 19 21 23 24 25 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.6711 \n",
      "Accuracy: 9383/10000 (93.83%)\n",
      "\n",
      "Round  14, Average loss 0.671 Test accuracy 93.830\n",
      "selected users: [ 0  1  4  5  7  9 11 12 13 14 15 16 17 20 22 23 24 25 26 27 30]\n",
      "\n",
      "Test set: Average loss: 0.8979 \n",
      "Accuracy: 9500/10000 (95.00%)\n",
      "\n",
      "Round  15, Average loss 0.898 Test accuracy 95.000\n",
      "selected users: [ 1  2  3  5  6  7  8  9 10 11 13 14 17 18 19 22 23 24 26 28 29]\n",
      "\n",
      "Test set: Average loss: 0.5747 \n",
      "Accuracy: 9478/10000 (94.78%)\n",
      "\n",
      "Round  16, Average loss 0.575 Test accuracy 94.780\n",
      "selected users: [ 0  2  3  4  5  6  8  9 11 13 14 15 16 18 20 22 24 25 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.5839 \n",
      "Accuracy: 9493/10000 (94.93%)\n",
      "\n",
      "Round  17, Average loss 0.584 Test accuracy 94.930\n",
      "selected users: [ 2  3  6  7  8  9 12 13 14 15 16 18 19 20 21 22 23 24 26 27 29]\n",
      "\n",
      "Test set: Average loss: 0.9684 \n",
      "Accuracy: 9473/10000 (94.73%)\n",
      "\n",
      "Round  18, Average loss 0.968 Test accuracy 94.730\n",
      "selected users: [ 0  3  4  5  6  7  9 10 12 13 16 17 18 19 20 21 23 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.3966 \n",
      "Accuracy: 9403/10000 (94.03%)\n",
      "\n",
      "Round  19, Average loss 0.397 Test accuracy 94.030\n",
      "selected users: [ 1  2  3  4  5  7  8  9 10 13 14 15 16 19 20 21 22 25 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4692 \n",
      "Accuracy: 9431/10000 (94.31%)\n",
      "\n",
      "Round  20, Average loss 0.469 Test accuracy 94.310\n",
      "selected users: [ 1  2  3  4  5  6  9 11 12 13 14 15 16 17 19 20 22 25 26 29 30]\n",
      "\n",
      "Test set: Average loss: 0.7611 \n",
      "Accuracy: 9498/10000 (94.98%)\n",
      "\n",
      "Round  21, Average loss 0.761 Test accuracy 94.980\n",
      "selected users: [ 0  1  3  4  5  6  7  8  9 10 11 14 15 17 19 21 22 23 24 27 28]\n",
      "\n",
      "Test set: Average loss: 1.1679 \n",
      "Accuracy: 9488/10000 (94.88%)\n",
      "\n",
      "Round  22, Average loss 1.168 Test accuracy 94.880\n",
      "selected users: [ 0  2  3  4  6  8  9 11 12 13 14 15 19 21 22 23 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.3961 \n",
      "Accuracy: 9444/10000 (94.44%)\n",
      "\n",
      "Round  23, Average loss 0.396 Test accuracy 94.440\n",
      "selected users: [ 2  5  6  7  8  9 10 12 13 14 15 16 18 20 21 22 23 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.7437 \n",
      "Accuracy: 9462/10000 (94.62%)\n",
      "\n",
      "Round  24, Average loss 0.744 Test accuracy 94.620\n",
      "selected users: [ 0  3  4  5  6  7  8  9 10 11 12 17 19 20 21 22 23 24 25 29 30]\n",
      "\n",
      "Test set: Average loss: 0.8085 \n",
      "Accuracy: 9485/10000 (94.85%)\n",
      "\n",
      "Round  25, Average loss 0.809 Test accuracy 94.850\n",
      "selected users: [ 0  1  2  3  4  5  7  8  9 10 11 12 14 15 19 20 21 23 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.7227 \n",
      "Accuracy: 9478/10000 (94.78%)\n",
      "\n",
      "Round  26, Average loss 0.723 Test accuracy 94.780\n",
      "selected users: [ 0  1  3  4  5  6  7  9 10 11 13 16 19 20 21 22 23 24 26 28 29]\n",
      "\n",
      "Test set: Average loss: 0.6370 \n",
      "Accuracy: 9434/10000 (94.34%)\n",
      "\n",
      "Round  27, Average loss 0.637 Test accuracy 94.340\n",
      "selected users: [ 0  2  3  4  8 10 11 12 13 14 15 18 19 21 22 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.6522 \n",
      "Accuracy: 9467/10000 (94.67%)\n",
      "\n",
      "Round  28, Average loss 0.652 Test accuracy 94.670\n",
      "selected users: [ 0  1  3  5  6  8 10 11 12 13 14 15 17 19 20 21 22 23 24 27 29]\n",
      "\n",
      "Test set: Average loss: 0.8971 \n",
      "Accuracy: 9480/10000 (94.80%)\n",
      "\n",
      "Round  29, Average loss 0.897 Test accuracy 94.800\n",
      "(m= 21 )  8 -th Trial!!\n",
      "selected users: [ 0  1  6  7  8  9 10 11 12 17 18 21 22 23 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  1  6  7  8  9 11 12 13 14 15 16 17 19 21 24 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 2.2857 \n",
      "Accuracy: 5300/10000 (53.00%)\n",
      "\n",
      "Round   1, Average loss 2.286 Test accuracy 53.000\n",
      "selected users: [ 0  2  3  5  6  7  8 10 12 13 15 16 18 19 20 21 22 24 25 27 29]\n",
      "\n",
      "Test set: Average loss: 0.9876 \n",
      "Accuracy: 9140/10000 (91.40%)\n",
      "\n",
      "Round   2, Average loss 0.988 Test accuracy 91.400\n",
      "selected users: [ 1  2  4  6  7 10 11 12 13 15 17 19 20 21 22 23 24 25 26 27 28]\n",
      "\n",
      "Test set: Average loss: 0.7292 \n",
      "Accuracy: 9330/10000 (93.30%)\n",
      "\n",
      "Round   3, Average loss 0.729 Test accuracy 93.300\n",
      "selected users: [ 0  1  2  3  4  8  9 11 12 13 14 15 17 19 21 22 24 25 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.6578 \n",
      "Accuracy: 9452/10000 (94.52%)\n",
      "\n",
      "Round   4, Average loss 0.658 Test accuracy 94.520\n",
      "selected users: [ 0  1  3  7  8  9 11 13 14 15 16 17 19 20 22 23 24 25 28 29 30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.7326 \n",
      "Accuracy: 9447/10000 (94.47%)\n",
      "\n",
      "Round   5, Average loss 0.733 Test accuracy 94.470\n",
      "selected users: [ 0  1  4  5  6  7  8 11 12 14 16 17 18 20 21 22 24 25 26 27 30]\n",
      "\n",
      "Test set: Average loss: 1.0119 \n",
      "Accuracy: 9497/10000 (94.97%)\n",
      "\n",
      "Round   6, Average loss 1.012 Test accuracy 94.970\n",
      "selected users: [ 1  3  5  6  8  9 10 11 13 14 15 17 19 20 22 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.7220 \n",
      "Accuracy: 9496/10000 (94.96%)\n",
      "\n",
      "Round   7, Average loss 0.722 Test accuracy 94.960\n",
      "selected users: [ 4  5  7  9 10 11 12 14 15 16 17 18 19 20 21 23 24 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.5695 \n",
      "Accuracy: 9474/10000 (94.74%)\n",
      "\n",
      "Round   8, Average loss 0.569 Test accuracy 94.740\n",
      "selected users: [ 0  1  3  4  6  7  9 11 13 16 17 18 19 21 23 24 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4926 \n",
      "Accuracy: 9439/10000 (94.39%)\n",
      "\n",
      "Round   9, Average loss 0.493 Test accuracy 94.390\n",
      "selected users: [ 0  1  3  4  5  7  8  9 11 12 15 16 17 18 19 20 21 22 25 26 30]\n",
      "\n",
      "Test set: Average loss: 0.5630 \n",
      "Accuracy: 9474/10000 (94.74%)\n",
      "\n",
      "Round  10, Average loss 0.563 Test accuracy 94.740\n",
      "selected users: [ 0  2  3  4  5  7  8  9 11 12 15 16 17 18 19 22 23 24 25 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4888 \n",
      "Accuracy: 9509/10000 (95.09%)\n",
      "\n",
      "Round  11, Average loss 0.489 Test accuracy 95.090\n",
      "selected users: [ 3  4  5  6  7  8  9 10 11 12 13 15 16 19 20 22 23 24 26 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4236 \n",
      "Accuracy: 9424/10000 (94.24%)\n",
      "\n",
      "Round  12, Average loss 0.424 Test accuracy 94.240\n",
      "selected users: [ 0  1  4  7 10 11 12 14 15 16 17 18 19 21 22 23 24 25 26 29 30]\n",
      "\n",
      "Test set: Average loss: 0.6116 \n",
      "Accuracy: 9463/10000 (94.63%)\n",
      "\n",
      "Round  13, Average loss 0.612 Test accuracy 94.630\n",
      "selected users: [ 0  1  3  4  5  7  8  9 10 13 15 16 17 18 21 22 23 24 25 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4641 \n",
      "Accuracy: 9420/10000 (94.20%)\n",
      "\n",
      "Round  14, Average loss 0.464 Test accuracy 94.200\n",
      "selected users: [ 1  3  6  7  8  9 10 11 12 13 15 16 20 21 22 23 24 25 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.7632 \n",
      "Accuracy: 9470/10000 (94.70%)\n",
      "\n",
      "Round  15, Average loss 0.763 Test accuracy 94.700\n",
      "selected users: [ 0  1  2  4  5  6  7  8  9 13 14 16 17 18 20 21 22 25 26 28 29]\n",
      "\n",
      "Test set: Average loss: 0.6340 \n",
      "Accuracy: 9467/10000 (94.67%)\n",
      "\n",
      "Round  16, Average loss 0.634 Test accuracy 94.670\n",
      "selected users: [ 1  3  5  7  8  9 14 15 16 18 19 20 22 23 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.6041 \n",
      "Accuracy: 9510/10000 (95.10%)\n",
      "\n",
      "Round  17, Average loss 0.604 Test accuracy 95.100\n",
      "selected users: [ 0  3  4  5  6  7  8  9 10 11 13 15 18 20 21 24 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.3585 \n",
      "Accuracy: 9463/10000 (94.63%)\n",
      "\n",
      "Round  18, Average loss 0.358 Test accuracy 94.630\n",
      "selected users: [ 0  1  2  3  6  7  9 10 14 15 16 17 20 21 22 23 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.6109 \n",
      "Accuracy: 9433/10000 (94.33%)\n",
      "\n",
      "Round  19, Average loss 0.611 Test accuracy 94.330\n",
      "selected users: [ 5  6  7  9 11 12 13 14 15 16 17 18 20 21 22 24 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.2469 \n",
      "Accuracy: 9422/10000 (94.22%)\n",
      "\n",
      "Round  20, Average loss 0.247 Test accuracy 94.220\n",
      "selected users: [ 0  1  4  5  6  8 10 11 12 13 15 17 18 20 21 23 24 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.4904 \n",
      "Accuracy: 9394/10000 (93.94%)\n",
      "\n",
      "Round  21, Average loss 0.490 Test accuracy 93.940\n",
      "selected users: [ 4  6  8 10 12 13 14 15 16 17 18 19 21 23 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.5529 \n",
      "Accuracy: 9449/10000 (94.49%)\n",
      "\n",
      "Round  22, Average loss 0.553 Test accuracy 94.490\n",
      "selected users: [ 0  2  7  8  9 11 12 13 14 16 17 18 19 20 21 22 24 25 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.5387 \n",
      "Accuracy: 9535/10000 (95.35%)\n",
      "\n",
      "Round  23, Average loss 1.539 Test accuracy 95.350\n",
      "selected users: [ 0  1  2  4  5  6  9 11 12 13 15 17 18 19 21 22 23 25 26 28 29]\n",
      "\n",
      "Test set: Average loss: 0.6628 \n",
      "Accuracy: 9469/10000 (94.69%)\n",
      "\n",
      "Round  24, Average loss 0.663 Test accuracy 94.690\n",
      "selected users: [ 1  2  3  5  6  7  9 10 11 12 15 16 17 18 20 21 22 23 26 27 28]\n",
      "\n",
      "Test set: Average loss: 0.3734 \n",
      "Accuracy: 9383/10000 (93.83%)\n",
      "\n",
      "Round  25, Average loss 0.373 Test accuracy 93.830\n",
      "selected users: [ 1  2  3  5  6  7  8  9 10 11 12 13 15 19 20 21 22 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2566 \n",
      "Accuracy: 9415/10000 (94.15%)\n",
      "\n",
      "Round  26, Average loss 0.257 Test accuracy 94.150\n",
      "selected users: [ 0  1  2  3  4  8  9 10 11 13 15 16 17 18 19 21 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.6325 \n",
      "Accuracy: 9455/10000 (94.55%)\n",
      "\n",
      "Round  27, Average loss 0.632 Test accuracy 94.550\n",
      "selected users: [ 1  4  5  8  9 10 11 12 13 14 15 18 19 20 21 23 24 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2477 \n",
      "Accuracy: 9409/10000 (94.09%)\n",
      "\n",
      "Round  28, Average loss 0.248 Test accuracy 94.090\n",
      "selected users: [ 2  3  5  6  7  8  9 10 12 13 14 15 17 18 19 20 22 23 25 28 30]\n",
      "\n",
      "Test set: Average loss: 0.6907 \n",
      "Accuracy: 9477/10000 (94.77%)\n",
      "\n",
      "Round  29, Average loss 0.691 Test accuracy 94.770\n",
      "(m= 21 )  9 -th Trial!!\n",
      "selected users: [ 0  1  2  3  4  7  8  9 10 11 13 16 17 20 21 22 23 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 2  4  5  6  7 10 11 12 13 15 18 21 22 23 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.2643 \n",
      "Accuracy: 6190/10000 (61.90%)\n",
      "\n",
      "Round   1, Average loss 2.264 Test accuracy 61.900\n",
      "selected users: [ 1  2  3  4  5  6  7  8  9 10 12 15 16 18 21 22 25 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 1.3705 \n",
      "Accuracy: 9114/10000 (91.14%)\n",
      "\n",
      "Round   2, Average loss 1.370 Test accuracy 91.140\n",
      "selected users: [ 0  1  2  3  4  6  7  9 10 11 12 15 17 18 19 21 24 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.6917 \n",
      "Accuracy: 9267/10000 (92.67%)\n",
      "\n",
      "Round   3, Average loss 0.692 Test accuracy 92.670\n",
      "selected users: [ 1  2  4  5  9 11 12 13 14 16 17 19 20 21 22 24 25 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.6179 \n",
      "Accuracy: 9363/10000 (93.63%)\n",
      "\n",
      "Round   4, Average loss 0.618 Test accuracy 93.630\n",
      "selected users: [ 1  2  3  4  5  6  7  9 10 11 13 15 16 17 18 21 24 25 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.8333 \n",
      "Accuracy: 9369/10000 (93.69%)\n",
      "\n",
      "Round   5, Average loss 0.833 Test accuracy 93.690\n",
      "selected users: [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 22 26]\n",
      "\n",
      "Test set: Average loss: 0.3169 \n",
      "Accuracy: 9280/10000 (92.80%)\n",
      "\n",
      "Round   6, Average loss 0.317 Test accuracy 92.800\n",
      "selected users: [ 0  2  3  4  6  8  9 11 13 15 16 17 19 20 22 24 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.6289 \n",
      "Accuracy: 9368/10000 (93.68%)\n",
      "\n",
      "Round   7, Average loss 0.629 Test accuracy 93.680\n",
      "selected users: [ 0  1  3  4  5  7  8  9 10 11 12 13 15 17 20 21 22 24 26 27 28]\n",
      "\n",
      "Test set: Average loss: 0.8602 \n",
      "Accuracy: 9392/10000 (93.92%)\n",
      "\n",
      "Round   8, Average loss 0.860 Test accuracy 93.920\n",
      "selected users: [ 0  1  3  4  5  6  7  8  9 10 12 14 15 17 18 21 23 24 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.0282 \n",
      "Accuracy: 9410/10000 (94.10%)\n",
      "\n",
      "Round   9, Average loss 1.028 Test accuracy 94.100\n",
      "selected users: [ 0  1  3  4  5  6  8  9 12 13 14 15 16 18 21 23 24 25 26 27 29]\n",
      "\n",
      "Test set: Average loss: 1.0700 \n",
      "Accuracy: 9411/10000 (94.11%)\n",
      "\n",
      "Round  10, Average loss 1.070 Test accuracy 94.110\n",
      "selected users: [ 0  1  2  3  4  8 11 13 14 15 16 17 19 21 22 23 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.8586 \n",
      "Accuracy: 9435/10000 (94.35%)\n",
      "\n",
      "Round  11, Average loss 0.859 Test accuracy 94.350\n",
      "selected users: [ 0  3  5  8  9 10 12 14 17 18 20 21 22 23 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4825 \n",
      "Accuracy: 9406/10000 (94.06%)\n",
      "\n",
      "Round  12, Average loss 0.483 Test accuracy 94.060\n",
      "selected users: [ 0  1  2  3  4  6  9 11 14 15 16 17 18 20 21 22 23 24 25 27 29]\n",
      "\n",
      "Test set: Average loss: 1.0478 \n",
      "Accuracy: 9456/10000 (94.56%)\n",
      "\n",
      "Round  13, Average loss 1.048 Test accuracy 94.560\n",
      "selected users: [ 1  3  5  6  7  9 10 11 12 13 14 15 17 18 19 20 22 24 26 28 29]\n",
      "\n",
      "Test set: Average loss: 0.6620 \n",
      "Accuracy: 9391/10000 (93.91%)\n",
      "\n",
      "Round  14, Average loss 0.662 Test accuracy 93.910\n",
      "selected users: [ 2  4  5  6  9 10 11 12 13 15 16 18 20 21 23 24 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.8808 \n",
      "Accuracy: 9432/10000 (94.32%)\n",
      "\n",
      "Round  15, Average loss 0.881 Test accuracy 94.320\n",
      "selected users: [ 0  1  2  5  6  7  8  9 11 12 14 15 16 17 18 19 21 22 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.7107 \n",
      "Accuracy: 9492/10000 (94.92%)\n",
      "\n",
      "Round  16, Average loss 0.711 Test accuracy 94.920\n",
      "selected users: [ 0  3  5  6  8  9 10 11 12 13 14 15 16 18 19 21 24 26 27 28 29]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.6768 \n",
      "Accuracy: 9439/10000 (94.39%)\n",
      "\n",
      "Round  17, Average loss 0.677 Test accuracy 94.390\n",
      "selected users: [ 1  2  3  5  6  8  9 10 11 13 14 15 17 18 19 20 21 23 24 26 28]\n",
      "\n",
      "Test set: Average loss: 0.7045 \n",
      "Accuracy: 9401/10000 (94.01%)\n",
      "\n",
      "Round  18, Average loss 0.705 Test accuracy 94.010\n",
      "selected users: [ 0  1  4  5  9 10 11 12 13 15 17 18 19 20 22 23 24 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.6492 \n",
      "Accuracy: 9461/10000 (94.61%)\n",
      "\n",
      "Round  19, Average loss 0.649 Test accuracy 94.610\n",
      "selected users: [ 0  2  3  4  5  6  7  8 11 13 14 15 16 17 19 22 23 25 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.8201 \n",
      "Accuracy: 9465/10000 (94.65%)\n",
      "\n",
      "Round  20, Average loss 0.820 Test accuracy 94.650\n",
      "selected users: [ 0  2  4  5  6  7  9 10 13 14 15 16 18 20 21 22 24 25 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.6722 \n",
      "Accuracy: 9390/10000 (93.90%)\n",
      "\n",
      "Round  21, Average loss 0.672 Test accuracy 93.900\n",
      "selected users: [ 0  1  2  4  5  6  8 13 14 15 17 18 19 20 21 22 23 24 25 27 30]\n",
      "\n",
      "Test set: Average loss: 0.8310 \n",
      "Accuracy: 9438/10000 (94.38%)\n",
      "\n",
      "Round  22, Average loss 0.831 Test accuracy 94.380\n",
      "selected users: [ 0  1  2  3  6  9 10 11 12 13 14 16 18 19 21 22 23 24 25 26 27]\n",
      "\n",
      "Test set: Average loss: 1.3965 \n",
      "Accuracy: 9478/10000 (94.78%)\n",
      "\n",
      "Round  23, Average loss 1.396 Test accuracy 94.780\n",
      "selected users: [ 1  2  4  5  6  7 10 11 12 13 14 15 17 18 20 21 22 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.5840 \n",
      "Accuracy: 9425/10000 (94.25%)\n",
      "\n",
      "Round  24, Average loss 0.584 Test accuracy 94.250\n",
      "selected users: [ 0  1  2  3  4  8  9 10 11 13 14 15 17 21 23 24 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.5633 \n",
      "Accuracy: 9439/10000 (94.39%)\n",
      "\n",
      "Round  25, Average loss 0.563 Test accuracy 94.390\n",
      "selected users: [ 0  1  2  3  4  7  8  9 11 12 15 19 20 22 23 24 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.9464 \n",
      "Accuracy: 9474/10000 (94.74%)\n",
      "\n",
      "Round  26, Average loss 0.946 Test accuracy 94.740\n",
      "selected users: [ 1  3  5  6  7  9 10 11 12 14 15 17 18 20 21 24 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.6773 \n",
      "Accuracy: 9407/10000 (94.07%)\n",
      "\n",
      "Round  27, Average loss 0.677 Test accuracy 94.070\n",
      "selected users: [ 0  1  2  3  4  5  6  7 12 14 16 17 18 21 22 23 24 25 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.9199 \n",
      "Accuracy: 9419/10000 (94.19%)\n",
      "\n",
      "Round  28, Average loss 0.920 Test accuracy 94.190\n",
      "selected users: [ 1  4  5  6  7  8  9 10 11 13 16 18 19 21 22 23 24 25 26 28 29]\n",
      "\n",
      "Test set: Average loss: 0.6738 \n",
      "Accuracy: 9430/10000 (94.30%)\n",
      "\n",
      "Round  29, Average loss 0.674 Test accuracy 94.300\n",
      "number of results: 24\n",
      "(m= 24 )  0 -th Trial!!\n",
      "selected users: [ 0  2  3  4  5  6  7  8  9 10 11 13 14 16 17 20 21 22 23 24 25 26 27 29]\n",
      "\n",
      "Test set: Average loss: 2.3020 \n",
      "Accuracy: 1234/10000 (12.34%)\n",
      "\n",
      "Round   0, Average loss 2.302 Test accuracy 12.340\n",
      "selected users: [ 1  2  3  5  6  7  8  9 11 12 13 14 17 18 19 20 21 22 23 24 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 2.1524 \n",
      "Accuracy: 8380/10000 (83.80%)\n",
      "\n",
      "Round   1, Average loss 2.152 Test accuracy 83.800\n",
      "selected users: [ 0  1  2  3  4  6  8 10 12 13 15 17 18 19 20 21 22 23 24 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6062 \n",
      "Accuracy: 9254/10000 (92.54%)\n",
      "\n",
      "Round   2, Average loss 1.606 Test accuracy 92.540\n",
      "selected users: [ 0  1  3  4  5  6  7  8  9 10 11 12 13 16 17 18 19 20 22 23 24 26 27 30]\n",
      "\n",
      "Test set: Average loss: 1.4380 \n",
      "Accuracy: 9427/10000 (94.27%)\n",
      "\n",
      "Round   3, Average loss 1.438 Test accuracy 94.270\n",
      "selected users: [ 0  2  3  4  6  8  9 10 11 12 13 14 15 17 18 20 21 22 23 24 25 26 27 29]\n",
      "\n",
      "Test set: Average loss: 1.3243 \n",
      "Accuracy: 9464/10000 (94.64%)\n",
      "\n",
      "Round   4, Average loss 1.324 Test accuracy 94.640\n",
      "selected users: [ 0  1  2  3  5  6  7  8 11 12 14 15 16 17 18 19 22 23 24 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6035 \n",
      "Accuracy: 9474/10000 (94.74%)\n",
      "\n",
      "Round   5, Average loss 1.604 Test accuracy 94.740\n",
      "selected users: [ 0  1  2  4  5  6  8  9 10 12 13 15 16 18 19 20 21 22 23 24 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3489 \n",
      "Accuracy: 9466/10000 (94.66%)\n",
      "\n",
      "Round   6, Average loss 1.349 Test accuracy 94.660\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 18 21 22 25 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3628 \n",
      "Accuracy: 9478/10000 (94.78%)\n",
      "\n",
      "Round   7, Average loss 1.363 Test accuracy 94.780\n",
      "selected users: [ 0  1  2  4  6  7  8  9 11 12 13 15 16 17 19 20 21 22 23 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.2685 \n",
      "Accuracy: 9462/10000 (94.62%)\n",
      "\n",
      "Round   8, Average loss 1.269 Test accuracy 94.620\n",
      "selected users: [ 0  1  2  4  5  7  8  9 10 11 12 13 14 17 18 21 22 23 24 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.3551 \n",
      "Accuracy: 9495/10000 (94.95%)\n",
      "\n",
      "Round   9, Average loss 1.355 Test accuracy 94.950\n",
      "selected users: [ 0  3  4  7  8  9 10 11 12 14 15 16 17 18 19 20 22 23 24 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.2595 \n",
      "Accuracy: 9525/10000 (95.25%)\n",
      "\n",
      "Round  10, Average loss 1.259 Test accuracy 95.250\n",
      "selected users: [ 0  1  2  3  5  6  7  8 11 12 13 14 15 17 19 20 21 22 23 24 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5181 \n",
      "Accuracy: 9454/10000 (94.54%)\n",
      "\n",
      "Round  11, Average loss 1.518 Test accuracy 94.540\n",
      "selected users: [ 2  3  4  5  6  7  8  9 10 12 13 16 17 18 19 20 21 22 23 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.4559 \n",
      "Accuracy: 9485/10000 (94.85%)\n",
      "\n",
      "Round  12, Average loss 1.456 Test accuracy 94.850\n",
      "selected users: [ 0  1  3  4  5  6  7  9 10 11 12 13 15 16 17 18 20 21 22 23 24 26 27 28]\n",
      "\n",
      "Test set: Average loss: 1.5586 \n",
      "Accuracy: 9488/10000 (94.88%)\n",
      "\n",
      "Round  13, Average loss 1.559 Test accuracy 94.880\n",
      "selected users: [ 0  1  2  3  4  5  8  9 10 13 14 16 17 18 20 21 22 23 24 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.1758 \n",
      "Accuracy: 9454/10000 (94.54%)\n",
      "\n",
      "Round  14, Average loss 1.176 Test accuracy 94.540\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 16 18 19 20 22 23 26 27 29]\n",
      "\n",
      "Test set: Average loss: 1.3561 \n",
      "Accuracy: 9452/10000 (94.52%)\n",
      "\n",
      "Round  15, Average loss 1.356 Test accuracy 94.520\n",
      "selected users: [ 0  1  2  3  4  5  6  7 10 11 12 13 14 15 16 17 22 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5822 \n",
      "Accuracy: 9488/10000 (94.88%)\n",
      "\n",
      "Round  16, Average loss 1.582 Test accuracy 94.880\n",
      "selected users: [ 1  2  3  4  5  7  8  9 10 11 13 14 15 16 17 18 19 20 21 22 24 25 26 28]\n",
      "\n",
      "Test set: Average loss: 1.3985 \n",
      "Accuracy: 9464/10000 (94.64%)\n",
      "\n",
      "Round  17, Average loss 1.398 Test accuracy 94.640\n",
      "selected users: [ 0  1  2  3  5  6  7  8 10 13 14 16 17 18 19 20 22 23 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.1937 \n",
      "Accuracy: 9454/10000 (94.54%)\n",
      "\n",
      "Round  18, Average loss 1.194 Test accuracy 94.540\n",
      "selected users: [ 0  2  3  4  5  7  8 10 12 13 14 15 16 17 19 20 22 23 24 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.8900 \n",
      "Accuracy: 9470/10000 (94.70%)\n",
      "\n",
      "Round  19, Average loss 0.890 Test accuracy 94.700\n",
      "selected users: [ 0  1  2  4  5  7  8 11 12 14 15 16 17 18 20 21 22 23 24 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.0950 \n",
      "Accuracy: 9482/10000 (94.82%)\n",
      "\n",
      "Round  20, Average loss 1.095 Test accuracy 94.820\n",
      "selected users: [ 0  2  5  6  7  8  9 11 12 13 14 15 17 19 20 21 22 23 24 25 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 1.2310 \n",
      "Accuracy: 9492/10000 (94.92%)\n",
      "\n",
      "Round  21, Average loss 1.231 Test accuracy 94.920\n",
      "selected users: [ 0  1  2  4  5  6  8  9 10 11 12 14 15 16 17 19 20 21 22 24 25 26 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5242 \n",
      "Accuracy: 9479/10000 (94.79%)\n",
      "\n",
      "Round  22, Average loss 1.524 Test accuracy 94.790\n",
      "selected users: [ 1  2  3  4  5  7 10 11 12 14 15 16 17 18 19 20 22 23 24 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.2008 \n",
      "Accuracy: 9483/10000 (94.83%)\n",
      "\n",
      "Round  23, Average loss 1.201 Test accuracy 94.830\n",
      "selected users: [ 0  1  2  4  5  6  7  8 10 12 13 14 15 17 19 20 22 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.4566 \n",
      "Accuracy: 9490/10000 (94.90%)\n",
      "\n",
      "Round  24, Average loss 1.457 Test accuracy 94.900\n",
      "selected users: [ 0  1  2  4  5  7  8  9 11 12 13 14 15 17 18 19 20 21 22 23 24 25 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3540 \n",
      "Accuracy: 9505/10000 (95.05%)\n",
      "\n",
      "Round  25, Average loss 1.354 Test accuracy 95.050\n",
      "selected users: [ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 17 20 21 22 24 25 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.5492 \n",
      "Accuracy: 9482/10000 (94.82%)\n",
      "\n",
      "Round  26, Average loss 1.549 Test accuracy 94.820\n",
      "selected users: [ 0  2  3  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20 22 23 24 25 29]\n",
      "\n",
      "Test set: Average loss: 1.2122 \n",
      "Accuracy: 9497/10000 (94.97%)\n",
      "\n",
      "Round  27, Average loss 1.212 Test accuracy 94.970\n",
      "selected users: [ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 16 17 19 20 21 22 23 26 29 30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.0937 \n",
      "Accuracy: 9504/10000 (95.04%)\n",
      "\n",
      "Round  28, Average loss 1.094 Test accuracy 95.040\n",
      "selected users: [ 0  3  4  7  8 10 11 12 13 14 15 16 17 19 21 22 23 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.9190 \n",
      "Accuracy: 9515/10000 (95.15%)\n",
      "\n",
      "Round  29, Average loss 0.919 Test accuracy 95.150\n",
      "(m= 24 )  1 -th Trial!!\n",
      "selected users: [ 0  1  2  3  4  6  7  8  9 10 11 12 14 15 16 17 18 20 22 24 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  1  3  4  5  6  9 10 11 13 14 15 16 17 18 19 20 21 22 23 24 25 28 30]\n",
      "\n",
      "Test set: Average loss: 2.2943 \n",
      "Accuracy: 5164/10000 (51.64%)\n",
      "\n",
      "Round   1, Average loss 2.294 Test accuracy 51.640\n",
      "selected users: [ 0  2  4  5  7  8  9 10 11 12 13 14 16 17 18 19 20 22 23 24 25 26 29 30]\n",
      "\n",
      "Test set: Average loss: 1.2801 \n",
      "Accuracy: 9161/10000 (91.61%)\n",
      "\n",
      "Round   2, Average loss 1.280 Test accuracy 91.610\n",
      "selected users: [ 1  2  5  6  7  8  9 10 11 13 15 16 17 18 19 20 21 22 24 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.7397 \n",
      "Accuracy: 9323/10000 (93.23%)\n",
      "\n",
      "Round   3, Average loss 0.740 Test accuracy 93.230\n",
      "selected users: [ 0  1  3  4  5  6  9 10 11 12 13 14 15 16 17 18 19 20 21 23 25 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.2993 \n",
      "Accuracy: 9509/10000 (95.09%)\n",
      "\n",
      "Round   4, Average loss 1.299 Test accuracy 95.090\n",
      "selected users: [ 0  1  2  3  5  7  8  9 11 12 13 14 15 16 17 18 20 21 22 24 25 26 27 29]\n",
      "\n",
      "Test set: Average loss: 0.9920 \n",
      "Accuracy: 9461/10000 (94.61%)\n",
      "\n",
      "Round   5, Average loss 0.992 Test accuracy 94.610\n",
      "selected users: [ 0  1  2  3  4  6  7  8  9 10 11 13 14 15 17 18 19 21 23 24 25 26 27 30]\n",
      "\n",
      "Test set: Average loss: 0.8644 \n",
      "Accuracy: 9499/10000 (94.99%)\n",
      "\n",
      "Round   6, Average loss 0.864 Test accuracy 94.990\n",
      "selected users: [ 0  1  2  3  4  5  7  8  9 10 11 12 13 14 15 17 18 20 22 23 25 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.8534 \n",
      "Accuracy: 9513/10000 (95.13%)\n",
      "\n",
      "Round   7, Average loss 0.853 Test accuracy 95.130\n",
      "selected users: [ 1  2  3  4  5  6  7  8 10 12 14 17 18 19 20 21 22 23 24 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.7883 \n",
      "Accuracy: 9535/10000 (95.35%)\n",
      "\n",
      "Round   8, Average loss 0.788 Test accuracy 95.350\n",
      "selected users: [ 1  3  4  5  6  7  8  9 11 12 13 14 15 16 18 19 21 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.6106 \n",
      "Accuracy: 9493/10000 (94.93%)\n",
      "\n",
      "Round   9, Average loss 0.611 Test accuracy 94.930\n",
      "selected users: [ 0  2  3  4  5  6  7  9 10 12 13 14 15 16 17 18 20 21 22 23 24 25 27 29]\n",
      "\n",
      "Test set: Average loss: 0.9542 \n",
      "Accuracy: 9526/10000 (95.26%)\n",
      "\n",
      "Round  10, Average loss 0.954 Test accuracy 95.260\n",
      "selected users: [ 0  2  3  5  7  8 10 11 12 13 14 16 17 19 20 21 22 23 24 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4240 \n",
      "Accuracy: 9434/10000 (94.34%)\n",
      "\n",
      "Round  11, Average loss 0.424 Test accuracy 94.340\n",
      "selected users: [ 0  2  3  4  6  7  8  9 10 12 13 14 16 17 18 22 23 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.6022 \n",
      "Accuracy: 9476/10000 (94.76%)\n",
      "\n",
      "Round  12, Average loss 0.602 Test accuracy 94.760\n",
      "selected users: [ 1  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 21 22 23 24 25 26 28 29]\n",
      "\n",
      "Test set: Average loss: 0.8367 \n",
      "Accuracy: 9365/10000 (93.65%)\n",
      "\n",
      "Round  13, Average loss 0.837 Test accuracy 93.650\n",
      "selected users: [ 0  1  2  3  4  6  7  8 10 11 12 13 15 17 18 19 20 21 23 24 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4406 \n",
      "Accuracy: 9356/10000 (93.56%)\n",
      "\n",
      "Round  14, Average loss 0.441 Test accuracy 93.560\n",
      "selected users: [ 0  1  3  4  5  6  7  8  9 10 12 14 15 16 17 18 19 20 21 23 25 27 28 30]\n",
      "\n",
      "Test set: Average loss: 1.0891 \n",
      "Accuracy: 9490/10000 (94.90%)\n",
      "\n",
      "Round  15, Average loss 1.089 Test accuracy 94.900\n",
      "selected users: [ 0  2  3  4  5  7  8  9 10 12 13 14 15 17 18 20 21 22 23 24 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.5440 \n",
      "Accuracy: 9492/10000 (94.92%)\n",
      "\n",
      "Round  16, Average loss 0.544 Test accuracy 94.920\n",
      "selected users: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 17 19 20 21 22 23 25 26 28 30]\n",
      "\n",
      "Test set: Average loss: 1.2621 \n",
      "Accuracy: 9502/10000 (95.02%)\n",
      "\n",
      "Round  17, Average loss 1.262 Test accuracy 95.020\n",
      "selected users: [ 0  1  2  3  5  7  9 10 11 12 13 14 15 16 17 19 20 21 22 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.6637 \n",
      "Accuracy: 9449/10000 (94.49%)\n",
      "\n",
      "Round  18, Average loss 0.664 Test accuracy 94.490\n",
      "selected users: [ 0  1  2  4  5  6  7  9 10 11 12 14 15 17 18 19 21 23 24 25 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.9476 \n",
      "Accuracy: 9512/10000 (95.12%)\n",
      "\n",
      "Round  19, Average loss 0.948 Test accuracy 95.120\n",
      "selected users: [ 0  1  2  3  4  8  9 10 11 12 13 14 15 16 17 18 19 20 21 24 25 26 28 30]\n",
      "\n",
      "Test set: Average loss: 0.5750 \n",
      "Accuracy: 9486/10000 (94.86%)\n",
      "\n",
      "Round  20, Average loss 0.575 Test accuracy 94.860\n",
      "selected users: [ 0  1  3  4  5  6  7  8  9 10 12 13 15 16 18 19 20 21 22 23 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.9770 \n",
      "Accuracy: 9382/10000 (93.82%)\n",
      "\n",
      "Round  21, Average loss 0.977 Test accuracy 93.820\n",
      "selected users: [ 0  1  2  4  5  8  9 10 11 12 14 15 16 17 18 21 22 23 24 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4563 \n",
      "Accuracy: 9467/10000 (94.67%)\n",
      "\n",
      "Round  22, Average loss 0.456 Test accuracy 94.670\n",
      "selected users: [ 0  1  3  4  5  6  7 11 12 13 14 15 16 17 19 20 22 23 24 25 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 1.0126 \n",
      "Accuracy: 9503/10000 (95.03%)\n",
      "\n",
      "Round  23, Average loss 1.013 Test accuracy 95.030\n",
      "selected users: [ 0  1  3  4  6  7  8 10 11 12 13 14 15 16 17 19 20 21 22 23 24 25 26 29]\n",
      "\n",
      "Test set: Average loss: 0.9044 \n",
      "Accuracy: 9509/10000 (95.09%)\n",
      "\n",
      "Round  24, Average loss 0.904 Test accuracy 95.090\n",
      "selected users: [ 0  1  2  3  4  5  6  7  9 12 13 14 16 17 18 19 21 22 23 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.2129 \n",
      "Accuracy: 9530/10000 (95.30%)\n",
      "\n",
      "Round  25, Average loss 1.213 Test accuracy 95.300\n",
      "selected users: [ 0  1  2  3  4  5  6  8  9 10 12 13 14 15 16 18 19 20 22 24 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.9220 \n",
      "Accuracy: 9542/10000 (95.42%)\n",
      "\n",
      "Round  26, Average loss 0.922 Test accuracy 95.420\n",
      "selected users: [ 0  1  2  3  5  6  7  8  9 10 11 12 15 16 17 18 20 21 22 23 25 27 28 30]\n",
      "\n",
      "Test set: Average loss: 1.1480 \n",
      "Accuracy: 9462/10000 (94.62%)\n",
      "\n",
      "Round  27, Average loss 1.148 Test accuracy 94.620\n",
      "selected users: [ 0  3  4  5  6  7  9 10 11 12 13 14 15 18 19 20 22 23 24 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.6349 \n",
      "Accuracy: 9535/10000 (95.35%)\n",
      "\n",
      "Round  28, Average loss 0.635 Test accuracy 95.350\n",
      "selected users: [ 0  2  4  5  6  7  8  9 10 11 13 15 16 17 18 19 20 21 22 24 25 26 28 30]\n",
      "\n",
      "Test set: Average loss: 0.7261 \n",
      "Accuracy: 9491/10000 (94.91%)\n",
      "\n",
      "Round  29, Average loss 0.726 Test accuracy 94.910\n",
      "(m= 24 )  2 -th Trial!!\n",
      "selected users: [ 1  4  5  6  8  9 10 11 13 15 17 18 19 20 21 22 23 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  1  2  3  5  6  8  9 10 11 12 13 14 15 16 17 20 21 22 24 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.1911 \n",
      "Accuracy: 7128/10000 (71.28%)\n",
      "\n",
      "Round   1, Average loss 2.191 Test accuracy 71.280\n",
      "selected users: [ 1  4  6  7  9 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.0934 \n",
      "Accuracy: 9403/10000 (94.03%)\n",
      "\n",
      "Round   2, Average loss 1.093 Test accuracy 94.030\n",
      "selected users: [ 1  2  4  5  6  7  8  9 10 11 12 14 15 16 18 19 20 22 23 24 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.6595 \n",
      "Accuracy: 9424/10000 (94.24%)\n",
      "\n",
      "Round   3, Average loss 0.659 Test accuracy 94.240\n",
      "selected users: [ 2  3  4  5  6  8  9 10 11 12 14 15 17 18 19 20 22 23 24 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3508 \n",
      "Accuracy: 9517/10000 (95.17%)\n",
      "\n",
      "Round   4, Average loss 1.351 Test accuracy 95.170\n",
      "selected users: [ 0  1  2  3  5  6  8  9 10 11 12 14 15 16 17 18 19 20 22 24 25 26 27 30]\n",
      "\n",
      "Test set: Average loss: 1.1541 \n",
      "Accuracy: 9493/10000 (94.93%)\n",
      "\n",
      "Round   5, Average loss 1.154 Test accuracy 94.930\n",
      "selected users: [ 0  1  2  5  7  9 11 12 13 14 15 16 17 18 20 21 22 23 24 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.5291 \n",
      "Accuracy: 9494/10000 (94.94%)\n",
      "\n",
      "Round   6, Average loss 0.529 Test accuracy 94.940\n",
      "selected users: [ 0  1  2  3  4  5  7  8  9 10 12 13 14 15 18 19 21 22 23 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4932 \n",
      "Accuracy: 9427/10000 (94.27%)\n",
      "\n",
      "Round   7, Average loss 0.493 Test accuracy 94.270\n",
      "selected users: [ 0  3  4  5  6  7  9 10 11 12 15 16 17 18 19 20 21 22 23 24 27 28 29 30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.6835 \n",
      "Accuracy: 9504/10000 (95.04%)\n",
      "\n",
      "Round   8, Average loss 0.684 Test accuracy 95.040\n",
      "selected users: [ 0  1  2  3  4  8  9 10 12 13 14 15 16 17 19 21 22 23 24 25 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.7215 \n",
      "Accuracy: 9528/10000 (95.28%)\n",
      "\n",
      "Round   9, Average loss 0.722 Test accuracy 95.280\n",
      "selected users: [ 0  1  3  4  6  7  8  9 11 12 13 14 15 16 17 18 21 23 24 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.6362 \n",
      "Accuracy: 9475/10000 (94.75%)\n",
      "\n",
      "Round  10, Average loss 0.636 Test accuracy 94.750\n",
      "selected users: [ 0  1  2  3  4  6  7  8  9 10 11 12 14 15 16 17 18 19 22 24 25 26 28 30]\n",
      "\n",
      "Test set: Average loss: 1.0976 \n",
      "Accuracy: 9525/10000 (95.25%)\n",
      "\n",
      "Round  11, Average loss 1.098 Test accuracy 95.250\n",
      "selected users: [ 0  1  2  3  4  6  7  8  9 12 13 14 15 16 17 18 20 22 24 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.7680 \n",
      "Accuracy: 9543/10000 (95.43%)\n",
      "\n",
      "Round  12, Average loss 0.768 Test accuracy 95.430\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 13 14 15 16 17 18 19 20 21 22 24 27 28 30]\n",
      "\n",
      "Test set: Average loss: 1.0697 \n",
      "Accuracy: 9550/10000 (95.50%)\n",
      "\n",
      "Round  13, Average loss 1.070 Test accuracy 95.500\n",
      "selected users: [ 0  1  4  5  6  7  9 10 11 12 13 14 15 17 18 19 20 21 22 23 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.7576 \n",
      "Accuracy: 9516/10000 (95.16%)\n",
      "\n",
      "Round  14, Average loss 0.758 Test accuracy 95.160\n",
      "selected users: [ 0  1  2  3  4  5  7  8 10 11 13 14 15 18 19 21 22 23 24 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4563 \n",
      "Accuracy: 9419/10000 (94.19%)\n",
      "\n",
      "Round  15, Average loss 0.456 Test accuracy 94.190\n",
      "selected users: [ 0  2  4  5  7  8  9 10 11 12 13 14 15 17 18 19 20 23 24 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.5344 \n",
      "Accuracy: 9502/10000 (95.02%)\n",
      "\n",
      "Round  16, Average loss 0.534 Test accuracy 95.020\n",
      "selected users: [ 1  2  3  4  5  6  7 10 11 12 14 15 16 17 19 21 22 23 24 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.6496 \n",
      "Accuracy: 9481/10000 (94.81%)\n",
      "\n",
      "Round  17, Average loss 0.650 Test accuracy 94.810\n",
      "selected users: [ 0  1  3  4  7  9 10 11 13 14 15 16 17 18 19 20 21 22 23 24 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.8673 \n",
      "Accuracy: 9549/10000 (95.49%)\n",
      "\n",
      "Round  18, Average loss 0.867 Test accuracy 95.490\n",
      "selected users: [ 0  1  2  3  4  5  6  8  9 10 11 12 14 15 16 17 18 20 21 24 25 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.7190 \n",
      "Accuracy: 9537/10000 (95.37%)\n",
      "\n",
      "Round  19, Average loss 0.719 Test accuracy 95.370\n",
      "selected users: [ 0  1  2  3  4  5  6  9 10 11 12 14 15 17 18 20 21 23 24 25 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.9069 \n",
      "Accuracy: 9532/10000 (95.32%)\n",
      "\n",
      "Round  20, Average loss 0.907 Test accuracy 95.320\n",
      "selected users: [ 1  2  3  4  5  6  7  8  9 10 11 12 14 15 17 18 19 20 23 24 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.5968 \n",
      "Accuracy: 9519/10000 (95.19%)\n",
      "\n",
      "Round  21, Average loss 0.597 Test accuracy 95.190\n",
      "selected users: [ 1  2  3  5  6  7  8 10 11 12 14 15 16 18 19 20 21 22 23 24 25 26 27 30]\n",
      "\n",
      "Test set: Average loss: 0.7247 \n",
      "Accuracy: 9468/10000 (94.68%)\n",
      "\n",
      "Round  22, Average loss 0.725 Test accuracy 94.680\n",
      "selected users: [ 0  1  2  3  4  5  7  8  9 11 12 14 15 18 19 20 21 22 23 24 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.8916 \n",
      "Accuracy: 9503/10000 (95.03%)\n",
      "\n",
      "Round  23, Average loss 0.892 Test accuracy 95.030\n",
      "selected users: [ 0  1  2  3  5  7  8  9 10 11 12 13 14 16 17 19 20 21 22 24 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.7323 \n",
      "Accuracy: 9482/10000 (94.82%)\n",
      "\n",
      "Round  24, Average loss 0.732 Test accuracy 94.820\n",
      "selected users: [ 0  1  2  3  4  7  9 10 12 13 14 15 16 17 18 20 22 23 24 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.7323 \n",
      "Accuracy: 9500/10000 (95.00%)\n",
      "\n",
      "Round  25, Average loss 0.732 Test accuracy 95.000\n",
      "selected users: [ 0  1  2  3  4  5  6  8 10 12 13 14 15 17 18 19 20 21 24 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.6669 \n",
      "Accuracy: 9507/10000 (95.07%)\n",
      "\n",
      "Round  26, Average loss 0.667 Test accuracy 95.070\n",
      "selected users: [ 1  2  4  5  6  7  8  9 10 11 13 14 15 16 18 19 20 21 22 23 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.5278 \n",
      "Accuracy: 9543/10000 (95.43%)\n",
      "\n",
      "Round  27, Average loss 0.528 Test accuracy 95.430\n",
      "selected users: [ 0  1  2  3  4  5  8 11 12 13 15 16 17 18 19 21 22 23 24 25 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.7043 \n",
      "Accuracy: 9456/10000 (94.56%)\n",
      "\n",
      "Round  28, Average loss 0.704 Test accuracy 94.560\n",
      "selected users: [ 2  3  5  6  7  8  9 10 11 12 13 15 16 17 18 19 21 22 23 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4934 \n",
      "Accuracy: 9472/10000 (94.72%)\n",
      "\n",
      "Round  29, Average loss 0.493 Test accuracy 94.720\n",
      "(m= 24 )  3 -th Trial!!\n",
      "selected users: [ 1  2  3  5  6  7  8  9 11 12 13 14 16 17 18 19 20 21 22 23 25 27 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  1  3  4  5  7  9 10 11 12 13 14 15 16 17 18 19 20 22 24 25 26 28 29]\n",
      "\n",
      "Test set: Average loss: 2.2906 \n",
      "Accuracy: 5301/10000 (53.01%)\n",
      "\n",
      "Round   1, Average loss 2.291 Test accuracy 53.010\n",
      "selected users: [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 18 19 20 23 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.4784 \n",
      "Accuracy: 9218/10000 (92.18%)\n",
      "\n",
      "Round   2, Average loss 1.478 Test accuracy 92.180\n",
      "selected users: [ 1  2  3  4  6  7  9 10 11 12 14 15 16 18 19 20 21 22 23 25 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 1.0738 \n",
      "Accuracy: 9467/10000 (94.67%)\n",
      "\n",
      "Round   3, Average loss 1.074 Test accuracy 94.670\n",
      "selected users: [ 0  2  3  5  6  7  8 10 11 12 13 14 15 16 17 18 19 21 22 24 25 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.4654 \n",
      "Accuracy: 9439/10000 (94.39%)\n",
      "\n",
      "Round   4, Average loss 0.465 Test accuracy 94.390\n",
      "selected users: [ 0  1  3  4  5  6  8  9 10 11 12 15 16 19 20 21 23 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.9640 \n",
      "Accuracy: 9479/10000 (94.79%)\n",
      "\n",
      "Round   5, Average loss 0.964 Test accuracy 94.790\n",
      "selected users: [ 0  2  4  5  6  7  9 11 12 13 14 15 16 17 18 19 20 21 22 25 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.9905 \n",
      "Accuracy: 9559/10000 (95.59%)\n",
      "\n",
      "Round   6, Average loss 0.990 Test accuracy 95.590\n",
      "selected users: [ 0  1  2  3  5  6  7  8  9 10 11 13 14 16 17 19 21 22 23 24 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.7088 \n",
      "Accuracy: 9465/10000 (94.65%)\n",
      "\n",
      "Round   7, Average loss 0.709 Test accuracy 94.650\n",
      "selected users: [ 1  2  4  6  9 10 11 12 13 14 15 16 17 18 19 20 21 23 24 25 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.8052 \n",
      "Accuracy: 9536/10000 (95.36%)\n",
      "\n",
      "Round   8, Average loss 0.805 Test accuracy 95.360\n",
      "selected users: [ 1  2  3  4  5  6  7  8  9 10 11 13 14 15 16 17 18 23 24 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.3797 \n",
      "Accuracy: 9524/10000 (95.24%)\n",
      "\n",
      "Round   9, Average loss 1.380 Test accuracy 95.240\n",
      "selected users: [ 0  3  4  5  6  7  8  9 10 14 15 17 18 19 20 21 22 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.5578 \n",
      "Accuracy: 9548/10000 (95.48%)\n",
      "\n",
      "Round  10, Average loss 0.558 Test accuracy 95.480\n",
      "selected users: [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 20 21 22 25 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 1.2001 \n",
      "Accuracy: 9547/10000 (95.47%)\n",
      "\n",
      "Round  11, Average loss 1.200 Test accuracy 95.470\n",
      "selected users: [ 0  2  3  4  5  7  8  9 11 12 13 15 17 18 19 20 21 23 24 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.2985 \n",
      "Accuracy: 9496/10000 (94.96%)\n",
      "\n",
      "Round  12, Average loss 0.298 Test accuracy 94.960\n",
      "selected users: [ 1  2  4  5  8  9 10 11 12 13 14 15 16 18 19 20 21 22 23 24 25 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4771 \n",
      "Accuracy: 9490/10000 (94.90%)\n",
      "\n",
      "Round  13, Average loss 0.477 Test accuracy 94.900\n",
      "selected users: [ 0  3  4  5  6  7  8 10 11 12 13 14 16 17 19 21 22 23 24 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.5624 \n",
      "Accuracy: 9523/10000 (95.23%)\n",
      "\n",
      "Round  14, Average loss 0.562 Test accuracy 95.230\n",
      "selected users: [ 1  2  4  5  7  8  9 10 11 12 13 14 15 17 18 19 20 22 23 24 25 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.8241 \n",
      "Accuracy: 9548/10000 (95.48%)\n",
      "\n",
      "Round  15, Average loss 0.824 Test accuracy 95.480\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 12 16 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.1298 \n",
      "Accuracy: 9562/10000 (95.62%)\n",
      "\n",
      "Round  16, Average loss 1.130 Test accuracy 95.620\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 13 14 16 17 18 19 21 23 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.7687 \n",
      "Accuracy: 9531/10000 (95.31%)\n",
      "\n",
      "Round  17, Average loss 0.769 Test accuracy 95.310\n",
      "selected users: [ 1  3  5  7  8  9 10 11 12 14 15 16 18 19 21 22 23 24 25 26 27 28 29 30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.7404 \n",
      "Accuracy: 9498/10000 (94.98%)\n",
      "\n",
      "Round  18, Average loss 0.740 Test accuracy 94.980\n",
      "selected users: [ 0  1  2  3  4  6  7  9 10 11 15 16 17 18 19 20 21 22 24 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.1082 \n",
      "Accuracy: 9553/10000 (95.53%)\n",
      "\n",
      "Round  19, Average loss 1.108 Test accuracy 95.530\n",
      "selected users: [ 1  2  3  4  7  8  9 10 12 13 15 16 18 20 21 22 23 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.7613 \n",
      "Accuracy: 9522/10000 (95.22%)\n",
      "\n",
      "Round  20, Average loss 0.761 Test accuracy 95.220\n",
      "selected users: [ 0  1  2  3  5  7  8  9 11 12 13 14 15 17 18 19 20 21 22 23 24 26 28 30]\n",
      "\n",
      "Test set: Average loss: 0.4875 \n",
      "Accuracy: 9487/10000 (94.87%)\n",
      "\n",
      "Round  21, Average loss 0.488 Test accuracy 94.870\n",
      "selected users: [ 0  1  2  3  5  7  8  9 12 13 14 16 18 19 21 22 23 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.7175 \n",
      "Accuracy: 9516/10000 (95.16%)\n",
      "\n",
      "Round  22, Average loss 0.717 Test accuracy 95.160\n",
      "selected users: [ 0  1  2  4  5  6  7  8 10 11 12 14 15 16 17 18 20 21 22 23 24 25 26 29]\n",
      "\n",
      "Test set: Average loss: 1.2966 \n",
      "Accuracy: 9517/10000 (95.17%)\n",
      "\n",
      "Round  23, Average loss 1.297 Test accuracy 95.170\n",
      "selected users: [ 0  1  2  3  4  6  7  8  9 10 11 12 13 14 15 17 18 20 22 24 25 26 27 30]\n",
      "\n",
      "Test set: Average loss: 0.8812 \n",
      "Accuracy: 9552/10000 (95.52%)\n",
      "\n",
      "Round  24, Average loss 0.881 Test accuracy 95.520\n",
      "selected users: [ 0  1  4  5  7  8  9 10 11 12 13 15 16 17 18 19 21 22 23 24 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.8636 \n",
      "Accuracy: 9535/10000 (95.35%)\n",
      "\n",
      "Round  25, Average loss 0.864 Test accuracy 95.350\n",
      "selected users: [ 2  6  7  8  9 10 11 12 13 14 15 16 17 18 19 21 22 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.1607 \n",
      "Accuracy: 9525/10000 (95.25%)\n",
      "\n",
      "Round  26, Average loss 1.161 Test accuracy 95.250\n",
      "selected users: [ 2  3  4  5  6  7  8 10 11 12 15 17 18 19 20 21 23 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.8775 \n",
      "Accuracy: 9531/10000 (95.31%)\n",
      "\n",
      "Round  27, Average loss 0.878 Test accuracy 95.310\n",
      "selected users: [ 1  2  3  4  5  6  7  9 10 11 12 13 14 15 16 17 19 21 23 25 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.7333 \n",
      "Accuracy: 9534/10000 (95.34%)\n",
      "\n",
      "Round  28, Average loss 0.733 Test accuracy 95.340\n",
      "selected users: [ 0  1  3  4  5  6  7  8  9 10 11 12 14 15 16 18 20 21 23 24 25 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.9457 \n",
      "Accuracy: 9495/10000 (94.95%)\n",
      "\n",
      "Round  29, Average loss 0.946 Test accuracy 94.950\n",
      "(m= 24 )  4 -th Trial!!\n",
      "selected users: [ 0  4  5  6  7  8  9 11 13 14 15 17 18 19 20 21 22 23 24 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 14 15 16 18 19 20 21 23 24 25 27 29 30]\n",
      "\n",
      "Test set: Average loss: 2.1918 \n",
      "Accuracy: 7616/10000 (76.16%)\n",
      "\n",
      "Round   1, Average loss 2.192 Test accuracy 76.160\n",
      "selected users: [ 0  3  4  5  6 10 11 12 13 14 15 16 17 18 19 20 23 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.8641 \n",
      "Accuracy: 9334/10000 (93.34%)\n",
      "\n",
      "Round   2, Average loss 0.864 Test accuracy 93.340\n",
      "selected users: [ 1  2  3  4  6  7  8  9 11 12 14 16 18 19 20 21 22 23 24 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.9319 \n",
      "Accuracy: 9452/10000 (94.52%)\n",
      "\n",
      "Round   3, Average loss 0.932 Test accuracy 94.520\n",
      "selected users: [ 1  2  3  7  8  9 10 11 12 14 15 16 17 19 20 21 22 23 24 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.0769 \n",
      "Accuracy: 9503/10000 (95.03%)\n",
      "\n",
      "Round   4, Average loss 1.077 Test accuracy 95.030\n",
      "selected users: [ 0  3  4  5  7  8  9 11 12 13 15 16 17 19 20 21 22 23 24 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.5481 \n",
      "Accuracy: 9485/10000 (94.85%)\n",
      "\n",
      "Round   5, Average loss 0.548 Test accuracy 94.850\n",
      "selected users: [ 1  2  4  5  6  8  9 10 11 12 15 16 18 19 20 21 22 23 24 25 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.9670 \n",
      "Accuracy: 9506/10000 (95.06%)\n",
      "\n",
      "Round   6, Average loss 0.967 Test accuracy 95.060\n",
      "selected users: [ 0  1  4  5  6  7  8  9 10 12 13 14 15 17 18 19 20 21 22 23 25 26 27 30]\n",
      "\n",
      "Test set: Average loss: 1.3358 \n",
      "Accuracy: 9549/10000 (95.49%)\n",
      "\n",
      "Round   7, Average loss 1.336 Test accuracy 95.490\n",
      "selected users: [ 0  2  5  6  7  8  9 12 13 14 15 16 17 19 20 21 23 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.9117 \n",
      "Accuracy: 9534/10000 (95.34%)\n",
      "\n",
      "Round   8, Average loss 0.912 Test accuracy 95.340\n",
      "selected users: [ 1  2  3  5  6  8  9 11 12 14 15 16 17 18 19 20 21 22 23 24 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4205 \n",
      "Accuracy: 9501/10000 (95.01%)\n",
      "\n",
      "Round   9, Average loss 0.420 Test accuracy 95.010\n",
      "selected users: [ 0  2  3  4  6  8  9 10 11 13 14 15 16 18 19 20 21 23 24 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.5001 \n",
      "Accuracy: 9510/10000 (95.10%)\n",
      "\n",
      "Round  10, Average loss 0.500 Test accuracy 95.100\n",
      "selected users: [ 0  1  2  3  8  9 10 11 12 14 16 17 18 19 20 21 22 23 24 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5399 \n",
      "Accuracy: 9418/10000 (94.18%)\n",
      "\n",
      "Round  11, Average loss 1.540 Test accuracy 94.180\n",
      "selected users: [ 1  2  3  4  5  6  7  8  9 12 13 14 15 17 18 21 22 23 24 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4838 \n",
      "Accuracy: 9535/10000 (95.35%)\n",
      "\n",
      "Round  12, Average loss 0.484 Test accuracy 95.350\n",
      "selected users: [ 0  2  5  6  7  8  9 10 11 12 14 15 17 18 19 20 21 23 24 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4051 \n",
      "Accuracy: 9479/10000 (94.79%)\n",
      "\n",
      "Round  13, Average loss 0.405 Test accuracy 94.790\n",
      "selected users: [ 1  2  4  5  7  8  9 10 11 12 14 15 16 17 18 19 20 21 23 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.6081 \n",
      "Accuracy: 9497/10000 (94.97%)\n",
      "\n",
      "Round  14, Average loss 0.608 Test accuracy 94.970\n",
      "selected users: [ 0  1  2  4  5  7  8  9 10 11 12 14 15 18 19 20 21 22 24 25 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.8207 \n",
      "Accuracy: 9541/10000 (95.41%)\n",
      "\n",
      "Round  15, Average loss 0.821 Test accuracy 95.410\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 11 12 14 15 16 17 18 19 20 22 25 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.9728 \n",
      "Accuracy: 9540/10000 (95.40%)\n",
      "\n",
      "Round  16, Average loss 0.973 Test accuracy 95.400\n",
      "selected users: [ 2  3  4  6  7  8  9 10 14 15 16 17 18 19 20 21 22 23 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.6960 \n",
      "Accuracy: 9554/10000 (95.54%)\n",
      "\n",
      "Round  17, Average loss 0.696 Test accuracy 95.540\n",
      "selected users: [ 0  2  3  4  6  7  8  9 10 11 12 13 14 16 17 19 20 21 23 24 25 26 27 30]\n",
      "\n",
      "Test set: Average loss: 0.5603 \n",
      "Accuracy: 9528/10000 (95.28%)\n",
      "\n",
      "Round  18, Average loss 0.560 Test accuracy 95.280\n",
      "selected users: [ 0  1  2  3  4  5  7  8  9 10 12 13 14 16 17 19 20 22 23 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.9884 \n",
      "Accuracy: 9560/10000 (95.60%)\n",
      "\n",
      "Round  19, Average loss 0.988 Test accuracy 95.600\n",
      "selected users: [ 1  2  4  5  6  7 10 11 12 13 14 15 16 17 19 20 21 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4811 \n",
      "Accuracy: 9514/10000 (95.14%)\n",
      "\n",
      "Round  20, Average loss 0.481 Test accuracy 95.140\n",
      "selected users: [ 0  4  5  7  8  9 10 11 12 13 15 16 17 19 20 21 23 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.6233 \n",
      "Accuracy: 9535/10000 (95.35%)\n",
      "\n",
      "Round  21, Average loss 0.623 Test accuracy 95.350\n",
      "selected users: [ 1  2  3  7  8  9 10 11 13 14 15 16 17 18 19 20 21 22 23 24 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.9964 \n",
      "Accuracy: 9572/10000 (95.72%)\n",
      "\n",
      "Round  22, Average loss 0.996 Test accuracy 95.720\n",
      "selected users: [ 0  1  3  4  5  6  7  8  9 11 12 14 15 16 17 18 19 21 24 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.8568 \n",
      "Accuracy: 9547/10000 (95.47%)\n",
      "\n",
      "Round  23, Average loss 0.857 Test accuracy 95.470\n",
      "selected users: [ 0  1  2  3  5  6  8  9 10 12 13 14 16 17 18 19 21 23 24 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.6615 \n",
      "Accuracy: 9511/10000 (95.11%)\n",
      "\n",
      "Round  24, Average loss 0.662 Test accuracy 95.110\n",
      "selected users: [ 1  3  4  5  6  7  8  9 10 11 12 13 15 16 17 18 19 21 22 24 25 26 27 30]\n",
      "\n",
      "Test set: Average loss: 0.8670 \n",
      "Accuracy: 9577/10000 (95.77%)\n",
      "\n",
      "Round  25, Average loss 0.867 Test accuracy 95.770\n",
      "selected users: [ 0  1  2  3  4  5  6  7 12 13 14 16 17 18 19 20 21 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.9637 \n",
      "Accuracy: 9553/10000 (95.53%)\n",
      "\n",
      "Round  26, Average loss 0.964 Test accuracy 95.530\n",
      "selected users: [ 1  2  3  6  7  8  9 10 11 12 13 15 16 19 20 21 22 23 24 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.6199 \n",
      "Accuracy: 9546/10000 (95.46%)\n",
      "\n",
      "Round  27, Average loss 0.620 Test accuracy 95.460\n",
      "selected users: [ 1  3  6  7  8  9 10 11 12 13 14 16 17 18 19 20 21 22 23 24 25 28 29 30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.9540 \n",
      "Accuracy: 9539/10000 (95.39%)\n",
      "\n",
      "Round  28, Average loss 0.954 Test accuracy 95.390\n",
      "selected users: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 16 18 19 20 21 22 25 26 27 28]\n",
      "\n",
      "Test set: Average loss: 0.9861 \n",
      "Accuracy: 9532/10000 (95.32%)\n",
      "\n",
      "Round  29, Average loss 0.986 Test accuracy 95.320\n",
      "(m= 24 )  5 -th Trial!!\n",
      "selected users: [ 0  1  2  5  6  7 11 12 13 14 16 17 18 19 20 21 22 23 24 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 982/10000 (9.82%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.820\n",
      "selected users: [ 0  2  3  4  5  6  7  8 10 11 12 13 15 16 17 18 21 22 23 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.2091 \n",
      "Accuracy: 6815/10000 (68.15%)\n",
      "\n",
      "Round   1, Average loss 2.209 Test accuracy 68.150\n",
      "selected users: [ 1  2  3  5  6  8  9 11 12 13 14 15 16 17 18 19 21 22 23 24 25 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3259 \n",
      "Accuracy: 9178/10000 (91.78%)\n",
      "\n",
      "Round   2, Average loss 1.326 Test accuracy 91.780\n",
      "selected users: [ 0  1  3  4  5  6  7  8 10 12 14 15 16 17 18 19 21 22 23 24 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3798 \n",
      "Accuracy: 9399/10000 (93.99%)\n",
      "\n",
      "Round   3, Average loss 1.380 Test accuracy 93.990\n",
      "selected users: [ 0  2  3  4  5  7  8  9 11 12 14 15 17 18 19 21 22 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.9041 \n",
      "Accuracy: 9410/10000 (94.10%)\n",
      "\n",
      "Round   4, Average loss 0.904 Test accuracy 94.100\n",
      "selected users: [ 0  1  2  4  7  8  9 10 12 13 14 15 16 17 19 20 21 22 23 24 25 27 28 30]\n",
      "\n",
      "Test set: Average loss: 1.4050 \n",
      "Accuracy: 9495/10000 (94.95%)\n",
      "\n",
      "Round   5, Average loss 1.405 Test accuracy 94.950\n",
      "selected users: [ 1  2  3  4  5  7  9 10 11 12 13 14 15 16 17 20 22 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.0349 \n",
      "Accuracy: 9449/10000 (94.49%)\n",
      "\n",
      "Round   6, Average loss 1.035 Test accuracy 94.490\n",
      "selected users: [ 0  1  2  4  5  6  7 11 12 13 14 15 16 17 18 19 20 22 23 24 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.1453 \n",
      "Accuracy: 9482/10000 (94.82%)\n",
      "\n",
      "Round   7, Average loss 1.145 Test accuracy 94.820\n",
      "selected users: [ 0  1  2  3  7  8  9 10 11 12 13 15 17 18 19 20 21 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5643 \n",
      "Accuracy: 9484/10000 (94.84%)\n",
      "\n",
      "Round   8, Average loss 1.564 Test accuracy 94.840\n",
      "selected users: [ 0  2  3  4  6  7  8  9 13 14 16 17 18 19 20 21 23 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.1861 \n",
      "Accuracy: 9474/10000 (94.74%)\n",
      "\n",
      "Round   9, Average loss 1.186 Test accuracy 94.740\n",
      "selected users: [ 1  2  3  4  5  6  7  8  9 11 12 13 14 15 16 18 19 20 21 24 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.1140 \n",
      "Accuracy: 9497/10000 (94.97%)\n",
      "\n",
      "Round  10, Average loss 1.114 Test accuracy 94.970\n",
      "selected users: [ 0  1  2  3  5  6  7 10 11 12 13 14 16 17 18 20 21 22 23 24 25 26 28 29]\n",
      "\n",
      "Test set: Average loss: 0.9199 \n",
      "Accuracy: 9439/10000 (94.39%)\n",
      "\n",
      "Round  11, Average loss 0.920 Test accuracy 94.390\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8 10 11 12 13 14 17 18 19 20 22 24 25 26 28 29]\n",
      "\n",
      "Test set: Average loss: 1.1576 \n",
      "Accuracy: 9462/10000 (94.62%)\n",
      "\n",
      "Round  12, Average loss 1.158 Test accuracy 94.620\n",
      "selected users: [ 0  2  4  5  6  7  8  9 11 13 14 17 18 19 20 21 22 23 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.1650 \n",
      "Accuracy: 9507/10000 (95.07%)\n",
      "\n",
      "Round  13, Average loss 1.165 Test accuracy 95.070\n",
      "selected users: [ 0  1  2  3  5  6  7  9 10 11 14 15 16 17 18 19 20 21 23 24 25 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.2237 \n",
      "Accuracy: 9456/10000 (94.56%)\n",
      "\n",
      "Round  14, Average loss 1.224 Test accuracy 94.560\n",
      "selected users: [ 2  3  6  7  8  9 10 11 12 13 14 15 16 18 20 21 22 23 24 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.0926 \n",
      "Accuracy: 9451/10000 (94.51%)\n",
      "\n",
      "Round  15, Average loss 1.093 Test accuracy 94.510\n",
      "selected users: [ 0  2  3  5  6  7  9 10 11 12 13 14 15 16 17 18 19 20 22 24 25 26 27 30]\n",
      "\n",
      "Test set: Average loss: 0.7870 \n",
      "Accuracy: 9488/10000 (94.88%)\n",
      "\n",
      "Round  16, Average loss 0.787 Test accuracy 94.880\n",
      "selected users: [ 0  1  2  3  4  5  6  7  9 10 11 12 13 14 15 17 19 21 22 23 25 26 28 29]\n",
      "\n",
      "Test set: Average loss: 1.1525 \n",
      "Accuracy: 9485/10000 (94.85%)\n",
      "\n",
      "Round  17, Average loss 1.153 Test accuracy 94.850\n",
      "selected users: [ 1  2  3  5  6  7  8 10 11 12 13 14 15 16 17 19 21 22 23 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.8580 \n",
      "Accuracy: 9457/10000 (94.57%)\n",
      "\n",
      "Round  18, Average loss 0.858 Test accuracy 94.570\n",
      "selected users: [ 1  3  4  5  6  7  9 10 11 13 14 15 16 17 18 19 20 21 22 23 25 26 27 29]\n",
      "\n",
      "Test set: Average loss: 1.3920 \n",
      "Accuracy: 9543/10000 (95.43%)\n",
      "\n",
      "Round  19, Average loss 1.392 Test accuracy 95.430\n",
      "selected users: [ 0  1  2  3  4  5  6  7  9 10 11 12 13 15 16 17 18 21 22 25 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 1.2383 \n",
      "Accuracy: 9511/10000 (95.11%)\n",
      "\n",
      "Round  20, Average loss 1.238 Test accuracy 95.110\n",
      "selected users: [ 1  2  3  4  5  7  8  9 10 11 12 13 14 19 20 21 22 23 24 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.1539 \n",
      "Accuracy: 9533/10000 (95.33%)\n",
      "\n",
      "Round  21, Average loss 1.154 Test accuracy 95.330\n",
      "selected users: [ 2  4  5  6  7  9 10 11 12 13 14 15 16 17 18 19 20 21 23 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.0488 \n",
      "Accuracy: 9481/10000 (94.81%)\n",
      "\n",
      "Round  22, Average loss 1.049 Test accuracy 94.810\n",
      "selected users: [ 1  2  3  4  5  6  7  8  9 12 14 16 17 18 20 21 22 23 24 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.1502 \n",
      "Accuracy: 9507/10000 (95.07%)\n",
      "\n",
      "Round  23, Average loss 1.150 Test accuracy 95.070\n",
      "selected users: [ 0  1  2  3  4  5  7  9 10 11 12 14 16 17 19 21 22 23 24 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.1143 \n",
      "Accuracy: 9488/10000 (94.88%)\n",
      "\n",
      "Round  24, Average loss 1.114 Test accuracy 94.880\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 11 14 15 16 17 19 21 22 23 24 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.2160 \n",
      "Accuracy: 9524/10000 (95.24%)\n",
      "\n",
      "Round  25, Average loss 1.216 Test accuracy 95.240\n",
      "selected users: [ 0  1  2  3  4  6  7  8  9 10 12 13 14 15 16 17 18 19 21 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.6457 \n",
      "Accuracy: 9464/10000 (94.64%)\n",
      "\n",
      "Round  26, Average loss 0.646 Test accuracy 94.640\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8 10 13 14 15 16 17 18 19 21 23 24 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.7193 \n",
      "Accuracy: 9414/10000 (94.14%)\n",
      "\n",
      "Round  27, Average loss 0.719 Test accuracy 94.140\n",
      "selected users: [ 0  1  2  3  4  6  7  8  9 10 12 13 14 16 18 19 21 22 23 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.0125 \n",
      "Accuracy: 9441/10000 (94.41%)\n",
      "\n",
      "Round  28, Average loss 1.013 Test accuracy 94.410\n",
      "selected users: [ 0  2  3  4  5  6  7  8  9 10 12 13 14 16 20 21 22 23 24 25 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 1.1002 \n",
      "Accuracy: 9495/10000 (94.95%)\n",
      "\n",
      "Round  29, Average loss 1.100 Test accuracy 94.950\n",
      "(m= 24 )  6 -th Trial!!\n",
      "selected users: [ 0  1  2  4  5  6  7  8  9 10 11 12 13 14 16 19 20 21 24 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  1  2  3  5  6  8 10 12 13 14 15 16 17 18 19 21 23 24 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 2.2963 \n",
      "Accuracy: 3871/10000 (38.71%)\n",
      "\n",
      "Round   1, Average loss 2.296 Test accuracy 38.710\n",
      "selected users: [ 0  1  2  4  5  6  7  8  9 10 11 12 15 17 18 19 20 21 24 25 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 1.6094 \n",
      "Accuracy: 9173/10000 (91.73%)\n",
      "\n",
      "Round   2, Average loss 1.609 Test accuracy 91.730\n",
      "selected users: [ 0  1  2  4  5  6  7  9 10 11 12 13 14 16 17 18 21 22 23 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.1543 \n",
      "Accuracy: 9359/10000 (93.59%)\n",
      "\n",
      "Round   3, Average loss 1.154 Test accuracy 93.590\n",
      "selected users: [ 1  2  3  5  6  7  8  9 10 12 14 15 16 17 18 19 21 22 23 25 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 1.0199 \n",
      "Accuracy: 9500/10000 (95.00%)\n",
      "\n",
      "Round   4, Average loss 1.020 Test accuracy 95.000\n",
      "selected users: [ 0  3  4  5  6  7  9 10 11 12 14 15 17 18 20 22 23 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.8022 \n",
      "Accuracy: 9518/10000 (95.18%)\n",
      "\n",
      "Round   5, Average loss 0.802 Test accuracy 95.180\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 11 12 13 15 16 17 18 20 21 22 24 25 26 27]\n",
      "\n",
      "Test set: Average loss: 2.2106 \n",
      "Accuracy: 6116/10000 (61.16%)\n",
      "\n",
      "Round   6, Average loss 2.211 Test accuracy 61.160\n",
      "selected users: [ 0  1  2  3  6  7  9 10 11 12 13 14 15 16 17 19 20 21 22 25 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 1.3831 \n",
      "Accuracy: 9523/10000 (95.23%)\n",
      "\n",
      "Round   7, Average loss 1.383 Test accuracy 95.230\n",
      "selected users: [ 0  2  3  6  7  8  9 10 11 12 13 15 16 17 18 19 20 21 22 23 24 25 29 30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.7084 \n",
      "Accuracy: 9522/10000 (95.22%)\n",
      "\n",
      "Round   8, Average loss 0.708 Test accuracy 95.220\n",
      "selected users: [ 0  1  2  3  6  8  9 10 11 12 15 16 17 18 19 20 21 22 23 24 25 26 27 29]\n",
      "\n",
      "Test set: Average loss: 1.2032 \n",
      "Accuracy: 9511/10000 (95.11%)\n",
      "\n",
      "Round   9, Average loss 1.203 Test accuracy 95.110\n",
      "selected users: [ 1  2  3  4  5  6  7  8  9 10 12 13 14 15 16 18 19 20 22 24 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.8122 \n",
      "Accuracy: 9510/10000 (95.10%)\n",
      "\n",
      "Round  10, Average loss 0.812 Test accuracy 95.100\n",
      "selected users: [ 1  3  5  6  7  8  9 10 12 14 15 16 17 18 19 20 21 22 23 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.9583 \n",
      "Accuracy: 9550/10000 (95.50%)\n",
      "\n",
      "Round  11, Average loss 0.958 Test accuracy 95.500\n",
      "selected users: [ 0  1  2  3  6  7 10 11 12 13 14 15 17 18 19 20 21 22 23 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.1908 \n",
      "Accuracy: 9525/10000 (95.25%)\n",
      "\n",
      "Round  12, Average loss 1.191 Test accuracy 95.250\n",
      "selected users: [ 0  1  2  3  4  5  7  8  9 10 11 12 13 14 15 16 18 19 20 23 25 26 28 30]\n",
      "\n",
      "Test set: Average loss: 0.8061 \n",
      "Accuracy: 9483/10000 (94.83%)\n",
      "\n",
      "Round  13, Average loss 0.806 Test accuracy 94.830\n",
      "selected users: [ 0  1  2  3  4  5  7  8 12 13 14 15 16 17 18 19 20 22 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.9865 \n",
      "Accuracy: 9531/10000 (95.31%)\n",
      "\n",
      "Round  14, Average loss 0.987 Test accuracy 95.310\n",
      "selected users: [ 0  3  4  5  6  7  8  9 10 11 13 14 15 16 17 18 19 20 21 24 25 26 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4365 \n",
      "Accuracy: 9486/10000 (94.86%)\n",
      "\n",
      "Round  15, Average loss 0.436 Test accuracy 94.860\n",
      "selected users: [ 0  1  3  5  7  8  9 10 11 12 14 15 16 17 18 20 22 23 24 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.8221 \n",
      "Accuracy: 9486/10000 (94.86%)\n",
      "\n",
      "Round  16, Average loss 0.822 Test accuracy 94.860\n",
      "selected users: [ 0  1  2  3  4  5  8  9 10 11 12 13 15 16 17 20 21 22 23 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.5604 \n",
      "Accuracy: 9416/10000 (94.16%)\n",
      "\n",
      "Round  17, Average loss 0.560 Test accuracy 94.160\n",
      "selected users: [ 0  1  2  3  5  6  7  8  9 10 11 12 14 16 17 18 19 20 22 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.9000 \n",
      "Accuracy: 9507/10000 (95.07%)\n",
      "\n",
      "Round  18, Average loss 0.900 Test accuracy 95.070\n",
      "selected users: [ 0  1  2  3  5  6  7  8  9 10 13 14 15 16 18 19 20 22 23 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.9272 \n",
      "Accuracy: 9430/10000 (94.30%)\n",
      "\n",
      "Round  19, Average loss 0.927 Test accuracy 94.300\n",
      "selected users: [ 0  1  2  4  5  6  7  8  9 10 11 13 14 16 17 18 19 20 21 23 25 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.0048 \n",
      "Accuracy: 9511/10000 (95.11%)\n",
      "\n",
      "Round  20, Average loss 1.005 Test accuracy 95.110\n",
      "selected users: [ 1  3  4  6  8  9 10 11 12 13 14 15 18 19 20 21 22 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.9552 \n",
      "Accuracy: 9505/10000 (95.05%)\n",
      "\n",
      "Round  21, Average loss 0.955 Test accuracy 95.050\n",
      "selected users: [ 2  3  4  5  8  9 10 11 12 13 14 15 17 19 20 22 23 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.7347 \n",
      "Accuracy: 9493/10000 (94.93%)\n",
      "\n",
      "Round  22, Average loss 0.735 Test accuracy 94.930\n",
      "selected users: [ 1  2  6  7  8  9 10 11 13 14 15 16 17 20 21 22 23 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.1317 \n",
      "Accuracy: 9519/10000 (95.19%)\n",
      "\n",
      "Round  23, Average loss 1.132 Test accuracy 95.190\n",
      "selected users: [ 0  1  2  4  5  6  7  9 10 12 13 14 15 17 18 19 20 22 24 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.2967 \n",
      "Accuracy: 9542/10000 (95.42%)\n",
      "\n",
      "Round  24, Average loss 1.297 Test accuracy 95.420\n",
      "selected users: [ 1  3  4  6  8  9 10 11 12 13 14 15 16 17 18 19 20 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.7248 \n",
      "Accuracy: 9498/10000 (94.98%)\n",
      "\n",
      "Round  25, Average loss 0.725 Test accuracy 94.980\n",
      "selected users: [ 1  2  3  4  5  7  8  9 12 13 14 16 17 18 19 20 21 22 23 24 25 26 29 30]\n",
      "\n",
      "Test set: Average loss: 0.7823 \n",
      "Accuracy: 9534/10000 (95.34%)\n",
      "\n",
      "Round  26, Average loss 0.782 Test accuracy 95.340\n",
      "selected users: [ 0  2  3  4  5  7  8  9 10 11 12 13 14 16 18 19 20 21 22 23 24 26 27 30]\n",
      "\n",
      "Test set: Average loss: 0.5951 \n",
      "Accuracy: 9522/10000 (95.22%)\n",
      "\n",
      "Round  27, Average loss 0.595 Test accuracy 95.220\n",
      "selected users: [ 0  1  3  4  5  6  8  9 10 12 13 14 15 16 17 18 19 20 22 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.4267 \n",
      "Accuracy: 9549/10000 (95.49%)\n",
      "\n",
      "Round  28, Average loss 1.427 Test accuracy 95.490\n",
      "selected users: [ 0  2  3  4  5  6  8  9 10 11 12 13 14 16 17 18 20 22 23 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.7383 \n",
      "Accuracy: 9519/10000 (95.19%)\n",
      "\n",
      "Round  29, Average loss 0.738 Test accuracy 95.190\n",
      "(m= 24 )  7 -th Trial!!\n",
      "selected users: [ 0  1  2  4  5  6  7  8  9 10 11 12 13 14 17 18 19 20 23 24 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 1  3  4  5  7  8 10 11 12 13 14 15 16 17 20 21 22 23 24 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 2.2590 \n",
      "Accuracy: 7399/10000 (73.99%)\n",
      "\n",
      "Round   1, Average loss 2.259 Test accuracy 73.990\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 11 13 14 15 16 18 19 20 22 23 25 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3545 \n",
      "Accuracy: 9367/10000 (93.67%)\n",
      "\n",
      "Round   2, Average loss 1.354 Test accuracy 93.670\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 13 14 15 16 19 21 22 23 24 25 26 28 30]\n",
      "\n",
      "Test set: Average loss: 1.0701 \n",
      "Accuracy: 9393/10000 (93.93%)\n",
      "\n",
      "Round   3, Average loss 1.070 Test accuracy 93.930\n",
      "selected users: [ 2  4  5  6  8  9 11 12 13 14 15 16 17 18 19 20 21 23 24 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.2095 \n",
      "Accuracy: 9509/10000 (95.09%)\n",
      "\n",
      "Round   4, Average loss 1.209 Test accuracy 95.090\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 11 16 17 18 19 20 21 23 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.7312 \n",
      "Accuracy: 9473/10000 (94.73%)\n",
      "\n",
      "Round   5, Average loss 0.731 Test accuracy 94.730\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 15 18 19 20 22 23 24 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.7026 \n",
      "Accuracy: 9512/10000 (95.12%)\n",
      "\n",
      "Round   6, Average loss 0.703 Test accuracy 95.120\n",
      "selected users: [ 0  1  3  4  5  8  9 10 11 12 13 14 16 17 18 19 20 21 22 24 25 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.7335 \n",
      "Accuracy: 9454/10000 (94.54%)\n",
      "\n",
      "Round   7, Average loss 0.733 Test accuracy 94.540\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8 12 13 15 17 18 19 20 21 22 23 24 25 26 27 30]\n",
      "\n",
      "Test set: Average loss: 1.0775 \n",
      "Accuracy: 9542/10000 (95.42%)\n",
      "\n",
      "Round   8, Average loss 1.077 Test accuracy 95.420\n",
      "selected users: [ 0  1  2  3  5  6  8  9 10 11 12 13 14 15 17 18 19 22 23 24 25 26 27 29]\n",
      "\n",
      "Test set: Average loss: 1.3865 \n",
      "Accuracy: 9551/10000 (95.51%)\n",
      "\n",
      "Round   9, Average loss 1.386 Test accuracy 95.510\n",
      "selected users: [ 0  2  3  4  5  6  8  9 10 11 14 15 16 17 20 21 22 23 24 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.3396 \n",
      "Accuracy: 9564/10000 (95.64%)\n",
      "\n",
      "Round  10, Average loss 1.340 Test accuracy 95.640\n",
      "selected users: [ 0  1  2  3  4  5  8 11 12 14 15 16 17 18 19 21 22 23 24 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.8193 \n",
      "Accuracy: 9520/10000 (95.20%)\n",
      "\n",
      "Round  11, Average loss 0.819 Test accuracy 95.200\n",
      "selected users: [ 1  2  4  6  7  8  9 10 12 13 15 16 17 18 20 21 22 23 24 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.7127 \n",
      "Accuracy: 9524/10000 (95.24%)\n",
      "\n",
      "Round  12, Average loss 0.713 Test accuracy 95.240\n",
      "selected users: [ 0  2  5  7  9 10 11 12 13 14 15 16 17 18 20 21 22 23 24 25 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.5396 \n",
      "Accuracy: 9499/10000 (94.99%)\n",
      "\n",
      "Round  13, Average loss 0.540 Test accuracy 94.990\n",
      "selected users: [ 0  1  2  3  4  5  6  7  9 11 12 13 14 16 17 18 19 20 23 24 25 26 27 28]\n",
      "\n",
      "Test set: Average loss: 1.1256 \n",
      "Accuracy: 9489/10000 (94.89%)\n",
      "\n",
      "Round  14, Average loss 1.126 Test accuracy 94.890\n",
      "selected users: [ 0  2  5  7  8 10 11 12 13 14 15 16 17 19 20 22 23 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.3697 \n",
      "Accuracy: 9454/10000 (94.54%)\n",
      "\n",
      "Round  15, Average loss 0.370 Test accuracy 94.540\n",
      "selected users: [ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 17 21 22 23 24 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.8354 \n",
      "Accuracy: 9516/10000 (95.16%)\n",
      "\n",
      "Round  16, Average loss 0.835 Test accuracy 95.160\n",
      "selected users: [ 1  2  3  4  5  6  7  9 10 11 12 13 14 17 18 19 21 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.6963 \n",
      "Accuracy: 9517/10000 (95.17%)\n",
      "\n",
      "Round  17, Average loss 0.696 Test accuracy 95.170\n",
      "selected users: [ 0  1  2  3  4  5  6  8 10 11 12 13 14 15 17 18 19 20 22 23 24 27 28 30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.1040 \n",
      "Accuracy: 9528/10000 (95.28%)\n",
      "\n",
      "Round  18, Average loss 1.104 Test accuracy 95.280\n",
      "selected users: [ 1  2  3  4  5  6  7  9 10 11 14 15 16 17 21 22 23 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.8420 \n",
      "Accuracy: 9543/10000 (95.43%)\n",
      "\n",
      "Round  19, Average loss 0.842 Test accuracy 95.430\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 16 17 21 22 24 25 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.0029 \n",
      "Accuracy: 9518/10000 (95.18%)\n",
      "\n",
      "Round  20, Average loss 1.003 Test accuracy 95.180\n",
      "selected users: [ 0  1  2  3  4  7  8  9 10 11 13 14 15 16 18 19 20 21 22 24 25 26 27 29]\n",
      "\n",
      "Test set: Average loss: 1.2791 \n",
      "Accuracy: 9544/10000 (95.44%)\n",
      "\n",
      "Round  21, Average loss 1.279 Test accuracy 95.440\n",
      "selected users: [ 1  2  3  4  5  6  8  9 11 12 15 16 17 18 19 20 21 22 24 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.2829 \n",
      "Accuracy: 9531/10000 (95.31%)\n",
      "\n",
      "Round  22, Average loss 1.283 Test accuracy 95.310\n",
      "selected users: [ 0  1  2  3  4  5  6  8  9 10 14 16 17 18 19 20 21 23 24 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.7485 \n",
      "Accuracy: 9516/10000 (95.16%)\n",
      "\n",
      "Round  23, Average loss 0.748 Test accuracy 95.160\n",
      "selected users: [ 0  1  2  3  4  6  7  8  9 10 11 12 13 14 15 17 19 20 22 24 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.9412 \n",
      "Accuracy: 9527/10000 (95.27%)\n",
      "\n",
      "Round  24, Average loss 0.941 Test accuracy 95.270\n",
      "selected users: [ 0  2  4  5  6  7  8  9 10 11 12 13 14 16 17 18 19 21 23 24 25 26 27 29]\n",
      "\n",
      "Test set: Average loss: 0.6998 \n",
      "Accuracy: 9519/10000 (95.19%)\n",
      "\n",
      "Round  25, Average loss 0.700 Test accuracy 95.190\n",
      "selected users: [ 0  1  2  4  7  8  9 10 11 12 14 15 16 17 18 19 20 22 24 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.0223 \n",
      "Accuracy: 9561/10000 (95.61%)\n",
      "\n",
      "Round  26, Average loss 1.022 Test accuracy 95.610\n",
      "selected users: [ 0  1  4  5  7  8 11 12 13 14 15 16 17 18 19 20 21 22 24 25 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.8493 \n",
      "Accuracy: 9531/10000 (95.31%)\n",
      "\n",
      "Round  27, Average loss 0.849 Test accuracy 95.310\n",
      "selected users: [ 0  1  2  3  4  5  6  8  9 10 11 12 14 15 16 18 20 21 22 24 25 26 27 30]\n",
      "\n",
      "Test set: Average loss: 1.0949 \n",
      "Accuracy: 9535/10000 (95.35%)\n",
      "\n",
      "Round  28, Average loss 1.095 Test accuracy 95.350\n",
      "selected users: [ 3  4  6  7  9 10 11 12 13 14 15 16 17 18 19 20 23 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.0787 \n",
      "Accuracy: 9528/10000 (95.28%)\n",
      "\n",
      "Round  29, Average loss 1.079 Test accuracy 95.280\n",
      "(m= 24 )  8 -th Trial!!\n",
      "selected users: [ 0  1  2  4  5  6  7  8  9 10 12 13 14 16 17 19 21 22 23 24 25 26 28 29]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 1  3  5  6  7  8 10 11 12 13 15 16 17 19 20 21 22 23 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.2740 \n",
      "Accuracy: 5852/10000 (58.52%)\n",
      "\n",
      "Round   1, Average loss 2.274 Test accuracy 58.520\n",
      "selected users: [ 0  1  2  3  5  6  7  8  9 11 12 13 14 15 16 19 20 21 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5956 \n",
      "Accuracy: 9271/10000 (92.71%)\n",
      "\n",
      "Round   2, Average loss 1.596 Test accuracy 92.710\n",
      "selected users: [ 0  2  4  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.1660 \n",
      "Accuracy: 9461/10000 (94.61%)\n",
      "\n",
      "Round   3, Average loss 1.166 Test accuracy 94.610\n",
      "selected users: [ 0  1  2  3  4  6  8  9 10 11 12 14 15 16 18 19 21 23 24 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.8370 \n",
      "Accuracy: 9409/10000 (94.09%)\n",
      "\n",
      "Round   4, Average loss 0.837 Test accuracy 94.090\n",
      "selected users: [ 0  1  2  3  4  6  7  9 10 11 12 14 15 16 17 18 20 22 23 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.9798 \n",
      "Accuracy: 9445/10000 (94.45%)\n",
      "\n",
      "Round   5, Average loss 0.980 Test accuracy 94.450\n",
      "selected users: [ 1  2  4  5  7  9 11 13 14 15 16 17 18 19 20 21 22 23 24 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.9757 \n",
      "Accuracy: 9480/10000 (94.80%)\n",
      "\n",
      "Round   6, Average loss 0.976 Test accuracy 94.800\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 11 12 13 14 16 17 18 19 20 23 25 26 27 30]\n",
      "\n",
      "Test set: Average loss: 1.2590 \n",
      "Accuracy: 9528/10000 (95.28%)\n",
      "\n",
      "Round   7, Average loss 1.259 Test accuracy 95.280\n",
      "selected users: [ 0  1  3  5  6  7  8 10 11 13 14 15 16 17 19 20 21 22 23 24 25 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.1019 \n",
      "Accuracy: 9508/10000 (95.08%)\n",
      "\n",
      "Round   8, Average loss 1.102 Test accuracy 95.080\n",
      "selected users: [ 0  1  2  3  4  5  7  8  9 10 11 13 15 16 17 18 19 20 22 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.1629 \n",
      "Accuracy: 9531/10000 (95.31%)\n",
      "\n",
      "Round   9, Average loss 1.163 Test accuracy 95.310\n",
      "selected users: [ 0  1  2  4  5  6  7  8 11 12 13 14 16 19 21 22 23 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.2052 \n",
      "Accuracy: 9507/10000 (95.07%)\n",
      "\n",
      "Round  10, Average loss 1.205 Test accuracy 95.070\n",
      "selected users: [ 1  2  6  7  8  9 10 11 12 13 14 15 16 17 18 20 21 22 24 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.2632 \n",
      "Accuracy: 9514/10000 (95.14%)\n",
      "\n",
      "Round  11, Average loss 1.263 Test accuracy 95.140\n",
      "selected users: [ 0  1  2  3  4  6  7  9 10 11 12 13 15 16 18 19 20 21 22 23 24 26 27 28]\n",
      "\n",
      "Test set: Average loss: 1.1228 \n",
      "Accuracy: 9491/10000 (94.91%)\n",
      "\n",
      "Round  12, Average loss 1.123 Test accuracy 94.910\n",
      "selected users: [ 0  4  5  8  9 10 11 12 13 14 15 16 17 19 20 21 22 23 24 25 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.4910 \n",
      "Accuracy: 9489/10000 (94.89%)\n",
      "\n",
      "Round  13, Average loss 0.491 Test accuracy 94.890\n",
      "selected users: [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 21 23 24 26 27]\n",
      "\n",
      "Test set: Average loss: 1.2612 \n",
      "Accuracy: 9503/10000 (95.03%)\n",
      "\n",
      "Round  14, Average loss 1.261 Test accuracy 95.030\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8 10 12 14 15 17 19 20 22 23 24 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.9566 \n",
      "Accuracy: 9537/10000 (95.37%)\n",
      "\n",
      "Round  15, Average loss 0.957 Test accuracy 95.370\n",
      "selected users: [ 0  2  3  4  6  7  8  9 10 11 12 13 15 16 17 18 19 23 24 25 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.8498 \n",
      "Accuracy: 9552/10000 (95.52%)\n",
      "\n",
      "Round  16, Average loss 0.850 Test accuracy 95.520\n",
      "selected users: [ 0  2  3  4  5  6  7  9 10 11 12 13 14 16 17 18 19 20 23 24 25 26 28 29]\n",
      "\n",
      "Test set: Average loss: 0.9897 \n",
      "Accuracy: 9502/10000 (95.02%)\n",
      "\n",
      "Round  17, Average loss 0.990 Test accuracy 95.020\n",
      "selected users: [ 1  2  3  5  8  9 10 12 13 14 15 16 18 19 20 21 22 23 24 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.4781 \n",
      "Accuracy: 9469/10000 (94.69%)\n",
      "\n",
      "Round  18, Average loss 0.478 Test accuracy 94.690\n",
      "selected users: [ 0  1  3  6  7  8  9 10 11 12 13 15 16 18 19 20 22 23 24 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.8855 \n",
      "Accuracy: 9481/10000 (94.81%)\n",
      "\n",
      "Round  19, Average loss 0.885 Test accuracy 94.810\n",
      "selected users: [ 0  1  2  3  4  5  6  8 10 11 13 14 15 17 18 20 22 23 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.1046 \n",
      "Accuracy: 9504/10000 (95.04%)\n",
      "\n",
      "Round  20, Average loss 1.105 Test accuracy 95.040\n",
      "selected users: [ 0  1  3  4  6  7  9 10 11 12 13 17 18 19 20 21 22 23 24 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.0562 \n",
      "Accuracy: 9506/10000 (95.06%)\n",
      "\n",
      "Round  21, Average loss 1.056 Test accuracy 95.060\n",
      "selected users: [ 0  1  2  3  4  6  8  9 10 11 12 14 15 17 18 19 21 22 24 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.2487 \n",
      "Accuracy: 9543/10000 (95.43%)\n",
      "\n",
      "Round  22, Average loss 1.249 Test accuracy 95.430\n",
      "selected users: [ 0  2  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 23 24 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.8083 \n",
      "Accuracy: 9447/10000 (94.47%)\n",
      "\n",
      "Round  23, Average loss 0.808 Test accuracy 94.470\n",
      "selected users: [ 1  3  4  5  6  7 10 11 12 13 14 15 16 17 18 19 20 22 23 24 25 26 27 28]\n",
      "\n",
      "Test set: Average loss: 0.9611 \n",
      "Accuracy: 9499/10000 (94.99%)\n",
      "\n",
      "Round  24, Average loss 0.961 Test accuracy 94.990\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 15 16 19 20 21 22 24 26 27 28]\n",
      "\n",
      "Test set: Average loss: 1.0808 \n",
      "Accuracy: 9460/10000 (94.60%)\n",
      "\n",
      "Round  25, Average loss 1.081 Test accuracy 94.600\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 11 12 13 14 15 16 17 19 20 22 23 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.0185 \n",
      "Accuracy: 9528/10000 (95.28%)\n",
      "\n",
      "Round  26, Average loss 1.019 Test accuracy 95.280\n",
      "selected users: [ 0  1  3  4  5  6  8 10 11 12 13 14 16 17 18 19 20 21 22 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.3505 \n",
      "Accuracy: 9524/10000 (95.24%)\n",
      "\n",
      "Round  27, Average loss 1.351 Test accuracy 95.240\n",
      "selected users: [ 0  1  2  4  5  7  8  9 10 12 13 14 15 16 17 19 20 21 23 25 26 27 28 30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.9784 \n",
      "Accuracy: 9497/10000 (94.97%)\n",
      "\n",
      "Round  28, Average loss 0.978 Test accuracy 94.970\n",
      "selected users: [ 1  2  3  4  5  6  7  8  9 11 13 14 15 16 17 18 19 22 23 24 25 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.1100 \n",
      "Accuracy: 9549/10000 (95.49%)\n",
      "\n",
      "Round  29, Average loss 1.110 Test accuracy 95.490\n",
      "(m= 24 )  9 -th Trial!!\n",
      "selected users: [ 1  3  4  5  6  7  8  9 10 11 12 14 15 16 18 20 22 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 1  2  4  5  6  7  8  9 10 11 12 13 14 16 18 19 20 22 24 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.2702 \n",
      "Accuracy: 6853/10000 (68.53%)\n",
      "\n",
      "Round   1, Average loss 2.270 Test accuracy 68.530\n",
      "selected users: [ 1  5  6  8  9 11 12 13 14 15 16 17 18 19 20 21 22 23 24 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3318 \n",
      "Accuracy: 9160/10000 (91.60%)\n",
      "\n",
      "Round   2, Average loss 1.332 Test accuracy 91.600\n",
      "selected users: [ 0  1  2  4  6  7  8  9 10 11 12 14 16 17 18 20 21 22 24 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.2589 \n",
      "Accuracy: 9361/10000 (93.61%)\n",
      "\n",
      "Round   3, Average loss 1.259 Test accuracy 93.610\n",
      "selected users: [ 0  3  4  5  8  9 10 12 13 14 15 16 17 18 19 21 23 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.7149 \n",
      "Accuracy: 9339/10000 (93.39%)\n",
      "\n",
      "Round   4, Average loss 0.715 Test accuracy 93.390\n",
      "selected users: [ 0  2  3  4  5  6  7  8  9 11 12 14 16 17 19 20 21 22 23 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.3128 \n",
      "Accuracy: 9447/10000 (94.47%)\n",
      "\n",
      "Round   5, Average loss 1.313 Test accuracy 94.470\n",
      "selected users: [ 0  1  3  5  8  9 10 11 12 13 14 15 16 17 18 19 20 22 23 24 25 26 27 28]\n",
      "\n",
      "Test set: Average loss: 0.7748 \n",
      "Accuracy: 9412/10000 (94.12%)\n",
      "\n",
      "Round   6, Average loss 0.775 Test accuracy 94.120\n",
      "selected users: [ 0  2  3  4  7  8 10 11 12 13 15 16 18 19 20 21 22 23 24 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.9829 \n",
      "Accuracy: 9428/10000 (94.28%)\n",
      "\n",
      "Round   7, Average loss 0.983 Test accuracy 94.280\n",
      "selected users: [ 0  1  3  4  6  7  8  9 10 12 13 15 16 18 19 20 21 23 24 25 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 0.9865 \n",
      "Accuracy: 9449/10000 (94.49%)\n",
      "\n",
      "Round   8, Average loss 0.986 Test accuracy 94.490\n",
      "selected users: [ 1  2  3  4  5  7  8  9 10 11 12 13 14 15 16 17 19 21 22 24 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.8563 \n",
      "Accuracy: 9463/10000 (94.63%)\n",
      "\n",
      "Round   9, Average loss 0.856 Test accuracy 94.630\n",
      "selected users: [ 0  2  3  5  6  7  8  9 10 11 12 14 15 17 19 20 21 22 24 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.6839 \n",
      "Accuracy: 9429/10000 (94.29%)\n",
      "\n",
      "Round  10, Average loss 0.684 Test accuracy 94.290\n",
      "selected users: [ 0  1  2  3  5  6  7  8  9 11 13 15 16 17 18 19 22 23 24 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.1233 \n",
      "Accuracy: 9452/10000 (94.52%)\n",
      "\n",
      "Round  11, Average loss 1.123 Test accuracy 94.520\n",
      "selected users: [ 0  1  4  5  6  8  9 10 11 12 14 15 16 17 20 21 22 23 24 25 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 1.5458 \n",
      "Accuracy: 9513/10000 (95.13%)\n",
      "\n",
      "Round  12, Average loss 1.546 Test accuracy 95.130\n",
      "selected users: [ 0  1  2  4  6  7  8  9 10 13 14 15 16 17 18 20 21 22 23 24 25 26 27 29]\n",
      "\n",
      "Test set: Average loss: 0.9943 \n",
      "Accuracy: 9487/10000 (94.87%)\n",
      "\n",
      "Round  13, Average loss 0.994 Test accuracy 94.870\n",
      "selected users: [ 0  1  3  4  5  6  7  9 12 13 16 17 18 19 20 21 22 23 24 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3393 \n",
      "Accuracy: 9521/10000 (95.21%)\n",
      "\n",
      "Round  14, Average loss 1.339 Test accuracy 95.210\n",
      "selected users: [ 0  1  3  4  5  6  8 10 11 12 13 14 16 18 20 21 22 23 24 25 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 1.1659 \n",
      "Accuracy: 9497/10000 (94.97%)\n",
      "\n",
      "Round  15, Average loss 1.166 Test accuracy 94.970\n",
      "selected users: [ 0  1  2  3  4  6  7  8  9 10 12 14 15 16 17 18 20 22 24 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.0835 \n",
      "Accuracy: 9493/10000 (94.93%)\n",
      "\n",
      "Round  16, Average loss 1.083 Test accuracy 94.930\n",
      "selected users: [ 0  1  2  3  4  5  6  9 11 12 14 15 17 19 20 21 23 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3594 \n",
      "Accuracy: 9510/10000 (95.10%)\n",
      "\n",
      "Round  17, Average loss 1.359 Test accuracy 95.100\n",
      "selected users: [ 0  1  3  4  7  8 10 11 12 13 14 15 17 18 19 20 21 22 23 24 25 26 27 30]\n",
      "\n",
      "Test set: Average loss: 1.2338 \n",
      "Accuracy: 9526/10000 (95.26%)\n",
      "\n",
      "Round  18, Average loss 1.234 Test accuracy 95.260\n",
      "selected users: [ 0  1  2  3  4  6  7  8  9 10 11 12 14 16 17 18 20 21 22 23 24 25 26 30]\n",
      "\n",
      "Test set: Average loss: 0.9447 \n",
      "Accuracy: 9539/10000 (95.39%)\n",
      "\n",
      "Round  19, Average loss 0.945 Test accuracy 95.390\n",
      "selected users: [ 0  2  3  4  5  6  7  8  9 10 11 13 14 15 18 20 22 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.8635 \n",
      "Accuracy: 9497/10000 (94.97%)\n",
      "\n",
      "Round  20, Average loss 0.864 Test accuracy 94.970\n",
      "selected users: [ 0  3  4  5  6  7  8  9 10 12 13 14 15 16 17 19 20 21 23 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.8375 \n",
      "Accuracy: 9442/10000 (94.42%)\n",
      "\n",
      "Round  21, Average loss 0.837 Test accuracy 94.420\n",
      "selected users: [ 0  1  3  4  5  6  7  9 10 11 12 15 17 18 20 21 22 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.4127 \n",
      "Accuracy: 9485/10000 (94.85%)\n",
      "\n",
      "Round  22, Average loss 1.413 Test accuracy 94.850\n",
      "selected users: [ 0  1  2  4  5  6  7  8  9 10 11 12 13 16 17 18 19 20 22 23 24 25 27 29]\n",
      "\n",
      "Test set: Average loss: 1.2054 \n",
      "Accuracy: 9451/10000 (94.51%)\n",
      "\n",
      "Round  23, Average loss 1.205 Test accuracy 94.510\n",
      "selected users: [ 0  2  3  4  5  6  8  9 10 14 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.0422 \n",
      "Accuracy: 9531/10000 (95.31%)\n",
      "\n",
      "Round  24, Average loss 1.042 Test accuracy 95.310\n",
      "selected users: [ 0  1  3  5  6  7  8  9 11 12 13 14 16 17 18 19 20 21 22 23 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 0.9230 \n",
      "Accuracy: 9465/10000 (94.65%)\n",
      "\n",
      "Round  25, Average loss 0.923 Test accuracy 94.650\n",
      "selected users: [ 0  1  2  3  4  5  9 10 11 13 14 15 16 19 20 21 22 24 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.1051 \n",
      "Accuracy: 9486/10000 (94.86%)\n",
      "\n",
      "Round  26, Average loss 1.105 Test accuracy 94.860\n",
      "selected users: [ 0  1  3  4  5  6  7  8  9 10 12 15 16 17 19 20 21 22 24 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.0966 \n",
      "Accuracy: 9483/10000 (94.83%)\n",
      "\n",
      "Round  27, Average loss 1.097 Test accuracy 94.830\n",
      "selected users: [ 0  1  3  5  7  8  9 10 11 13 14 15 16 17 18 21 22 23 24 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 0.9649 \n",
      "Accuracy: 9503/10000 (95.03%)\n",
      "\n",
      "Round  28, Average loss 0.965 Test accuracy 95.030\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8 11 13 14 15 16 19 20 21 23 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.0359 \n",
      "Accuracy: 9479/10000 (94.79%)\n",
      "\n",
      "Round  29, Average loss 1.036 Test accuracy 94.790\n",
      "number of results: 27\n",
      "(m= 27 )  0 -th Trial!!\n",
      "selected users: [ 0  1  2  4  7  8  9 10 11 12 13 14 15 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 2  3  4  6  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.2535 \n",
      "Accuracy: 7879/10000 (78.79%)\n",
      "\n",
      "Round   1, Average loss 2.254 Test accuracy 78.790\n",
      "selected users: [ 0  1  2  3  5  6  7  9 10 11 12 13 14 15 16 17 18 19 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.8315 \n",
      "Accuracy: 9255/10000 (92.55%)\n",
      "\n",
      "Round   2, Average loss 1.832 Test accuracy 92.550\n",
      "selected users: [ 0  1  2  3  5  6  7  8  9 11 12 13 14 15 16 17 19 20 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.4886 \n",
      "Accuracy: 9443/10000 (94.43%)\n",
      "\n",
      "Round   3, Average loss 1.489 Test accuracy 94.430\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 11 12 13 15 16 17 18 19 21 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3068 \n",
      "Accuracy: 9500/10000 (95.00%)\n",
      "\n",
      "Round   4, Average loss 1.307 Test accuracy 95.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  9 12 13 14 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6262 \n",
      "Accuracy: 9523/10000 (95.23%)\n",
      "\n",
      "Round   5, Average loss 1.626 Test accuracy 95.230\n",
      "selected users: [ 0  1  2  3  4  6  7  8 10 11 12 13 14 15 16 17 18 19 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3766 \n",
      "Accuracy: 9523/10000 (95.23%)\n",
      "\n",
      "Round   6, Average loss 1.377 Test accuracy 95.230\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 12 13 14 15 16 17 20 21 22 23 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6932 \n",
      "Accuracy: 9489/10000 (94.89%)\n",
      "\n",
      "Round   7, Average loss 1.693 Test accuracy 94.890\n",
      "selected users: [ 0  2  3  4  5  6  7  8  9 10 12 13 14 15 17 18 19 20 21 23 24 25 26 27\n",
      " 28 29 30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.0877 \n",
      "Accuracy: 9549/10000 (95.49%)\n",
      "\n",
      "Round   8, Average loss 1.088 Test accuracy 95.490\n",
      "selected users: [ 0  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 18 19 20 21 23 24 25 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.9159 \n",
      "Accuracy: 9521/10000 (95.21%)\n",
      "\n",
      "Round   9, Average loss 0.916 Test accuracy 95.210\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 11 12 13 14 15 16 17 19 20 21 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.2439 \n",
      "Accuracy: 9504/10000 (95.04%)\n",
      "\n",
      "Round  10, Average loss 1.244 Test accuracy 95.040\n",
      "selected users: [ 1  2  3  4  5  6  7  8 10 12 13 14 15 16 17 18 19 20 21 22 23 24 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.1207 \n",
      "Accuracy: 9519/10000 (95.19%)\n",
      "\n",
      "Round  11, Average loss 1.121 Test accuracy 95.190\n",
      "selected users: [ 0  2  3  4  5  6  7 10 11 12 13 14 15 16 17 18 19 20 21 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.8847 \n",
      "Accuracy: 9467/10000 (94.67%)\n",
      "\n",
      "Round  12, Average loss 0.885 Test accuracy 94.670\n",
      "selected users: [ 0  1  2  3  4  5  7  8  9 10 11 12 13 16 17 18 19 20 21 22 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3756 \n",
      "Accuracy: 9503/10000 (95.03%)\n",
      "\n",
      "Round  13, Average loss 1.376 Test accuracy 95.030\n",
      "selected users: [ 0  1  2  3  6  7  8  9 10 11 12 14 15 16 17 18 19 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6102 \n",
      "Accuracy: 9531/10000 (95.31%)\n",
      "\n",
      "Round  14, Average loss 1.610 Test accuracy 95.310\n",
      "selected users: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 16 17 18 19 20 21 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3093 \n",
      "Accuracy: 9473/10000 (94.73%)\n",
      "\n",
      "Round  15, Average loss 1.309 Test accuracy 94.730\n",
      "selected users: [ 0  1  2  3  4  5  6  7  9 10 11 13 14 15 16 17 18 19 20 21 22 23 25 26\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.2978 \n",
      "Accuracy: 9520/10000 (95.20%)\n",
      "\n",
      "Round  16, Average loss 1.298 Test accuracy 95.200\n",
      "selected users: [ 1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 18 19 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3340 \n",
      "Accuracy: 9485/10000 (94.85%)\n",
      "\n",
      "Round  17, Average loss 1.334 Test accuracy 94.850\n",
      "selected users: [ 1  2  3  4  5  6  7  8  9 10 11 12 14 15 16 17 19 20 21 22 23 24 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.1647 \n",
      "Accuracy: 9542/10000 (95.42%)\n",
      "\n",
      "Round  18, Average loss 1.165 Test accuracy 95.420\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 15 16 19 20 21 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.2059 \n",
      "Accuracy: 9501/10000 (95.01%)\n",
      "\n",
      "Round  19, Average loss 1.206 Test accuracy 95.010\n",
      "selected users: [ 0  2  3  4  5  6  7  8  9 10 13 14 15 16 17 18 19 20 21 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.1227 \n",
      "Accuracy: 9474/10000 (94.74%)\n",
      "\n",
      "Round  20, Average loss 1.123 Test accuracy 94.740\n",
      "selected users: [ 0  1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.5678 \n",
      "Accuracy: 9521/10000 (95.21%)\n",
      "\n",
      "Round  21, Average loss 1.568 Test accuracy 95.210\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 16 17 18 19 20 21 22 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5282 \n",
      "Accuracy: 9478/10000 (94.78%)\n",
      "\n",
      "Round  22, Average loss 1.528 Test accuracy 94.780\n",
      "selected users: [ 0  1  3  4  5  6  7 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.2841 \n",
      "Accuracy: 9507/10000 (95.07%)\n",
      "\n",
      "Round  23, Average loss 1.284 Test accuracy 95.070\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 12 13 14 15 16 17 18 19 21 22 23 25 26\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.2421 \n",
      "Accuracy: 9474/10000 (94.74%)\n",
      "\n",
      "Round  24, Average loss 1.242 Test accuracy 94.740\n",
      "selected users: [ 1  2  3  4  6  7  8  9 10 11 12 13 14 15 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 30]\n",
      "\n",
      "Test set: Average loss: 1.2231 \n",
      "Accuracy: 9510/10000 (95.10%)\n",
      "\n",
      "Round  25, Average loss 1.223 Test accuracy 95.100\n",
      "selected users: [ 0  1  2  4  5  6  7  8 10 11 12 13 14 15 16 17 18 19 20 21 22 24 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.2410 \n",
      "Accuracy: 9472/10000 (94.72%)\n",
      "\n",
      "Round  26, Average loss 1.241 Test accuracy 94.720\n",
      "selected users: [ 0  2  3  4  5  6  7  8  9 10 11 12 13 14 15 17 18 19 20 21 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.8950 \n",
      "Accuracy: 9509/10000 (95.09%)\n",
      "\n",
      "Round  27, Average loss 0.895 Test accuracy 95.090\n",
      "selected users: [ 0  1  2  5  7  8  9 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.0328 \n",
      "Accuracy: 9504/10000 (95.04%)\n",
      "\n",
      "Round  28, Average loss 1.033 Test accuracy 95.040\n",
      "selected users: [ 0  1  2  3  4  5  6  8  9 11 12 13 14 16 17 18 19 20 21 22 23 24 25 26\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3360 \n",
      "Accuracy: 9516/10000 (95.16%)\n",
      "\n",
      "Round  29, Average loss 1.336 Test accuracy 95.160\n",
      "(m= 27 )  1 -th Trial!!\n",
      "selected users: [ 0  1  4  5  6  7  8  9 10 11 12 13 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3021 \n",
      "Accuracy: 1903/10000 (19.03%)\n",
      "\n",
      "Round   0, Average loss 2.302 Test accuracy 19.030\n",
      "selected users: [ 0  2  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 30]\n",
      "\n",
      "Test set: Average loss: 2.2267 \n",
      "Accuracy: 7957/10000 (79.57%)\n",
      "\n",
      "Round   1, Average loss 2.227 Test accuracy 79.570\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 17 20 21 22 23 24 25 26\n",
      " 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7670 \n",
      "Accuracy: 9416/10000 (94.16%)\n",
      "\n",
      "Round   2, Average loss 1.767 Test accuracy 94.160\n",
      "selected users: [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 17 19 20 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6553 \n",
      "Accuracy: 9444/10000 (94.44%)\n",
      "\n",
      "Round   3, Average loss 1.655 Test accuracy 94.440\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 13 14 16 17 18 20 21 22 23 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6618 \n",
      "Accuracy: 9476/10000 (94.76%)\n",
      "\n",
      "Round   4, Average loss 1.662 Test accuracy 94.760\n",
      "selected users: [ 0  1  2  3  4  6  7  8  9 10 11 12 13 14 15 16 17 19 21 22 23 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5355 \n",
      "Accuracy: 9476/10000 (94.76%)\n",
      "\n",
      "Round   5, Average loss 1.536 Test accuracy 94.760\n",
      "selected users: [ 1  2  3  4  5  6  7  9 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5974 \n",
      "Accuracy: 9467/10000 (94.67%)\n",
      "\n",
      "Round   6, Average loss 1.597 Test accuracy 94.670\n",
      "selected users: [ 0  1  2  3  4  5  7  8  9 10 11 12 13 15 16 17 18 19 20 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6794 \n",
      "Accuracy: 9460/10000 (94.60%)\n",
      "\n",
      "Round   7, Average loss 1.679 Test accuracy 94.600\n",
      "selected users: [ 1  2  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 22 23 24 25 26\n",
      " 27 28 30]\n",
      "\n",
      "Test set: Average loss: 1.4746 \n",
      "Accuracy: 9489/10000 (94.89%)\n",
      "\n",
      "Round   8, Average loss 1.475 Test accuracy 94.890\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 14 16 17 19 20 21 22 23 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6648 \n",
      "Accuracy: 9479/10000 (94.79%)\n",
      "\n",
      "Round   9, Average loss 1.665 Test accuracy 94.790\n",
      "selected users: [ 0  3  4  5  6  7  8  9 10 11 12 13 14 15 17 18 19 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3239 \n",
      "Accuracy: 9492/10000 (94.92%)\n",
      "\n",
      "Round  10, Average loss 1.324 Test accuracy 94.920\n",
      "selected users: [ 0  1  2  3  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 22 24 25 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.4525 \n",
      "Accuracy: 9488/10000 (94.88%)\n",
      "\n",
      "Round  11, Average loss 1.453 Test accuracy 94.880\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 17 18 21 22 23 24 25 26\n",
      " 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5064 \n",
      "Accuracy: 9480/10000 (94.80%)\n",
      "\n",
      "Round  12, Average loss 1.506 Test accuracy 94.800\n",
      "selected users: [ 0  1  3  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 30]\n",
      "\n",
      "Test set: Average loss: 1.4808 \n",
      "Accuracy: 9489/10000 (94.89%)\n",
      "\n",
      "Round  13, Average loss 1.481 Test accuracy 94.890\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8 10 11 13 14 15 16 17 18 19 20 21 22 23 25 26\n",
      " 27 28 30]\n",
      "\n",
      "Test set: Average loss: 1.6194 \n",
      "Accuracy: 9479/10000 (94.79%)\n",
      "\n",
      "Round  14, Average loss 1.619 Test accuracy 94.790\n",
      "selected users: [ 0  1  2  4  5  6  7  8  9 10 11 12 14 15 16 17 18 19 20 21 22 24 25 26\n",
      " 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.6889 \n",
      "Accuracy: 9482/10000 (94.82%)\n",
      "\n",
      "Round  15, Average loss 1.689 Test accuracy 94.820\n",
      "selected users: [ 2  3  4  6  7  8  9 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29 30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.4599 \n",
      "Accuracy: 9502/10000 (95.02%)\n",
      "\n",
      "Round  16, Average loss 1.460 Test accuracy 95.020\n",
      "selected users: [ 1  3  4  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.4735 \n",
      "Accuracy: 9503/10000 (95.03%)\n",
      "\n",
      "Round  17, Average loss 1.473 Test accuracy 95.030\n",
      "selected users: [ 1  2  3  4  5  6  7 10 11 12 13 14 15 16 17 18 19 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3477 \n",
      "Accuracy: 9494/10000 (94.94%)\n",
      "\n",
      "Round  18, Average loss 1.348 Test accuracy 94.940\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 19 21 22 23 24 25 26\n",
      " 27 28 30]\n",
      "\n",
      "Test set: Average loss: 1.5643 \n",
      "Accuracy: 9513/10000 (95.13%)\n",
      "\n",
      "Round  19, Average loss 1.564 Test accuracy 95.130\n",
      "selected users: [ 1  2  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3414 \n",
      "Accuracy: 9471/10000 (94.71%)\n",
      "\n",
      "Round  20, Average loss 1.341 Test accuracy 94.710\n",
      "selected users: [ 0  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 20 21 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.2935 \n",
      "Accuracy: 9487/10000 (94.87%)\n",
      "\n",
      "Round  21, Average loss 1.293 Test accuracy 94.870\n",
      "selected users: [ 0  1  2  3  4  5  7  9 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.6303 \n",
      "Accuracy: 9499/10000 (94.99%)\n",
      "\n",
      "Round  22, Average loss 1.630 Test accuracy 94.990\n",
      "selected users: [ 0  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 19 20 21 22 23 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.2174 \n",
      "Accuracy: 9473/10000 (94.73%)\n",
      "\n",
      "Round  23, Average loss 1.217 Test accuracy 94.730\n",
      "selected users: [ 1  2  3  4  5  6  7  9 10 11 13 14 15 16 17 18 19 20 21 22 23 24 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.4291 \n",
      "Accuracy: 9509/10000 (95.09%)\n",
      "\n",
      "Round  24, Average loss 1.429 Test accuracy 95.090\n",
      "selected users: [ 0  1  3  4  5  7  9 10 11 12 13 14 15 16 17 18 19 20 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.4657 \n",
      "Accuracy: 9491/10000 (94.91%)\n",
      "\n",
      "Round  25, Average loss 1.466 Test accuracy 94.910\n",
      "selected users: [ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 16 17 19 20 21 22 23 24 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5039 \n",
      "Accuracy: 9484/10000 (94.84%)\n",
      "\n",
      "Round  26, Average loss 1.504 Test accuracy 94.840\n",
      "selected users: [ 0  1  2  4  6  7  8  9 10 12 13 14 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5096 \n",
      "Accuracy: 9510/10000 (95.10%)\n",
      "\n",
      "Round  27, Average loss 1.510 Test accuracy 95.100\n",
      "selected users: [ 0  2  3  4  5  6  7  8  9 10 11 12 13 14 15 17 18 19 20 21 23 24 25 26\n",
      " 27 28 30]\n",
      "\n",
      "Test set: Average loss: 1.4003 \n",
      "Accuracy: 9492/10000 (94.92%)\n",
      "\n",
      "Round  28, Average loss 1.400 Test accuracy 94.920\n",
      "selected users: [ 1  2  4  5  6  7  8  9 10 11 12 13 14 15 16 18 20 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.4520 \n",
      "Accuracy: 9492/10000 (94.92%)\n",
      "\n",
      "Round  29, Average loss 1.452 Test accuracy 94.920\n",
      "(m= 27 )  2 -th Trial!!\n",
      "selected users: [ 0  1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 24 25 26\n",
      " 27 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  1  2  3  4  5  6  8  9 10 11 12 14 15 16 17 18 19 20 21 23 24 25 26\n",
      " 27 28 29]\n",
      "\n",
      "Test set: Average loss: 2.2952 \n",
      "Accuracy: 5454/10000 (54.54%)\n",
      "\n",
      "Round   1, Average loss 2.295 Test accuracy 54.540\n",
      "selected users: [ 0  2  3  4  6  7  8  9 10 11 12 13 14 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 30]\n",
      "\n",
      "Test set: Average loss: 1.6006 \n",
      "Accuracy: 9256/10000 (92.56%)\n",
      "\n",
      "Round   2, Average loss 1.601 Test accuracy 92.560\n",
      "selected users: [ 0  3  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.4382 \n",
      "Accuracy: 9426/10000 (94.26%)\n",
      "\n",
      "Round   3, Average loss 1.438 Test accuracy 94.260\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 16 18 19 20 22 23 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.4760 \n",
      "Accuracy: 9504/10000 (95.04%)\n",
      "\n",
      "Round   4, Average loss 1.476 Test accuracy 95.040\n",
      "selected users: [ 0  1  2  3  4  5  6  8  9 10 11 12 14 15 16 17 18 19 20 21 22 23 25 26\n",
      " 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.4345 \n",
      "Accuracy: 9533/10000 (95.33%)\n",
      "\n",
      "Round   5, Average loss 1.434 Test accuracy 95.330\n",
      "selected users: [ 1  3  4  5  6  7  9 10 11 12 13 14 15 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3308 \n",
      "Accuracy: 9537/10000 (95.37%)\n",
      "\n",
      "Round   6, Average loss 1.331 Test accuracy 95.370\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 12 13 14 15 16 17 18 20 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5590 \n",
      "Accuracy: 9510/10000 (95.10%)\n",
      "\n",
      "Round   7, Average loss 1.559 Test accuracy 95.100\n",
      "selected users: [ 0  1  2  4  5  6  7  8  9 11 12 13 14 15 16 17 18 19 21 22 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5072 \n",
      "Accuracy: 9504/10000 (95.04%)\n",
      "\n",
      "Round   8, Average loss 1.507 Test accuracy 95.040\n",
      "selected users: [ 0  1  2  3  4  6  7  9 10 11 12 13 14 15 17 18 19 20 21 22 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3688 \n",
      "Accuracy: 9521/10000 (95.21%)\n",
      "\n",
      "Round   9, Average loss 1.369 Test accuracy 95.210\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8 11 12 13 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.4815 \n",
      "Accuracy: 9485/10000 (94.85%)\n",
      "\n",
      "Round  10, Average loss 1.482 Test accuracy 94.850\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 13 15 16 17 18 19 21 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.2414 \n",
      "Accuracy: 9488/10000 (94.88%)\n",
      "\n",
      "Round  11, Average loss 1.241 Test accuracy 94.880\n",
      "selected users: [ 0  1  2  4  5  7  8  9 10 11 12 13 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 30]\n",
      "\n",
      "Test set: Average loss: 1.3872 \n",
      "Accuracy: 9521/10000 (95.21%)\n",
      "\n",
      "Round  12, Average loss 1.387 Test accuracy 95.210\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 23 25\n",
      " 26 27 30]\n",
      "\n",
      "Test set: Average loss: 1.3124 \n",
      "Accuracy: 9519/10000 (95.19%)\n",
      "\n",
      "Round  13, Average loss 1.312 Test accuracy 95.190\n",
      "selected users: [ 0  1  2  3  4  6  7  8  9 10 11 12 13 15 16 17 19 20 21 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.2447 \n",
      "Accuracy: 9509/10000 (95.09%)\n",
      "\n",
      "Round  14, Average loss 1.245 Test accuracy 95.090\n",
      "selected users: [ 0  1  2  3  4  5  6  8  9 10 11 12 13 14 16 17 18 19 20 21 23 24 25 26\n",
      " 27 28 30]\n",
      "\n",
      "Test set: Average loss: 1.3923 \n",
      "Accuracy: 9511/10000 (95.11%)\n",
      "\n",
      "Round  15, Average loss 1.392 Test accuracy 95.110\n",
      "selected users: [ 0  1  2  3  4  5  6  7  9 10 12 13 14 15 16 17 19 20 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6180 \n",
      "Accuracy: 9515/10000 (95.15%)\n",
      "\n",
      "Round  16, Average loss 1.618 Test accuracy 95.150\n",
      "selected users: [ 0  1  2  4  5  6  7  8  9 11 12 13 14 15 16 17 18 19 20 22 23 24 25 26\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.4379 \n",
      "Accuracy: 9508/10000 (95.08%)\n",
      "\n",
      "Round  17, Average loss 1.438 Test accuracy 95.080\n",
      "selected users: [ 0  1  2  3  4  6  7  9 10 11 12 13 14 15 16 18 19 20 21 22 23 24 25 26\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.2211 \n",
      "Accuracy: 9528/10000 (95.28%)\n",
      "\n",
      "Round  18, Average loss 1.221 Test accuracy 95.280\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 13 15 16 17 18 19 20 21 22 24 25 26\n",
      " 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3234 \n",
      "Accuracy: 9493/10000 (94.93%)\n",
      "\n",
      "Round  19, Average loss 1.323 Test accuracy 94.930\n",
      "selected users: [ 0  1  3  4  6  7  8  9 10 11 12 13 15 16 17 18 20 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.4284 \n",
      "Accuracy: 9516/10000 (95.16%)\n",
      "\n",
      "Round  20, Average loss 1.428 Test accuracy 95.160\n",
      "selected users: [ 0  1  2  4  5  6  7  8 10 11 12 13 14 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5271 \n",
      "Accuracy: 9518/10000 (95.18%)\n",
      "\n",
      "Round  21, Average loss 1.527 Test accuracy 95.180\n",
      "selected users: [ 0  1  2  4  5  6  7  9 11 12 13 14 15 16 17 18 19 20 21 22 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.4919 \n",
      "Accuracy: 9521/10000 (95.21%)\n",
      "\n",
      "Round  22, Average loss 1.492 Test accuracy 95.210\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 12 13 15 16 17 18 19 20 21 22 24 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3210 \n",
      "Accuracy: 9520/10000 (95.20%)\n",
      "\n",
      "Round  23, Average loss 1.321 Test accuracy 95.200\n",
      "selected users: [ 0  2  3  4  6  7  8  9 10 11 12 14 15 16 17 18 19 20 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.1342 \n",
      "Accuracy: 9527/10000 (95.27%)\n",
      "\n",
      "Round  24, Average loss 1.134 Test accuracy 95.270\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 20 23 24 25 26\n",
      " 28 29 30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.3044 \n",
      "Accuracy: 9500/10000 (95.00%)\n",
      "\n",
      "Round  25, Average loss 1.304 Test accuracy 95.000\n",
      "selected users: [ 0  2  3  4  5  7  8 10 11 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.6482 \n",
      "Accuracy: 9470/10000 (94.70%)\n",
      "\n",
      "Round  26, Average loss 0.648 Test accuracy 94.700\n",
      "selected users: [ 0  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 18 19 20 22 23 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.1447 \n",
      "Accuracy: 9488/10000 (94.88%)\n",
      "\n",
      "Round  27, Average loss 1.145 Test accuracy 94.880\n",
      "selected users: [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 20 21 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3489 \n",
      "Accuracy: 9441/10000 (94.41%)\n",
      "\n",
      "Round  28, Average loss 1.349 Test accuracy 94.410\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8 10 11 12 13 14 15 17 18 19 20 21 22 23 25 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.2226 \n",
      "Accuracy: 9502/10000 (95.02%)\n",
      "\n",
      "Round  29, Average loss 1.223 Test accuracy 95.020\n",
      "(m= 27 )  3 -th Trial!!\n",
      "selected users: [ 0  1  3  4  5  7  8  9 10 11 12 13 14 16 17 18 19 20 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  2  3  4  5  7  8  9 11 12 13 14 15 16 17 19 20 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.2816 \n",
      "Accuracy: 7408/10000 (74.08%)\n",
      "\n",
      "Round   1, Average loss 2.282 Test accuracy 74.080\n",
      "selected users: [ 1  2  3  4  5  6  7  9 10 11 12 13 14 15 16 17 18 19 20 22 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7344 \n",
      "Accuracy: 9395/10000 (93.95%)\n",
      "\n",
      "Round   2, Average loss 1.734 Test accuracy 93.950\n",
      "selected users: [ 0  1  2  3  5  7  8  9 10 11 12 13 15 16 17 18 19 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3576 \n",
      "Accuracy: 9416/10000 (94.16%)\n",
      "\n",
      "Round   3, Average loss 1.358 Test accuracy 94.160\n",
      "selected users: [ 0  1  2  3  4  5  7  8  9 10 11 12 13 14 15 16 18 19 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.4456 \n",
      "Accuracy: 9443/10000 (94.43%)\n",
      "\n",
      "Round   4, Average loss 1.446 Test accuracy 94.430\n",
      "selected users: [ 0  1  2  3  5  8  9 10 11 12 13 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.9434 \n",
      "Accuracy: 9424/10000 (94.24%)\n",
      "\n",
      "Round   5, Average loss 0.943 Test accuracy 94.240\n",
      "selected users: [ 0  2  3  4  5  6  7  8  9 10 11 12 14 16 17 19 20 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3329 \n",
      "Accuracy: 9497/10000 (94.97%)\n",
      "\n",
      "Round   6, Average loss 1.333 Test accuracy 94.970\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8 10 12 13 14 15 16 17 18 19 20 21 22 23 25 26\n",
      " 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.4524 \n",
      "Accuracy: 9484/10000 (94.84%)\n",
      "\n",
      "Round   7, Average loss 1.452 Test accuracy 94.840\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8 10 11 12 13 14 16 17 18 19 20 22 23 24 25 26\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3588 \n",
      "Accuracy: 9485/10000 (94.85%)\n",
      "\n",
      "Round   8, Average loss 1.359 Test accuracy 94.850\n",
      "selected users: [ 0  1  3  4  5  6  7  8  9 10 11 13 14 15 16 17 18 19 20 22 23 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5141 \n",
      "Accuracy: 9516/10000 (95.16%)\n",
      "\n",
      "Round   9, Average loss 1.514 Test accuracy 95.160\n",
      "selected users: [ 0  1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 19 20 22 23 24 25 26\n",
      " 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.6393 \n",
      "Accuracy: 9490/10000 (94.90%)\n",
      "\n",
      "Round  10, Average loss 1.639 Test accuracy 94.900\n",
      "selected users: [ 0  1  2  3  4  5  6  7  9 10 12 13 14 15 16 17 18 19 21 22 23 24 25 26\n",
      " 27 28 30]\n",
      "\n",
      "Test set: Average loss: 1.4527 \n",
      "Accuracy: 9490/10000 (94.90%)\n",
      "\n",
      "Round  11, Average loss 1.453 Test accuracy 94.900\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 21 22 23 24 25 26\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.2336 \n",
      "Accuracy: 9498/10000 (94.98%)\n",
      "\n",
      "Round  12, Average loss 1.234 Test accuracy 94.980\n",
      "selected users: [ 0  1  2  4  5  6  7 10 11 12 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3060 \n",
      "Accuracy: 9486/10000 (94.86%)\n",
      "\n",
      "Round  13, Average loss 1.306 Test accuracy 94.860\n",
      "selected users: [ 0  2  3  4  5  6  7  8  9 10 12 13 14 15 16 17 19 20 21 22 23 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3569 \n",
      "Accuracy: 9514/10000 (95.14%)\n",
      "\n",
      "Round  14, Average loss 1.357 Test accuracy 95.140\n",
      "selected users: [ 1  2  4  5  6  7  8  9 10 11 12 13 14 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.4973 \n",
      "Accuracy: 9504/10000 (95.04%)\n",
      "\n",
      "Round  15, Average loss 1.497 Test accuracy 95.040\n",
      "selected users: [ 0  1  2  3  5  6  7  8 10 12 13 14 15 16 17 18 19 20 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3254 \n",
      "Accuracy: 9482/10000 (94.82%)\n",
      "\n",
      "Round  16, Average loss 1.325 Test accuracy 94.820\n",
      "selected users: [ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 19 20 21 22 23 24 25 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.4890 \n",
      "Accuracy: 9513/10000 (95.13%)\n",
      "\n",
      "Round  17, Average loss 1.489 Test accuracy 95.130\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 17 19 20 21 22 23 24 25\n",
      " 27 28 30]\n",
      "\n",
      "Test set: Average loss: 1.5165 \n",
      "Accuracy: 9478/10000 (94.78%)\n",
      "\n",
      "Round  18, Average loss 1.517 Test accuracy 94.780\n",
      "selected users: [ 0  1  2  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3830 \n",
      "Accuracy: 9475/10000 (94.75%)\n",
      "\n",
      "Round  19, Average loss 1.383 Test accuracy 94.750\n",
      "selected users: [ 0  2  3  4  5  6  7  9 10 12 13 14 15 16 17 19 20 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.2936 \n",
      "Accuracy: 9505/10000 (95.05%)\n",
      "\n",
      "Round  20, Average loss 1.294 Test accuracy 95.050\n",
      "selected users: [ 0  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 18 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.2094 \n",
      "Accuracy: 9498/10000 (94.98%)\n",
      "\n",
      "Round  21, Average loss 1.209 Test accuracy 94.980\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 12 13 14 15 16 17 18 19 20 22 23 24 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.2257 \n",
      "Accuracy: 9525/10000 (95.25%)\n",
      "\n",
      "Round  22, Average loss 1.226 Test accuracy 95.250\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 15 16 17 18 19 20 22 24 25 26\n",
      " 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.4643 \n",
      "Accuracy: 9504/10000 (95.04%)\n",
      "\n",
      "Round  23, Average loss 1.464 Test accuracy 95.040\n",
      "selected users: [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 24 26\n",
      " 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.2065 \n",
      "Accuracy: 9482/10000 (94.82%)\n",
      "\n",
      "Round  24, Average loss 1.207 Test accuracy 94.820\n",
      "selected users: [ 2  3  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.2278 \n",
      "Accuracy: 9514/10000 (95.14%)\n",
      "\n",
      "Round  25, Average loss 1.228 Test accuracy 95.140\n",
      "selected users: [ 0  1  2  3  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5043 \n",
      "Accuracy: 9513/10000 (95.13%)\n",
      "\n",
      "Round  26, Average loss 1.504 Test accuracy 95.130\n",
      "selected users: [ 1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.3107 \n",
      "Accuracy: 9487/10000 (94.87%)\n",
      "\n",
      "Round  27, Average loss 1.311 Test accuracy 94.870\n",
      "selected users: [ 1  3  4  5  6  7  9 10 11 12 13 14 15 16 17 18 19 20 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5031 \n",
      "Accuracy: 9518/10000 (95.18%)\n",
      "\n",
      "Round  28, Average loss 1.503 Test accuracy 95.180\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 13 16 17 19 20 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5957 \n",
      "Accuracy: 9461/10000 (94.61%)\n",
      "\n",
      "Round  29, Average loss 1.596 Test accuracy 94.610\n",
      "(m= 27 )  4 -th Trial!!\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 21 22 24 25 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 17 18 19 20 22 23 24 25 26\n",
      " 27 28 29]\n",
      "\n",
      "Test set: Average loss: 2.2675 \n",
      "Accuracy: 7443/10000 (74.43%)\n",
      "\n",
      "Round   1, Average loss 2.267 Test accuracy 74.430\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 17 18 19 20 22 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.9123 \n",
      "Accuracy: 9291/10000 (92.91%)\n",
      "\n",
      "Round   2, Average loss 1.912 Test accuracy 92.910\n",
      "selected users: [ 0  1  3  4  5  6  7  8  9 10 11 12 15 16 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29 30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.5662 \n",
      "Accuracy: 9464/10000 (94.64%)\n",
      "\n",
      "Round   3, Average loss 1.566 Test accuracy 94.640\n",
      "selected users: [ 0  1  2  4  5  6  7  8  9 11 12 13 14 15 16 17 18 19 20 21 23 24 25 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3688 \n",
      "Accuracy: 9481/10000 (94.81%)\n",
      "\n",
      "Round   4, Average loss 1.369 Test accuracy 94.810\n",
      "selected users: [ 0  1  3  4  6  7  9 10 11 12 13 14 15 16 17 18 19 20 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3811 \n",
      "Accuracy: 9527/10000 (95.27%)\n",
      "\n",
      "Round   5, Average loss 1.381 Test accuracy 95.270\n",
      "selected users: [ 0  1  2  5  6  7  8  9 10 11 12 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.3163 \n",
      "Accuracy: 9502/10000 (95.02%)\n",
      "\n",
      "Round   6, Average loss 1.316 Test accuracy 95.020\n",
      "selected users: [ 1  2  3  4  7  8  9 10 11 12 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.2109 \n",
      "Accuracy: 9532/10000 (95.32%)\n",
      "\n",
      "Round   7, Average loss 1.211 Test accuracy 95.320\n",
      "selected users: [ 0  1  2  3  4  6  7  8  9 10 11 12 13 14 15 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.3632 \n",
      "Accuracy: 9512/10000 (95.12%)\n",
      "\n",
      "Round   8, Average loss 1.363 Test accuracy 95.120\n",
      "selected users: [ 0  1  2  3  6  7  8  9 10 11 12 14 15 16 17 18 19 20 21 22 23 24 25 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.4367 \n",
      "Accuracy: 9502/10000 (95.02%)\n",
      "\n",
      "Round   9, Average loss 1.437 Test accuracy 95.020\n",
      "selected users: [ 0  1  2  4  5  6  7  8  9 10 11 13 14 15 16 17 18 19 20 21 23 24 25 26\n",
      " 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.3473 \n",
      "Accuracy: 9524/10000 (95.24%)\n",
      "\n",
      "Round  10, Average loss 1.347 Test accuracy 95.240\n",
      "selected users: [ 0  1  2  3  4  6  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 24 25 26\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.1849 \n",
      "Accuracy: 9506/10000 (95.06%)\n",
      "\n",
      "Round  11, Average loss 1.185 Test accuracy 95.060\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 11 12 13 14 15 16 17 19 21 22 23 24 25 26\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.2505 \n",
      "Accuracy: 9512/10000 (95.12%)\n",
      "\n",
      "Round  12, Average loss 1.251 Test accuracy 95.120\n",
      "selected users: [ 0  1  2  3  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 23 24 25 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.8073 \n",
      "Accuracy: 9477/10000 (94.77%)\n",
      "\n",
      "Round  13, Average loss 0.807 Test accuracy 94.770\n",
      "selected users: [ 0  1  2  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 21 22 23 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.4368 \n",
      "Accuracy: 9471/10000 (94.71%)\n",
      "\n",
      "Round  14, Average loss 1.437 Test accuracy 94.710\n",
      "selected users: [ 0  1  2  3  4  5  6  7 10 11 12 13 14 15 16 18 19 20 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.2944 \n",
      "Accuracy: 9497/10000 (94.97%)\n",
      "\n",
      "Round  15, Average loss 1.294 Test accuracy 94.970\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 17 18 20 22 23 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.2854 \n",
      "Accuracy: 9518/10000 (95.18%)\n",
      "\n",
      "Round  16, Average loss 1.285 Test accuracy 95.180\n",
      "selected users: [ 0  1  2  4  5  6  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 28 29]\n",
      "\n",
      "Test set: Average loss: 1.4180 \n",
      "Accuracy: 9495/10000 (94.95%)\n",
      "\n",
      "Round  17, Average loss 1.418 Test accuracy 94.950\n",
      "selected users: [ 0  1  2  3  4  5  6  8  9 10 12 13 14 15 16 17 18 19 20 21 22 24 25 26\n",
      " 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.7294 \n",
      "Accuracy: 9512/10000 (95.12%)\n",
      "\n",
      "Round  18, Average loss 1.729 Test accuracy 95.120\n",
      "selected users: [ 0  1  2  3  4  5  6  9 10 11 12 13 14 15 16 17 18 19 20 22 23 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5219 \n",
      "Accuracy: 9522/10000 (95.22%)\n",
      "\n",
      "Round  19, Average loss 1.522 Test accuracy 95.220\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 19 20 21 22 23 25\n",
      " 26 27 28]\n",
      "\n",
      "Test set: Average loss: 1.4190 \n",
      "Accuracy: 9500/10000 (95.00%)\n",
      "\n",
      "Round  20, Average loss 1.419 Test accuracy 95.000\n",
      "selected users: [ 0  1  2  3  4  5  6  8  9 10 11 12 13 14 16 17 18 19 20 21 22 23 25 26\n",
      " 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.6693 \n",
      "Accuracy: 9527/10000 (95.27%)\n",
      "\n",
      "Round  21, Average loss 1.669 Test accuracy 95.270\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8 10 11 13 14 15 16 17 18 19 20 21 22 23 25 26\n",
      " 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.5647 \n",
      "Accuracy: 9506/10000 (95.06%)\n",
      "\n",
      "Round  22, Average loss 1.565 Test accuracy 95.060\n",
      "selected users: [ 0  1  3  4  5  6  7  8  9 10 11 12 13 15 16 17 18 19 20 21 22 23 25 26\n",
      " 27 28 30]\n",
      "\n",
      "Test set: Average loss: 1.3374 \n",
      "Accuracy: 9529/10000 (95.29%)\n",
      "\n",
      "Round  23, Average loss 1.337 Test accuracy 95.290\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 14 15 16 17 18 21 22 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.1066 \n",
      "Accuracy: 9471/10000 (94.71%)\n",
      "\n",
      "Round  24, Average loss 1.107 Test accuracy 94.710\n",
      "selected users: [ 0  2  3  4  5  6  7 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.8646 \n",
      "Accuracy: 9483/10000 (94.83%)\n",
      "\n",
      "Round  25, Average loss 0.865 Test accuracy 94.830\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8 10 13 14 15 16 17 18 19 20 21 22 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.2631 \n",
      "Accuracy: 9478/10000 (94.78%)\n",
      "\n",
      "Round  26, Average loss 1.263 Test accuracy 94.780\n",
      "selected users: [ 0  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 19 20 22 23 24 25 26\n",
      " 27 28 30]\n",
      "\n",
      "Test set: Average loss: 1.1027 \n",
      "Accuracy: 9525/10000 (95.25%)\n",
      "\n",
      "Round  27, Average loss 1.103 Test accuracy 95.250\n",
      "selected users: [ 0  2  3  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 29]\n",
      "\n",
      "Test set: Average loss: 1.0653 \n",
      "Accuracy: 9523/10000 (95.23%)\n",
      "\n",
      "Round  28, Average loss 1.065 Test accuracy 95.230\n",
      "selected users: [ 0  1  3  4  5  6  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 29]\n",
      "\n",
      "Test set: Average loss: 1.7215 \n",
      "Accuracy: 9507/10000 (95.07%)\n",
      "\n",
      "Round  29, Average loss 1.722 Test accuracy 95.070\n",
      "(m= 27 )  5 -th Trial!!\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 14 15 16 17 18 20 21 22 24 25 26\n",
      " 27 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 20 22 23 24 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.2897 \n",
      "Accuracy: 5685/10000 (56.85%)\n",
      "\n",
      "Round   1, Average loss 2.290 Test accuracy 56.850\n",
      "selected users: [ 0  1  2  4  5  6  8  9 10 11 12 13 14 15 16 17 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.8872 \n",
      "Accuracy: 9347/10000 (93.47%)\n",
      "\n",
      "Round   2, Average loss 1.887 Test accuracy 93.470\n",
      "selected users: [ 0  1  2  3  4  5  7  8  9 10 11 12 13 15 16 17 18 20 21 22 23 24 25 26\n",
      " 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.4777 \n",
      "Accuracy: 9486/10000 (94.86%)\n",
      "\n",
      "Round   3, Average loss 1.478 Test accuracy 94.860\n",
      "selected users: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28]\n",
      "\n",
      "Test set: Average loss: 1.3133 \n",
      "Accuracy: 9493/10000 (94.93%)\n",
      "\n",
      "Round   4, Average loss 1.313 Test accuracy 94.930\n",
      "selected users: [ 0  1  2  3  4  5  7  9 10 11 12 13 14 15 16 17 19 20 21 22 23 24 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.2348 \n",
      "Accuracy: 9502/10000 (95.02%)\n",
      "\n",
      "Round   5, Average loss 1.235 Test accuracy 95.020\n",
      "selected users: [ 0  1  2  4  6  7  8  9 10 11 12 13 14 15 16 17 19 20 21 22 23 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.4259 \n",
      "Accuracy: 9525/10000 (95.25%)\n",
      "\n",
      "Round   6, Average loss 1.426 Test accuracy 95.250\n",
      "selected users: [ 0  1  2  3  4  5  7  8  9 10 11 13 14 15 16 19 20 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3422 \n",
      "Accuracy: 9531/10000 (95.31%)\n",
      "\n",
      "Round   7, Average loss 1.342 Test accuracy 95.310\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 12 13 14 15 16 18 19 20 21 22 23 24 26\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.0741 \n",
      "Accuracy: 9503/10000 (95.03%)\n",
      "\n",
      "Round   8, Average loss 1.074 Test accuracy 95.030\n",
      "selected users: [ 0  1  2  4  5  6  7  8  9 10 11 12 13 15 16 17 18 19 20 21 22 23 24 25\n",
      " 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.3349 \n",
      "Accuracy: 9560/10000 (95.60%)\n",
      "\n",
      "Round   9, Average loss 1.335 Test accuracy 95.600\n",
      "selected users: [ 0  1  2  4  5  6  7  8  9 10 11 12 13 14 15 17 18 19 20 21 22 23 24 25\n",
      " 27 28 30]\n",
      "\n",
      "Test set: Average loss: 1.4534 \n",
      "Accuracy: 9540/10000 (95.40%)\n",
      "\n",
      "Round  10, Average loss 1.453 Test accuracy 95.400\n",
      "selected users: [ 0  1  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.6938 \n",
      "Accuracy: 9504/10000 (95.04%)\n",
      "\n",
      "Round  11, Average loss 1.694 Test accuracy 95.040\n",
      "selected users: [ 0  2  3  4  5  6  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 25 26\n",
      " 28 29 30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.2418 \n",
      "Accuracy: 9461/10000 (94.61%)\n",
      "\n",
      "Round  12, Average loss 1.242 Test accuracy 94.610\n",
      "selected users: [ 1  2  3  4  6  7  8 10 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.9979 \n",
      "Accuracy: 9506/10000 (95.06%)\n",
      "\n",
      "Round  13, Average loss 0.998 Test accuracy 95.060\n",
      "selected users: [ 0  1  2  4  5  6  7  9 10 11 12 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.6852 \n",
      "Accuracy: 9512/10000 (95.12%)\n",
      "\n",
      "Round  14, Average loss 1.685 Test accuracy 95.120\n",
      "selected users: [ 0  1  3  4  5  6  9 10 11 12 13 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5988 \n",
      "Accuracy: 9496/10000 (94.96%)\n",
      "\n",
      "Round  15, Average loss 1.599 Test accuracy 94.960\n",
      "selected users: [ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 17 18 19 20 21 23 24 25 26\n",
      " 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.6056 \n",
      "Accuracy: 9517/10000 (95.17%)\n",
      "\n",
      "Round  16, Average loss 1.606 Test accuracy 95.170\n",
      "selected users: [ 1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 24 25 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.0853 \n",
      "Accuracy: 9508/10000 (95.08%)\n",
      "\n",
      "Round  17, Average loss 1.085 Test accuracy 95.080\n",
      "selected users: [ 0  1  2  3  6  7  8  9 10 11 12 14 15 16 17 18 19 20 21 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.4056 \n",
      "Accuracy: 9485/10000 (94.85%)\n",
      "\n",
      "Round  18, Average loss 1.406 Test accuracy 94.850\n",
      "selected users: [ 0  1  2  3  4  7  8  9 10 11 12 13 14 15 16 17 18 19 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3914 \n",
      "Accuracy: 9533/10000 (95.33%)\n",
      "\n",
      "Round  19, Average loss 1.391 Test accuracy 95.330\n",
      "selected users: [ 0  1  2  3  4  5  6  7  9 10 11 12 13 14 15 16 18 19 20 21 22 23 25 26\n",
      " 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.5390 \n",
      "Accuracy: 9511/10000 (95.11%)\n",
      "\n",
      "Round  20, Average loss 1.539 Test accuracy 95.110\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 11 12 14 15 17 18 19 20 21 22 23 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.4389 \n",
      "Accuracy: 9532/10000 (95.32%)\n",
      "\n",
      "Round  21, Average loss 1.439 Test accuracy 95.320\n",
      "selected users: [ 0  1  2  3  4  6  7  8  9 10 12 13 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.5206 \n",
      "Accuracy: 9456/10000 (94.56%)\n",
      "\n",
      "Round  22, Average loss 1.521 Test accuracy 94.560\n",
      "selected users: [ 0  1  2  3  4  5  7  8  9 10 11 12 13 14 15 17 18 19 20 21 22 23 24 25\n",
      " 26 28 30]\n",
      "\n",
      "Test set: Average loss: 1.2893 \n",
      "Accuracy: 9515/10000 (95.15%)\n",
      "\n",
      "Round  23, Average loss 1.289 Test accuracy 95.150\n",
      "selected users: [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 17 18 19 20 21 22 23 25 26\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.2383 \n",
      "Accuracy: 9508/10000 (95.08%)\n",
      "\n",
      "Round  24, Average loss 1.238 Test accuracy 95.080\n",
      "selected users: [ 0  2  4  5  6  7  8  9 10 11 13 14 15 16 17 18 19 20 21 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.0936 \n",
      "Accuracy: 9485/10000 (94.85%)\n",
      "\n",
      "Round  25, Average loss 1.094 Test accuracy 94.850\n",
      "selected users: [ 0  1  2  3  4  5  6  9 10 11 12 13 14 15 17 18 19 20 21 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3322 \n",
      "Accuracy: 9509/10000 (95.09%)\n",
      "\n",
      "Round  26, Average loss 1.332 Test accuracy 95.090\n",
      "selected users: [ 0  1  3  4  6  7  8  9 10 11 12 13 14 15 16 17 19 20 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.2675 \n",
      "Accuracy: 9537/10000 (95.37%)\n",
      "\n",
      "Round  27, Average loss 1.268 Test accuracy 95.370\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8 10 11 12 13 14 15 16 18 19 20 21 22 23 25 26\n",
      " 27 28 30]\n",
      "\n",
      "Test set: Average loss: 1.3618 \n",
      "Accuracy: 9514/10000 (95.14%)\n",
      "\n",
      "Round  28, Average loss 1.362 Test accuracy 95.140\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 12 13 15 16 18 19 20 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3941 \n",
      "Accuracy: 9474/10000 (94.74%)\n",
      "\n",
      "Round  29, Average loss 1.394 Test accuracy 94.740\n",
      "(m= 27 )  6 -th Trial!!\n",
      "selected users: [ 0  1  2  3  4  5  6  9 11 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 15 16 18 19 20 21 22 23 24 25 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.2168 \n",
      "Accuracy: 7443/10000 (74.43%)\n",
      "\n",
      "Round   1, Average loss 2.217 Test accuracy 74.430\n",
      "selected users: [ 0  1  2  3  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 27 28]\n",
      "\n",
      "Test set: Average loss: 1.7575 \n",
      "Accuracy: 9284/10000 (92.84%)\n",
      "\n",
      "Round   2, Average loss 1.757 Test accuracy 92.840\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 17 18 19 20 21 23 24 25\n",
      " 27 28 30]\n",
      "\n",
      "Test set: Average loss: 1.7209 \n",
      "Accuracy: 9390/10000 (93.90%)\n",
      "\n",
      "Round   3, Average loss 1.721 Test accuracy 93.900\n",
      "selected users: [ 0  1  2  3  4  5  7  8  9 10 11 12 14 15 16 17 19 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7101 \n",
      "Accuracy: 9430/10000 (94.30%)\n",
      "\n",
      "Round   4, Average loss 1.710 Test accuracy 94.300\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 16 17 18 20 22 23 24 25 26\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6755 \n",
      "Accuracy: 9405/10000 (94.05%)\n",
      "\n",
      "Round   5, Average loss 1.675 Test accuracy 94.050\n",
      "selected users: [ 0  1  2  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 20 21 22 23 25 26\n",
      " 27 28 30]\n",
      "\n",
      "Test set: Average loss: 1.7259 \n",
      "Accuracy: 9434/10000 (94.34%)\n",
      "\n",
      "Round   6, Average loss 1.726 Test accuracy 94.340\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 14 15 16 18 19 21 22 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6266 \n",
      "Accuracy: 9434/10000 (94.34%)\n",
      "\n",
      "Round   7, Average loss 1.627 Test accuracy 94.340\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 13 14 15 16 17 18 19 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6863 \n",
      "Accuracy: 9475/10000 (94.75%)\n",
      "\n",
      "Round   8, Average loss 1.686 Test accuracy 94.750\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 24 25\n",
      " 26 27 28]\n",
      "\n",
      "Test set: Average loss: 1.6709 \n",
      "Accuracy: 9429/10000 (94.29%)\n",
      "\n",
      "Round   9, Average loss 1.671 Test accuracy 94.290\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 21 22 23 24\n",
      " 26 27 30]\n",
      "\n",
      "Test set: Average loss: 1.5541 \n",
      "Accuracy: 9468/10000 (94.68%)\n",
      "\n",
      "Round  10, Average loss 1.554 Test accuracy 94.680\n",
      "selected users: [ 1  2  3  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 23 24 25 26\n",
      " 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5348 \n",
      "Accuracy: 9461/10000 (94.61%)\n",
      "\n",
      "Round  11, Average loss 1.535 Test accuracy 94.610\n",
      "selected users: [ 0  1  2  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 23 24 25\n",
      " 26 27 28]\n",
      "\n",
      "Test set: Average loss: 1.6159 \n",
      "Accuracy: 9440/10000 (94.40%)\n",
      "\n",
      "Round  12, Average loss 1.616 Test accuracy 94.400\n",
      "selected users: [ 0  1  2  4  5  6  7  9 10 11 12 13 14 15 16 17 18 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6393 \n",
      "Accuracy: 9452/10000 (94.52%)\n",
      "\n",
      "Round  13, Average loss 1.639 Test accuracy 94.520\n",
      "selected users: [ 1  2  3  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 21 22 23 24 25 26\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.4407 \n",
      "Accuracy: 9458/10000 (94.58%)\n",
      "\n",
      "Round  14, Average loss 1.441 Test accuracy 94.580\n",
      "selected users: [ 0  2  3  4  6  7  8  9 10 11 12 13 14 16 17 18 19 20 21 22 23 24 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.4223 \n",
      "Accuracy: 9461/10000 (94.61%)\n",
      "\n",
      "Round  15, Average loss 1.422 Test accuracy 94.610\n",
      "selected users: [ 1  2  3  4  5  6  7  8  9 10 11 13 14 15 16 17 18 19 20 22 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5761 \n",
      "Accuracy: 9489/10000 (94.89%)\n",
      "\n",
      "Round  16, Average loss 1.576 Test accuracy 94.890\n",
      "selected users: [ 0  1  2  3  5  6  7  8  9 10 11 12 14 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6423 \n",
      "Accuracy: 9464/10000 (94.64%)\n",
      "\n",
      "Round  17, Average loss 1.642 Test accuracy 94.640\n",
      "selected users: [ 1  2  3  4  5  6  7  8  9 10 11 12 14 15 16 17 18 19 21 22 23 24 25 26\n",
      " 27 28 30]\n",
      "\n",
      "Test set: Average loss: 1.5687 \n",
      "Accuracy: 9478/10000 (94.78%)\n",
      "\n",
      "Round  18, Average loss 1.569 Test accuracy 94.780\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 23 25\n",
      " 26 29 30]\n",
      "\n",
      "Test set: Average loss: 1.4163 \n",
      "Accuracy: 9411/10000 (94.11%)\n",
      "\n",
      "Round  19, Average loss 1.416 Test accuracy 94.110\n",
      "selected users: [ 0  1  2  4  5  6  7  8  9 10 11 12 13 14 15 17 18 19 20 21 22 23 24 25\n",
      " 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.6934 \n",
      "Accuracy: 9466/10000 (94.66%)\n",
      "\n",
      "Round  20, Average loss 1.693 Test accuracy 94.660\n",
      "selected users: [ 0  1  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 22 24 25 26 27\n",
      " 28 29 30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.6307 \n",
      "Accuracy: 9452/10000 (94.52%)\n",
      "\n",
      "Round  21, Average loss 1.631 Test accuracy 94.520\n",
      "selected users: [ 0  2  3  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.3612 \n",
      "Accuracy: 9462/10000 (94.62%)\n",
      "\n",
      "Round  22, Average loss 1.361 Test accuracy 94.620\n",
      "selected users: [ 1  2  3  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 25 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.4403 \n",
      "Accuracy: 9469/10000 (94.69%)\n",
      "\n",
      "Round  23, Average loss 1.440 Test accuracy 94.690\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8 11 12 14 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7378 \n",
      "Accuracy: 9438/10000 (94.38%)\n",
      "\n",
      "Round  24, Average loss 1.738 Test accuracy 94.380\n",
      "selected users: [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 16 17 18 19 21 22 23 24 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6044 \n",
      "Accuracy: 9404/10000 (94.04%)\n",
      "\n",
      "Round  25, Average loss 1.604 Test accuracy 94.040\n",
      "selected users: [ 0  1  2  3  4  5  7  8 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 26\n",
      " 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.4861 \n",
      "Accuracy: 9448/10000 (94.48%)\n",
      "\n",
      "Round  26, Average loss 1.486 Test accuracy 94.480\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 14 15 16 17 18 19 20 21 22 23 25\n",
      " 26 27 29]\n",
      "\n",
      "Test set: Average loss: 1.7540 \n",
      "Accuracy: 9476/10000 (94.76%)\n",
      "\n",
      "Round  27, Average loss 1.754 Test accuracy 94.760\n",
      "selected users: [ 0  1  2  3  5  6  7  8  9 11 12 13 14 15 17 18 20 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5808 \n",
      "Accuracy: 9449/10000 (94.49%)\n",
      "\n",
      "Round  28, Average loss 1.581 Test accuracy 94.490\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 12 13 14 15 16 17 18 19 20 21 23 24 25\n",
      " 26 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5267 \n",
      "Accuracy: 9434/10000 (94.34%)\n",
      "\n",
      "Round  29, Average loss 1.527 Test accuracy 94.340\n",
      "(m= 27 )  7 -th Trial!!\n",
      "selected users: [ 0  2  3  4  5  6  7  9 10 11 12 13 14 15 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8 10 11 12 13 14 15 16 18 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.2484 \n",
      "Accuracy: 7101/10000 (71.01%)\n",
      "\n",
      "Round   1, Average loss 2.248 Test accuracy 71.010\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 12 13 14 15 16 17 20 21 22 23 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.8491 \n",
      "Accuracy: 9321/10000 (93.21%)\n",
      "\n",
      "Round   2, Average loss 1.849 Test accuracy 93.210\n",
      "selected users: [ 0  1  2  3  4  7  9 10 11 12 13 14 15 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6149 \n",
      "Accuracy: 9446/10000 (94.46%)\n",
      "\n",
      "Round   3, Average loss 1.615 Test accuracy 94.460\n",
      "selected users: [ 0  1  2  3  4  5  6  7  9 10 11 12 13 14 15 17 18 19 20 21 22 23 25 26\n",
      " 27 28 30]\n",
      "\n",
      "Test set: Average loss: 1.7330 \n",
      "Accuracy: 9521/10000 (95.21%)\n",
      "\n",
      "Round   4, Average loss 1.733 Test accuracy 95.210\n",
      "selected users: [ 0  2  3  5  6  7  8  9 10 11 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.1322 \n",
      "Accuracy: 9436/10000 (94.36%)\n",
      "\n",
      "Round   5, Average loss 1.132 Test accuracy 94.360\n",
      "selected users: [ 0  1  2  3  4  5  6  8  9 11 12 14 15 16 17 18 19 20 21 22 23 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7625 \n",
      "Accuracy: 9495/10000 (94.95%)\n",
      "\n",
      "Round   6, Average loss 1.763 Test accuracy 94.950\n",
      "selected users: [ 0  1  2  3  4  5  6  7  9 10 11 12 13 15 16 17 18 19 20 21 22 23 24 25\n",
      " 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.5310 \n",
      "Accuracy: 9491/10000 (94.91%)\n",
      "\n",
      "Round   7, Average loss 1.531 Test accuracy 94.910\n",
      "selected users: [ 0  1  2  3  4  6  7  8 10 11 12 13 14 15 16 18 19 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3923 \n",
      "Accuracy: 9487/10000 (94.87%)\n",
      "\n",
      "Round   8, Average loss 1.392 Test accuracy 94.870\n",
      "selected users: [ 0  1  2  3  4  5  6  8  9 10 11 12 13 14 15 18 19 20 21 22 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7606 \n",
      "Accuracy: 9475/10000 (94.75%)\n",
      "\n",
      "Round   9, Average loss 1.761 Test accuracy 94.750\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8 10 11 12 14 15 16 17 19 20 21 22 23 24 25 26\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3106 \n",
      "Accuracy: 9451/10000 (94.51%)\n",
      "\n",
      "Round  10, Average loss 1.311 Test accuracy 94.510\n",
      "selected users: [ 0  1  3  4  6  7  8  9 10 11 12 13 14 15 16 17 18 20 21 22 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6330 \n",
      "Accuracy: 9495/10000 (94.95%)\n",
      "\n",
      "Round  11, Average loss 1.633 Test accuracy 94.950\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 12 13 14 15 16 18 19 21 22 23 24 25 26\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3887 \n",
      "Accuracy: 9512/10000 (95.12%)\n",
      "\n",
      "Round  12, Average loss 1.389 Test accuracy 95.120\n",
      "selected users: [ 1  2  3  4  5  6  7  8 10 11 12 13 14 15 16 17 18 19 20 22 23 24 25 26\n",
      " 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.6240 \n",
      "Accuracy: 9464/10000 (94.64%)\n",
      "\n",
      "Round  13, Average loss 1.624 Test accuracy 94.640\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 12 13 14 15 16 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.7127 \n",
      "Accuracy: 9484/10000 (94.84%)\n",
      "\n",
      "Round  14, Average loss 1.713 Test accuracy 94.840\n",
      "selected users: [ 0  1  2  5  6  7  9 10 11 12 13 14 15 16 17 18 20 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.4046 \n",
      "Accuracy: 9488/10000 (94.88%)\n",
      "\n",
      "Round  15, Average loss 1.405 Test accuracy 94.880\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 11 12 13 14 15 16 17 18 19 21 22 24 25 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.4522 \n",
      "Accuracy: 9509/10000 (95.09%)\n",
      "\n",
      "Round  16, Average loss 1.452 Test accuracy 95.090\n",
      "selected users: [ 0  1  2  3  4  5  7  8  9 10 11 12 13 14 15 17 18 19 20 21 22 24 25 26\n",
      " 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.4257 \n",
      "Accuracy: 9516/10000 (95.16%)\n",
      "\n",
      "Round  17, Average loss 1.426 Test accuracy 95.160\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8 10 11 12 13 14 15 16 17 18 19 20 21 24 25 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3325 \n",
      "Accuracy: 9464/10000 (94.64%)\n",
      "\n",
      "Round  18, Average loss 1.333 Test accuracy 94.640\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 20 21 24 25 26\n",
      " 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.4034 \n",
      "Accuracy: 9445/10000 (94.45%)\n",
      "\n",
      "Round  19, Average loss 1.403 Test accuracy 94.450\n",
      "selected users: [ 0  1  3  4  6  7  8  9 11 12 13 14 15 16 17 18 19 20 21 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.4919 \n",
      "Accuracy: 9494/10000 (94.94%)\n",
      "\n",
      "Round  20, Average loss 1.492 Test accuracy 94.940\n",
      "selected users: [ 0  1  2  3  4  6  7  8  9 10 11 13 14 15 16 17 18 19 21 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3876 \n",
      "Accuracy: 9474/10000 (94.74%)\n",
      "\n",
      "Round  21, Average loss 1.388 Test accuracy 94.740\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 12 13 15 16 17 18 19 20 22 23 24 25 26\n",
      " 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5003 \n",
      "Accuracy: 9516/10000 (95.16%)\n",
      "\n",
      "Round  22, Average loss 1.500 Test accuracy 95.160\n",
      "selected users: [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 30]\n",
      "\n",
      "Test set: Average loss: 1.2596 \n",
      "Accuracy: 9491/10000 (94.91%)\n",
      "\n",
      "Round  23, Average loss 1.260 Test accuracy 94.910\n",
      "selected users: [ 0  1  2  4  5  6  7  8 10 11 12 13 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.7957 \n",
      "Accuracy: 9465/10000 (94.65%)\n",
      "\n",
      "Round  24, Average loss 1.796 Test accuracy 94.650\n",
      "selected users: [ 0  1  2  4  5  7  8  9 10 11 12 14 15 16 17 18 19 20 21 22 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.4228 \n",
      "Accuracy: 9501/10000 (95.01%)\n",
      "\n",
      "Round  25, Average loss 1.423 Test accuracy 95.010\n",
      "selected users: [ 0  1  2  3  4  5  8  9 10 11 12 13 14 15 16 17 18 20 21 22 23 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.2430 \n",
      "Accuracy: 9474/10000 (94.74%)\n",
      "\n",
      "Round  26, Average loss 1.243 Test accuracy 94.740\n",
      "selected users: [ 0  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.0329 \n",
      "Accuracy: 9512/10000 (95.12%)\n",
      "\n",
      "Round  27, Average loss 1.033 Test accuracy 95.120\n",
      "selected users: [ 1  2  3  4  5  6  7  8  9 10 11 13 14 15 17 18 19 20 21 22 23 24 25 26\n",
      " 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.4982 \n",
      "Accuracy: 9481/10000 (94.81%)\n",
      "\n",
      "Round  28, Average loss 1.498 Test accuracy 94.810\n",
      "selected users: [ 0  1  2  3  4  6  7  8  9 10 12 13 14 15 16 17 18 19 20 22 23 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5130 \n",
      "Accuracy: 9485/10000 (94.85%)\n",
      "\n",
      "Round  29, Average loss 1.513 Test accuracy 94.850\n",
      "(m= 27 )  8 -th Trial!!\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 16 17 18 19 21 22 23 24 25\n",
      " 26 27 30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  2  4  5  7  8  9 10 11 12 13 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.2693 \n",
      "Accuracy: 6677/10000 (66.77%)\n",
      "\n",
      "Round   1, Average loss 2.269 Test accuracy 66.770\n",
      "selected users: [ 2  3  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 30]\n",
      "\n",
      "Test set: Average loss: 1.5527 \n",
      "Accuracy: 9394/10000 (93.94%)\n",
      "\n",
      "Round   2, Average loss 1.553 Test accuracy 93.940\n",
      "selected users: [ 0  2  3  4  5  6  7  8  9 10 12 13 14 15 16 18 19 20 21 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.2203 \n",
      "Accuracy: 9482/10000 (94.82%)\n",
      "\n",
      "Round   3, Average loss 1.220 Test accuracy 94.820\n",
      "selected users: [ 0  1  2  3  4  5  6  8  9 10 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.4348 \n",
      "Accuracy: 9506/10000 (95.06%)\n",
      "\n",
      "Round   4, Average loss 1.435 Test accuracy 95.060\n",
      "selected users: [ 0  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.2510 \n",
      "Accuracy: 9525/10000 (95.25%)\n",
      "\n",
      "Round   5, Average loss 1.251 Test accuracy 95.250\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8 10 11 12 13 14 16 17 18 19 21 22 23 24 25 26\n",
      " 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3995 \n",
      "Accuracy: 9542/10000 (95.42%)\n",
      "\n",
      "Round   6, Average loss 1.400 Test accuracy 95.420\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 13 14 17 18 19 20 21 22 23 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.4361 \n",
      "Accuracy: 9547/10000 (95.47%)\n",
      "\n",
      "Round   7, Average loss 1.436 Test accuracy 95.470\n",
      "selected users: [ 0  1  3  4  5  6  7  8  9 10 12 13 14 15 16 17 18 19 20 21 22 23 24 26\n",
      " 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.3985 \n",
      "Accuracy: 9535/10000 (95.35%)\n",
      "\n",
      "Round   8, Average loss 1.398 Test accuracy 95.350\n",
      "selected users: [ 0  1  2  4  5  6  8  9 10 11 12 13 14 15 16 17 18 20 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.4252 \n",
      "Accuracy: 9530/10000 (95.30%)\n",
      "\n",
      "Round   9, Average loss 1.425 Test accuracy 95.300\n",
      "selected users: [ 0  1  3  4  5  6  8  9 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.7979 \n",
      "Accuracy: 9507/10000 (95.07%)\n",
      "\n",
      "Round  10, Average loss 1.798 Test accuracy 95.070\n",
      "selected users: [ 0  2  3  4  5  6  8 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.0726 \n",
      "Accuracy: 9486/10000 (94.86%)\n",
      "\n",
      "Round  11, Average loss 1.073 Test accuracy 94.860\n",
      "selected users: [ 0  1  2  3  4  5  6  7  9 11 12 13 14 15 16 17 18 20 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.4467 \n",
      "Accuracy: 9498/10000 (94.98%)\n",
      "\n",
      "Round  12, Average loss 1.447 Test accuracy 94.980\n",
      "selected users: [ 0  1  2  4  5  6  7  9 10 11 12 13 14 15 16 18 19 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.4853 \n",
      "Accuracy: 9537/10000 (95.37%)\n",
      "\n",
      "Round  13, Average loss 1.485 Test accuracy 95.370\n",
      "selected users: [ 0  2  3  4  5  6  7  8  9 10 11 12 13 15 16 17 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.1972 \n",
      "Accuracy: 9531/10000 (95.31%)\n",
      "\n",
      "Round  14, Average loss 1.197 Test accuracy 95.310\n",
      "selected users: [ 0  1  2  3  4  5  8  9 10 11 12 13 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.1156 \n",
      "Accuracy: 9431/10000 (94.31%)\n",
      "\n",
      "Round  15, Average loss 1.116 Test accuracy 94.310\n",
      "selected users: [ 0  1  2  3  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 22 23 24 25 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3780 \n",
      "Accuracy: 9532/10000 (95.32%)\n",
      "\n",
      "Round  16, Average loss 1.378 Test accuracy 95.320\n",
      "selected users: [ 0  1  2  4  5  6  7  8  9 10 11 12 13 15 16 17 18 20 21 22 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5322 \n",
      "Accuracy: 9530/10000 (95.30%)\n",
      "\n",
      "Round  17, Average loss 1.532 Test accuracy 95.300\n",
      "selected users: [ 0  1  2  3  4  5  6  8 10 12 13 14 15 16 17 18 19 20 21 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.2032 \n",
      "Accuracy: 9513/10000 (95.13%)\n",
      "\n",
      "Round  18, Average loss 1.203 Test accuracy 95.130\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 16 17 18 19 21 22 23 24 25 26\n",
      " 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.2958 \n",
      "Accuracy: 9517/10000 (95.17%)\n",
      "\n",
      "Round  19, Average loss 1.296 Test accuracy 95.170\n",
      "selected users: [ 0  2  3  4  6  7  8  9 10 11 12 14 15 16 17 18 19 20 21 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.8761 \n",
      "Accuracy: 9529/10000 (95.29%)\n",
      "\n",
      "Round  20, Average loss 0.876 Test accuracy 95.290\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 12 13 14 15 16 17 18 19 20 21 23 24 25 26\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3892 \n",
      "Accuracy: 9479/10000 (94.79%)\n",
      "\n",
      "Round  21, Average loss 1.389 Test accuracy 94.790\n",
      "selected users: [ 0  2  3  4  5  6  7  8  9 10 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 27 28 30]\n",
      "\n",
      "Test set: Average loss: 1.1895 \n",
      "Accuracy: 9521/10000 (95.21%)\n",
      "\n",
      "Round  22, Average loss 1.190 Test accuracy 95.210\n",
      "selected users: [ 0  2  4  5  6  7  8  9 10 11 12 14 15 16 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.0174 \n",
      "Accuracy: 9523/10000 (95.23%)\n",
      "\n",
      "Round  23, Average loss 1.017 Test accuracy 95.230\n",
      "selected users: [ 0  1  3  4  5  6  8 10 11 12 13 14 15 16 17 18 20 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3645 \n",
      "Accuracy: 9511/10000 (95.11%)\n",
      "\n",
      "Round  24, Average loss 1.364 Test accuracy 95.110\n",
      "selected users: [ 0  2  4  5  6  7  8 10 11 12 13 14 15 16 17 18 19 20 21 22 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.1663 \n",
      "Accuracy: 9515/10000 (95.15%)\n",
      "\n",
      "Round  25, Average loss 1.166 Test accuracy 95.150\n",
      "selected users: [ 0  1  2  3  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 22 23 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.0001 \n",
      "Accuracy: 9508/10000 (95.08%)\n",
      "\n",
      "Round  26, Average loss 1.000 Test accuracy 95.080\n",
      "selected users: [ 0  2  3  5  6  7  8  9 10 12 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.8630 \n",
      "Accuracy: 9493/10000 (94.93%)\n",
      "\n",
      "Round  27, Average loss 0.863 Test accuracy 94.930\n",
      "selected users: [ 0  2  3  4  5  6  7  9 10 11 12 13 14 16 17 18 19 20 21 22 23 24 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.0835 \n",
      "Accuracy: 9520/10000 (95.20%)\n",
      "\n",
      "Round  28, Average loss 1.084 Test accuracy 95.200\n",
      "selected users: [ 0  1  2  3  4  5  6  8 10 11 12 13 14 15 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.6388 \n",
      "Accuracy: 9513/10000 (95.13%)\n",
      "\n",
      "Round  29, Average loss 1.639 Test accuracy 95.130\n",
      "(m= 27 )  9 -th Trial!!\n",
      "selected users: [ 0  2  3  4  5  7  8  9 10 11 12 13 14 15 17 18 19 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3025 \n",
      "Accuracy: 1336/10000 (13.36%)\n",
      "\n",
      "Round   0, Average loss 2.302 Test accuracy 13.360\n",
      "selected users: [ 0  2  3  4  5  6  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.1989 \n",
      "Accuracy: 7675/10000 (76.75%)\n",
      "\n",
      "Round   1, Average loss 2.199 Test accuracy 76.750\n",
      "selected users: [ 0  1  2  3  5  6  8  9 10 12 13 14 15 16 17 19 20 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7514 \n",
      "Accuracy: 9340/10000 (93.40%)\n",
      "\n",
      "Round   2, Average loss 1.751 Test accuracy 93.400\n",
      "selected users: [ 1  2  4  5  6  8  9 10 11 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6245 \n",
      "Accuracy: 9443/10000 (94.43%)\n",
      "\n",
      "Round   3, Average loss 1.624 Test accuracy 94.430\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 12 13 14 15 16 17 18 19 20 21 22 24 25 26\n",
      " 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.8036 \n",
      "Accuracy: 9455/10000 (94.55%)\n",
      "\n",
      "Round   4, Average loss 1.804 Test accuracy 94.550\n",
      "selected users: [ 0  1  2  3  4  5  6  7 10 11 12 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.5398 \n",
      "Accuracy: 9462/10000 (94.62%)\n",
      "\n",
      "Round   5, Average loss 1.540 Test accuracy 94.620\n",
      "selected users: [ 0  1  3  4  5  6  7  8  9 11 12 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5946 \n",
      "Accuracy: 9481/10000 (94.81%)\n",
      "\n",
      "Round   6, Average loss 1.595 Test accuracy 94.810\n",
      "selected users: [ 0  1  2  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 24 25 26\n",
      " 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3645 \n",
      "Accuracy: 9487/10000 (94.87%)\n",
      "\n",
      "Round   7, Average loss 1.364 Test accuracy 94.870\n",
      "selected users: [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 18 19 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3790 \n",
      "Accuracy: 9487/10000 (94.87%)\n",
      "\n",
      "Round   8, Average loss 1.379 Test accuracy 94.870\n",
      "selected users: [ 1  2  3  4  5  6  7  8  9 10 12 13 14 15 16 18 19 20 21 22 23 24 25 27\n",
      " 28 29 30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.4386 \n",
      "Accuracy: 9508/10000 (95.08%)\n",
      "\n",
      "Round   9, Average loss 1.439 Test accuracy 95.080\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 12 13 14 16 17 18 19 20 21 22 23 25 26\n",
      " 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.7310 \n",
      "Accuracy: 9484/10000 (94.84%)\n",
      "\n",
      "Round  10, Average loss 1.731 Test accuracy 94.840\n",
      "selected users: [ 0  2  3  5  6  7  8  9 10 11 12 13 14 15 17 18 19 20 21 22 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.0963 \n",
      "Accuracy: 9479/10000 (94.79%)\n",
      "\n",
      "Round  11, Average loss 1.096 Test accuracy 94.790\n",
      "selected users: [ 0  2  3  4  7  8  9 10 11 12 13 14 15 16 17 19 20 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3143 \n",
      "Accuracy: 9494/10000 (94.94%)\n",
      "\n",
      "Round  12, Average loss 1.314 Test accuracy 94.940\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 15 16 17 18 20 21 22 23 24 26\n",
      " 27 28 30]\n",
      "\n",
      "Test set: Average loss: 1.4897 \n",
      "Accuracy: 9466/10000 (94.66%)\n",
      "\n",
      "Round  13, Average loss 1.490 Test accuracy 94.660\n",
      "selected users: [ 0  1  2  3  4  5  7  8  9 10 11 12 13 14 15 16 18 19 21 22 23 24 25 26\n",
      " 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3283 \n",
      "Accuracy: 9497/10000 (94.97%)\n",
      "\n",
      "Round  14, Average loss 1.328 Test accuracy 94.970\n",
      "selected users: [ 0  1  2  3  4  5  6  7  9 10 11 12 13 14 15 16 17 19 20 21 22 23 24 25\n",
      " 26 28 30]\n",
      "\n",
      "Test set: Average loss: 1.4635 \n",
      "Accuracy: 9477/10000 (94.77%)\n",
      "\n",
      "Round  15, Average loss 1.464 Test accuracy 94.770\n",
      "selected users: [ 0  1  2  3  5  6  7  8 10 11 12 13 14 15 16 17 18 19 20 21 22 23 25 26\n",
      " 27 28 30]\n",
      "\n",
      "Test set: Average loss: 1.5015 \n",
      "Accuracy: 9471/10000 (94.71%)\n",
      "\n",
      "Round  16, Average loss 1.501 Test accuracy 94.710\n",
      "selected users: [ 1  2  3  4  5  7  8  9 10 11 12 13 14 15 17 19 20 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.3617 \n",
      "Accuracy: 9503/10000 (95.03%)\n",
      "\n",
      "Round  17, Average loss 1.362 Test accuracy 95.030\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 16 17 18 19 20 21 22 23 26\n",
      " 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.1930 \n",
      "Accuracy: 9486/10000 (94.86%)\n",
      "\n",
      "Round  18, Average loss 1.193 Test accuracy 94.860\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 19 20 21 23 24 25\n",
      " 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.4729 \n",
      "Accuracy: 9459/10000 (94.59%)\n",
      "\n",
      "Round  19, Average loss 1.473 Test accuracy 94.590\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8 10 11 12 13 14 15 17 18 19 20 21 22 23 24 25\n",
      " 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.4141 \n",
      "Accuracy: 9494/10000 (94.94%)\n",
      "\n",
      "Round  20, Average loss 1.414 Test accuracy 94.940\n",
      "selected users: [ 0  1  2  3  4  5  7  8  9 11 12 13 14 15 16 17 18 19 20 22 23 24 25 26\n",
      " 27 28 30]\n",
      "\n",
      "Test set: Average loss: 1.2808 \n",
      "Accuracy: 9495/10000 (94.95%)\n",
      "\n",
      "Round  21, Average loss 1.281 Test accuracy 94.950\n",
      "selected users: [ 1  2  3  4  5  7  8  9 10 11 12 13 15 16 17 18 19 20 21 22 23 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.2867 \n",
      "Accuracy: 9494/10000 (94.94%)\n",
      "\n",
      "Round  22, Average loss 1.287 Test accuracy 94.940\n",
      "selected users: [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 16 17 18 19 20 22 23 24 25 26\n",
      " 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.4810 \n",
      "Accuracy: 9496/10000 (94.96%)\n",
      "\n",
      "Round  23, Average loss 1.481 Test accuracy 94.960\n",
      "selected users: [ 0  1  2  3  4  6  7  8  9 11 12 13 14 15 16 18 19 20 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.4332 \n",
      "Accuracy: 9473/10000 (94.73%)\n",
      "\n",
      "Round  24, Average loss 1.433 Test accuracy 94.730\n",
      "selected users: [ 0  1  2  3  5  7  8  9 10 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 0.9619 \n",
      "Accuracy: 9437/10000 (94.37%)\n",
      "\n",
      "Round  25, Average loss 0.962 Test accuracy 94.370\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 18 19 20 22 23 24 25\n",
      " 27 28 30]\n",
      "\n",
      "Test set: Average loss: 1.4756 \n",
      "Accuracy: 9464/10000 (94.64%)\n",
      "\n",
      "Round  26, Average loss 1.476 Test accuracy 94.640\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 14 15 17 18 19 20 21 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.2936 \n",
      "Accuracy: 9505/10000 (95.05%)\n",
      "\n",
      "Round  27, Average loss 1.294 Test accuracy 95.050\n",
      "selected users: [ 0  1  2  4  5  6  7  8  9 10 11 12 13 16 17 18 19 20 21 22 24 25 26 27\n",
      " 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5128 \n",
      "Accuracy: 9494/10000 (94.94%)\n",
      "\n",
      "Round  28, Average loss 1.513 Test accuracy 94.940\n",
      "selected users: [ 0  1  2  4  5  6  7  8 10 11 12 13 14 15 16 17 18 19 20 21 23 24 25 26\n",
      " 27 28 30]\n",
      "\n",
      "Test set: Average loss: 1.3885 \n",
      "Accuracy: 9504/10000 (95.04%)\n",
      "\n",
      "Round  29, Average loss 1.389 Test accuracy 95.040\n",
      "number of results: 30\n",
      "(m= 30 )  0 -th Trial!!\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.2971 \n",
      "Accuracy: 5046/10000 (50.46%)\n",
      "\n",
      "Round   1, Average loss 2.297 Test accuracy 50.460\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.9943 \n",
      "Accuracy: 9225/10000 (92.25%)\n",
      "\n",
      "Round   2, Average loss 1.994 Test accuracy 92.250\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7864 \n",
      "Accuracy: 9404/10000 (94.04%)\n",
      "\n",
      "Round   3, Average loss 1.786 Test accuracy 94.040\n",
      "selected users: [ 0  1  2  3  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7269 \n",
      "Accuracy: 9442/10000 (94.42%)\n",
      "\n",
      "Round   4, Average loss 1.727 Test accuracy 94.420\n",
      "selected users: [ 0  1  2  3  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7233 \n",
      "Accuracy: 9463/10000 (94.63%)\n",
      "\n",
      "Round   5, Average loss 1.723 Test accuracy 94.630\n",
      "selected users: [ 0  1  2  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7749 \n",
      "Accuracy: 9469/10000 (94.69%)\n",
      "\n",
      "Round   6, Average loss 1.775 Test accuracy 94.690\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7399 \n",
      "Accuracy: 9494/10000 (94.94%)\n",
      "\n",
      "Round   7, Average loss 1.740 Test accuracy 94.940\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6449 \n",
      "Accuracy: 9479/10000 (94.79%)\n",
      "\n",
      "Round   8, Average loss 1.645 Test accuracy 94.790\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6202 \n",
      "Accuracy: 9486/10000 (94.86%)\n",
      "\n",
      "Round   9, Average loss 1.620 Test accuracy 94.860\n",
      "selected users: [ 0  1  2  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5954 \n",
      "Accuracy: 9502/10000 (95.02%)\n",
      "\n",
      "Round  10, Average loss 1.595 Test accuracy 95.020\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7412 \n",
      "Accuracy: 9486/10000 (94.86%)\n",
      "\n",
      "Round  11, Average loss 1.741 Test accuracy 94.860\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6024 \n",
      "Accuracy: 9493/10000 (94.93%)\n",
      "\n",
      "Round  12, Average loss 1.602 Test accuracy 94.930\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7732 \n",
      "Accuracy: 9496/10000 (94.96%)\n",
      "\n",
      "Round  13, Average loss 1.773 Test accuracy 94.960\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5980 \n",
      "Accuracy: 9497/10000 (94.97%)\n",
      "\n",
      "Round  14, Average loss 1.598 Test accuracy 94.970\n",
      "selected users: [ 0  1  2  3  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5188 \n",
      "Accuracy: 9515/10000 (95.15%)\n",
      "\n",
      "Round  15, Average loss 1.519 Test accuracy 95.150\n",
      "selected users: [ 0  1  2  3  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6320 \n",
      "Accuracy: 9524/10000 (95.24%)\n",
      "\n",
      "Round  16, Average loss 1.632 Test accuracy 95.240\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.6947 \n",
      "Accuracy: 9480/10000 (94.80%)\n",
      "\n",
      "Round  17, Average loss 1.695 Test accuracy 94.800\n",
      "selected users: [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5664 \n",
      "Accuracy: 9495/10000 (94.95%)\n",
      "\n",
      "Round  18, Average loss 1.566 Test accuracy 94.950\n",
      "selected users: [ 0  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.4521 \n",
      "Accuracy: 9513/10000 (95.13%)\n",
      "\n",
      "Round  19, Average loss 1.452 Test accuracy 95.130\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7600 \n",
      "Accuracy: 9500/10000 (95.00%)\n",
      "\n",
      "Round  20, Average loss 1.760 Test accuracy 95.000\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6395 \n",
      "Accuracy: 9537/10000 (95.37%)\n",
      "\n",
      "Round  21, Average loss 1.640 Test accuracy 95.370\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6878 \n",
      "Accuracy: 9512/10000 (95.12%)\n",
      "\n",
      "Round  22, Average loss 1.688 Test accuracy 95.120\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7064 \n",
      "Accuracy: 9489/10000 (94.89%)\n",
      "\n",
      "Round  23, Average loss 1.706 Test accuracy 94.890\n",
      "selected users: [ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7879 \n",
      "Accuracy: 9482/10000 (94.82%)\n",
      "\n",
      "Round  24, Average loss 1.788 Test accuracy 94.820\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5832 \n",
      "Accuracy: 9504/10000 (95.04%)\n",
      "\n",
      "Round  25, Average loss 1.583 Test accuracy 95.040\n",
      "selected users: [ 0  1  2  3  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5827 \n",
      "Accuracy: 9523/10000 (95.23%)\n",
      "\n",
      "Round  26, Average loss 1.583 Test accuracy 95.230\n",
      "selected users: [ 0  1  2  3  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6208 \n",
      "Accuracy: 9502/10000 (95.02%)\n",
      "\n",
      "Round  27, Average loss 1.621 Test accuracy 95.020\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7421 \n",
      "Accuracy: 9472/10000 (94.72%)\n",
      "\n",
      "Round  28, Average loss 1.742 Test accuracy 94.720\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5594 \n",
      "Accuracy: 9513/10000 (95.13%)\n",
      "\n",
      "Round  29, Average loss 1.559 Test accuracy 95.130\n",
      "(m= 30 )  1 -th Trial!!\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.2871 \n",
      "Accuracy: 6544/10000 (65.44%)\n",
      "\n",
      "Round   1, Average loss 2.287 Test accuracy 65.440\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.9614 \n",
      "Accuracy: 9326/10000 (93.26%)\n",
      "\n",
      "Round   2, Average loss 1.961 Test accuracy 93.260\n",
      "selected users: [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.8277 \n",
      "Accuracy: 9415/10000 (94.15%)\n",
      "\n",
      "Round   3, Average loss 1.828 Test accuracy 94.150\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.8418 \n",
      "Accuracy: 9435/10000 (94.35%)\n",
      "\n",
      "Round   4, Average loss 1.842 Test accuracy 94.350\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6800 \n",
      "Accuracy: 9455/10000 (94.55%)\n",
      "\n",
      "Round   5, Average loss 1.680 Test accuracy 94.550\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6989 \n",
      "Accuracy: 9471/10000 (94.71%)\n",
      "\n",
      "Round   6, Average loss 1.699 Test accuracy 94.710\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7325 \n",
      "Accuracy: 9455/10000 (94.55%)\n",
      "\n",
      "Round   7, Average loss 1.733 Test accuracy 94.550\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7691 \n",
      "Accuracy: 9422/10000 (94.22%)\n",
      "\n",
      "Round   8, Average loss 1.769 Test accuracy 94.220\n",
      "selected users: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7417 \n",
      "Accuracy: 9458/10000 (94.58%)\n",
      "\n",
      "Round   9, Average loss 1.742 Test accuracy 94.580\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7475 \n",
      "Accuracy: 9445/10000 (94.45%)\n",
      "\n",
      "Round  10, Average loss 1.747 Test accuracy 94.450\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7515 \n",
      "Accuracy: 9508/10000 (95.08%)\n",
      "\n",
      "Round  11, Average loss 1.752 Test accuracy 95.080\n",
      "selected users: [ 0  1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7680 \n",
      "Accuracy: 9469/10000 (94.69%)\n",
      "\n",
      "Round  12, Average loss 1.768 Test accuracy 94.690\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7980 \n",
      "Accuracy: 9443/10000 (94.43%)\n",
      "\n",
      "Round  13, Average loss 1.798 Test accuracy 94.430\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6712 \n",
      "Accuracy: 9499/10000 (94.99%)\n",
      "\n",
      "Round  14, Average loss 1.671 Test accuracy 94.990\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7374 \n",
      "Accuracy: 9499/10000 (94.99%)\n",
      "\n",
      "Round  15, Average loss 1.737 Test accuracy 94.990\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.8279 \n",
      "Accuracy: 9485/10000 (94.85%)\n",
      "\n",
      "Round  16, Average loss 1.828 Test accuracy 94.850\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.8091 \n",
      "Accuracy: 9513/10000 (95.13%)\n",
      "\n",
      "Round  17, Average loss 1.809 Test accuracy 95.130\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7447 \n",
      "Accuracy: 9481/10000 (94.81%)\n",
      "\n",
      "Round  18, Average loss 1.745 Test accuracy 94.810\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6702 \n",
      "Accuracy: 9504/10000 (95.04%)\n",
      "\n",
      "Round  19, Average loss 1.670 Test accuracy 95.040\n",
      "selected users: [ 0  1  2  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7913 \n",
      "Accuracy: 9490/10000 (94.90%)\n",
      "\n",
      "Round  20, Average loss 1.791 Test accuracy 94.900\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.8043 \n",
      "Accuracy: 9471/10000 (94.71%)\n",
      "\n",
      "Round  21, Average loss 1.804 Test accuracy 94.710\n",
      "selected users: [ 0  1  2  3  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6279 \n",
      "Accuracy: 9441/10000 (94.41%)\n",
      "\n",
      "Round  22, Average loss 1.628 Test accuracy 94.410\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7223 \n",
      "Accuracy: 9496/10000 (94.96%)\n",
      "\n",
      "Round  23, Average loss 1.722 Test accuracy 94.960\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 23 24\n",
      " 25 26 27 28 29 30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.6539 \n",
      "Accuracy: 9477/10000 (94.77%)\n",
      "\n",
      "Round  24, Average loss 1.654 Test accuracy 94.770\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7467 \n",
      "Accuracy: 9476/10000 (94.76%)\n",
      "\n",
      "Round  25, Average loss 1.747 Test accuracy 94.760\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5682 \n",
      "Accuracy: 9511/10000 (95.11%)\n",
      "\n",
      "Round  26, Average loss 1.568 Test accuracy 95.110\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 1.6452 \n",
      "Accuracy: 9525/10000 (95.25%)\n",
      "\n",
      "Round  27, Average loss 1.645 Test accuracy 95.250\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7289 \n",
      "Accuracy: 9486/10000 (94.86%)\n",
      "\n",
      "Round  28, Average loss 1.729 Test accuracy 94.860\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6311 \n",
      "Accuracy: 9459/10000 (94.59%)\n",
      "\n",
      "Round  29, Average loss 1.631 Test accuracy 94.590\n",
      "(m= 30 )  2 -th Trial!!\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3014 \n",
      "Accuracy: 2063/10000 (20.63%)\n",
      "\n",
      "Round   0, Average loss 2.301 Test accuracy 20.630\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.2713 \n",
      "Accuracy: 6232/10000 (62.32%)\n",
      "\n",
      "Round   1, Average loss 2.271 Test accuracy 62.320\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.0371 \n",
      "Accuracy: 9168/10000 (91.68%)\n",
      "\n",
      "Round   2, Average loss 2.037 Test accuracy 91.680\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.8813 \n",
      "Accuracy: 9394/10000 (93.94%)\n",
      "\n",
      "Round   3, Average loss 1.881 Test accuracy 93.940\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 1.8440 \n",
      "Accuracy: 9467/10000 (94.67%)\n",
      "\n",
      "Round   4, Average loss 1.844 Test accuracy 94.670\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.8065 \n",
      "Accuracy: 9462/10000 (94.62%)\n",
      "\n",
      "Round   5, Average loss 1.807 Test accuracy 94.620\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7672 \n",
      "Accuracy: 9455/10000 (94.55%)\n",
      "\n",
      "Round   6, Average loss 1.767 Test accuracy 94.550\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.8395 \n",
      "Accuracy: 9433/10000 (94.33%)\n",
      "\n",
      "Round   7, Average loss 1.840 Test accuracy 94.330\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7999 \n",
      "Accuracy: 9464/10000 (94.64%)\n",
      "\n",
      "Round   8, Average loss 1.800 Test accuracy 94.640\n",
      "selected users: [ 0  1  2  3  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7681 \n",
      "Accuracy: 9433/10000 (94.33%)\n",
      "\n",
      "Round   9, Average loss 1.768 Test accuracy 94.330\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7869 \n",
      "Accuracy: 9446/10000 (94.46%)\n",
      "\n",
      "Round  10, Average loss 1.787 Test accuracy 94.460\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.8608 \n",
      "Accuracy: 9438/10000 (94.38%)\n",
      "\n",
      "Round  11, Average loss 1.861 Test accuracy 94.380\n",
      "selected users: [ 0  1  2  3  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7419 \n",
      "Accuracy: 9456/10000 (94.56%)\n",
      "\n",
      "Round  12, Average loss 1.742 Test accuracy 94.560\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.8060 \n",
      "Accuracy: 9472/10000 (94.72%)\n",
      "\n",
      "Round  13, Average loss 1.806 Test accuracy 94.720\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7782 \n",
      "Accuracy: 9461/10000 (94.61%)\n",
      "\n",
      "Round  14, Average loss 1.778 Test accuracy 94.610\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7954 \n",
      "Accuracy: 9439/10000 (94.39%)\n",
      "\n",
      "Round  15, Average loss 1.795 Test accuracy 94.390\n",
      "selected users: [ 0  1  2  3  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7838 \n",
      "Accuracy: 9471/10000 (94.71%)\n",
      "\n",
      "Round  16, Average loss 1.784 Test accuracy 94.710\n",
      "selected users: [ 0  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6438 \n",
      "Accuracy: 9473/10000 (94.73%)\n",
      "\n",
      "Round  17, Average loss 1.644 Test accuracy 94.730\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.8043 \n",
      "Accuracy: 9429/10000 (94.29%)\n",
      "\n",
      "Round  18, Average loss 1.804 Test accuracy 94.290\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.8035 \n",
      "Accuracy: 9471/10000 (94.71%)\n",
      "\n",
      "Round  19, Average loss 1.803 Test accuracy 94.710\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.8108 \n",
      "Accuracy: 9445/10000 (94.45%)\n",
      "\n",
      "Round  20, Average loss 1.811 Test accuracy 94.450\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.8150 \n",
      "Accuracy: 9448/10000 (94.48%)\n",
      "\n",
      "Round  21, Average loss 1.815 Test accuracy 94.480\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7965 \n",
      "Accuracy: 9440/10000 (94.40%)\n",
      "\n",
      "Round  22, Average loss 1.797 Test accuracy 94.400\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7671 \n",
      "Accuracy: 9458/10000 (94.58%)\n",
      "\n",
      "Round  23, Average loss 1.767 Test accuracy 94.580\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.8587 \n",
      "Accuracy: 9419/10000 (94.19%)\n",
      "\n",
      "Round  24, Average loss 1.859 Test accuracy 94.190\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7439 \n",
      "Accuracy: 9437/10000 (94.37%)\n",
      "\n",
      "Round  25, Average loss 1.744 Test accuracy 94.370\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.8102 \n",
      "Accuracy: 9450/10000 (94.50%)\n",
      "\n",
      "Round  26, Average loss 1.810 Test accuracy 94.500\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.8312 \n",
      "Accuracy: 9462/10000 (94.62%)\n",
      "\n",
      "Round  27, Average loss 1.831 Test accuracy 94.620\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.8082 \n",
      "Accuracy: 9447/10000 (94.47%)\n",
      "\n",
      "Round  28, Average loss 1.808 Test accuracy 94.470\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.8261 \n",
      "Accuracy: 9423/10000 (94.23%)\n",
      "\n",
      "Round  29, Average loss 1.826 Test accuracy 94.230\n",
      "(m= 30 )  3 -th Trial!!\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 24\n",
      " 25 26 27 28 29 30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.2890 \n",
      "Accuracy: 6606/10000 (66.06%)\n",
      "\n",
      "Round   1, Average loss 2.289 Test accuracy 66.060\n",
      "selected users: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.8628 \n",
      "Accuracy: 9313/10000 (93.13%)\n",
      "\n",
      "Round   2, Average loss 1.863 Test accuracy 93.130\n",
      "selected users: [ 0  1  2  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7289 \n",
      "Accuracy: 9453/10000 (94.53%)\n",
      "\n",
      "Round   3, Average loss 1.729 Test accuracy 94.530\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6851 \n",
      "Accuracy: 9497/10000 (94.97%)\n",
      "\n",
      "Round   4, Average loss 1.685 Test accuracy 94.970\n",
      "selected users: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7271 \n",
      "Accuracy: 9463/10000 (94.63%)\n",
      "\n",
      "Round   5, Average loss 1.727 Test accuracy 94.630\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6867 \n",
      "Accuracy: 9503/10000 (95.03%)\n",
      "\n",
      "Round   6, Average loss 1.687 Test accuracy 95.030\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7779 \n",
      "Accuracy: 9449/10000 (94.49%)\n",
      "\n",
      "Round   7, Average loss 1.778 Test accuracy 94.490\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7468 \n",
      "Accuracy: 9468/10000 (94.68%)\n",
      "\n",
      "Round   8, Average loss 1.747 Test accuracy 94.680\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7614 \n",
      "Accuracy: 9476/10000 (94.76%)\n",
      "\n",
      "Round   9, Average loss 1.761 Test accuracy 94.760\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7565 \n",
      "Accuracy: 9460/10000 (94.60%)\n",
      "\n",
      "Round  10, Average loss 1.757 Test accuracy 94.600\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.9080 \n",
      "Accuracy: 9409/10000 (94.09%)\n",
      "\n",
      "Round  11, Average loss 1.908 Test accuracy 94.090\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7861 \n",
      "Accuracy: 9437/10000 (94.37%)\n",
      "\n",
      "Round  12, Average loss 1.786 Test accuracy 94.370\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7854 \n",
      "Accuracy: 9475/10000 (94.75%)\n",
      "\n",
      "Round  13, Average loss 1.785 Test accuracy 94.750\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.9465 \n",
      "Accuracy: 9426/10000 (94.26%)\n",
      "\n",
      "Round  14, Average loss 1.947 Test accuracy 94.260\n",
      "selected users: [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7953 \n",
      "Accuracy: 9437/10000 (94.37%)\n",
      "\n",
      "Round  15, Average loss 1.795 Test accuracy 94.370\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7704 \n",
      "Accuracy: 9491/10000 (94.91%)\n",
      "\n",
      "Round  16, Average loss 1.770 Test accuracy 94.910\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7332 \n",
      "Accuracy: 9385/10000 (93.85%)\n",
      "\n",
      "Round  17, Average loss 1.733 Test accuracy 93.850\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7216 \n",
      "Accuracy: 9475/10000 (94.75%)\n",
      "\n",
      "Round  18, Average loss 1.722 Test accuracy 94.750\n",
      "selected users: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6680 \n",
      "Accuracy: 9480/10000 (94.80%)\n",
      "\n",
      "Round  19, Average loss 1.668 Test accuracy 94.800\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7523 \n",
      "Accuracy: 9460/10000 (94.60%)\n",
      "\n",
      "Round  20, Average loss 1.752 Test accuracy 94.600\n",
      "selected users: [ 0  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6393 \n",
      "Accuracy: 9491/10000 (94.91%)\n",
      "\n",
      "Round  21, Average loss 1.639 Test accuracy 94.910\n",
      "selected users: [ 0  1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7982 \n",
      "Accuracy: 9451/10000 (94.51%)\n",
      "\n",
      "Round  22, Average loss 1.798 Test accuracy 94.510\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7782 \n",
      "Accuracy: 9489/10000 (94.89%)\n",
      "\n",
      "Round  23, Average loss 1.778 Test accuracy 94.890\n",
      "selected users: [ 0  1  2  3  4  5  6  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.8591 \n",
      "Accuracy: 9391/10000 (93.91%)\n",
      "\n",
      "Round  24, Average loss 1.859 Test accuracy 93.910\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6480 \n",
      "Accuracy: 9502/10000 (95.02%)\n",
      "\n",
      "Round  25, Average loss 1.648 Test accuracy 95.020\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7225 \n",
      "Accuracy: 9523/10000 (95.23%)\n",
      "\n",
      "Round  26, Average loss 1.722 Test accuracy 95.230\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7431 \n",
      "Accuracy: 9482/10000 (94.82%)\n",
      "\n",
      "Round  27, Average loss 1.743 Test accuracy 94.820\n",
      "selected users: [ 0  1  2  3  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5998 \n",
      "Accuracy: 9497/10000 (94.97%)\n",
      "\n",
      "Round  28, Average loss 1.600 Test accuracy 94.970\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6323 \n",
      "Accuracy: 9484/10000 (94.84%)\n",
      "\n",
      "Round  29, Average loss 1.632 Test accuracy 94.840\n",
      "(m= 30 )  4 -th Trial!!\n",
      "selected users: [ 0  1  2  3  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  1  2  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.2920 \n",
      "Accuracy: 5549/10000 (55.49%)\n",
      "\n",
      "Round   1, Average loss 2.292 Test accuracy 55.490\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.9377 \n",
      "Accuracy: 9291/10000 (92.91%)\n",
      "\n",
      "Round   2, Average loss 1.938 Test accuracy 92.910\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.8268 \n",
      "Accuracy: 9306/10000 (93.06%)\n",
      "\n",
      "Round   3, Average loss 1.827 Test accuracy 93.060\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7697 \n",
      "Accuracy: 9454/10000 (94.54%)\n",
      "\n",
      "Round   4, Average loss 1.770 Test accuracy 94.540\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7708 \n",
      "Accuracy: 9447/10000 (94.47%)\n",
      "\n",
      "Round   5, Average loss 1.771 Test accuracy 94.470\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7439 \n",
      "Accuracy: 9444/10000 (94.44%)\n",
      "\n",
      "Round   6, Average loss 1.744 Test accuracy 94.440\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7020 \n",
      "Accuracy: 9503/10000 (95.03%)\n",
      "\n",
      "Round   7, Average loss 1.702 Test accuracy 95.030\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.7759 \n",
      "Accuracy: 9425/10000 (94.25%)\n",
      "\n",
      "Round   8, Average loss 1.776 Test accuracy 94.250\n",
      "selected users: [ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7760 \n",
      "Accuracy: 9459/10000 (94.59%)\n",
      "\n",
      "Round   9, Average loss 1.776 Test accuracy 94.590\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7340 \n",
      "Accuracy: 9506/10000 (95.06%)\n",
      "\n",
      "Round  10, Average loss 1.734 Test accuracy 95.060\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7003 \n",
      "Accuracy: 9504/10000 (95.04%)\n",
      "\n",
      "Round  11, Average loss 1.700 Test accuracy 95.040\n",
      "selected users: [ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6855 \n",
      "Accuracy: 9462/10000 (94.62%)\n",
      "\n",
      "Round  12, Average loss 1.685 Test accuracy 94.620\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6843 \n",
      "Accuracy: 9498/10000 (94.98%)\n",
      "\n",
      "Round  13, Average loss 1.684 Test accuracy 94.980\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7615 \n",
      "Accuracy: 9480/10000 (94.80%)\n",
      "\n",
      "Round  14, Average loss 1.762 Test accuracy 94.800\n",
      "selected users: [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6089 \n",
      "Accuracy: 9484/10000 (94.84%)\n",
      "\n",
      "Round  15, Average loss 1.609 Test accuracy 94.840\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5485 \n",
      "Accuracy: 9481/10000 (94.81%)\n",
      "\n",
      "Round  16, Average loss 1.549 Test accuracy 94.810\n",
      "selected users: [ 0  1  2  3  4  5  6  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7583 \n",
      "Accuracy: 9446/10000 (94.46%)\n",
      "\n",
      "Round  17, Average loss 1.758 Test accuracy 94.460\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7178 \n",
      "Accuracy: 9487/10000 (94.87%)\n",
      "\n",
      "Round  18, Average loss 1.718 Test accuracy 94.870\n",
      "selected users: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6728 \n",
      "Accuracy: 9478/10000 (94.78%)\n",
      "\n",
      "Round  19, Average loss 1.673 Test accuracy 94.780\n",
      "selected users: [ 0  1  2  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7390 \n",
      "Accuracy: 9500/10000 (95.00%)\n",
      "\n",
      "Round  20, Average loss 1.739 Test accuracy 95.000\n",
      "selected users: [ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7820 \n",
      "Accuracy: 9469/10000 (94.69%)\n",
      "\n",
      "Round  21, Average loss 1.782 Test accuracy 94.690\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5583 \n",
      "Accuracy: 9521/10000 (95.21%)\n",
      "\n",
      "Round  22, Average loss 1.558 Test accuracy 95.210\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6818 \n",
      "Accuracy: 9474/10000 (94.74%)\n",
      "\n",
      "Round  23, Average loss 1.682 Test accuracy 94.740\n",
      "selected users: [ 0  1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7208 \n",
      "Accuracy: 9441/10000 (94.41%)\n",
      "\n",
      "Round  24, Average loss 1.721 Test accuracy 94.410\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7300 \n",
      "Accuracy: 9524/10000 (95.24%)\n",
      "\n",
      "Round  25, Average loss 1.730 Test accuracy 95.240\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7478 \n",
      "Accuracy: 9495/10000 (94.95%)\n",
      "\n",
      "Round  26, Average loss 1.748 Test accuracy 94.950\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7073 \n",
      "Accuracy: 9495/10000 (94.95%)\n",
      "\n",
      "Round  27, Average loss 1.707 Test accuracy 94.950\n",
      "selected users: [ 0  1  2  3  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7202 \n",
      "Accuracy: 9487/10000 (94.87%)\n",
      "\n",
      "Round  28, Average loss 1.720 Test accuracy 94.870\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7643 \n",
      "Accuracy: 9467/10000 (94.67%)\n",
      "\n",
      "Round  29, Average loss 1.764 Test accuracy 94.670\n",
      "(m= 30 )  5 -th Trial!!\n",
      "selected users: [ 0  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3010 \n",
      "Accuracy: 1180/10000 (11.80%)\n",
      "\n",
      "Round   1, Average loss 2.301 Test accuracy 11.800\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 2.1538 \n",
      "Accuracy: 8424/10000 (84.24%)\n",
      "\n",
      "Round   2, Average loss 2.154 Test accuracy 84.240\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.8455 \n",
      "Accuracy: 9401/10000 (94.01%)\n",
      "\n",
      "Round   3, Average loss 1.845 Test accuracy 94.010\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7344 \n",
      "Accuracy: 9415/10000 (94.15%)\n",
      "\n",
      "Round   4, Average loss 1.734 Test accuracy 94.150\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7540 \n",
      "Accuracy: 9470/10000 (94.70%)\n",
      "\n",
      "Round   5, Average loss 1.754 Test accuracy 94.700\n",
      "selected users: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6360 \n",
      "Accuracy: 9476/10000 (94.76%)\n",
      "\n",
      "Round   6, Average loss 1.636 Test accuracy 94.760\n",
      "selected users: [ 0  1  2  3  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7619 \n",
      "Accuracy: 9499/10000 (94.99%)\n",
      "\n",
      "Round   7, Average loss 1.762 Test accuracy 94.990\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7221 \n",
      "Accuracy: 9484/10000 (94.84%)\n",
      "\n",
      "Round   8, Average loss 1.722 Test accuracy 94.840\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6468 \n",
      "Accuracy: 9528/10000 (95.28%)\n",
      "\n",
      "Round   9, Average loss 1.647 Test accuracy 95.280\n",
      "selected users: [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6991 \n",
      "Accuracy: 9477/10000 (94.77%)\n",
      "\n",
      "Round  10, Average loss 1.699 Test accuracy 94.770\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7179 \n",
      "Accuracy: 9494/10000 (94.94%)\n",
      "\n",
      "Round  11, Average loss 1.718 Test accuracy 94.940\n",
      "selected users: [ 0  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5146 \n",
      "Accuracy: 9537/10000 (95.37%)\n",
      "\n",
      "Round  12, Average loss 1.515 Test accuracy 95.370\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7172 \n",
      "Accuracy: 9460/10000 (94.60%)\n",
      "\n",
      "Round  13, Average loss 1.717 Test accuracy 94.600\n",
      "selected users: [ 0  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6485 \n",
      "Accuracy: 9488/10000 (94.88%)\n",
      "\n",
      "Round  14, Average loss 1.649 Test accuracy 94.880\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 24\n",
      " 25 26 27 28 29 30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.7148 \n",
      "Accuracy: 9491/10000 (94.91%)\n",
      "\n",
      "Round  15, Average loss 1.715 Test accuracy 94.910\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6907 \n",
      "Accuracy: 9488/10000 (94.88%)\n",
      "\n",
      "Round  16, Average loss 1.691 Test accuracy 94.880\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5597 \n",
      "Accuracy: 9516/10000 (95.16%)\n",
      "\n",
      "Round  17, Average loss 1.560 Test accuracy 95.160\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7403 \n",
      "Accuracy: 9506/10000 (95.06%)\n",
      "\n",
      "Round  18, Average loss 1.740 Test accuracy 95.060\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6377 \n",
      "Accuracy: 9514/10000 (95.14%)\n",
      "\n",
      "Round  19, Average loss 1.638 Test accuracy 95.140\n",
      "selected users: [ 0  1  2  3  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6468 \n",
      "Accuracy: 9465/10000 (94.65%)\n",
      "\n",
      "Round  20, Average loss 1.647 Test accuracy 94.650\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6940 \n",
      "Accuracy: 9495/10000 (94.95%)\n",
      "\n",
      "Round  21, Average loss 1.694 Test accuracy 94.950\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6982 \n",
      "Accuracy: 9508/10000 (95.08%)\n",
      "\n",
      "Round  22, Average loss 1.698 Test accuracy 95.080\n",
      "selected users: [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6685 \n",
      "Accuracy: 9480/10000 (94.80%)\n",
      "\n",
      "Round  23, Average loss 1.668 Test accuracy 94.800\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7327 \n",
      "Accuracy: 9502/10000 (95.02%)\n",
      "\n",
      "Round  24, Average loss 1.733 Test accuracy 95.020\n",
      "selected users: [ 0  1  2  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7958 \n",
      "Accuracy: 9442/10000 (94.42%)\n",
      "\n",
      "Round  25, Average loss 1.796 Test accuracy 94.420\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7165 \n",
      "Accuracy: 9519/10000 (95.19%)\n",
      "\n",
      "Round  26, Average loss 1.716 Test accuracy 95.190\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7298 \n",
      "Accuracy: 9502/10000 (95.02%)\n",
      "\n",
      "Round  27, Average loss 1.730 Test accuracy 95.020\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7029 \n",
      "Accuracy: 9504/10000 (95.04%)\n",
      "\n",
      "Round  28, Average loss 1.703 Test accuracy 95.040\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7410 \n",
      "Accuracy: 9511/10000 (95.11%)\n",
      "\n",
      "Round  29, Average loss 1.741 Test accuracy 95.110\n",
      "(m= 30 )  6 -th Trial!!\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  1  2  3  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.2746 \n",
      "Accuracy: 6331/10000 (63.31%)\n",
      "\n",
      "Round   1, Average loss 2.275 Test accuracy 63.310\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7725 \n",
      "Accuracy: 9350/10000 (93.50%)\n",
      "\n",
      "Round   2, Average loss 1.773 Test accuracy 93.500\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.8153 \n",
      "Accuracy: 9463/10000 (94.63%)\n",
      "\n",
      "Round   3, Average loss 1.815 Test accuracy 94.630\n",
      "selected users: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6525 \n",
      "Accuracy: 9497/10000 (94.97%)\n",
      "\n",
      "Round   4, Average loss 1.652 Test accuracy 94.970\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7719 \n",
      "Accuracy: 9485/10000 (94.85%)\n",
      "\n",
      "Round   5, Average loss 1.772 Test accuracy 94.850\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6227 \n",
      "Accuracy: 9519/10000 (95.19%)\n",
      "\n",
      "Round   6, Average loss 1.623 Test accuracy 95.190\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7545 \n",
      "Accuracy: 9483/10000 (94.83%)\n",
      "\n",
      "Round   7, Average loss 1.755 Test accuracy 94.830\n",
      "selected users: [ 0  1  2  3  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6454 \n",
      "Accuracy: 9477/10000 (94.77%)\n",
      "\n",
      "Round   8, Average loss 1.645 Test accuracy 94.770\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7384 \n",
      "Accuracy: 9503/10000 (95.03%)\n",
      "\n",
      "Round   9, Average loss 1.738 Test accuracy 95.030\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5680 \n",
      "Accuracy: 9523/10000 (95.23%)\n",
      "\n",
      "Round  10, Average loss 1.568 Test accuracy 95.230\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.8321 \n",
      "Accuracy: 9461/10000 (94.61%)\n",
      "\n",
      "Round  11, Average loss 1.832 Test accuracy 94.610\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5713 \n",
      "Accuracy: 9494/10000 (94.94%)\n",
      "\n",
      "Round  12, Average loss 1.571 Test accuracy 94.940\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7332 \n",
      "Accuracy: 9489/10000 (94.89%)\n",
      "\n",
      "Round  13, Average loss 1.733 Test accuracy 94.890\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7231 \n",
      "Accuracy: 9469/10000 (94.69%)\n",
      "\n",
      "Round  14, Average loss 1.723 Test accuracy 94.690\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7356 \n",
      "Accuracy: 9476/10000 (94.76%)\n",
      "\n",
      "Round  15, Average loss 1.736 Test accuracy 94.760\n",
      "selected users: [ 0  1  2  3  4  5  6  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6513 \n",
      "Accuracy: 9496/10000 (94.96%)\n",
      "\n",
      "Round  16, Average loss 1.651 Test accuracy 94.960\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6353 \n",
      "Accuracy: 9485/10000 (94.85%)\n",
      "\n",
      "Round  17, Average loss 1.635 Test accuracy 94.850\n",
      "selected users: [ 0  1  2  3  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6624 \n",
      "Accuracy: 9452/10000 (94.52%)\n",
      "\n",
      "Round  18, Average loss 1.662 Test accuracy 94.520\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5846 \n",
      "Accuracy: 9488/10000 (94.88%)\n",
      "\n",
      "Round  19, Average loss 1.585 Test accuracy 94.880\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6794 \n",
      "Accuracy: 9486/10000 (94.86%)\n",
      "\n",
      "Round  20, Average loss 1.679 Test accuracy 94.860\n",
      "selected users: [ 0  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5481 \n",
      "Accuracy: 9488/10000 (94.88%)\n",
      "\n",
      "Round  21, Average loss 1.548 Test accuracy 94.880\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 26 27 28 29 30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.6028 \n",
      "Accuracy: 9508/10000 (95.08%)\n",
      "\n",
      "Round  22, Average loss 1.603 Test accuracy 95.080\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6303 \n",
      "Accuracy: 9484/10000 (94.84%)\n",
      "\n",
      "Round  23, Average loss 1.630 Test accuracy 94.840\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.8130 \n",
      "Accuracy: 9468/10000 (94.68%)\n",
      "\n",
      "Round  24, Average loss 1.813 Test accuracy 94.680\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6536 \n",
      "Accuracy: 9488/10000 (94.88%)\n",
      "\n",
      "Round  25, Average loss 1.654 Test accuracy 94.880\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.7709 \n",
      "Accuracy: 9484/10000 (94.84%)\n",
      "\n",
      "Round  26, Average loss 1.771 Test accuracy 94.840\n",
      "selected users: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5062 \n",
      "Accuracy: 9496/10000 (94.96%)\n",
      "\n",
      "Round  27, Average loss 1.506 Test accuracy 94.960\n",
      "selected users: [ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6382 \n",
      "Accuracy: 9476/10000 (94.76%)\n",
      "\n",
      "Round  28, Average loss 1.638 Test accuracy 94.760\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6348 \n",
      "Accuracy: 9509/10000 (95.09%)\n",
      "\n",
      "Round  29, Average loss 1.635 Test accuracy 95.090\n",
      "(m= 30 )  7 -th Trial!!\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.2946 \n",
      "Accuracy: 3757/10000 (37.57%)\n",
      "\n",
      "Round   1, Average loss 2.295 Test accuracy 37.570\n",
      "selected users: [ 0  1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.9674 \n",
      "Accuracy: 9299/10000 (92.99%)\n",
      "\n",
      "Round   2, Average loss 1.967 Test accuracy 92.990\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7078 \n",
      "Accuracy: 9480/10000 (94.80%)\n",
      "\n",
      "Round   3, Average loss 1.708 Test accuracy 94.800\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7402 \n",
      "Accuracy: 9484/10000 (94.84%)\n",
      "\n",
      "Round   4, Average loss 1.740 Test accuracy 94.840\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7111 \n",
      "Accuracy: 9489/10000 (94.89%)\n",
      "\n",
      "Round   5, Average loss 1.711 Test accuracy 94.890\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7023 \n",
      "Accuracy: 9472/10000 (94.72%)\n",
      "\n",
      "Round   6, Average loss 1.702 Test accuracy 94.720\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6560 \n",
      "Accuracy: 9503/10000 (95.03%)\n",
      "\n",
      "Round   7, Average loss 1.656 Test accuracy 95.030\n",
      "selected users: [ 0  1  2  3  4  5  6  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.8277 \n",
      "Accuracy: 9483/10000 (94.83%)\n",
      "\n",
      "Round   8, Average loss 1.828 Test accuracy 94.830\n",
      "selected users: [ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6157 \n",
      "Accuracy: 9510/10000 (95.10%)\n",
      "\n",
      "Round   9, Average loss 1.616 Test accuracy 95.100\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6960 \n",
      "Accuracy: 9499/10000 (94.99%)\n",
      "\n",
      "Round  10, Average loss 1.696 Test accuracy 94.990\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6003 \n",
      "Accuracy: 9517/10000 (95.17%)\n",
      "\n",
      "Round  11, Average loss 1.600 Test accuracy 95.170\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6037 \n",
      "Accuracy: 9489/10000 (94.89%)\n",
      "\n",
      "Round  12, Average loss 1.604 Test accuracy 94.890\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5809 \n",
      "Accuracy: 9514/10000 (95.14%)\n",
      "\n",
      "Round  13, Average loss 1.581 Test accuracy 95.140\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6419 \n",
      "Accuracy: 9509/10000 (95.09%)\n",
      "\n",
      "Round  14, Average loss 1.642 Test accuracy 95.090\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6227 \n",
      "Accuracy: 9531/10000 (95.31%)\n",
      "\n",
      "Round  15, Average loss 1.623 Test accuracy 95.310\n",
      "selected users: [ 0  1  2  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7085 \n",
      "Accuracy: 9543/10000 (95.43%)\n",
      "\n",
      "Round  16, Average loss 1.708 Test accuracy 95.430\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5882 \n",
      "Accuracy: 9489/10000 (94.89%)\n",
      "\n",
      "Round  17, Average loss 1.588 Test accuracy 94.890\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.7780 \n",
      "Accuracy: 9490/10000 (94.90%)\n",
      "\n",
      "Round  18, Average loss 1.778 Test accuracy 94.900\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6692 \n",
      "Accuracy: 9492/10000 (94.92%)\n",
      "\n",
      "Round  19, Average loss 1.669 Test accuracy 94.920\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5639 \n",
      "Accuracy: 9495/10000 (94.95%)\n",
      "\n",
      "Round  20, Average loss 1.564 Test accuracy 94.950\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29]\n",
      "\n",
      "Test set: Average loss: 1.8249 \n",
      "Accuracy: 9495/10000 (94.95%)\n",
      "\n",
      "Round  21, Average loss 1.825 Test accuracy 94.950\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6066 \n",
      "Accuracy: 9467/10000 (94.67%)\n",
      "\n",
      "Round  22, Average loss 1.607 Test accuracy 94.670\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.8653 \n",
      "Accuracy: 9479/10000 (94.79%)\n",
      "\n",
      "Round  23, Average loss 1.865 Test accuracy 94.790\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6214 \n",
      "Accuracy: 9495/10000 (94.95%)\n",
      "\n",
      "Round  24, Average loss 1.621 Test accuracy 94.950\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6963 \n",
      "Accuracy: 9492/10000 (94.92%)\n",
      "\n",
      "Round  25, Average loss 1.696 Test accuracy 94.920\n",
      "selected users: [ 0  1  2  3  4  5  6  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6753 \n",
      "Accuracy: 9509/10000 (95.09%)\n",
      "\n",
      "Round  26, Average loss 1.675 Test accuracy 95.090\n",
      "selected users: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6316 \n",
      "Accuracy: 9520/10000 (95.20%)\n",
      "\n",
      "Round  27, Average loss 1.632 Test accuracy 95.200\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 30]\n",
      "\n",
      "Test set: Average loss: 1.5988 \n",
      "Accuracy: 9508/10000 (95.08%)\n",
      "\n",
      "Round  28, Average loss 1.599 Test accuracy 95.080\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.6189 \n",
      "Accuracy: 9513/10000 (95.13%)\n",
      "\n",
      "Round  29, Average loss 1.619 Test accuracy 95.130\n",
      "(m= 30 )  8 -th Trial!!\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3002 \n",
      "Accuracy: 4784/10000 (47.84%)\n",
      "\n",
      "Round   1, Average loss 2.300 Test accuracy 47.840\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.0048 \n",
      "Accuracy: 9281/10000 (92.81%)\n",
      "\n",
      "Round   2, Average loss 2.005 Test accuracy 92.810\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.9669 \n",
      "Accuracy: 9417/10000 (94.17%)\n",
      "\n",
      "Round   3, Average loss 1.967 Test accuracy 94.170\n",
      "selected users: [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7794 \n",
      "Accuracy: 9440/10000 (94.40%)\n",
      "\n",
      "Round   4, Average loss 1.779 Test accuracy 94.400\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.8074 \n",
      "Accuracy: 9468/10000 (94.68%)\n",
      "\n",
      "Round   5, Average loss 1.807 Test accuracy 94.680\n",
      "selected users: [ 0  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.5421 \n",
      "Accuracy: 9451/10000 (94.51%)\n",
      "\n",
      "Round   6, Average loss 1.542 Test accuracy 94.510\n",
      "selected users: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6925 \n",
      "Accuracy: 9485/10000 (94.85%)\n",
      "\n",
      "Round   7, Average loss 1.693 Test accuracy 94.850\n",
      "selected users: [ 0  1  2  3  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7027 \n",
      "Accuracy: 9462/10000 (94.62%)\n",
      "\n",
      "Round   8, Average loss 1.703 Test accuracy 94.620\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.8808 \n",
      "Accuracy: 9435/10000 (94.35%)\n",
      "\n",
      "Round   9, Average loss 1.881 Test accuracy 94.350\n",
      "selected users: [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7335 \n",
      "Accuracy: 9410/10000 (94.10%)\n",
      "\n",
      "Round  10, Average loss 1.733 Test accuracy 94.100\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7855 \n",
      "Accuracy: 9462/10000 (94.62%)\n",
      "\n",
      "Round  11, Average loss 1.785 Test accuracy 94.620\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7951 \n",
      "Accuracy: 9455/10000 (94.55%)\n",
      "\n",
      "Round  12, Average loss 1.795 Test accuracy 94.550\n",
      "selected users: [ 0  1  2  3  4  5  6  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.8609 \n",
      "Accuracy: 9441/10000 (94.41%)\n",
      "\n",
      "Round  13, Average loss 1.861 Test accuracy 94.410\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.8261 \n",
      "Accuracy: 9440/10000 (94.40%)\n",
      "\n",
      "Round  14, Average loss 1.826 Test accuracy 94.400\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7582 \n",
      "Accuracy: 9463/10000 (94.63%)\n",
      "\n",
      "Round  15, Average loss 1.758 Test accuracy 94.630\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7778 \n",
      "Accuracy: 9401/10000 (94.01%)\n",
      "\n",
      "Round  16, Average loss 1.778 Test accuracy 94.010\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7615 \n",
      "Accuracy: 9471/10000 (94.71%)\n",
      "\n",
      "Round  17, Average loss 1.762 Test accuracy 94.710\n",
      "selected users: [ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7903 \n",
      "Accuracy: 9476/10000 (94.76%)\n",
      "\n",
      "Round  18, Average loss 1.790 Test accuracy 94.760\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7849 \n",
      "Accuracy: 9489/10000 (94.89%)\n",
      "\n",
      "Round  19, Average loss 1.785 Test accuracy 94.890\n",
      "selected users: [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.8011 \n",
      "Accuracy: 9464/10000 (94.64%)\n",
      "\n",
      "Round  20, Average loss 1.801 Test accuracy 94.640\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7979 \n",
      "Accuracy: 9469/10000 (94.69%)\n",
      "\n",
      "Round  21, Average loss 1.798 Test accuracy 94.690\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6950 \n",
      "Accuracy: 9480/10000 (94.80%)\n",
      "\n",
      "Round  22, Average loss 1.695 Test accuracy 94.800\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7556 \n",
      "Accuracy: 9498/10000 (94.98%)\n",
      "\n",
      "Round  23, Average loss 1.756 Test accuracy 94.980\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7241 \n",
      "Accuracy: 9446/10000 (94.46%)\n",
      "\n",
      "Round  24, Average loss 1.724 Test accuracy 94.460\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7399 \n",
      "Accuracy: 9470/10000 (94.70%)\n",
      "\n",
      "Round  25, Average loss 1.740 Test accuracy 94.700\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7738 \n",
      "Accuracy: 9460/10000 (94.60%)\n",
      "\n",
      "Round  26, Average loss 1.774 Test accuracy 94.600\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.8451 \n",
      "Accuracy: 9442/10000 (94.42%)\n",
      "\n",
      "Round  27, Average loss 1.845 Test accuracy 94.420\n",
      "selected users: [ 0  1  2  3  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6994 \n",
      "Accuracy: 9488/10000 (94.88%)\n",
      "\n",
      "Round  28, Average loss 1.699 Test accuracy 94.880\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7348 \n",
      "Accuracy: 9490/10000 (94.90%)\n",
      "\n",
      "Round  29, Average loss 1.735 Test accuracy 94.900\n",
      "(m= 30 )  9 -th Trial!!\n",
      "selected users: [ 0  1  2  3  4  5  6  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.303 Test accuracy 9.800\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 2.2859 \n",
      "Accuracy: 6987/10000 (69.87%)\n",
      "\n",
      "Round   1, Average loss 2.286 Test accuracy 69.870\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.9449 \n",
      "Accuracy: 8995/10000 (89.95%)\n",
      "\n",
      "Round   2, Average loss 1.945 Test accuracy 89.950\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7441 \n",
      "Accuracy: 9436/10000 (94.36%)\n",
      "\n",
      "Round   3, Average loss 1.744 Test accuracy 94.360\n",
      "selected users: [ 0  1  2  3  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6995 \n",
      "Accuracy: 9441/10000 (94.41%)\n",
      "\n",
      "Round   4, Average loss 1.700 Test accuracy 94.410\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7145 \n",
      "Accuracy: 9418/10000 (94.18%)\n",
      "\n",
      "Round   5, Average loss 1.715 Test accuracy 94.180\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 24\n",
      " 25 26 27 28 29 30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.8206 \n",
      "Accuracy: 9466/10000 (94.66%)\n",
      "\n",
      "Round   6, Average loss 1.821 Test accuracy 94.660\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7726 \n",
      "Accuracy: 9473/10000 (94.73%)\n",
      "\n",
      "Round   7, Average loss 1.773 Test accuracy 94.730\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7582 \n",
      "Accuracy: 9480/10000 (94.80%)\n",
      "\n",
      "Round   8, Average loss 1.758 Test accuracy 94.800\n",
      "selected users: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6816 \n",
      "Accuracy: 9486/10000 (94.86%)\n",
      "\n",
      "Round   9, Average loss 1.682 Test accuracy 94.860\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6397 \n",
      "Accuracy: 9454/10000 (94.54%)\n",
      "\n",
      "Round  10, Average loss 1.640 Test accuracy 94.540\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7274 \n",
      "Accuracy: 9481/10000 (94.81%)\n",
      "\n",
      "Round  11, Average loss 1.727 Test accuracy 94.810\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7806 \n",
      "Accuracy: 9521/10000 (95.21%)\n",
      "\n",
      "Round  12, Average loss 1.781 Test accuracy 95.210\n",
      "selected users: [ 0  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6571 \n",
      "Accuracy: 9468/10000 (94.68%)\n",
      "\n",
      "Round  13, Average loss 1.657 Test accuracy 94.680\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6319 \n",
      "Accuracy: 9514/10000 (95.14%)\n",
      "\n",
      "Round  14, Average loss 1.632 Test accuracy 95.140\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7212 \n",
      "Accuracy: 9470/10000 (94.70%)\n",
      "\n",
      "Round  15, Average loss 1.721 Test accuracy 94.700\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6937 \n",
      "Accuracy: 9529/10000 (95.29%)\n",
      "\n",
      "Round  16, Average loss 1.694 Test accuracy 95.290\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.8259 \n",
      "Accuracy: 9484/10000 (94.84%)\n",
      "\n",
      "Round  17, Average loss 1.826 Test accuracy 94.840\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.8052 \n",
      "Accuracy: 9488/10000 (94.88%)\n",
      "\n",
      "Round  18, Average loss 1.805 Test accuracy 94.880\n",
      "selected users: [ 0  1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7818 \n",
      "Accuracy: 9493/10000 (94.93%)\n",
      "\n",
      "Round  19, Average loss 1.782 Test accuracy 94.930\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7507 \n",
      "Accuracy: 9511/10000 (95.11%)\n",
      "\n",
      "Round  20, Average loss 1.751 Test accuracy 95.110\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.8231 \n",
      "Accuracy: 9490/10000 (94.90%)\n",
      "\n",
      "Round  21, Average loss 1.823 Test accuracy 94.900\n",
      "selected users: [ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.8608 \n",
      "Accuracy: 9490/10000 (94.90%)\n",
      "\n",
      "Round  22, Average loss 1.861 Test accuracy 94.900\n",
      "selected users: [ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.8119 \n",
      "Accuracy: 9469/10000 (94.69%)\n",
      "\n",
      "Round  23, Average loss 1.812 Test accuracy 94.690\n",
      "selected users: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7371 \n",
      "Accuracy: 9477/10000 (94.77%)\n",
      "\n",
      "Round  24, Average loss 1.737 Test accuracy 94.770\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6411 \n",
      "Accuracy: 9522/10000 (95.22%)\n",
      "\n",
      "Round  25, Average loss 1.641 Test accuracy 95.220\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.8424 \n",
      "Accuracy: 9435/10000 (94.35%)\n",
      "\n",
      "Round  26, Average loss 1.842 Test accuracy 94.350\n",
      "selected users: [ 0  1  2  3  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7277 \n",
      "Accuracy: 9442/10000 (94.42%)\n",
      "\n",
      "Round  27, Average loss 1.728 Test accuracy 94.420\n",
      "selected users: [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.6657 \n",
      "Accuracy: 9473/10000 (94.73%)\n",
      "\n",
      "Round  28, Average loss 1.666 Test accuracy 94.730\n",
      "selected users: [ 0  1  2  3  4  5  6  7  8  9 10 11 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "\n",
      "Test set: Average loss: 1.7799 \n",
      "Accuracy: 9477/10000 (94.77%)\n",
      "\n",
      "Round  29, Average loss 1.780 Test accuracy 94.770\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "# training\n",
    "loss_train_arr = []\n",
    "loss_test_arr = []\n",
    "acc_test_arr = []\n",
    "net_best = None\n",
    "best_loss = None\n",
    "\n",
    "N_trials = 10\n",
    "N_epochs = 30\n",
    "\n",
    "m_array = np.array([6,9,12,15,18,21,24,27,30]) # m is the number of received result @ master\n",
    "loss_test_arr = np.empty((len(m_array),N_trials,N_epochs))\n",
    "acc_test_arr  = np.empty((len(m_array),N_trials,N_epochs))\n",
    "\n",
    "for m_idx in range(len(m_array)):   \n",
    "    \n",
    "    m = m_array[m_idx] # m is the number of received result @ master\n",
    "    print('number of results:',m)\n",
    "    \n",
    "    for trial_idx in range(N_trials):\n",
    "        print('(m=',m,') ',trial_idx,'-th Trial!!')\n",
    "        \n",
    "        net_glob = CNNMnist2(args=args)\n",
    "        net_glob.cuda()\n",
    "        net_glob.train()\n",
    "\n",
    "        # copy weights\n",
    "        w_glob = net_glob.state_dict()\n",
    "\n",
    "        for iter in range(N_epochs): #args.epochs\n",
    "            w_locals, loss_locals = [], []\n",
    "            idxs_users = np.random.choice(range(N), m, replace=False)\n",
    "            idxs_users = np.sort(idxs_users)\n",
    "            print('selected users:',idxs_users)\n",
    "\n",
    "            dec_z_array = []\n",
    "            for idx in idxs_users: #for idx in range(N):\n",
    "        #         print(idx)\n",
    "                local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[idx,:,:], label=y_tilde[idx,:,:])\n",
    "                w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                w_locals.append(copy.deepcopy(w))\n",
    "                loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "            # update global weights\n",
    "            #w_glob = FedAvg(w_locals)\n",
    "            w_glob = FedAvg_with_BACC_Dec(w_locals, alpha_array, dec_z_array)\n",
    "\n",
    "            # copy weight to net_glob\n",
    "            net_glob.load_state_dict(w_glob)\n",
    "\n",
    "            # print loss\n",
    "        #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "        #     loss_train_arr.append(loss_train)\n",
    "\n",
    "            acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "            acc_test_arr[m_idx][trial_idx][iter] = acc_test\n",
    "            loss_test_arr[m_idx][trial_idx][iter] = loss_test\n",
    "            if iter % 1 ==0:\n",
    "                print('Round {:3d}, Average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_test, acc_test))\n",
    "            #print(loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 10, 30)\n",
      "(9, 30)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# m_array = np.array([6,9,12,15,18,21,24,27,30])\n",
    "\n",
    "plot_acc = np.mean(acc_test_arr, axis=1)\n",
    "print(acc_test_arr.shape)\n",
    "print(plot_acc.shape)\n",
    "\n",
    "plt.plot(plot_acc[0,:],label='n=N-s=6')\n",
    "plt.plot(plot_acc[1,:],label='n=N-s=9')\n",
    "plt.plot(plot_acc[2,:],label='n=N-s=12')\n",
    "plt.plot(plot_acc[4,:],label='n=N-s=18')\n",
    "plt.plot(plot_acc[6,:],label='n=N-s=24')\n",
    "plt.plot(plot_acc[8,:],label='n=N-s=30')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Test accuracy(%)')\n",
    "plt.show()\n",
    "\n",
    "import pickle\n",
    "\n",
    "filehandler = open(\"./plot/MNIST_LeNet_N31_K12_test_acc\",\"wb\")\n",
    "pickle.dump(acc_test_arr,filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
