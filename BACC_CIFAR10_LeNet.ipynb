{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import basic libraries\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Import PyTorch\n",
    "import torch # import main library\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn # import modules\n",
    "from torch.autograd import Function # import Function to create custom activations\n",
    "from torch.nn.parameter import Parameter # import Parameter to create custom activations with learnable parameters\n",
    "from torch import optim # import optimizers for demonstrations\n",
    "import torch.nn.functional as F # import torch functions\n",
    "from torchvision import datasets, transforms # import transformations to use for demo\n",
    "from torch.nn.parameter import Parameter # import Parameter to create custom activations with learnable parameters\n",
    "\n",
    "from models.activ_func import *\n",
    "from models.Nets import *\n",
    "from models.test import test_img\n",
    "from models.Update import *\n",
    "\n",
    "\n",
    "from utils.sampling import mnist_iid, mnist_noniid, cifar_iid\n",
    "from utils.options import args_parser\n",
    "from models.Update import LocalUpdate\n",
    "from models.Nets import MLP, CNNMnist, CNNCifar, LeNet, CNNMnist2, CNNMnist3\n",
    "from models.Fed import FedAvg\n",
    "from models.Fed import FedQAvg, Quantization, Quantization_Finite, my_score, my_score_Finite\n",
    "from models.test import test_img\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from utils.functions import *\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "class my_argument:    \n",
    "    epochs    = 16    #\"rounds of training\"\n",
    "    num_users = 4  # \"number of users: K\"\n",
    "    frac      = 0.5 #\"the fraction of clients: C\"\n",
    "    local_ep  = 1 #\"the number of local epochs: E\"\n",
    "    local_bs  = 50  #\"local batch size: B\"\n",
    "    bs        = 50 #\"test batch size\"\n",
    "    lr        = 0.001 #\"learning rate\"\n",
    "    momentum  = 0.5 # \"SGD momentum (default: 0.5)\"\n",
    "    weight_decay = 5e-4\n",
    "    split     = 'user' # \"train-test split type, user or sample\"\n",
    "    opt='ADAM'\n",
    "    loss='Custom' # 'Custom' or 'Default'\n",
    "\n",
    "    # model arguments\n",
    "    model = 'cnn'\n",
    "    kernel_num=9 #, help='number of each kind of kernel')\n",
    "    kernel_sizes='3,4,5' #  help='comma-separated kernel size to use for convolution')\n",
    "    norm='batch_norm' #, help=\"batch_norm, layer_norm, or None\")\n",
    "    num_filters=32 #, help=\"number of filters for conv nets\")\n",
    "    max_pool='True' #help=\"Whether use max pooling rather than strided convolutions\")\n",
    "\n",
    "    # other arguments\n",
    "    dataset='cifar' #, help=\"name of dataset\")\n",
    "    iid=1\n",
    "    num_classes=10#, help=\"number of classes\")\n",
    "    num_channels=1#, help=\"number of channels of imges\")\n",
    "    gpu=1#, help=\"GPU ID, -1 for CPU\")\n",
    "    stopping_rounds=10#, help='rounds of early stopping')\n",
    "    verbose='False'#, help='verbose print')\n",
    "    seed=1#, help='random seed (default: 1)')\n",
    "    \n",
    "args = my_argument()\n",
    "\n",
    "args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)\n",
    "args.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(args.device)\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Normalize the test set same as training set without augmentation\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# trainset = torchvision.datasets.CIFAR10(\n",
    "#     root=\"./data/cifar\", train=True, download=True, transform=transform_train)\n",
    "# trainloader = torch.utils.data.DataLoader(\n",
    "#     trainset, batch_size=args.bs, shuffle=True, num_workers=2)\n",
    "\n",
    "# testset = torchvision.datasets.CIFAR10(\n",
    "#     root=\"./data/cifar\", train=False, download=True, transform=transform_test)\n",
    "# testloader = torch.utils.data.DataLoader(\n",
    "#     testset, batch_size=args.bs, shuffle=False, num_workers=2)\n",
    "\n",
    "dataset_train = datasets.CIFAR10('../data/cifar', train=True, download=True, transform=transform_train)\n",
    "dataset_test = datasets.CIFAR10('../data/cifar', train=False, download=True, transform=transform_test)\n",
    "if args.iid:\n",
    "    dict_users = cifar_iid(dataset_train, args.num_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, testloader, criterion):\n",
    "    test_loss = 0\n",
    "    accuracy = 0\n",
    "    for images, labels in testloader:\n",
    "\n",
    "#         images.resize_(images.shape[0], 3*32*32)\n",
    "\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        output = model.forward(images)\n",
    "        test_loss += criterion(output, labels).item()\n",
    "\n",
    "        ps = torch.exp(output)\n",
    "        equality = (labels.data == ps.max(dim=1)[1])\n",
    "        accuracy += equality.type(torch.FloatTensor).mean()\n",
    "\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Train LeNet with uncoded Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.6077 \n",
      "Accuracy: 4660/10000 (46.60%)\n",
      "\n",
      "Round   0, Average loss 1.536\n",
      "\n",
      "Test set: Average loss: 1.1219 \n",
      "Accuracy: 5991/10000 (59.91%)\n",
      "\n",
      "Round   1, Average loss 1.306\n",
      "\n",
      "Test set: Average loss: 1.0416 \n",
      "Accuracy: 6298/10000 (62.98%)\n",
      "\n",
      "Round   2, Average loss 1.177\n",
      "\n",
      "Test set: Average loss: 0.9871 \n",
      "Accuracy: 6578/10000 (65.78%)\n",
      "\n",
      "Round   3, Average loss 1.107\n",
      "\n",
      "Test set: Average loss: 0.9544 \n",
      "Accuracy: 6708/10000 (67.08%)\n",
      "\n",
      "Round   4, Average loss 1.060\n",
      "\n",
      "Test set: Average loss: 0.9183 \n",
      "Accuracy: 6810/10000 (68.10%)\n",
      "\n",
      "Round   5, Average loss 1.026\n",
      "\n",
      "Test set: Average loss: 0.8934 \n",
      "Accuracy: 6858/10000 (68.58%)\n",
      "\n",
      "Round   6, Average loss 1.004\n",
      "\n",
      "Test set: Average loss: 0.8718 \n",
      "Accuracy: 6935/10000 (69.35%)\n",
      "\n",
      "Round   7, Average loss 0.984\n",
      "\n",
      "Test set: Average loss: 0.8556 \n",
      "Accuracy: 7036/10000 (70.36%)\n",
      "\n",
      "Round   8, Average loss 0.967\n",
      "\n",
      "Test set: Average loss: 0.8596 \n",
      "Accuracy: 6992/10000 (69.92%)\n",
      "\n",
      "Round   9, Average loss 0.955\n",
      "\n",
      "Test set: Average loss: 0.8473 \n",
      "Accuracy: 7060/10000 (70.60%)\n",
      "\n",
      "Round  10, Average loss 0.943\n",
      "\n",
      "Test set: Average loss: 0.8396 \n",
      "Accuracy: 7077/10000 (70.77%)\n",
      "\n",
      "Round  11, Average loss 0.935\n",
      "\n",
      "Test set: Average loss: 0.8384 \n",
      "Accuracy: 7103/10000 (71.03%)\n",
      "\n",
      "Round  12, Average loss 0.928\n",
      "\n",
      "Test set: Average loss: 0.8269 \n",
      "Accuracy: 7148/10000 (71.48%)\n",
      "\n",
      "Round  13, Average loss 0.918\n",
      "\n",
      "Test set: Average loss: 0.8194 \n",
      "Accuracy: 7157/10000 (71.57%)\n",
      "\n",
      "Round  14, Average loss 0.911\n",
      "\n",
      "Test set: Average loss: 0.8262 \n",
      "Accuracy: 7163/10000 (71.63%)\n",
      "\n",
      "Round  15, Average loss 0.906\n",
      "\n",
      "Test set: Average loss: 0.8236 \n",
      "Accuracy: 7155/10000 (71.55%)\n",
      "\n",
      "Round  16, Average loss 0.900\n",
      "\n",
      "Test set: Average loss: 0.8084 \n",
      "Accuracy: 7230/10000 (72.30%)\n",
      "\n",
      "Round  17, Average loss 0.895\n",
      "\n",
      "Test set: Average loss: 0.8071 \n",
      "Accuracy: 7226/10000 (72.26%)\n",
      "\n",
      "Round  18, Average loss 0.894\n",
      "\n",
      "Test set: Average loss: 0.8109 \n",
      "Accuracy: 7191/10000 (71.91%)\n",
      "\n",
      "Round  19, Average loss 0.889\n",
      "\n",
      "Test set: Average loss: 0.8043 \n",
      "Accuracy: 7201/10000 (72.01%)\n",
      "\n",
      "Round  20, Average loss 0.888\n",
      "\n",
      "Test set: Average loss: 0.7967 \n",
      "Accuracy: 7260/10000 (72.60%)\n",
      "\n",
      "Round  21, Average loss 0.882\n",
      "\n",
      "Test set: Average loss: 0.8021 \n",
      "Accuracy: 7246/10000 (72.46%)\n",
      "\n",
      "Round  22, Average loss 0.880\n",
      "\n",
      "Test set: Average loss: 0.8053 \n",
      "Accuracy: 7239/10000 (72.39%)\n",
      "\n",
      "Round  23, Average loss 0.877\n",
      "\n",
      "Test set: Average loss: 0.7952 \n",
      "Accuracy: 7258/10000 (72.58%)\n",
      "\n",
      "Round  24, Average loss 0.875\n",
      "\n",
      "Test set: Average loss: 0.7930 \n",
      "Accuracy: 7248/10000 (72.48%)\n",
      "\n",
      "Round  25, Average loss 0.871\n",
      "\n",
      "Test set: Average loss: 0.8164 \n",
      "Accuracy: 7186/10000 (71.86%)\n",
      "\n",
      "Round  26, Average loss 0.873\n",
      "\n",
      "Test set: Average loss: 0.7925 \n",
      "Accuracy: 7294/10000 (72.94%)\n",
      "\n",
      "Round  27, Average loss 0.869\n",
      "\n",
      "Test set: Average loss: 0.7964 \n",
      "Accuracy: 7254/10000 (72.54%)\n",
      "\n",
      "Round  28, Average loss 0.869\n",
      "\n",
      "Test set: Average loss: 0.7953 \n",
      "Accuracy: 7254/10000 (72.54%)\n",
      "\n",
      "Round  29, Average loss 0.863\n",
      "\n",
      "Test set: Average loss: 0.7964 \n",
      "Accuracy: 7285/10000 (72.85%)\n",
      "\n",
      "Round  30, Average loss 0.863\n",
      "\n",
      "Test set: Average loss: 0.7817 \n",
      "Accuracy: 7304/10000 (73.04%)\n",
      "\n",
      "Round  31, Average loss 0.862\n",
      "\n",
      "Test set: Average loss: 0.7905 \n",
      "Accuracy: 7283/10000 (72.83%)\n",
      "\n",
      "Round  32, Average loss 0.859\n",
      "\n",
      "Test set: Average loss: 0.7841 \n",
      "Accuracy: 7281/10000 (72.81%)\n",
      "\n",
      "Round  33, Average loss 0.857\n",
      "\n",
      "Test set: Average loss: 0.7830 \n",
      "Accuracy: 7270/10000 (72.70%)\n",
      "\n",
      "Round  34, Average loss 0.857\n",
      "\n",
      "Test set: Average loss: 0.7782 \n",
      "Accuracy: 7342/10000 (73.42%)\n",
      "\n",
      "Round  35, Average loss 0.856\n",
      "\n",
      "Test set: Average loss: 0.8032 \n",
      "Accuracy: 7238/10000 (72.38%)\n",
      "\n",
      "Round  36, Average loss 0.854\n",
      "\n",
      "Test set: Average loss: 0.7880 \n",
      "Accuracy: 7288/10000 (72.88%)\n",
      "\n",
      "Round  37, Average loss 0.853\n",
      "\n",
      "Test set: Average loss: 0.7767 \n",
      "Accuracy: 7336/10000 (73.36%)\n",
      "\n",
      "Round  38, Average loss 0.853\n",
      "\n",
      "Test set: Average loss: 0.7823 \n",
      "Accuracy: 7317/10000 (73.17%)\n",
      "\n",
      "Round  39, Average loss 0.852\n",
      "\n",
      "Test set: Average loss: 0.7928 \n",
      "Accuracy: 7298/10000 (72.98%)\n",
      "\n",
      "Round  40, Average loss 0.853\n",
      "\n",
      "Test set: Average loss: 0.7817 \n",
      "Accuracy: 7315/10000 (73.15%)\n",
      "\n",
      "Round  41, Average loss 0.850\n",
      "\n",
      "Test set: Average loss: 0.7798 \n",
      "Accuracy: 7329/10000 (73.29%)\n",
      "\n",
      "Round  42, Average loss 0.848\n",
      "\n",
      "Test set: Average loss: 0.7833 \n",
      "Accuracy: 7281/10000 (72.81%)\n",
      "\n",
      "Round  43, Average loss 0.846\n",
      "\n",
      "Test set: Average loss: 0.7826 \n",
      "Accuracy: 7300/10000 (73.00%)\n",
      "\n",
      "Round  44, Average loss 0.847\n",
      "\n",
      "Test set: Average loss: 0.7677 \n",
      "Accuracy: 7379/10000 (73.79%)\n",
      "\n",
      "Round  45, Average loss 0.845\n",
      "\n",
      "Test set: Average loss: 0.7761 \n",
      "Accuracy: 7307/10000 (73.07%)\n",
      "\n",
      "Round  46, Average loss 0.847\n",
      "\n",
      "Test set: Average loss: 0.7680 \n",
      "Accuracy: 7366/10000 (73.66%)\n",
      "\n",
      "Round  47, Average loss 0.845\n",
      "\n",
      "Test set: Average loss: 0.7713 \n",
      "Accuracy: 7341/10000 (73.41%)\n",
      "\n",
      "Round  48, Average loss 0.844\n",
      "\n",
      "Test set: Average loss: 0.7667 \n",
      "Accuracy: 7348/10000 (73.48%)\n",
      "\n",
      "Round  49, Average loss 0.842\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "N_epochs = 50\n",
    "\n",
    "net_glob = CNNCifar(args=args)\n",
    "net_glob.cuda()\n",
    "\n",
    "acc_test_FedAvg = np.empty(N_epochs)\n",
    "loss_test_FedAvg = np.empty(N_epochs)\n",
    "\n",
    "for iter in range(N_epochs):\n",
    "    w_locals, loss_locals = [], []\n",
    "\n",
    "    m = args.num_users\n",
    "    \n",
    "    idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "    \n",
    "    for idx in idxs_users:\n",
    "        local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])\n",
    "        w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))\n",
    "        w_locals.append(copy.deepcopy(w))\n",
    "        loss_locals.append(copy.deepcopy(loss))\n",
    "        \n",
    "    # update global weights\n",
    "    w_glob = FedAvg(w_locals)\n",
    "\n",
    "    # copy weight to net_glob\n",
    "    net_glob.load_state_dict(w_glob)\n",
    "    \n",
    "    acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "\n",
    "    # print loss\n",
    "    loss_avg = sum(loss_locals) / len(loss_locals)\n",
    "    print('Round {:3d}, Average loss {:.3f}'.format(iter, loss_avg))\n",
    "#     loss_train.append(loss_avg)\n",
    "    \n",
    "    acc_test_FedAvg[iter] = acc_test\n",
    "    loss_test_FedAvg[iter] = loss_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. BACC without grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1. K=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "class my_argument:    \n",
    "    epochs    = 16    #\"rounds of training\"\n",
    "    num_users = 2  # \"number of users: K\"\n",
    "    frac      = 0.5 #\"the fraction of clients: C\"\n",
    "    local_ep  = 5 #\"the number of local epochs: E\"\n",
    "    local_bs  = 50  #\"local batch size: B\"\n",
    "    bs        = 50 #\"test batch size\"\n",
    "    lr        = 0.001 #\"learning rate\"\n",
    "    momentum  = 0.5 # \"SGD momentum (default: 0.5)\"\n",
    "    weight_decay = 5e-4\n",
    "    split     = 'user' # \"train-test split type, user or sample\"\n",
    "    opt='ADAM'\n",
    "    loss='Custom' # 'Custom' or 'Default'\n",
    "\n",
    "    # model arguments\n",
    "    model = 'cnn'\n",
    "    kernel_num=9 #, help='number of each kind of kernel')\n",
    "    kernel_sizes='3,4,5' #  help='comma-separated kernel size to use for convolution')\n",
    "    norm='batch_norm' #, help=\"batch_norm, layer_norm, or None\")\n",
    "    num_filters=32 #, help=\"number of filters for conv nets\")\n",
    "    max_pool='True' #help=\"Whether use max pooling rather than strided convolutions\")\n",
    "\n",
    "    # other arguments\n",
    "    dataset='cifar' #, help=\"name of dataset\")\n",
    "    iid=1\n",
    "    num_classes=10#, help=\"number of classes\")\n",
    "    num_channels=1#, help=\"number of channels of imges\")\n",
    "    gpu=1#, help=\"GPU ID, -1 for CPU\")\n",
    "    stopping_rounds=10#, help='rounds of early stopping')\n",
    "    verbose='False'#, help='verbose print')\n",
    "    seed=1#, help='random seed (default: 1)')\n",
    "    \n",
    "args = my_argument()\n",
    "\n",
    "args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)\n",
    "args.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(args.device)\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Normalize the test set same as training set without augmentation\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# trainset = torchvision.datasets.CIFAR10(\n",
    "#     root=\"./data/cifar\", train=True, download=True, transform=transform_train)\n",
    "# trainloader = torch.utils.data.DataLoader(\n",
    "#     trainset, batch_size=args.bs, shuffle=True, num_workers=2)\n",
    "\n",
    "# testset = torchvision.datasets.CIFAR10(\n",
    "#     root=\"./data/cifar\", train=False, download=True, transform=transform_test)\n",
    "# testloader = torch.utils.data.DataLoader(\n",
    "#     testset, batch_size=args.bs, shuffle=False, num_workers=2)\n",
    "\n",
    "dataset_train = datasets.CIFAR10('../data/cifar', train=True, download=True, transform=transform_train)\n",
    "dataset_test = datasets.CIFAR10('../data/cifar', train=False, download=True, transform=transform_test)\n",
    "if args.iid:\n",
    "    dict_users = cifar_iid(dataset_train, args.num_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of X: (50000, 3072)\n",
      "size of Y: (50000, 10)\n",
      "(50000, 10)\n",
      "2.0725500337056313\n"
     ]
    }
   ],
   "source": [
    "encoding_input_array_np = np.empty((len(dataset_train),32*32*3))\n",
    "encoding_label_array_np = np.empty((len(dataset_train),args.num_classes))\n",
    "print(\"size of X:\" ,encoding_input_array_np.shape)\n",
    "print(\"size of Y:\" ,encoding_label_array_np.shape)\n",
    "\n",
    "Size_submatrices = int(50000/args.num_users)\n",
    "\n",
    "\n",
    "for i in range(args.num_users):\n",
    "    \n",
    "    stt_pos = i*Size_submatrices\n",
    "    end_pos = (i+1)*Size_submatrices\n",
    "#     print(i,stt_pos,end_pos)\n",
    "    Temp_train = DataLoader(DatasetSplit(dataset_train, dict_users[i]), batch_size=Size_submatrices, shuffle=True)\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(Temp_train):\n",
    "        \n",
    "        images_np = images.detach().cpu().numpy()\n",
    "        \n",
    "#         print(np.size(images_np))\n",
    "        encoding_input_array_np[stt_pos:end_pos,:] = np.reshape(images_np, (Size_submatrices,32*32*3))\n",
    "#         print(encoding_input_array_np[stt_pos:end_pos,:].shape)\n",
    "\n",
    "        onehot_labels = torch.nn.functional.one_hot(labels,num_classes=args.num_classes)\n",
    "        labels_np = onehot_labels.detach().cpu().numpy()\n",
    "#         print(labels_np.shape)\n",
    "        encoding_label_array_np[stt_pos:end_pos,:] = labels_np\n",
    "    \n",
    "print(np.shape(encoding_label_array_np))\n",
    "\n",
    "avg_power = np.sum(encoding_input_array_np*encoding_input_array_np, axis=1)/np.shape(encoding_input_array_np)[1]\n",
    "avg_power = np.sum(avg_power)/np.shape(encoding_input_array_np)[0]\n",
    "print(avg_power)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K=2, T=3, w/o inPowerAlignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "sigma = 0.1\n",
      "\n",
      "\n",
      "\n",
      "Learning Rate = 0.001\n",
      "\n",
      "\n",
      "\n",
      "z_array: [-0.81  0.81]\n",
      "2.3175440334168256\n",
      "2.317544033416827\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 25000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 25000 \n",
      "\n",
      "(T, sigma)= 3 0.1 )  0 -th Trial!!\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.9010 \n",
      "Accuracy: 3314/10000 (33.14%)\n",
      "\n",
      "Round   0, Average loss 1.901 Test accuracy 33.140\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6722 \n",
      "Accuracy: 4233/10000 (42.33%)\n",
      "\n",
      "Round   1, Average loss 1.672 Test accuracy 42.330\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6410 \n",
      "Accuracy: 4415/10000 (44.15%)\n",
      "\n",
      "Round   2, Average loss 1.641 Test accuracy 44.150\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6058 \n",
      "Accuracy: 4589/10000 (45.89%)\n",
      "\n",
      "Round   3, Average loss 1.606 Test accuracy 45.890\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5672 \n",
      "Accuracy: 4804/10000 (48.04%)\n",
      "\n",
      "Round   4, Average loss 1.567 Test accuracy 48.040\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5435 \n",
      "Accuracy: 4888/10000 (48.88%)\n",
      "\n",
      "Round   5, Average loss 1.544 Test accuracy 48.880\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5327 \n",
      "Accuracy: 4956/10000 (49.56%)\n",
      "\n",
      "Round   6, Average loss 1.533 Test accuracy 49.560\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5446 \n",
      "Accuracy: 4880/10000 (48.80%)\n",
      "\n",
      "Round   7, Average loss 1.545 Test accuracy 48.800\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5214 \n",
      "Accuracy: 5003/10000 (50.03%)\n",
      "\n",
      "Round   8, Average loss 1.521 Test accuracy 50.030\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4940 \n",
      "Accuracy: 5123/10000 (51.23%)\n",
      "\n",
      "Round   9, Average loss 1.494 Test accuracy 51.230\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5121 \n",
      "Accuracy: 5011/10000 (50.11%)\n",
      "\n",
      "Round  10, Average loss 1.512 Test accuracy 50.110\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4970 \n",
      "Accuracy: 5113/10000 (51.13%)\n",
      "\n",
      "Round  11, Average loss 1.497 Test accuracy 51.130\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5058 \n",
      "Accuracy: 5072/10000 (50.72%)\n",
      "\n",
      "Round  12, Average loss 1.506 Test accuracy 50.720\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5040 \n",
      "Accuracy: 5083/10000 (50.83%)\n",
      "\n",
      "Round  13, Average loss 1.504 Test accuracy 50.830\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4768 \n",
      "Accuracy: 5190/10000 (51.90%)\n",
      "\n",
      "Round  14, Average loss 1.477 Test accuracy 51.900\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4711 \n",
      "Accuracy: 5235/10000 (52.35%)\n",
      "\n",
      "Round  15, Average loss 1.471 Test accuracy 52.350\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4828 \n",
      "Accuracy: 5211/10000 (52.11%)\n",
      "\n",
      "Round  16, Average loss 1.483 Test accuracy 52.110\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4701 \n",
      "Accuracy: 5279/10000 (52.79%)\n",
      "\n",
      "Round  17, Average loss 1.470 Test accuracy 52.790\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5081 \n",
      "Accuracy: 5138/10000 (51.38%)\n",
      "\n",
      "Round  18, Average loss 1.508 Test accuracy 51.380\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4857 \n",
      "Accuracy: 5225/10000 (52.25%)\n",
      "\n",
      "Round  19, Average loss 1.486 Test accuracy 52.250\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4920 \n",
      "Accuracy: 5179/10000 (51.79%)\n",
      "\n",
      "Round  20, Average loss 1.492 Test accuracy 51.790\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4678 \n",
      "Accuracy: 5289/10000 (52.89%)\n",
      "\n",
      "Round  21, Average loss 1.468 Test accuracy 52.890\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4755 \n",
      "Accuracy: 5237/10000 (52.37%)\n",
      "\n",
      "Round  22, Average loss 1.475 Test accuracy 52.370\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4847 \n",
      "Accuracy: 5224/10000 (52.24%)\n",
      "\n",
      "Round  23, Average loss 1.485 Test accuracy 52.240\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4723 \n",
      "Accuracy: 5240/10000 (52.40%)\n",
      "\n",
      "Round  24, Average loss 1.472 Test accuracy 52.400\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4638 \n",
      "Accuracy: 5298/10000 (52.98%)\n",
      "\n",
      "Round  25, Average loss 1.464 Test accuracy 52.980\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4719 \n",
      "Accuracy: 5290/10000 (52.90%)\n",
      "\n",
      "Round  26, Average loss 1.472 Test accuracy 52.900\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4680 \n",
      "Accuracy: 5288/10000 (52.88%)\n",
      "\n",
      "Round  27, Average loss 1.468 Test accuracy 52.880\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4757 \n",
      "Accuracy: 5241/10000 (52.41%)\n",
      "\n",
      "Round  28, Average loss 1.476 Test accuracy 52.410\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4680 \n",
      "Accuracy: 5289/10000 (52.89%)\n",
      "\n",
      "Round  29, Average loss 1.468 Test accuracy 52.890\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4719 \n",
      "Accuracy: 5313/10000 (53.13%)\n",
      "\n",
      "Round  30, Average loss 1.472 Test accuracy 53.130\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4785 \n",
      "Accuracy: 5265/10000 (52.65%)\n",
      "\n",
      "Round  31, Average loss 1.479 Test accuracy 52.650\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4901 \n",
      "Accuracy: 5220/10000 (52.20%)\n",
      "\n",
      "Round  32, Average loss 1.490 Test accuracy 52.200\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4883 \n",
      "Accuracy: 5242/10000 (52.42%)\n",
      "\n",
      "Round  33, Average loss 1.488 Test accuracy 52.420\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4893 \n",
      "Accuracy: 5224/10000 (52.24%)\n",
      "\n",
      "Round  34, Average loss 1.489 Test accuracy 52.240\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4981 \n",
      "Accuracy: 5212/10000 (52.12%)\n",
      "\n",
      "Round  35, Average loss 1.498 Test accuracy 52.120\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4841 \n",
      "Accuracy: 5269/10000 (52.69%)\n",
      "\n",
      "Round  36, Average loss 1.484 Test accuracy 52.690\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4881 \n",
      "Accuracy: 5217/10000 (52.17%)\n",
      "\n",
      "Round  37, Average loss 1.488 Test accuracy 52.170\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4940 \n",
      "Accuracy: 5186/10000 (51.86%)\n",
      "\n",
      "Round  38, Average loss 1.494 Test accuracy 51.860\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4839 \n",
      "Accuracy: 5252/10000 (52.52%)\n",
      "\n",
      "Round  39, Average loss 1.484 Test accuracy 52.520\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4907 \n",
      "Accuracy: 5210/10000 (52.10%)\n",
      "\n",
      "Round  40, Average loss 1.491 Test accuracy 52.100\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4825 \n",
      "Accuracy: 5236/10000 (52.36%)\n",
      "\n",
      "Round  41, Average loss 1.482 Test accuracy 52.360\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4892 \n",
      "Accuracy: 5181/10000 (51.81%)\n",
      "\n",
      "Round  42, Average loss 1.489 Test accuracy 51.810\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4831 \n",
      "Accuracy: 5221/10000 (52.21%)\n",
      "\n",
      "Round  43, Average loss 1.483 Test accuracy 52.210\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4791 \n",
      "Accuracy: 5247/10000 (52.47%)\n",
      "\n",
      "Round  44, Average loss 1.479 Test accuracy 52.470\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4731 \n",
      "Accuracy: 5267/10000 (52.67%)\n",
      "\n",
      "Round  45, Average loss 1.473 Test accuracy 52.670\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4785 \n",
      "Accuracy: 5234/10000 (52.34%)\n",
      "\n",
      "Round  46, Average loss 1.479 Test accuracy 52.340\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4808 \n",
      "Accuracy: 5225/10000 (52.25%)\n",
      "\n",
      "Round  47, Average loss 1.481 Test accuracy 52.250\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4691 \n",
      "Accuracy: 5284/10000 (52.84%)\n",
      "\n",
      "Round  48, Average loss 1.469 Test accuracy 52.840\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4713 \n",
      "Accuracy: 5281/10000 (52.81%)\n",
      "\n",
      "Round  49, Average loss 1.471 Test accuracy 52.810\n",
      "\n",
      "\n",
      "\n",
      "sigma = 0.3\n",
      "\n",
      "\n",
      "\n",
      "Learning Rate = 0.001\n",
      "\n",
      "\n",
      "\n",
      "z_array: [-0.81  0.81]\n",
      "1.2192623088642704\n",
      "1.2192623088642716\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 25000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 25000 \n",
      "\n",
      "(T, sigma)= 3 0.3 )  0 -th Trial!!\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.9684 \n",
      "Accuracy: 3519/10000 (35.19%)\n",
      "\n",
      "Round   0, Average loss 1.968 Test accuracy 35.190\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6557 \n",
      "Accuracy: 4255/10000 (42.55%)\n",
      "\n",
      "Round   1, Average loss 1.656 Test accuracy 42.550\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5400 \n",
      "Accuracy: 4811/10000 (48.11%)\n",
      "\n",
      "Round   2, Average loss 1.540 Test accuracy 48.110\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4900 \n",
      "Accuracy: 5091/10000 (50.91%)\n",
      "\n",
      "Round   3, Average loss 1.490 Test accuracy 50.910\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4771 \n",
      "Accuracy: 5182/10000 (51.82%)\n",
      "\n",
      "Round   4, Average loss 1.477 Test accuracy 51.820\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4683 \n",
      "Accuracy: 5263/10000 (52.63%)\n",
      "\n",
      "Round   5, Average loss 1.468 Test accuracy 52.630\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4772 \n",
      "Accuracy: 5201/10000 (52.01%)\n",
      "\n",
      "Round   6, Average loss 1.477 Test accuracy 52.010\n",
      "selected users: [0 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.4908 \n",
      "Accuracy: 5145/10000 (51.45%)\n",
      "\n",
      "Round   7, Average loss 1.491 Test accuracy 51.450\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4913 \n",
      "Accuracy: 5146/10000 (51.46%)\n",
      "\n",
      "Round   8, Average loss 1.491 Test accuracy 51.460\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5006 \n",
      "Accuracy: 5148/10000 (51.48%)\n",
      "\n",
      "Round   9, Average loss 1.501 Test accuracy 51.480\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4943 \n",
      "Accuracy: 5144/10000 (51.44%)\n",
      "\n",
      "Round  10, Average loss 1.494 Test accuracy 51.440\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4802 \n",
      "Accuracy: 5204/10000 (52.04%)\n",
      "\n",
      "Round  11, Average loss 1.480 Test accuracy 52.040\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4951 \n",
      "Accuracy: 5153/10000 (51.53%)\n",
      "\n",
      "Round  12, Average loss 1.495 Test accuracy 51.530\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5064 \n",
      "Accuracy: 5099/10000 (50.99%)\n",
      "\n",
      "Round  13, Average loss 1.506 Test accuracy 50.990\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5009 \n",
      "Accuracy: 5091/10000 (50.91%)\n",
      "\n",
      "Round  14, Average loss 1.501 Test accuracy 50.910\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5079 \n",
      "Accuracy: 5069/10000 (50.69%)\n",
      "\n",
      "Round  15, Average loss 1.508 Test accuracy 50.690\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4861 \n",
      "Accuracy: 5136/10000 (51.36%)\n",
      "\n",
      "Round  16, Average loss 1.486 Test accuracy 51.360\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4785 \n",
      "Accuracy: 5180/10000 (51.80%)\n",
      "\n",
      "Round  17, Average loss 1.478 Test accuracy 51.800\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5081 \n",
      "Accuracy: 5072/10000 (50.72%)\n",
      "\n",
      "Round  18, Average loss 1.508 Test accuracy 50.720\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5008 \n",
      "Accuracy: 5078/10000 (50.78%)\n",
      "\n",
      "Round  19, Average loss 1.501 Test accuracy 50.780\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4873 \n",
      "Accuracy: 5150/10000 (51.50%)\n",
      "\n",
      "Round  20, Average loss 1.487 Test accuracy 51.500\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4897 \n",
      "Accuracy: 5148/10000 (51.48%)\n",
      "\n",
      "Round  21, Average loss 1.490 Test accuracy 51.480\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4886 \n",
      "Accuracy: 5176/10000 (51.76%)\n",
      "\n",
      "Round  22, Average loss 1.489 Test accuracy 51.760\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4959 \n",
      "Accuracy: 5154/10000 (51.54%)\n",
      "\n",
      "Round  23, Average loss 1.496 Test accuracy 51.540\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4917 \n",
      "Accuracy: 5165/10000 (51.65%)\n",
      "\n",
      "Round  24, Average loss 1.492 Test accuracy 51.650\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4813 \n",
      "Accuracy: 5236/10000 (52.36%)\n",
      "\n",
      "Round  25, Average loss 1.481 Test accuracy 52.360\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4953 \n",
      "Accuracy: 5160/10000 (51.60%)\n",
      "\n",
      "Round  26, Average loss 1.495 Test accuracy 51.600\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4946 \n",
      "Accuracy: 5169/10000 (51.69%)\n",
      "\n",
      "Round  27, Average loss 1.495 Test accuracy 51.690\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4987 \n",
      "Accuracy: 5146/10000 (51.46%)\n",
      "\n",
      "Round  28, Average loss 1.499 Test accuracy 51.460\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5089 \n",
      "Accuracy: 5139/10000 (51.39%)\n",
      "\n",
      "Round  29, Average loss 1.509 Test accuracy 51.390\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4866 \n",
      "Accuracy: 5241/10000 (52.41%)\n",
      "\n",
      "Round  30, Average loss 1.487 Test accuracy 52.410\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5043 \n",
      "Accuracy: 5145/10000 (51.45%)\n",
      "\n",
      "Round  31, Average loss 1.504 Test accuracy 51.450\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5056 \n",
      "Accuracy: 5171/10000 (51.71%)\n",
      "\n",
      "Round  32, Average loss 1.506 Test accuracy 51.710\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4998 \n",
      "Accuracy: 5193/10000 (51.93%)\n",
      "\n",
      "Round  33, Average loss 1.500 Test accuracy 51.930\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5017 \n",
      "Accuracy: 5162/10000 (51.62%)\n",
      "\n",
      "Round  34, Average loss 1.502 Test accuracy 51.620\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4920 \n",
      "Accuracy: 5208/10000 (52.08%)\n",
      "\n",
      "Round  35, Average loss 1.492 Test accuracy 52.080\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4785 \n",
      "Accuracy: 5225/10000 (52.25%)\n",
      "\n",
      "Round  36, Average loss 1.478 Test accuracy 52.250\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4708 \n",
      "Accuracy: 5251/10000 (52.51%)\n",
      "\n",
      "Round  37, Average loss 1.471 Test accuracy 52.510\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4700 \n",
      "Accuracy: 5275/10000 (52.75%)\n",
      "\n",
      "Round  38, Average loss 1.470 Test accuracy 52.750\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4623 \n",
      "Accuracy: 5318/10000 (53.18%)\n",
      "\n",
      "Round  39, Average loss 1.462 Test accuracy 53.180\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4888 \n",
      "Accuracy: 5167/10000 (51.67%)\n",
      "\n",
      "Round  40, Average loss 1.489 Test accuracy 51.670\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4809 \n",
      "Accuracy: 5205/10000 (52.05%)\n",
      "\n",
      "Round  41, Average loss 1.481 Test accuracy 52.050\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4773 \n",
      "Accuracy: 5182/10000 (51.82%)\n",
      "\n",
      "Round  42, Average loss 1.477 Test accuracy 51.820\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4723 \n",
      "Accuracy: 5217/10000 (52.17%)\n",
      "\n",
      "Round  43, Average loss 1.472 Test accuracy 52.170\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4847 \n",
      "Accuracy: 5231/10000 (52.31%)\n",
      "\n",
      "Round  44, Average loss 1.485 Test accuracy 52.310\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4831 \n",
      "Accuracy: 5221/10000 (52.21%)\n",
      "\n",
      "Round  45, Average loss 1.483 Test accuracy 52.210\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4868 \n",
      "Accuracy: 5199/10000 (51.99%)\n",
      "\n",
      "Round  46, Average loss 1.487 Test accuracy 51.990\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4937 \n",
      "Accuracy: 5163/10000 (51.63%)\n",
      "\n",
      "Round  47, Average loss 1.494 Test accuracy 51.630\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4776 \n",
      "Accuracy: 5240/10000 (52.40%)\n",
      "\n",
      "Round  48, Average loss 1.478 Test accuracy 52.400\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4731 \n",
      "Accuracy: 5271/10000 (52.71%)\n",
      "\n",
      "Round  49, Average loss 1.473 Test accuracy 52.710\n",
      "\n",
      "\n",
      "\n",
      "sigma = 1\n",
      "\n",
      "\n",
      "\n",
      "Learning Rate = 0.001\n",
      "\n",
      "\n",
      "\n",
      "z_array: [-0.81  0.81]\n",
      "0.4838626198316927\n",
      "0.48386261983169315\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 25000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 25000 \n",
      "\n",
      "(T, sigma)= 3 1 )  0 -th Trial!!\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.9067 \n",
      "Accuracy: 3340/10000 (33.40%)\n",
      "\n",
      "Round   0, Average loss 1.907 Test accuracy 33.400\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7118 \n",
      "Accuracy: 3992/10000 (39.92%)\n",
      "\n",
      "Round   1, Average loss 1.712 Test accuracy 39.920\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6793 \n",
      "Accuracy: 4176/10000 (41.76%)\n",
      "\n",
      "Round   2, Average loss 1.679 Test accuracy 41.760\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6818 \n",
      "Accuracy: 4187/10000 (41.87%)\n",
      "\n",
      "Round   3, Average loss 1.682 Test accuracy 41.870\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6760 \n",
      "Accuracy: 4250/10000 (42.50%)\n",
      "\n",
      "Round   4, Average loss 1.676 Test accuracy 42.500\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6646 \n",
      "Accuracy: 4319/10000 (43.19%)\n",
      "\n",
      "Round   5, Average loss 1.665 Test accuracy 43.190\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6652 \n",
      "Accuracy: 4347/10000 (43.47%)\n",
      "\n",
      "Round   6, Average loss 1.665 Test accuracy 43.470\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6594 \n",
      "Accuracy: 4361/10000 (43.61%)\n",
      "\n",
      "Round   7, Average loss 1.659 Test accuracy 43.610\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6570 \n",
      "Accuracy: 4394/10000 (43.94%)\n",
      "\n",
      "Round   8, Average loss 1.657 Test accuracy 43.940\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6513 \n",
      "Accuracy: 4364/10000 (43.64%)\n",
      "\n",
      "Round   9, Average loss 1.651 Test accuracy 43.640\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6576 \n",
      "Accuracy: 4399/10000 (43.99%)\n",
      "\n",
      "Round  10, Average loss 1.658 Test accuracy 43.990\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6475 \n",
      "Accuracy: 4484/10000 (44.84%)\n",
      "\n",
      "Round  11, Average loss 1.648 Test accuracy 44.840\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6653 \n",
      "Accuracy: 4329/10000 (43.29%)\n",
      "\n",
      "Round  12, Average loss 1.665 Test accuracy 43.290\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6579 \n",
      "Accuracy: 4357/10000 (43.57%)\n",
      "\n",
      "Round  13, Average loss 1.658 Test accuracy 43.570\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6532 \n",
      "Accuracy: 4390/10000 (43.90%)\n",
      "\n",
      "Round  14, Average loss 1.653 Test accuracy 43.900\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6558 \n",
      "Accuracy: 4382/10000 (43.82%)\n",
      "\n",
      "Round  15, Average loss 1.656 Test accuracy 43.820\n",
      "selected users: [0 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.6358 \n",
      "Accuracy: 4464/10000 (44.64%)\n",
      "\n",
      "Round  16, Average loss 1.636 Test accuracy 44.640\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6550 \n",
      "Accuracy: 4392/10000 (43.92%)\n",
      "\n",
      "Round  17, Average loss 1.655 Test accuracy 43.920\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6443 \n",
      "Accuracy: 4409/10000 (44.09%)\n",
      "\n",
      "Round  18, Average loss 1.644 Test accuracy 44.090\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6348 \n",
      "Accuracy: 4484/10000 (44.84%)\n",
      "\n",
      "Round  19, Average loss 1.635 Test accuracy 44.840\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6350 \n",
      "Accuracy: 4469/10000 (44.69%)\n",
      "\n",
      "Round  20, Average loss 1.635 Test accuracy 44.690\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6432 \n",
      "Accuracy: 4413/10000 (44.13%)\n",
      "\n",
      "Round  21, Average loss 1.643 Test accuracy 44.130\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6476 \n",
      "Accuracy: 4394/10000 (43.94%)\n",
      "\n",
      "Round  22, Average loss 1.648 Test accuracy 43.940\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6357 \n",
      "Accuracy: 4475/10000 (44.75%)\n",
      "\n",
      "Round  23, Average loss 1.636 Test accuracy 44.750\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6492 \n",
      "Accuracy: 4399/10000 (43.99%)\n",
      "\n",
      "Round  24, Average loss 1.649 Test accuracy 43.990\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6352 \n",
      "Accuracy: 4493/10000 (44.93%)\n",
      "\n",
      "Round  25, Average loss 1.635 Test accuracy 44.930\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6387 \n",
      "Accuracy: 4465/10000 (44.65%)\n",
      "\n",
      "Round  26, Average loss 1.639 Test accuracy 44.650\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6439 \n",
      "Accuracy: 4463/10000 (44.63%)\n",
      "\n",
      "Round  27, Average loss 1.644 Test accuracy 44.630\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6268 \n",
      "Accuracy: 4528/10000 (45.28%)\n",
      "\n",
      "Round  28, Average loss 1.627 Test accuracy 45.280\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6288 \n",
      "Accuracy: 4478/10000 (44.78%)\n",
      "\n",
      "Round  29, Average loss 1.629 Test accuracy 44.780\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6319 \n",
      "Accuracy: 4479/10000 (44.79%)\n",
      "\n",
      "Round  30, Average loss 1.632 Test accuracy 44.790\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6259 \n",
      "Accuracy: 4500/10000 (45.00%)\n",
      "\n",
      "Round  31, Average loss 1.626 Test accuracy 45.000\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6266 \n",
      "Accuracy: 4529/10000 (45.29%)\n",
      "\n",
      "Round  32, Average loss 1.627 Test accuracy 45.290\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6281 \n",
      "Accuracy: 4498/10000 (44.98%)\n",
      "\n",
      "Round  33, Average loss 1.628 Test accuracy 44.980\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6241 \n",
      "Accuracy: 4525/10000 (45.25%)\n",
      "\n",
      "Round  34, Average loss 1.624 Test accuracy 45.250\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6324 \n",
      "Accuracy: 4480/10000 (44.80%)\n",
      "\n",
      "Round  35, Average loss 1.632 Test accuracy 44.800\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6261 \n",
      "Accuracy: 4508/10000 (45.08%)\n",
      "\n",
      "Round  36, Average loss 1.626 Test accuracy 45.080\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6218 \n",
      "Accuracy: 4558/10000 (45.58%)\n",
      "\n",
      "Round  37, Average loss 1.622 Test accuracy 45.580\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6274 \n",
      "Accuracy: 4537/10000 (45.37%)\n",
      "\n",
      "Round  38, Average loss 1.627 Test accuracy 45.370\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6292 \n",
      "Accuracy: 4514/10000 (45.14%)\n",
      "\n",
      "Round  39, Average loss 1.629 Test accuracy 45.140\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6232 \n",
      "Accuracy: 4563/10000 (45.63%)\n",
      "\n",
      "Round  40, Average loss 1.623 Test accuracy 45.630\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6234 \n",
      "Accuracy: 4570/10000 (45.70%)\n",
      "\n",
      "Round  41, Average loss 1.623 Test accuracy 45.700\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6339 \n",
      "Accuracy: 4538/10000 (45.38%)\n",
      "\n",
      "Round  42, Average loss 1.634 Test accuracy 45.380\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6397 \n",
      "Accuracy: 4522/10000 (45.22%)\n",
      "\n",
      "Round  43, Average loss 1.640 Test accuracy 45.220\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6288 \n",
      "Accuracy: 4518/10000 (45.18%)\n",
      "\n",
      "Round  44, Average loss 1.629 Test accuracy 45.180\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6395 \n",
      "Accuracy: 4492/10000 (44.92%)\n",
      "\n",
      "Round  45, Average loss 1.640 Test accuracy 44.920\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6396 \n",
      "Accuracy: 4498/10000 (44.98%)\n",
      "\n",
      "Round  46, Average loss 1.640 Test accuracy 44.980\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6251 \n",
      "Accuracy: 4555/10000 (45.55%)\n",
      "\n",
      "Round  47, Average loss 1.625 Test accuracy 45.550\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6248 \n",
      "Accuracy: 4542/10000 (45.42%)\n",
      "\n",
      "Round  48, Average loss 1.625 Test accuracy 45.420\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6148 \n",
      "Accuracy: 4600/10000 (46.00%)\n",
      "\n",
      "Round  49, Average loss 1.615 Test accuracy 46.000\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "K = 2\n",
    "T = 3\n",
    "sigma = 0.1\n",
    "Noise_Alloc = [0,2,4]\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "\n",
    "N_array = [2]\n",
    "B_array = [0.5]\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 50\n",
    "\n",
    "lr_array = [0.001] # 0.001 is the bset\n",
    "\n",
    "sigma_array = [0.1, 0.3, 1]\n",
    "# lr_array = [0.1, 0.01, 0.005, 0.001,0.0005]\n",
    "\n",
    "\n",
    "loss_test_arr_K2_G1_v3 = np.zeros((len(sigma_array),len(lr_array),N_trials,N_epochs))\n",
    "acc_test_arr_K2_G1_v3  = np.zeros((len(sigma_array),len(lr_array),N_trials,N_epochs))\n",
    "\n",
    "N = 2\n",
    "for sigma_idx in range(len(sigma_array)):\n",
    "    \n",
    "    sigma = sigma_array[sigma_idx]\n",
    "    \n",
    "    print('\\n\\n')\n",
    "    print('sigma =',sigma)\n",
    "    print('\\n\\n')\n",
    "           \n",
    "        \n",
    "    # print(\"alpha_array: \",alpha_array,'\\n')\n",
    "    \n",
    "    \n",
    "    for lr_idx in range(len(lr_array)):\n",
    "        \n",
    "        args.lr = lr_array[lr_idx]\n",
    "        \n",
    "        print('Learning Rate =',args.lr)\n",
    "        print('\\n\\n')\n",
    "#         while(len(z_array)<N):\n",
    "#             z_tmp = np.random.uniform(-1,1,1)\n",
    "#             MIS_tmp = MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_tmp], 1,sigma)\n",
    "#             if MIS_tmp < B and MIS_tmp > 0.1:\n",
    "#                 z_array.append(z_tmp[0])\n",
    "#         \n",
    "#         z_array = np.sort(z_array)\n",
    "#         print(N_idx,'!!!')\n",
    "#         if N_idx==0:\n",
    "# #             z_array = np.array([-0.94,-0.534,0.534, 0.94])\n",
    "# #         elif N_idx==1:\n",
    "# #             z_array = np.array([-0.94, -0.73, 0.73, 0.94])\n",
    "#         elif N_idx==1:\n",
    "#             z_array = np.array([-0.94, -0.125, 0.125, 0.94])\n",
    "#         else:\n",
    "# #             z_array = np.array([-0.94, -0.73, -0.534, -0.125, 0.125, 0.534, 0.73, 0.94])\n",
    "# #             z_array = np.array([-0.9, -0.81, -0.22, -0.20, 0.20, 0.22, 0.81, 0.9])\n",
    "        \n",
    "        z_array = np.array([-0.81, 0.81])\n",
    "        \n",
    "        print('z_array:',z_array)\n",
    "        if sigma != 0:\n",
    "            for j in range(len(z_array)):\n",
    "                print(MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_array[j]], 1,sigma))\n",
    "\n",
    "        \n",
    "        _Noise_label = np.ones((25000*T,10)) * 0.1\n",
    "\n",
    "        X_tilde,a,b = BACC_Enc_Data_v3(encoding_input_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde,a,b = BACC_Enc_Data_v3(encoding_label_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "\n",
    "        m = N # m is the number of received result @ master\n",
    "    #     print('number of results:',m)\n",
    "\n",
    "        for trial_idx in range(N_trials):\n",
    "            print('(T, sigma)=',T,sigma,') ',trial_idx,'-th Trial!!')\n",
    "\n",
    "            net_glob = CNNCifar(args=args)\n",
    "            net_glob.cuda()\n",
    "            net_glob.train()\n",
    "\n",
    "            # copy weights\n",
    "            w_glob = net_glob.state_dict()\n",
    "\n",
    "            for iter in range(N_epochs): #args.epochs\n",
    "                w_locals, loss_locals = [], []\n",
    "                idxs_users = np.random.choice(range(N), m, replace=False)\n",
    "                idxs_users = np.sort(idxs_users)\n",
    "                print('selected users:',idxs_users)\n",
    "                \n",
    "                coded_net = BACC_Enc_Model_withNoise_v4(net_glob.cuda(), N, K, T, sigma, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "                dec_z_array = []\n",
    "                for idx in idxs_users: #for idx in range(N):\n",
    "            #         print(idx)\n",
    "                    local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[idx,:,:], label=y_tilde[idx,:,:])\n",
    "                    w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "#                     w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                    w_locals.append(copy.deepcopy(w))\n",
    "                    loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                    dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "                # update global weights\n",
    "                #w_glob = FedAvg(w_locals)\n",
    "                w_glob = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "\n",
    "\n",
    "                # copy weight to net_glob\n",
    "                net_glob.load_state_dict(w_glob)\n",
    "\n",
    "                # print loss\n",
    "            #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "            #     loss_train_arr.append(loss_train)\n",
    "\n",
    "                acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "                acc_test_arr_K2_G1_v3[sigma_idx][lr_idx][trial_idx][iter] = acc_test\n",
    "                loss_test_arr_K2_G1_v3[sigma_idx][lr_idx][trial_idx][iter] = loss_test\n",
    "                \n",
    "#                 PATH = \"./save_models/CIFAR10_LeNet_BACC_woPowerAlign_K2_sigma_E50_iter\"+str(iter)+\".pt\"\n",
    "#                 torch.save(net_glob.state_dict(), PATH)\n",
    "                \n",
    "                if iter % 1 ==0:\n",
    "                    print('Round {:3d}, Average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_test, acc_test))\n",
    "                #print(loss_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K=2, T=3, w/ inPowerAlignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "sigma = 0.1\n",
      "\n",
      "\n",
      "\n",
      "Learning Rate = 0.001\n",
      "\n",
      "\n",
      "\n",
      "z_array: [-0.81  0.81]\n",
      "2.3175440334168256\n",
      "2.317544033416827\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 25000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 25000 \n",
      "\n",
      "Adjust the power of X_tilde\n",
      "0 0.41072267777558713\n",
      "power after adjusting = 2.0725500337056313\n",
      "1 0.41129426227703575\n",
      "power after adjusting = 2.0725500337056317\n",
      "\n",
      "(T, sigma)= 3 0.1 )  0 -th Trial!!\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.1024 \n",
      "Accuracy: 3673/10000 (36.73%)\n",
      "\n",
      "Round   0, Average loss 2.102 Test accuracy 36.730\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6246 \n",
      "Accuracy: 5641/10000 (56.41%)\n",
      "\n",
      "Round   1, Average loss 1.625 Test accuracy 56.410\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5448 \n",
      "Accuracy: 5992/10000 (59.92%)\n",
      "\n",
      "Round   2, Average loss 1.545 Test accuracy 59.920\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5263 \n",
      "Accuracy: 6083/10000 (60.83%)\n",
      "\n",
      "Round   3, Average loss 1.526 Test accuracy 60.830\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5090 \n",
      "Accuracy: 6125/10000 (61.25%)\n",
      "\n",
      "Round   4, Average loss 1.509 Test accuracy 61.250\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4920 \n",
      "Accuracy: 6173/10000 (61.73%)\n",
      "\n",
      "Round   5, Average loss 1.492 Test accuracy 61.730\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4782 \n",
      "Accuracy: 6242/10000 (62.42%)\n",
      "\n",
      "Round   6, Average loss 1.478 Test accuracy 62.420\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4830 \n",
      "Accuracy: 6243/10000 (62.43%)\n",
      "\n",
      "Round   7, Average loss 1.483 Test accuracy 62.430\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4669 \n",
      "Accuracy: 6292/10000 (62.92%)\n",
      "\n",
      "Round   8, Average loss 1.467 Test accuracy 62.920\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4644 \n",
      "Accuracy: 6319/10000 (63.19%)\n",
      "\n",
      "Round   9, Average loss 1.464 Test accuracy 63.190\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4715 \n",
      "Accuracy: 6291/10000 (62.91%)\n",
      "\n",
      "Round  10, Average loss 1.472 Test accuracy 62.910\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4712 \n",
      "Accuracy: 6314/10000 (63.14%)\n",
      "\n",
      "Round  11, Average loss 1.471 Test accuracy 63.140\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4600 \n",
      "Accuracy: 6351/10000 (63.51%)\n",
      "\n",
      "Round  12, Average loss 1.460 Test accuracy 63.510\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4614 \n",
      "Accuracy: 6321/10000 (63.21%)\n",
      "\n",
      "Round  13, Average loss 1.461 Test accuracy 63.210\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4529 \n",
      "Accuracy: 6382/10000 (63.82%)\n",
      "\n",
      "Round  14, Average loss 1.453 Test accuracy 63.820\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4549 \n",
      "Accuracy: 6367/10000 (63.67%)\n",
      "\n",
      "Round  15, Average loss 1.455 Test accuracy 63.670\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4567 \n",
      "Accuracy: 6358/10000 (63.58%)\n",
      "\n",
      "Round  16, Average loss 1.457 Test accuracy 63.580\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4537 \n",
      "Accuracy: 6308/10000 (63.08%)\n",
      "\n",
      "Round  17, Average loss 1.454 Test accuracy 63.080\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4590 \n",
      "Accuracy: 6271/10000 (62.71%)\n",
      "\n",
      "Round  18, Average loss 1.459 Test accuracy 62.710\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4623 \n",
      "Accuracy: 6276/10000 (62.76%)\n",
      "\n",
      "Round  19, Average loss 1.462 Test accuracy 62.760\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4531 \n",
      "Accuracy: 6322/10000 (63.22%)\n",
      "\n",
      "Round  20, Average loss 1.453 Test accuracy 63.220\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4578 \n",
      "Accuracy: 6312/10000 (63.12%)\n",
      "\n",
      "Round  21, Average loss 1.458 Test accuracy 63.120\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4535 \n",
      "Accuracy: 6360/10000 (63.60%)\n",
      "\n",
      "Round  22, Average loss 1.453 Test accuracy 63.600\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4441 \n",
      "Accuracy: 6382/10000 (63.82%)\n",
      "\n",
      "Round  23, Average loss 1.444 Test accuracy 63.820\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4509 \n",
      "Accuracy: 6389/10000 (63.89%)\n",
      "\n",
      "Round  24, Average loss 1.451 Test accuracy 63.890\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4463 \n",
      "Accuracy: 6354/10000 (63.54%)\n",
      "\n",
      "Round  25, Average loss 1.446 Test accuracy 63.540\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4497 \n",
      "Accuracy: 6339/10000 (63.39%)\n",
      "\n",
      "Round  26, Average loss 1.450 Test accuracy 63.390\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4584 \n",
      "Accuracy: 6359/10000 (63.59%)\n",
      "\n",
      "Round  27, Average loss 1.458 Test accuracy 63.590\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4404 \n",
      "Accuracy: 6338/10000 (63.38%)\n",
      "\n",
      "Round  28, Average loss 1.440 Test accuracy 63.380\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4515 \n",
      "Accuracy: 6336/10000 (63.36%)\n",
      "\n",
      "Round  29, Average loss 1.451 Test accuracy 63.360\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4501 \n",
      "Accuracy: 6317/10000 (63.17%)\n",
      "\n",
      "Round  30, Average loss 1.450 Test accuracy 63.170\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4560 \n",
      "Accuracy: 6346/10000 (63.46%)\n",
      "\n",
      "Round  31, Average loss 1.456 Test accuracy 63.460\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4648 \n",
      "Accuracy: 6337/10000 (63.37%)\n",
      "\n",
      "Round  32, Average loss 1.465 Test accuracy 63.370\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4546 \n",
      "Accuracy: 6293/10000 (62.93%)\n",
      "\n",
      "Round  33, Average loss 1.455 Test accuracy 62.930\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4581 \n",
      "Accuracy: 6307/10000 (63.07%)\n",
      "\n",
      "Round  34, Average loss 1.458 Test accuracy 63.070\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4500 \n",
      "Accuracy: 6316/10000 (63.16%)\n",
      "\n",
      "Round  35, Average loss 1.450 Test accuracy 63.160\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4530 \n",
      "Accuracy: 6281/10000 (62.81%)\n",
      "\n",
      "Round  36, Average loss 1.453 Test accuracy 62.810\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4489 \n",
      "Accuracy: 6280/10000 (62.80%)\n",
      "\n",
      "Round  37, Average loss 1.449 Test accuracy 62.800\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4561 \n",
      "Accuracy: 6317/10000 (63.17%)\n",
      "\n",
      "Round  38, Average loss 1.456 Test accuracy 63.170\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4606 \n",
      "Accuracy: 6330/10000 (63.30%)\n",
      "\n",
      "Round  39, Average loss 1.461 Test accuracy 63.300\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4634 \n",
      "Accuracy: 6299/10000 (62.99%)\n",
      "\n",
      "Round  40, Average loss 1.463 Test accuracy 62.990\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4452 \n",
      "Accuracy: 6337/10000 (63.37%)\n",
      "\n",
      "Round  41, Average loss 1.445 Test accuracy 63.370\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4510 \n",
      "Accuracy: 6315/10000 (63.15%)\n",
      "\n",
      "Round  42, Average loss 1.451 Test accuracy 63.150\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4487 \n",
      "Accuracy: 6310/10000 (63.10%)\n",
      "\n",
      "Round  43, Average loss 1.449 Test accuracy 63.100\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4533 \n",
      "Accuracy: 6321/10000 (63.21%)\n",
      "\n",
      "Round  44, Average loss 1.453 Test accuracy 63.210\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4593 \n",
      "Accuracy: 6335/10000 (63.35%)\n",
      "\n",
      "Round  45, Average loss 1.459 Test accuracy 63.350\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4502 \n",
      "Accuracy: 6323/10000 (63.23%)\n",
      "\n",
      "Round  46, Average loss 1.450 Test accuracy 63.230\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4625 \n",
      "Accuracy: 6288/10000 (62.88%)\n",
      "\n",
      "Round  47, Average loss 1.462 Test accuracy 62.880\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4525 \n",
      "Accuracy: 6306/10000 (63.06%)\n",
      "\n",
      "Round  48, Average loss 1.452 Test accuracy 63.060\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.4536 \n",
      "Accuracy: 6316/10000 (63.16%)\n",
      "\n",
      "Round  49, Average loss 1.454 Test accuracy 63.160\n",
      "\n",
      "\n",
      "\n",
      "sigma = 0.3\n",
      "\n",
      "\n",
      "\n",
      "Learning Rate = 0.001\n",
      "\n",
      "\n",
      "\n",
      "z_array: [-0.81  0.81]\n",
      "1.2192623088642704\n",
      "1.2192623088642716\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 25000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 25000 \n",
      "\n",
      "Adjust the power of X_tilde\n",
      "0 0.44846753100185377\n",
      "power after adjusting = 2.072550033705631\n",
      "1 0.4491139373169974\n",
      "power after adjusting = 2.0725500337056313\n",
      "\n",
      "(T, sigma)= 3 0.3 )  0 -th Trial!!\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.0370 \n",
      "Accuracy: 4244/10000 (42.44%)\n",
      "\n",
      "Round   0, Average loss 2.037 Test accuracy 42.440\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6459 \n",
      "Accuracy: 5530/10000 (55.30%)\n",
      "\n",
      "Round   1, Average loss 1.646 Test accuracy 55.300\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5806 \n",
      "Accuracy: 5634/10000 (56.34%)\n",
      "\n",
      "Round   2, Average loss 1.581 Test accuracy 56.340\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5619 \n",
      "Accuracy: 5771/10000 (57.71%)\n",
      "\n",
      "Round   3, Average loss 1.562 Test accuracy 57.710\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5547 \n",
      "Accuracy: 5799/10000 (57.99%)\n",
      "\n",
      "Round   4, Average loss 1.555 Test accuracy 57.990\n",
      "selected users: [0 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.5451 \n",
      "Accuracy: 5799/10000 (57.99%)\n",
      "\n",
      "Round   5, Average loss 1.545 Test accuracy 57.990\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5431 \n",
      "Accuracy: 5895/10000 (58.95%)\n",
      "\n",
      "Round   6, Average loss 1.543 Test accuracy 58.950\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5400 \n",
      "Accuracy: 5957/10000 (59.57%)\n",
      "\n",
      "Round   7, Average loss 1.540 Test accuracy 59.570\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5266 \n",
      "Accuracy: 5926/10000 (59.26%)\n",
      "\n",
      "Round   8, Average loss 1.527 Test accuracy 59.260\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5364 \n",
      "Accuracy: 5971/10000 (59.71%)\n",
      "\n",
      "Round   9, Average loss 1.536 Test accuracy 59.710\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5320 \n",
      "Accuracy: 5970/10000 (59.70%)\n",
      "\n",
      "Round  10, Average loss 1.532 Test accuracy 59.700\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5371 \n",
      "Accuracy: 5928/10000 (59.28%)\n",
      "\n",
      "Round  11, Average loss 1.537 Test accuracy 59.280\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5339 \n",
      "Accuracy: 5915/10000 (59.15%)\n",
      "\n",
      "Round  12, Average loss 1.534 Test accuracy 59.150\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5384 \n",
      "Accuracy: 5913/10000 (59.13%)\n",
      "\n",
      "Round  13, Average loss 1.538 Test accuracy 59.130\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5423 \n",
      "Accuracy: 5954/10000 (59.54%)\n",
      "\n",
      "Round  14, Average loss 1.542 Test accuracy 59.540\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5344 \n",
      "Accuracy: 5951/10000 (59.51%)\n",
      "\n",
      "Round  15, Average loss 1.534 Test accuracy 59.510\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5362 \n",
      "Accuracy: 5937/10000 (59.37%)\n",
      "\n",
      "Round  16, Average loss 1.536 Test accuracy 59.370\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5301 \n",
      "Accuracy: 5959/10000 (59.59%)\n",
      "\n",
      "Round  17, Average loss 1.530 Test accuracy 59.590\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5261 \n",
      "Accuracy: 5965/10000 (59.65%)\n",
      "\n",
      "Round  18, Average loss 1.526 Test accuracy 59.650\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5312 \n",
      "Accuracy: 5901/10000 (59.01%)\n",
      "\n",
      "Round  19, Average loss 1.531 Test accuracy 59.010\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5200 \n",
      "Accuracy: 5992/10000 (59.92%)\n",
      "\n",
      "Round  20, Average loss 1.520 Test accuracy 59.920\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5262 \n",
      "Accuracy: 5979/10000 (59.79%)\n",
      "\n",
      "Round  21, Average loss 1.526 Test accuracy 59.790\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5198 \n",
      "Accuracy: 5986/10000 (59.86%)\n",
      "\n",
      "Round  22, Average loss 1.520 Test accuracy 59.860\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5263 \n",
      "Accuracy: 5947/10000 (59.47%)\n",
      "\n",
      "Round  23, Average loss 1.526 Test accuracy 59.470\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5281 \n",
      "Accuracy: 5931/10000 (59.31%)\n",
      "\n",
      "Round  24, Average loss 1.528 Test accuracy 59.310\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5271 \n",
      "Accuracy: 5888/10000 (58.88%)\n",
      "\n",
      "Round  25, Average loss 1.527 Test accuracy 58.880\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5196 \n",
      "Accuracy: 5920/10000 (59.20%)\n",
      "\n",
      "Round  26, Average loss 1.520 Test accuracy 59.200\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5143 \n",
      "Accuracy: 5910/10000 (59.10%)\n",
      "\n",
      "Round  27, Average loss 1.514 Test accuracy 59.100\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5158 \n",
      "Accuracy: 5913/10000 (59.13%)\n",
      "\n",
      "Round  28, Average loss 1.516 Test accuracy 59.130\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5246 \n",
      "Accuracy: 5903/10000 (59.03%)\n",
      "\n",
      "Round  29, Average loss 1.525 Test accuracy 59.030\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5082 \n",
      "Accuracy: 5942/10000 (59.42%)\n",
      "\n",
      "Round  30, Average loss 1.508 Test accuracy 59.420\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5211 \n",
      "Accuracy: 5895/10000 (58.95%)\n",
      "\n",
      "Round  31, Average loss 1.521 Test accuracy 58.950\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5290 \n",
      "Accuracy: 5884/10000 (58.84%)\n",
      "\n",
      "Round  32, Average loss 1.529 Test accuracy 58.840\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5190 \n",
      "Accuracy: 5899/10000 (58.99%)\n",
      "\n",
      "Round  33, Average loss 1.519 Test accuracy 58.990\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5150 \n",
      "Accuracy: 5883/10000 (58.83%)\n",
      "\n",
      "Round  34, Average loss 1.515 Test accuracy 58.830\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5172 \n",
      "Accuracy: 5969/10000 (59.69%)\n",
      "\n",
      "Round  35, Average loss 1.517 Test accuracy 59.690\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5168 \n",
      "Accuracy: 5930/10000 (59.30%)\n",
      "\n",
      "Round  36, Average loss 1.517 Test accuracy 59.300\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5189 \n",
      "Accuracy: 5906/10000 (59.06%)\n",
      "\n",
      "Round  37, Average loss 1.519 Test accuracy 59.060\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5113 \n",
      "Accuracy: 5919/10000 (59.19%)\n",
      "\n",
      "Round  38, Average loss 1.511 Test accuracy 59.190\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5198 \n",
      "Accuracy: 5891/10000 (58.91%)\n",
      "\n",
      "Round  39, Average loss 1.520 Test accuracy 58.910\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5181 \n",
      "Accuracy: 5879/10000 (58.79%)\n",
      "\n",
      "Round  40, Average loss 1.518 Test accuracy 58.790\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5072 \n",
      "Accuracy: 5920/10000 (59.20%)\n",
      "\n",
      "Round  41, Average loss 1.507 Test accuracy 59.200\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5091 \n",
      "Accuracy: 5881/10000 (58.81%)\n",
      "\n",
      "Round  42, Average loss 1.509 Test accuracy 58.810\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5134 \n",
      "Accuracy: 5893/10000 (58.93%)\n",
      "\n",
      "Round  43, Average loss 1.513 Test accuracy 58.930\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5147 \n",
      "Accuracy: 5951/10000 (59.51%)\n",
      "\n",
      "Round  44, Average loss 1.515 Test accuracy 59.510\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5177 \n",
      "Accuracy: 5948/10000 (59.48%)\n",
      "\n",
      "Round  45, Average loss 1.518 Test accuracy 59.480\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5153 \n",
      "Accuracy: 5926/10000 (59.26%)\n",
      "\n",
      "Round  46, Average loss 1.515 Test accuracy 59.260\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5184 \n",
      "Accuracy: 5919/10000 (59.19%)\n",
      "\n",
      "Round  47, Average loss 1.518 Test accuracy 59.190\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5131 \n",
      "Accuracy: 5922/10000 (59.22%)\n",
      "\n",
      "Round  48, Average loss 1.513 Test accuracy 59.220\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5295 \n",
      "Accuracy: 5876/10000 (58.76%)\n",
      "\n",
      "Round  49, Average loss 1.529 Test accuracy 58.760\n",
      "\n",
      "\n",
      "\n",
      "sigma = 1\n",
      "\n",
      "\n",
      "\n",
      "Learning Rate = 0.001\n",
      "\n",
      "\n",
      "\n",
      "z_array: [-0.81  0.81]\n",
      "0.4838626198316927\n",
      "0.48386261983169315\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 25000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 25000 \n",
      "\n",
      "Adjust the power of X_tilde\n",
      "0 0.87841814858843\n",
      "power after adjusting = 2.0725500337056313\n",
      "1 0.8788434298555792\n",
      "power after adjusting = 2.0725500337056317\n",
      "\n",
      "(T, sigma)= 3 1 )  0 -th Trial!!\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.1194 \n",
      "Accuracy: 3035/10000 (30.35%)\n",
      "\n",
      "Round   0, Average loss 2.119 Test accuracy 30.350\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7865 \n",
      "Accuracy: 4231/10000 (42.31%)\n",
      "\n",
      "Round   1, Average loss 1.786 Test accuracy 42.310\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7363 \n",
      "Accuracy: 4431/10000 (44.31%)\n",
      "\n",
      "Round   2, Average loss 1.736 Test accuracy 44.310\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7185 \n",
      "Accuracy: 4464/10000 (44.64%)\n",
      "\n",
      "Round   3, Average loss 1.718 Test accuracy 44.640\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7097 \n",
      "Accuracy: 4507/10000 (45.07%)\n",
      "\n",
      "Round   4, Average loss 1.710 Test accuracy 45.070\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7053 \n",
      "Accuracy: 4502/10000 (45.02%)\n",
      "\n",
      "Round   5, Average loss 1.705 Test accuracy 45.020\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7077 \n",
      "Accuracy: 4500/10000 (45.00%)\n",
      "\n",
      "Round   6, Average loss 1.708 Test accuracy 45.000\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7031 \n",
      "Accuracy: 4508/10000 (45.08%)\n",
      "\n",
      "Round   7, Average loss 1.703 Test accuracy 45.080\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6993 \n",
      "Accuracy: 4539/10000 (45.39%)\n",
      "\n",
      "Round   8, Average loss 1.699 Test accuracy 45.390\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6983 \n",
      "Accuracy: 4573/10000 (45.73%)\n",
      "\n",
      "Round   9, Average loss 1.698 Test accuracy 45.730\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6887 \n",
      "Accuracy: 4629/10000 (46.29%)\n",
      "\n",
      "Round  10, Average loss 1.689 Test accuracy 46.290\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6918 \n",
      "Accuracy: 4606/10000 (46.06%)\n",
      "\n",
      "Round  11, Average loss 1.692 Test accuracy 46.060\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6933 \n",
      "Accuracy: 4598/10000 (45.98%)\n",
      "\n",
      "Round  12, Average loss 1.693 Test accuracy 45.980\n",
      "selected users: [0 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.6918 \n",
      "Accuracy: 4628/10000 (46.28%)\n",
      "\n",
      "Round  13, Average loss 1.692 Test accuracy 46.280\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6815 \n",
      "Accuracy: 4660/10000 (46.60%)\n",
      "\n",
      "Round  14, Average loss 1.682 Test accuracy 46.600\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6910 \n",
      "Accuracy: 4591/10000 (45.91%)\n",
      "\n",
      "Round  15, Average loss 1.691 Test accuracy 45.910\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6931 \n",
      "Accuracy: 4643/10000 (46.43%)\n",
      "\n",
      "Round  16, Average loss 1.693 Test accuracy 46.430\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7034 \n",
      "Accuracy: 4596/10000 (45.96%)\n",
      "\n",
      "Round  17, Average loss 1.703 Test accuracy 45.960\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6899 \n",
      "Accuracy: 4693/10000 (46.93%)\n",
      "\n",
      "Round  18, Average loss 1.690 Test accuracy 46.930\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6896 \n",
      "Accuracy: 4647/10000 (46.47%)\n",
      "\n",
      "Round  19, Average loss 1.690 Test accuracy 46.470\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7005 \n",
      "Accuracy: 4581/10000 (45.81%)\n",
      "\n",
      "Round  20, Average loss 1.701 Test accuracy 45.810\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6934 \n",
      "Accuracy: 4641/10000 (46.41%)\n",
      "\n",
      "Round  21, Average loss 1.693 Test accuracy 46.410\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6930 \n",
      "Accuracy: 4651/10000 (46.51%)\n",
      "\n",
      "Round  22, Average loss 1.693 Test accuracy 46.510\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6983 \n",
      "Accuracy: 4580/10000 (45.80%)\n",
      "\n",
      "Round  23, Average loss 1.698 Test accuracy 45.800\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7004 \n",
      "Accuracy: 4597/10000 (45.97%)\n",
      "\n",
      "Round  24, Average loss 1.700 Test accuracy 45.970\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7053 \n",
      "Accuracy: 4530/10000 (45.30%)\n",
      "\n",
      "Round  25, Average loss 1.705 Test accuracy 45.300\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6933 \n",
      "Accuracy: 4648/10000 (46.48%)\n",
      "\n",
      "Round  26, Average loss 1.693 Test accuracy 46.480\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7013 \n",
      "Accuracy: 4570/10000 (45.70%)\n",
      "\n",
      "Round  27, Average loss 1.701 Test accuracy 45.700\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7034 \n",
      "Accuracy: 4596/10000 (45.96%)\n",
      "\n",
      "Round  28, Average loss 1.703 Test accuracy 45.960\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6940 \n",
      "Accuracy: 4660/10000 (46.60%)\n",
      "\n",
      "Round  29, Average loss 1.694 Test accuracy 46.600\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6947 \n",
      "Accuracy: 4633/10000 (46.33%)\n",
      "\n",
      "Round  30, Average loss 1.695 Test accuracy 46.330\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7018 \n",
      "Accuracy: 4661/10000 (46.61%)\n",
      "\n",
      "Round  31, Average loss 1.702 Test accuracy 46.610\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6973 \n",
      "Accuracy: 4660/10000 (46.60%)\n",
      "\n",
      "Round  32, Average loss 1.697 Test accuracy 46.600\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6991 \n",
      "Accuracy: 4633/10000 (46.33%)\n",
      "\n",
      "Round  33, Average loss 1.699 Test accuracy 46.330\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7044 \n",
      "Accuracy: 4632/10000 (46.32%)\n",
      "\n",
      "Round  34, Average loss 1.704 Test accuracy 46.320\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7047 \n",
      "Accuracy: 4709/10000 (47.09%)\n",
      "\n",
      "Round  35, Average loss 1.705 Test accuracy 47.090\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7042 \n",
      "Accuracy: 4611/10000 (46.11%)\n",
      "\n",
      "Round  36, Average loss 1.704 Test accuracy 46.110\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7045 \n",
      "Accuracy: 4629/10000 (46.29%)\n",
      "\n",
      "Round  37, Average loss 1.705 Test accuracy 46.290\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7065 \n",
      "Accuracy: 4617/10000 (46.17%)\n",
      "\n",
      "Round  38, Average loss 1.707 Test accuracy 46.170\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7040 \n",
      "Accuracy: 4622/10000 (46.22%)\n",
      "\n",
      "Round  39, Average loss 1.704 Test accuracy 46.220\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7002 \n",
      "Accuracy: 4689/10000 (46.89%)\n",
      "\n",
      "Round  40, Average loss 1.700 Test accuracy 46.890\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7042 \n",
      "Accuracy: 4672/10000 (46.72%)\n",
      "\n",
      "Round  41, Average loss 1.704 Test accuracy 46.720\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6968 \n",
      "Accuracy: 4701/10000 (47.01%)\n",
      "\n",
      "Round  42, Average loss 1.697 Test accuracy 47.010\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6966 \n",
      "Accuracy: 4691/10000 (46.91%)\n",
      "\n",
      "Round  43, Average loss 1.697 Test accuracy 46.910\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7021 \n",
      "Accuracy: 4638/10000 (46.38%)\n",
      "\n",
      "Round  44, Average loss 1.702 Test accuracy 46.380\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7067 \n",
      "Accuracy: 4595/10000 (45.95%)\n",
      "\n",
      "Round  45, Average loss 1.707 Test accuracy 45.950\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7201 \n",
      "Accuracy: 4527/10000 (45.27%)\n",
      "\n",
      "Round  46, Average loss 1.720 Test accuracy 45.270\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7078 \n",
      "Accuracy: 4564/10000 (45.64%)\n",
      "\n",
      "Round  47, Average loss 1.708 Test accuracy 45.640\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7167 \n",
      "Accuracy: 4531/10000 (45.31%)\n",
      "\n",
      "Round  48, Average loss 1.717 Test accuracy 45.310\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7125 \n",
      "Accuracy: 4541/10000 (45.41%)\n",
      "\n",
      "Round  49, Average loss 1.713 Test accuracy 45.410\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "K = 2\n",
    "T = 3\n",
    "sigma = 0.1\n",
    "Noise_Alloc = [0,2,4]\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "\n",
    "N_array = [2]\n",
    "B_array = [0.5]\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 50\n",
    "\n",
    "lr_array = [0.001] # 0.001 is the bset\n",
    "\n",
    "sigma_array = [0.1,0.3,1]\n",
    "# lr_array = [0.1, 0.01, 0.005, 0.001,0.0005]\n",
    "\n",
    "\n",
    "loss_test_arr_K2_G1_powerAlign = np.zeros((len(sigma_array),len(lr_array),N_trials,N_epochs))\n",
    "acc_test_arr_K2_G1_powerAlign  = np.zeros((len(sigma_array),len(lr_array),N_trials,N_epochs))\n",
    "\n",
    "N = 2\n",
    "for sigma_idx in range(len(sigma_array)):\n",
    "    \n",
    "    sigma = sigma_array[sigma_idx]\n",
    "    \n",
    "    print('\\n\\n')\n",
    "    print('sigma =',sigma)\n",
    "    print('\\n\\n')\n",
    "           \n",
    "        \n",
    "    # print(\"alpha_array: \",alpha_array,'\\n')\n",
    "    \n",
    "    \n",
    "    for lr_idx in range(len(lr_array)):\n",
    "        \n",
    "        args.lr = lr_array[lr_idx]\n",
    "        \n",
    "        print('Learning Rate =',args.lr)\n",
    "        print('\\n\\n')\n",
    "#         while(len(z_array)<N):\n",
    "#             z_tmp = np.random.uniform(-1,1,1)\n",
    "#             MIS_tmp = MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_tmp], 1,sigma)\n",
    "#             if MIS_tmp < B and MIS_tmp > 0.1:\n",
    "#                 z_array.append(z_tmp[0])\n",
    "#         \n",
    "#         z_array = np.sort(z_array)\n",
    "#         print(N_idx,'!!!')\n",
    "#         if N_idx==0:\n",
    "# #             z_array = np.array([-0.94,-0.534,0.534, 0.94])\n",
    "# #         elif N_idx==1:\n",
    "# #             z_array = np.array([-0.94, -0.73, 0.73, 0.94])\n",
    "#         elif N_idx==1:\n",
    "#             z_array = np.array([-0.94, -0.125, 0.125, 0.94])\n",
    "#         else:\n",
    "# #             z_array = np.array([-0.94, -0.73, -0.534, -0.125, 0.125, 0.534, 0.73, 0.94])\n",
    "# #             z_array = np.array([-0.9, -0.81, -0.22, -0.20, 0.20, 0.22, 0.81, 0.9])\n",
    "        \n",
    "        z_array = np.array([-0.81, 0.81])\n",
    "        \n",
    "        print('z_array:',z_array)\n",
    "        if sigma != 0:\n",
    "            for j in range(len(z_array)):\n",
    "                print(MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_array[j]], 1,sigma))\n",
    "\n",
    "        \n",
    "        _Noise_label = np.ones((25000*T,10)) * 0.1\n",
    "\n",
    "        X_tilde,a,b = BACC_Enc_Data_v3(encoding_input_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde,a,b = BACC_Enc_Data_v3(encoding_label_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "        \n",
    "        print(\"Adjust the power of X_tilde\")\n",
    "        \n",
    "        for p_idx in range(N):\n",
    "            tmp_power = np.sum(X_tilde[p_idx,:,:] * X_tilde[p_idx,:,:], axis=1)/np.shape(X_tilde)[2]\n",
    "            tmp_power = np.sum(tmp_power)/np.shape(X_tilde)[1]\n",
    "            print(p_idx, tmp_power)\n",
    "            \n",
    "            X_tilde[p_idx,:,:] = X_tilde[p_idx,:,:] * np.sqrt(avg_power/tmp_power)\n",
    "            tmp_power = np.sum(X_tilde[p_idx,:,:] * X_tilde[p_idx,:,:], axis=1)/np.shape(X_tilde)[2]\n",
    "            tmp_power = np.sum(tmp_power)/np.shape(X_tilde)[1]\n",
    "            print('power after adjusting =',tmp_power)\n",
    "        print()\n",
    "        \n",
    "        m = N # m is the number of received result @ master\n",
    "    #     print('number of results:',m)\n",
    "\n",
    "        for trial_idx in range(N_trials):\n",
    "            print('(T, sigma)=',T,sigma,') ',trial_idx,'-th Trial!!')\n",
    "\n",
    "            net_glob = CNNCifar(args=args)\n",
    "            net_glob.cuda()\n",
    "            net_glob.train()\n",
    "\n",
    "            # copy weights\n",
    "            w_glob = net_glob.state_dict()\n",
    "\n",
    "            for iter in range(N_epochs): #args.epochs\n",
    "                w_locals, loss_locals = [], []\n",
    "                idxs_users = np.random.choice(range(N), m, replace=False)\n",
    "                idxs_users = np.sort(idxs_users)\n",
    "                print('selected users:',idxs_users)\n",
    "                \n",
    "                coded_net = BACC_Enc_Model_withNoise_v4(net_glob.cuda(), N, K, T, sigma, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "                dec_z_array = []\n",
    "                for idx in idxs_users: #for idx in range(N):\n",
    "            #         print(idx)\n",
    "                    local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[idx,:,:], label=y_tilde[idx,:,:])\n",
    "                    w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "#                     w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                    w_locals.append(copy.deepcopy(w))\n",
    "                    loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                    dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "                # update global weights\n",
    "                #w_glob = FedAvg(w_locals)\n",
    "                w_glob = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "\n",
    "                # copy weight to net_glob\n",
    "                net_glob.load_state_dict(w_glob)\n",
    "\n",
    "                # print loss\n",
    "            #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "            #     loss_train_arr.append(loss_train)\n",
    "\n",
    "                acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "                acc_test_arr_K2_G1_powerAlign[sigma_idx][lr_idx][trial_idx][iter] = acc_test\n",
    "                loss_test_arr_K2_G1_powerAlign[sigma_idx][lr_idx][trial_idx][iter] = loss_test\n",
    "                \n",
    "#                 PATH = \"./save_models/CIFAR10_LeNet_BACC_PowerAlign_K2_sigma_E50_iter\"+str(iter)+\".pt\"\n",
    "#                 torch.save(net_glob.state_dict(), PATH)\n",
    "                \n",
    "                if iter % 1 ==0:\n",
    "                    print('Round {:3d}, Average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_test, acc_test))\n",
    "                #print(loss_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differential Privacy (Gaussian Noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of X: (50000, 3072)\n",
      "size of Y: (50000, 10)\n",
      "(50000, 10)\n",
      "2.0729636636110373\n"
     ]
    }
   ],
   "source": [
    "encoding_input_array_np = np.empty((len(dataset_train),32*32*3))\n",
    "encoding_label_array_np = np.empty((len(dataset_train),args.num_classes))\n",
    "print(\"size of X:\" ,encoding_input_array_np.shape)\n",
    "print(\"size of Y:\" ,encoding_label_array_np.shape)\n",
    "\n",
    "Size_submatrices = int(50000/args.num_users)\n",
    "\n",
    "sigma = 0.1\n",
    "\n",
    "for i in range(args.num_users):\n",
    "    \n",
    "    stt_pos = i*Size_submatrices\n",
    "    end_pos = (i+1)*Size_submatrices\n",
    "#     print(i,stt_pos,end_pos)\n",
    "    Temp_train = DataLoader(DatasetSplit(dataset_train, dict_users[i]), batch_size=Size_submatrices, shuffle=True)\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(Temp_train):\n",
    "        \n",
    "        images_np = images.detach().cpu().numpy()\n",
    "        \n",
    "#         print(np.size(images_np))\n",
    "        encoding_input_array_np[stt_pos:end_pos,:] = np.reshape(images_np, (Size_submatrices,32*32*3)) \n",
    "#         encoding_input_array_np[stt_pos:end_pos,:] = np.reshape(images_np, (Size_submatrices,32*32*3)) + np.random.normal(0,sigma,size=(Size_submatrices,32*32*3))\n",
    "#         print(encoding_input_array_np[stt_pos:end_pos,:].shape)\n",
    "\n",
    "        onehot_labels = torch.nn.functional.one_hot(labels,num_classes=args.num_classes)\n",
    "        labels_np = onehot_labels.detach().cpu().numpy()\n",
    "#         print(labels_np.shape)\n",
    "        encoding_label_array_np[stt_pos:end_pos,:] = labels_np\n",
    "    \n",
    "print(np.shape(encoding_label_array_np))\n",
    "\n",
    "avg_power = np.sum(encoding_input_array_np*encoding_input_array_np, axis=1)/np.shape(encoding_input_array_np)[1]\n",
    "avg_power = np.sum(avg_power)/np.shape(encoding_input_array_np)[0]\n",
    "print(avg_power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "sigma = 0.1\n",
      "\n",
      "\n",
      "\n",
      "Learning Rate = 5e-05\n",
      "\n",
      "\n",
      "\n",
      "(sigma, lr)= 0.1 5e-05 )  0 -th Trial!!\n",
      "\n",
      "Test set: Average loss: 1.7073 \n",
      "Accuracy: 3840/10000 (38.40%)\n",
      "\n",
      "Round   0, Average loss 1.707 Test accuracy 38.400\n",
      "\n",
      "Test set: Average loss: 1.5744 \n",
      "Accuracy: 4267/10000 (42.67%)\n",
      "\n",
      "Round   1, Average loss 1.574 Test accuracy 42.670\n",
      "\n",
      "Test set: Average loss: 1.5049 \n",
      "Accuracy: 4593/10000 (45.93%)\n",
      "\n",
      "Round   2, Average loss 1.505 Test accuracy 45.930\n",
      "\n",
      "Test set: Average loss: 1.4592 \n",
      "Accuracy: 4785/10000 (47.85%)\n",
      "\n",
      "Round   3, Average loss 1.459 Test accuracy 47.850\n",
      "\n",
      "Test set: Average loss: 1.4230 \n",
      "Accuracy: 4936/10000 (49.36%)\n",
      "\n",
      "Round   4, Average loss 1.423 Test accuracy 49.360\n",
      "\n",
      "Test set: Average loss: 1.3922 \n",
      "Accuracy: 5037/10000 (50.37%)\n",
      "\n",
      "Round   5, Average loss 1.392 Test accuracy 50.370\n",
      "\n",
      "Test set: Average loss: 1.3661 \n",
      "Accuracy: 5144/10000 (51.44%)\n",
      "\n",
      "Round   6, Average loss 1.366 Test accuracy 51.440\n",
      "\n",
      "Test set: Average loss: 1.3430 \n",
      "Accuracy: 5221/10000 (52.21%)\n",
      "\n",
      "Round   7, Average loss 1.343 Test accuracy 52.210\n",
      "\n",
      "Test set: Average loss: 1.3224 \n",
      "Accuracy: 5297/10000 (52.97%)\n",
      "\n",
      "Round   8, Average loss 1.322 Test accuracy 52.970\n",
      "\n",
      "Test set: Average loss: 1.3042 \n",
      "Accuracy: 5358/10000 (53.58%)\n",
      "\n",
      "Round   9, Average loss 1.304 Test accuracy 53.580\n",
      "\n",
      "Test set: Average loss: 1.2881 \n",
      "Accuracy: 5430/10000 (54.30%)\n",
      "\n",
      "Round  10, Average loss 1.288 Test accuracy 54.300\n",
      "\n",
      "Test set: Average loss: 1.2740 \n",
      "Accuracy: 5464/10000 (54.64%)\n",
      "\n",
      "Round  11, Average loss 1.274 Test accuracy 54.640\n",
      "\n",
      "Test set: Average loss: 1.2617 \n",
      "Accuracy: 5531/10000 (55.31%)\n",
      "\n",
      "Round  12, Average loss 1.262 Test accuracy 55.310\n",
      "\n",
      "Test set: Average loss: 1.2504 \n",
      "Accuracy: 5585/10000 (55.85%)\n",
      "\n",
      "Round  13, Average loss 1.250 Test accuracy 55.850\n",
      "\n",
      "Test set: Average loss: 1.2399 \n",
      "Accuracy: 5630/10000 (56.30%)\n",
      "\n",
      "Round  14, Average loss 1.240 Test accuracy 56.300\n",
      "\n",
      "Test set: Average loss: 1.2315 \n",
      "Accuracy: 5655/10000 (56.55%)\n",
      "\n",
      "Round  15, Average loss 1.231 Test accuracy 56.550\n",
      "\n",
      "Test set: Average loss: 1.2235 \n",
      "Accuracy: 5701/10000 (57.01%)\n",
      "\n",
      "Round  16, Average loss 1.224 Test accuracy 57.010\n",
      "\n",
      "Test set: Average loss: 1.2164 \n",
      "Accuracy: 5737/10000 (57.37%)\n",
      "\n",
      "Round  17, Average loss 1.216 Test accuracy 57.370\n",
      "\n",
      "Test set: Average loss: 1.2095 \n",
      "Accuracy: 5749/10000 (57.49%)\n",
      "\n",
      "Round  18, Average loss 1.209 Test accuracy 57.490\n",
      "\n",
      "Test set: Average loss: 1.2031 \n",
      "Accuracy: 5773/10000 (57.73%)\n",
      "\n",
      "Round  19, Average loss 1.203 Test accuracy 57.730\n",
      "\n",
      "Test set: Average loss: 1.1973 \n",
      "Accuracy: 5802/10000 (58.02%)\n",
      "\n",
      "Round  20, Average loss 1.197 Test accuracy 58.020\n",
      "\n",
      "Test set: Average loss: 1.1917 \n",
      "Accuracy: 5819/10000 (58.19%)\n",
      "\n",
      "Round  21, Average loss 1.192 Test accuracy 58.190\n",
      "\n",
      "Test set: Average loss: 1.1861 \n",
      "Accuracy: 5831/10000 (58.31%)\n",
      "\n",
      "Round  22, Average loss 1.186 Test accuracy 58.310\n",
      "\n",
      "Test set: Average loss: 1.1816 \n",
      "Accuracy: 5854/10000 (58.54%)\n",
      "\n",
      "Round  23, Average loss 1.182 Test accuracy 58.540\n",
      "\n",
      "Test set: Average loss: 1.1774 \n",
      "Accuracy: 5859/10000 (58.59%)\n",
      "\n",
      "Round  24, Average loss 1.177 Test accuracy 58.590\n",
      "\n",
      "Test set: Average loss: 1.1739 \n",
      "Accuracy: 5871/10000 (58.71%)\n",
      "\n",
      "Round  25, Average loss 1.174 Test accuracy 58.710\n",
      "\n",
      "Test set: Average loss: 1.1703 \n",
      "Accuracy: 5888/10000 (58.88%)\n",
      "\n",
      "Round  26, Average loss 1.170 Test accuracy 58.880\n",
      "\n",
      "Test set: Average loss: 1.1675 \n",
      "Accuracy: 5906/10000 (59.06%)\n",
      "\n",
      "Round  27, Average loss 1.167 Test accuracy 59.060\n",
      "\n",
      "Test set: Average loss: 1.1647 \n",
      "Accuracy: 5916/10000 (59.16%)\n",
      "\n",
      "Round  28, Average loss 1.165 Test accuracy 59.160\n",
      "\n",
      "Test set: Average loss: 1.1622 \n",
      "Accuracy: 5940/10000 (59.40%)\n",
      "\n",
      "Round  29, Average loss 1.162 Test accuracy 59.400\n",
      "\n",
      "Test set: Average loss: 1.1599 \n",
      "Accuracy: 5958/10000 (59.58%)\n",
      "\n",
      "Round  30, Average loss 1.160 Test accuracy 59.580\n",
      "\n",
      "Test set: Average loss: 1.1577 \n",
      "Accuracy: 5959/10000 (59.59%)\n",
      "\n",
      "Round  31, Average loss 1.158 Test accuracy 59.590\n",
      "\n",
      "Test set: Average loss: 1.1561 \n",
      "Accuracy: 5966/10000 (59.66%)\n",
      "\n",
      "Round  32, Average loss 1.156 Test accuracy 59.660\n",
      "\n",
      "Test set: Average loss: 1.1546 \n",
      "Accuracy: 5970/10000 (59.70%)\n",
      "\n",
      "Round  33, Average loss 1.155 Test accuracy 59.700\n",
      "\n",
      "Test set: Average loss: 1.1527 \n",
      "Accuracy: 5981/10000 (59.81%)\n",
      "\n",
      "Round  34, Average loss 1.153 Test accuracy 59.810\n",
      "\n",
      "Test set: Average loss: 1.1515 \n",
      "Accuracy: 5994/10000 (59.94%)\n",
      "\n",
      "Round  35, Average loss 1.151 Test accuracy 59.940\n",
      "\n",
      "Test set: Average loss: 1.1503 \n",
      "Accuracy: 5999/10000 (59.99%)\n",
      "\n",
      "Round  36, Average loss 1.150 Test accuracy 59.990\n",
      "\n",
      "Test set: Average loss: 1.1494 \n",
      "Accuracy: 6022/10000 (60.22%)\n",
      "\n",
      "Round  37, Average loss 1.149 Test accuracy 60.220\n",
      "\n",
      "Test set: Average loss: 1.1485 \n",
      "Accuracy: 6038/10000 (60.38%)\n",
      "\n",
      "Round  38, Average loss 1.148 Test accuracy 60.380\n",
      "\n",
      "Test set: Average loss: 1.1481 \n",
      "Accuracy: 6035/10000 (60.35%)\n",
      "\n",
      "Round  39, Average loss 1.148 Test accuracy 60.350\n",
      "\n",
      "Test set: Average loss: 1.1483 \n",
      "Accuracy: 6024/10000 (60.24%)\n",
      "\n",
      "Round  40, Average loss 1.148 Test accuracy 60.240\n",
      "\n",
      "Test set: Average loss: 1.1481 \n",
      "Accuracy: 6022/10000 (60.22%)\n",
      "\n",
      "Round  41, Average loss 1.148 Test accuracy 60.220\n",
      "\n",
      "Test set: Average loss: 1.1480 \n",
      "Accuracy: 6026/10000 (60.26%)\n",
      "\n",
      "Round  42, Average loss 1.148 Test accuracy 60.260\n",
      "\n",
      "Test set: Average loss: 1.1485 \n",
      "Accuracy: 6023/10000 (60.23%)\n",
      "\n",
      "Round  43, Average loss 1.149 Test accuracy 60.230\n",
      "\n",
      "Test set: Average loss: 1.1488 \n",
      "Accuracy: 6023/10000 (60.23%)\n",
      "\n",
      "Round  44, Average loss 1.149 Test accuracy 60.230\n",
      "\n",
      "Test set: Average loss: 1.1496 \n",
      "Accuracy: 6024/10000 (60.24%)\n",
      "\n",
      "Round  45, Average loss 1.150 Test accuracy 60.240\n",
      "\n",
      "Test set: Average loss: 1.1500 \n",
      "Accuracy: 6034/10000 (60.34%)\n",
      "\n",
      "Round  46, Average loss 1.150 Test accuracy 60.340\n",
      "\n",
      "Test set: Average loss: 1.1509 \n",
      "Accuracy: 6039/10000 (60.39%)\n",
      "\n",
      "Round  47, Average loss 1.151 Test accuracy 60.390\n",
      "\n",
      "Test set: Average loss: 1.1515 \n",
      "Accuracy: 6041/10000 (60.41%)\n",
      "\n",
      "Round  48, Average loss 1.152 Test accuracy 60.410\n",
      "\n",
      "Test set: Average loss: 1.1524 \n",
      "Accuracy: 6041/10000 (60.41%)\n",
      "\n",
      "Round  49, Average loss 1.152 Test accuracy 60.410\n",
      "\n",
      "\n",
      "\n",
      "sigma = 0.3\n",
      "\n",
      "\n",
      "\n",
      "Learning Rate = 5e-05\n",
      "\n",
      "\n",
      "\n",
      "(sigma, lr)= 0.3 5e-05 )  0 -th Trial!!\n",
      "\n",
      "Test set: Average loss: 1.7140 \n",
      "Accuracy: 3742/10000 (37.42%)\n",
      "\n",
      "Round   0, Average loss 1.714 Test accuracy 37.420\n",
      "\n",
      "Test set: Average loss: 1.6045 \n",
      "Accuracy: 4091/10000 (40.91%)\n",
      "\n",
      "Round   1, Average loss 1.604 Test accuracy 40.910\n",
      "\n",
      "Test set: Average loss: 1.5397 \n",
      "Accuracy: 4353/10000 (43.53%)\n",
      "\n",
      "Round   2, Average loss 1.540 Test accuracy 43.530\n",
      "\n",
      "Test set: Average loss: 1.4963 \n",
      "Accuracy: 4542/10000 (45.42%)\n",
      "\n",
      "Round   3, Average loss 1.496 Test accuracy 45.420\n",
      "\n",
      "Test set: Average loss: 1.4640 \n",
      "Accuracy: 4668/10000 (46.68%)\n",
      "\n",
      "Round   4, Average loss 1.464 Test accuracy 46.680\n",
      "\n",
      "Test set: Average loss: 1.4372 \n",
      "Accuracy: 4795/10000 (47.95%)\n",
      "\n",
      "Round   5, Average loss 1.437 Test accuracy 47.950\n",
      "\n",
      "Test set: Average loss: 1.4142 \n",
      "Accuracy: 4894/10000 (48.94%)\n",
      "\n",
      "Round   6, Average loss 1.414 Test accuracy 48.940\n",
      "\n",
      "Test set: Average loss: 1.3942 \n",
      "Accuracy: 4992/10000 (49.92%)\n",
      "\n",
      "Round   7, Average loss 1.394 Test accuracy 49.920\n",
      "\n",
      "Test set: Average loss: 1.3761 \n",
      "Accuracy: 5072/10000 (50.72%)\n",
      "\n",
      "Round   8, Average loss 1.376 Test accuracy 50.720\n",
      "\n",
      "Test set: Average loss: 1.3607 \n",
      "Accuracy: 5136/10000 (51.36%)\n",
      "\n",
      "Round   9, Average loss 1.361 Test accuracy 51.360\n",
      "\n",
      "Test set: Average loss: 1.3470 \n",
      "Accuracy: 5188/10000 (51.88%)\n",
      "\n",
      "Round  10, Average loss 1.347 Test accuracy 51.880\n",
      "\n",
      "Test set: Average loss: 1.3354 \n",
      "Accuracy: 5242/10000 (52.42%)\n",
      "\n",
      "Round  11, Average loss 1.335 Test accuracy 52.420\n",
      "\n",
      "Test set: Average loss: 1.3248 \n",
      "Accuracy: 5282/10000 (52.82%)\n",
      "\n",
      "Round  12, Average loss 1.325 Test accuracy 52.820\n",
      "\n",
      "Test set: Average loss: 1.3157 \n",
      "Accuracy: 5323/10000 (53.23%)\n",
      "\n",
      "Round  13, Average loss 1.316 Test accuracy 53.230\n",
      "\n",
      "Test set: Average loss: 1.3072 \n",
      "Accuracy: 5351/10000 (53.51%)\n",
      "\n",
      "Round  14, Average loss 1.307 Test accuracy 53.510\n",
      "\n",
      "Test set: Average loss: 1.3005 \n",
      "Accuracy: 5385/10000 (53.85%)\n",
      "\n",
      "Round  15, Average loss 1.300 Test accuracy 53.850\n",
      "\n",
      "Test set: Average loss: 1.2932 \n",
      "Accuracy: 5410/10000 (54.10%)\n",
      "\n",
      "Round  16, Average loss 1.293 Test accuracy 54.100\n",
      "\n",
      "Test set: Average loss: 1.2868 \n",
      "Accuracy: 5463/10000 (54.63%)\n",
      "\n",
      "Round  17, Average loss 1.287 Test accuracy 54.630\n",
      "\n",
      "Test set: Average loss: 1.2809 \n",
      "Accuracy: 5476/10000 (54.76%)\n",
      "\n",
      "Round  18, Average loss 1.281 Test accuracy 54.760\n",
      "\n",
      "Test set: Average loss: 1.2757 \n",
      "Accuracy: 5485/10000 (54.85%)\n",
      "\n",
      "Round  19, Average loss 1.276 Test accuracy 54.850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.2706 \n",
      "Accuracy: 5511/10000 (55.11%)\n",
      "\n",
      "Round  20, Average loss 1.271 Test accuracy 55.110\n",
      "\n",
      "Test set: Average loss: 1.2662 \n",
      "Accuracy: 5506/10000 (55.06%)\n",
      "\n",
      "Round  21, Average loss 1.266 Test accuracy 55.060\n",
      "\n",
      "Test set: Average loss: 1.2622 \n",
      "Accuracy: 5523/10000 (55.23%)\n",
      "\n",
      "Round  22, Average loss 1.262 Test accuracy 55.230\n",
      "\n",
      "Test set: Average loss: 1.2588 \n",
      "Accuracy: 5539/10000 (55.39%)\n",
      "\n",
      "Round  23, Average loss 1.259 Test accuracy 55.390\n",
      "\n",
      "Test set: Average loss: 1.2554 \n",
      "Accuracy: 5548/10000 (55.48%)\n",
      "\n",
      "Round  24, Average loss 1.255 Test accuracy 55.480\n",
      "\n",
      "Test set: Average loss: 1.2524 \n",
      "Accuracy: 5558/10000 (55.58%)\n",
      "\n",
      "Round  25, Average loss 1.252 Test accuracy 55.580\n",
      "\n",
      "Test set: Average loss: 1.2492 \n",
      "Accuracy: 5579/10000 (55.79%)\n",
      "\n",
      "Round  26, Average loss 1.249 Test accuracy 55.790\n",
      "\n",
      "Test set: Average loss: 1.2462 \n",
      "Accuracy: 5588/10000 (55.88%)\n",
      "\n",
      "Round  27, Average loss 1.246 Test accuracy 55.880\n",
      "\n",
      "Test set: Average loss: 1.2441 \n",
      "Accuracy: 5586/10000 (55.86%)\n",
      "\n",
      "Round  28, Average loss 1.244 Test accuracy 55.860\n",
      "\n",
      "Test set: Average loss: 1.2421 \n",
      "Accuracy: 5592/10000 (55.92%)\n",
      "\n",
      "Round  29, Average loss 1.242 Test accuracy 55.920\n",
      "\n",
      "Test set: Average loss: 1.2403 \n",
      "Accuracy: 5608/10000 (56.08%)\n",
      "\n",
      "Round  30, Average loss 1.240 Test accuracy 56.080\n",
      "\n",
      "Test set: Average loss: 1.2382 \n",
      "Accuracy: 5624/10000 (56.24%)\n",
      "\n",
      "Round  31, Average loss 1.238 Test accuracy 56.240\n",
      "\n",
      "Test set: Average loss: 1.2379 \n",
      "Accuracy: 5627/10000 (56.27%)\n",
      "\n",
      "Round  32, Average loss 1.238 Test accuracy 56.270\n",
      "\n",
      "Test set: Average loss: 1.2359 \n",
      "Accuracy: 5642/10000 (56.42%)\n",
      "\n",
      "Round  33, Average loss 1.236 Test accuracy 56.420\n",
      "\n",
      "Test set: Average loss: 1.2350 \n",
      "Accuracy: 5647/10000 (56.47%)\n",
      "\n",
      "Round  34, Average loss 1.235 Test accuracy 56.470\n",
      "\n",
      "Test set: Average loss: 1.2335 \n",
      "Accuracy: 5670/10000 (56.70%)\n",
      "\n",
      "Round  35, Average loss 1.233 Test accuracy 56.700\n",
      "\n",
      "Test set: Average loss: 1.2332 \n",
      "Accuracy: 5680/10000 (56.80%)\n",
      "\n",
      "Round  36, Average loss 1.233 Test accuracy 56.800\n",
      "\n",
      "Test set: Average loss: 1.2329 \n",
      "Accuracy: 5677/10000 (56.77%)\n",
      "\n",
      "Round  37, Average loss 1.233 Test accuracy 56.770\n",
      "\n",
      "Test set: Average loss: 1.2326 \n",
      "Accuracy: 5695/10000 (56.95%)\n",
      "\n",
      "Round  38, Average loss 1.233 Test accuracy 56.950\n",
      "\n",
      "Test set: Average loss: 1.2323 \n",
      "Accuracy: 5697/10000 (56.97%)\n",
      "\n",
      "Round  39, Average loss 1.232 Test accuracy 56.970\n",
      "\n",
      "Test set: Average loss: 1.2332 \n",
      "Accuracy: 5698/10000 (56.98%)\n",
      "\n",
      "Round  40, Average loss 1.233 Test accuracy 56.980\n",
      "\n",
      "Test set: Average loss: 1.2326 \n",
      "Accuracy: 5708/10000 (57.08%)\n",
      "\n",
      "Round  41, Average loss 1.233 Test accuracy 57.080\n",
      "\n",
      "Test set: Average loss: 1.2334 \n",
      "Accuracy: 5707/10000 (57.07%)\n",
      "\n",
      "Round  42, Average loss 1.233 Test accuracy 57.070\n",
      "\n",
      "Test set: Average loss: 1.2335 \n",
      "Accuracy: 5714/10000 (57.14%)\n",
      "\n",
      "Round  43, Average loss 1.233 Test accuracy 57.140\n",
      "\n",
      "Test set: Average loss: 1.2338 \n",
      "Accuracy: 5726/10000 (57.26%)\n",
      "\n",
      "Round  44, Average loss 1.234 Test accuracy 57.260\n",
      "\n",
      "Test set: Average loss: 1.2346 \n",
      "Accuracy: 5741/10000 (57.41%)\n",
      "\n",
      "Round  45, Average loss 1.235 Test accuracy 57.410\n",
      "\n",
      "Test set: Average loss: 1.2341 \n",
      "Accuracy: 5750/10000 (57.50%)\n",
      "\n",
      "Round  46, Average loss 1.234 Test accuracy 57.500\n",
      "\n",
      "Test set: Average loss: 1.2346 \n",
      "Accuracy: 5757/10000 (57.57%)\n",
      "\n",
      "Round  47, Average loss 1.235 Test accuracy 57.570\n",
      "\n",
      "Test set: Average loss: 1.2354 \n",
      "Accuracy: 5769/10000 (57.69%)\n",
      "\n",
      "Round  48, Average loss 1.235 Test accuracy 57.690\n",
      "\n",
      "Test set: Average loss: 1.2363 \n",
      "Accuracy: 5755/10000 (57.55%)\n",
      "\n",
      "Round  49, Average loss 1.236 Test accuracy 57.550\n",
      "\n",
      "\n",
      "\n",
      "sigma = 1\n",
      "\n",
      "\n",
      "\n",
      "Learning Rate = 5e-05\n",
      "\n",
      "\n",
      "\n",
      "(sigma, lr)= 1 5e-05 )  0 -th Trial!!\n",
      "\n",
      "Test set: Average loss: 1.8171 \n",
      "Accuracy: 3535/10000 (35.35%)\n",
      "\n",
      "Round   0, Average loss 1.817 Test accuracy 35.350\n",
      "\n",
      "Test set: Average loss: 1.6974 \n",
      "Accuracy: 3888/10000 (38.88%)\n",
      "\n",
      "Round   1, Average loss 1.697 Test accuracy 38.880\n",
      "\n",
      "Test set: Average loss: 1.6392 \n",
      "Accuracy: 4138/10000 (41.38%)\n",
      "\n",
      "Round   2, Average loss 1.639 Test accuracy 41.380\n",
      "\n",
      "Test set: Average loss: 1.5998 \n",
      "Accuracy: 4288/10000 (42.88%)\n",
      "\n",
      "Round   3, Average loss 1.600 Test accuracy 42.880\n",
      "\n",
      "Test set: Average loss: 1.5739 \n",
      "Accuracy: 4404/10000 (44.04%)\n",
      "\n",
      "Round   4, Average loss 1.574 Test accuracy 44.040\n",
      "\n",
      "Test set: Average loss: 1.5548 \n",
      "Accuracy: 4447/10000 (44.47%)\n",
      "\n",
      "Round   5, Average loss 1.555 Test accuracy 44.470\n",
      "\n",
      "Test set: Average loss: 1.5358 \n",
      "Accuracy: 4522/10000 (45.22%)\n",
      "\n",
      "Round   6, Average loss 1.536 Test accuracy 45.220\n",
      "\n",
      "Test set: Average loss: 1.5208 \n",
      "Accuracy: 4587/10000 (45.87%)\n",
      "\n",
      "Round   7, Average loss 1.521 Test accuracy 45.870\n",
      "\n",
      "Test set: Average loss: 1.5064 \n",
      "Accuracy: 4643/10000 (46.43%)\n",
      "\n",
      "Round   8, Average loss 1.506 Test accuracy 46.430\n",
      "\n",
      "Test set: Average loss: 1.4927 \n",
      "Accuracy: 4692/10000 (46.92%)\n",
      "\n",
      "Round   9, Average loss 1.493 Test accuracy 46.920\n",
      "\n",
      "Test set: Average loss: 1.4794 \n",
      "Accuracy: 4738/10000 (47.38%)\n",
      "\n",
      "Round  10, Average loss 1.479 Test accuracy 47.380\n",
      "\n",
      "Test set: Average loss: 1.4668 \n",
      "Accuracy: 4802/10000 (48.02%)\n",
      "\n",
      "Round  11, Average loss 1.467 Test accuracy 48.020\n",
      "\n",
      "Test set: Average loss: 1.4552 \n",
      "Accuracy: 4848/10000 (48.48%)\n",
      "\n",
      "Round  12, Average loss 1.455 Test accuracy 48.480\n",
      "\n",
      "Test set: Average loss: 1.4449 \n",
      "Accuracy: 4892/10000 (48.92%)\n",
      "\n",
      "Round  13, Average loss 1.445 Test accuracy 48.920\n",
      "\n",
      "Test set: Average loss: 1.4359 \n",
      "Accuracy: 4933/10000 (49.33%)\n",
      "\n",
      "Round  14, Average loss 1.436 Test accuracy 49.330\n",
      "\n",
      "Test set: Average loss: 1.4276 \n",
      "Accuracy: 4953/10000 (49.53%)\n",
      "\n",
      "Round  15, Average loss 1.428 Test accuracy 49.530\n",
      "\n",
      "Test set: Average loss: 1.4197 \n",
      "Accuracy: 4977/10000 (49.77%)\n",
      "\n",
      "Round  16, Average loss 1.420 Test accuracy 49.770\n",
      "\n",
      "Test set: Average loss: 1.4120 \n",
      "Accuracy: 5005/10000 (50.05%)\n",
      "\n",
      "Round  17, Average loss 1.412 Test accuracy 50.050\n",
      "\n",
      "Test set: Average loss: 1.4048 \n",
      "Accuracy: 5031/10000 (50.31%)\n",
      "\n",
      "Round  18, Average loss 1.405 Test accuracy 50.310\n",
      "\n",
      "Test set: Average loss: 1.3984 \n",
      "Accuracy: 5036/10000 (50.36%)\n",
      "\n",
      "Round  19, Average loss 1.398 Test accuracy 50.360\n",
      "\n",
      "Test set: Average loss: 1.3920 \n",
      "Accuracy: 5063/10000 (50.63%)\n",
      "\n",
      "Round  20, Average loss 1.392 Test accuracy 50.630\n",
      "\n",
      "Test set: Average loss: 1.3871 \n",
      "Accuracy: 5078/10000 (50.78%)\n",
      "\n",
      "Round  21, Average loss 1.387 Test accuracy 50.780\n",
      "\n",
      "Test set: Average loss: 1.3824 \n",
      "Accuracy: 5080/10000 (50.80%)\n",
      "\n",
      "Round  22, Average loss 1.382 Test accuracy 50.800\n",
      "\n",
      "Test set: Average loss: 1.3785 \n",
      "Accuracy: 5097/10000 (50.97%)\n",
      "\n",
      "Round  23, Average loss 1.378 Test accuracy 50.970\n",
      "\n",
      "Test set: Average loss: 1.3747 \n",
      "Accuracy: 5128/10000 (51.28%)\n",
      "\n",
      "Round  24, Average loss 1.375 Test accuracy 51.280\n",
      "\n",
      "Test set: Average loss: 1.3713 \n",
      "Accuracy: 5148/10000 (51.48%)\n",
      "\n",
      "Round  25, Average loss 1.371 Test accuracy 51.480\n",
      "\n",
      "Test set: Average loss: 1.3680 \n",
      "Accuracy: 5162/10000 (51.62%)\n",
      "\n",
      "Round  26, Average loss 1.368 Test accuracy 51.620\n",
      "\n",
      "Test set: Average loss: 1.3647 \n",
      "Accuracy: 5166/10000 (51.66%)\n",
      "\n",
      "Round  27, Average loss 1.365 Test accuracy 51.660\n",
      "\n",
      "Test set: Average loss: 1.3622 \n",
      "Accuracy: 5191/10000 (51.91%)\n",
      "\n",
      "Round  28, Average loss 1.362 Test accuracy 51.910\n",
      "\n",
      "Test set: Average loss: 1.3603 \n",
      "Accuracy: 5205/10000 (52.05%)\n",
      "\n",
      "Round  29, Average loss 1.360 Test accuracy 52.050\n",
      "\n",
      "Test set: Average loss: 1.3592 \n",
      "Accuracy: 5196/10000 (51.96%)\n",
      "\n",
      "Round  30, Average loss 1.359 Test accuracy 51.960\n",
      "\n",
      "Test set: Average loss: 1.3581 \n",
      "Accuracy: 5209/10000 (52.09%)\n",
      "\n",
      "Round  31, Average loss 1.358 Test accuracy 52.090\n",
      "\n",
      "Test set: Average loss: 1.3566 \n",
      "Accuracy: 5219/10000 (52.19%)\n",
      "\n",
      "Round  32, Average loss 1.357 Test accuracy 52.190\n",
      "\n",
      "Test set: Average loss: 1.3554 \n",
      "Accuracy: 5236/10000 (52.36%)\n",
      "\n",
      "Round  33, Average loss 1.355 Test accuracy 52.360\n",
      "\n",
      "Test set: Average loss: 1.3552 \n",
      "Accuracy: 5248/10000 (52.48%)\n",
      "\n",
      "Round  34, Average loss 1.355 Test accuracy 52.480\n",
      "\n",
      "Test set: Average loss: 1.3554 \n",
      "Accuracy: 5227/10000 (52.27%)\n",
      "\n",
      "Round  35, Average loss 1.355 Test accuracy 52.270\n",
      "\n",
      "Test set: Average loss: 1.3560 \n",
      "Accuracy: 5236/10000 (52.36%)\n",
      "\n",
      "Round  36, Average loss 1.356 Test accuracy 52.360\n",
      "\n",
      "Test set: Average loss: 1.3563 \n",
      "Accuracy: 5248/10000 (52.48%)\n",
      "\n",
      "Round  37, Average loss 1.356 Test accuracy 52.480\n",
      "\n",
      "Test set: Average loss: 1.3557 \n",
      "Accuracy: 5251/10000 (52.51%)\n",
      "\n",
      "Round  38, Average loss 1.356 Test accuracy 52.510\n",
      "\n",
      "Test set: Average loss: 1.3569 \n",
      "Accuracy: 5241/10000 (52.41%)\n",
      "\n",
      "Round  39, Average loss 1.357 Test accuracy 52.410\n",
      "\n",
      "Test set: Average loss: 1.3582 \n",
      "Accuracy: 5238/10000 (52.38%)\n",
      "\n",
      "Round  40, Average loss 1.358 Test accuracy 52.380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.3593 \n",
      "Accuracy: 5240/10000 (52.40%)\n",
      "\n",
      "Round  41, Average loss 1.359 Test accuracy 52.400\n",
      "\n",
      "Test set: Average loss: 1.3601 \n",
      "Accuracy: 5235/10000 (52.35%)\n",
      "\n",
      "Round  42, Average loss 1.360 Test accuracy 52.350\n",
      "\n",
      "Test set: Average loss: 1.3604 \n",
      "Accuracy: 5233/10000 (52.33%)\n",
      "\n",
      "Round  43, Average loss 1.360 Test accuracy 52.330\n",
      "\n",
      "Test set: Average loss: 1.3617 \n",
      "Accuracy: 5247/10000 (52.47%)\n",
      "\n",
      "Round  44, Average loss 1.362 Test accuracy 52.470\n",
      "\n",
      "Test set: Average loss: 1.3629 \n",
      "Accuracy: 5253/10000 (52.53%)\n",
      "\n",
      "Round  45, Average loss 1.363 Test accuracy 52.530\n",
      "\n",
      "Test set: Average loss: 1.3651 \n",
      "Accuracy: 5244/10000 (52.44%)\n",
      "\n",
      "Round  46, Average loss 1.365 Test accuracy 52.440\n",
      "\n",
      "Test set: Average loss: 1.3672 \n",
      "Accuracy: 5230/10000 (52.30%)\n",
      "\n",
      "Round  47, Average loss 1.367 Test accuracy 52.300\n",
      "\n",
      "Test set: Average loss: 1.3681 \n",
      "Accuracy: 5232/10000 (52.32%)\n",
      "\n",
      "Round  48, Average loss 1.368 Test accuracy 52.320\n",
      "\n",
      "Test set: Average loss: 1.3706 \n",
      "Accuracy: 5221/10000 (52.21%)\n",
      "\n",
      "Round  49, Average loss 1.371 Test accuracy 52.210\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "N = 2\n",
    "N_trials = 1\n",
    "N_epochs = 50\n",
    "\n",
    "lr_array = [0.00005] # 0.001 is the bset\n",
    "\n",
    "sigma_array = [0.1,0.3,1]\n",
    "# lr_array = [0.1, 0.01, 0.005, 0.001,0.0005]\n",
    "\n",
    "\n",
    "loss_test_arr_K2_DP_v1 = np.zeros((len(sigma_array),len(lr_array),N_trials,N_epochs))\n",
    "acc_test_arr_K2_DP_v1  = np.zeros((len(sigma_array),len(lr_array),N_trials,N_epochs))\n",
    "\n",
    "\n",
    "for sigma_idx in range(len(sigma_array)):\n",
    "    \n",
    "    sigma = sigma_array[sigma_idx]\n",
    "    \n",
    "    print('\\n\\n')\n",
    "    print('sigma =',sigma)\n",
    "    print('\\n\\n')\n",
    "           \n",
    "        \n",
    "    # print(\"alpha_array: \",alpha_array,'\\n')\n",
    "    \n",
    "    \n",
    "    for lr_idx in range(len(lr_array)):\n",
    "        \n",
    "        args.lr = lr_array[lr_idx]\n",
    "        \n",
    "        print('Learning Rate =',args.lr)\n",
    "        print('\\n\\n')\n",
    "        \n",
    "        X_tilde = np.reshape(encoding_input_array_np, (N,Size_submatrices, 32*32*3)) + np.random.normal(0,sigma,size=(N,Size_submatrices, 32*32*3))\n",
    "        y_tilde = np.reshape(encoding_label_array_np, (N,Size_submatrices, 10))\n",
    "\n",
    "#         X_tilde,a,b = BACC_Enc_Data_v3(encoding_input_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "#         y_tilde,a,b = BACC_Enc_Data_v3(encoding_label_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "        \n",
    "#         print(\"Adjust the power of X_tilde\")\n",
    "        \n",
    "#         for p_idx in range(N):\n",
    "#             tmp_power = np.sum(X_tilde[p_idx,:,:] * X_tilde[p_idx,:,:], axis=1)/np.shape(X_tilde)[2]\n",
    "#             tmp_power = np.sum(tmp_power)/np.shape(X_tilde)[1]\n",
    "#             print(p_idx, tmp_power)\n",
    "            \n",
    "#             X_tilde[p_idx,:,:] = X_tilde[p_idx,:,:] * np.sqrt(avg_power/tmp_power)\n",
    "#             tmp_power = np.sum(X_tilde[p_idx,:,:] * X_tilde[p_idx,:,:], axis=1)/np.shape(X_tilde)[2]\n",
    "#             tmp_power = np.sum(tmp_power)/np.shape(X_tilde)[1]\n",
    "#             print('power after adjusting =',tmp_power)\n",
    "#         print()\n",
    "        \n",
    "        m = N # m is the number of received result @ master\n",
    "    #     print('number of results:',m)\n",
    "\n",
    "        for trial_idx in range(N_trials):\n",
    "            print('(sigma, lr)=',sigma,args.lr,') ',trial_idx,'-th Trial!!')\n",
    "\n",
    "            net_glob = CNNCifar(args=args)\n",
    "            net_glob.cuda()\n",
    "            net_glob.train()\n",
    "\n",
    "            # copy weights\n",
    "            w_glob = net_glob.state_dict()\n",
    "\n",
    "            for iter in range(N_epochs): #args.epochs\n",
    "                w_locals, loss_locals = [], []\n",
    "                idxs_users = np.random.choice(range(N), m, replace=False)\n",
    "                idxs_users = np.sort(idxs_users)\n",
    "                \n",
    "                dec_z_array = []\n",
    "                for idx in idxs_users: #for idx in range(N):\n",
    "            #         print(idx)\n",
    "                    local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[idx,:,:], label=y_tilde[idx,:,:])\n",
    "#                     w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "                    w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                    w_locals.append(copy.deepcopy(w))\n",
    "                    loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                    dec_z_array.append(z_array[idx])\n",
    "             \n",
    "                                \n",
    "                w_glob = copy.deepcopy(w_locals[0])\n",
    "                for k in w_glob.keys():\n",
    "                    for G_idx in range(1,N):\n",
    "                        w_glob[k] += w_locals[G_idx][k]\n",
    "                    w_glob[k] = torch.div(w_glob[k], N)\n",
    "\n",
    "                # copy weight to net_glob\n",
    "                net_glob.load_state_dict(w_glob)\n",
    "\n",
    "                # print loss\n",
    "            #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "            #     loss_train_arr.append(loss_train)\n",
    "\n",
    "                acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "                acc_test_arr_K2_DP_v1[sigma_idx][lr_idx][trial_idx][iter] = acc_test\n",
    "                loss_test_arr_K2_DP_v1[sigma_idx][lr_idx][trial_idx][iter] = loss_test\n",
    "                \n",
    "#                 PATH = \"./save_models/CIFAR10_LeNet_BACC_PowerAlign_K2_sigma_E50_iter\"+str(iter)+\".pt\"\n",
    "#                 torch.save(net_glob.state_dict(), PATH)\n",
    "                \n",
    "                if iter % 1 ==0:\n",
    "                    print('Round {:3d}, Average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_test, acc_test))\n",
    "                #print(loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filehandler = open(\"./plot/CIFAR10_LeNet_K2_N2_FedAvg\",\"wb\")\n",
    "pickle.dump(acc_test_FedAvg,filehandler)\n",
    "filehandler = open(\"./plot/CIFAR10_LeNet_K2_N2_T3_G1\",\"wb\")\n",
    "pickle.dump(acc_test_arr_K2_G1_v3,filehandler)\n",
    "filehandler = open(\"./plot/CIFAR10_LeNet_K2_N2_T3_G1_PowerAlign\",\"wb\")\n",
    "pickle.dump(acc_test_arr_K2_G1_powerAlign,filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEGCAYAAABiq/5QAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOydd3hcxbn/P7N9tavei2W5yA13GRsMBpveS8AkhHCB0C4JJCQkARIICbkkhEuSm0J+QCghNNMxNs0E7BhjY3Dvtlzkprpqq5W27/z+GFmyrOKVrLUkaz7PM8/ZPXvOnHfO7n5nzjsz7wgpJRqNRqMZPBj62gCNRqPRHF+08Gs0Gs0gQwu/RqPRDDK08Gs0Gs0gQwu/RqPRDDJMfW1ANKSlpcmCgoIendvY2IjD4ehdgwYAutyDj8Fadl3uzlm9erVLSpl+5P6YCb8QYjTw2mG7hgO/BJKAW4Gq5v0/l1J+0FVeBQUFrFq1qkd2LFmyhNmzZ/fo3IGMLvfgY7CWXZe7c4QQezvaHzPhl1JuByY3X9wIHATeAW4C/iSlfDxW19ZoNBpN5xwvH//ZwC4pZYe1j0aj0WiOH+J4zNwVQjwHrJFS/k0I8SvgRsANrALukVLWdnDObcBtAJmZmUXz5s3r0bU9Hg9Op7OHlg9cdLkHH4O17LrcnTNnzpzVUspp7T6QUsY0ARbABWQ2v88EjKinjUeA546WR1FRkewpixcv7vG5Axld7sHHYC27LnfnAKtkB5p6PFw9F6Ja+xXNFU2FlDIspYwA/wCmHwcbNBqNRtPM8RD+a4FXD70RQmQf9tmVwKbjYINGo9FomonpOH4hRBxwLnD7YbsfE0JMBiRQcsRnGo1Go4kxMRV+KWUTkHrEvutjeU2NRqOJNd5AmIUbSpmYl8TorPi+NqfbDIiZuxqNRtNfWLytkgfnb+JArReDgG+enM+Pzx1Fery103PW7a/jH0t3U1LdyLjsBMbnJjI+N4Gx2QnEWTqX4UhEIgGjQfRqGbTwazSaHrNmXy0vfbmXG2cWMDEvqcf5SCnZeLCe4goPQ1PjGJbmIMVhQYjeFTx/KIxBCMzG7ndvltV7eXjBFj7cVM7IDCfP3TiNz4tdvLhiLwvWl/K9OSP47mnDsJmNLWVasr2KJ/+zi5V7akiwmZiYl8Rn2yp5Y/UBAISAEelOClId+ENhGnwhPP4QnkNbf4iXbp7B6YVpvXoftPBrNJpus7+micc+3s6C9aUAvL+hjEevmsCVU/K6lU9lg4931x7kzdUH2FHhafNZgs3EsHQnw1LjGJbmZHi6g2FpKjms7aVLSkmVx8/+mib21TRRWuejvN5HWb2PcreX8nofLk+ARLuZyyblcHVRHhPzEo9auYTCEf61Yi9/WLSdUETy0/NHc+us4VhMBs4ak8n1pwzltx9s47GPtvPyl/v42QWjCUckT/1nN9srGshOtPHAxWP51vR8nFYTUkrK3T42HXSz6WA9m0vr2V/TRJzVSILdTG6SHafVhNNmwmk1kZts79Y9jQYt/BpNH7G1zI0vGGZSXhKGY3yUl1JS54+wcnc1e1yN7HE1stvVyL7qJlIcFgoznRRmOBmZEU9hppPUHramG3xB/r5kF88u24NBwA/OLmRuUR73vLGeH722nq1lDdx7wZguXRP+UJjPtqpW7392VBGOSKbmJ/HbKycwfVgy+2u97KlqbCnH1yW1vLuutE0emQlWhqU5GJIcx879Pn63din7aprwBsNtjkuKM5OVYCM70caE3CSyEmzsdnl4fdV+XvxyL4UZTq4uyuPKKblkJNiQUlLdGFDXrlL38D87qtha5ubMUen85vLx5KfGtbnG8HQnz9wwjeU7Xfzm/a38cN46AEZlOvnD3ElcOikHi6n1CUMIQXainexEO+eOy+z2d9AbaOHXaI4zUkqeXbaHRz7YipSQEW/l3HGZnHdSFqcOT20jEp3hD4XZeKCelXtq+GpPDWv21dLgC8HiLwGwmAwUpMaRnxJHdWOAd9YcpMEfajk/Oc5MmtOK0SAwGQVGgwGTQWA0CGxmI2kOC2nxVlIdFlKdVlKdFg7Uevnzv3fg8gT4xpRcfnL+aHKSVGv05Vtm8OsFm3l66W62lzfwl29NITHO3Mbm4ooG5n29n7fXHKC2KUhWgo3bzxjOVUV5jEhvnYE6MiOeOaPbltcbCLO3plWMd1c1ssflYcmOKqwywtj8OE4vTCM/RZV5SIqdnCR7p/5zty/I+xvKeHP1AX734TYe+3g7hRlOSuu8uH2t98lsFIxId/L366Zy4fisLivLmSPTWHjX6SzaXI7dYuTMUem97qrqLbTwa05opJQs2VHFm6sPYDUZyE60kZVoJzvBRnaSjexEO4l2c6ctVCkltU1B9rg8zWLTSG1TgES7hVSHhWSHhRSHmeQ4C5kJthYh7IxgOMIv52/m1a/2ceH4LM47KZNPtlTwztqDvLxyH/FWE7PHZDB5SBIdmVTbGOCrkhrW7qvDH4oAqmV56aQcDO5yzjt1MsPSHOQk2duUSUpJhdtPcWUDxRUeiis91HsDhMKScEQSihzaRqhrCrCr0oPL42+5xiFOLkjmuRtPbufPNxsN/M8VExibncBD8zdzxd+/4B//NY2cJBsLN5Tx2tf7Wb23FrNRcN64LK45eQinj0yLutPSbjEyJiuBMVkJ7T5TUSrbRyXoigSbmWun53Pt9Hx2VXl4a/UBNpe6ObkgRbmT0h0MT3OQm2TH1I3+AKNBcOGE7KMf2Mdo4df0OcUVDdQ0BrCajVhNBpWaXyfZzd364x0iGI6wYH1pi581zWnFajJQ7vYRjrSPT+WwGFt8qk6bmXirCY8/xB5XI/XeYMtxJoMgKc5MvTdIMNw+nzNHpXPXWSOZVpDS7rP6piDfe2U1X+ys5vtzRnDPuaMxGARXTsnDFwzzxU4XizZX8O+tFS2+8yMxCBiXk8B1M4YyfVgKJxckk+pUo0mWLKnmjFHtQq8Dyr2QlWgjK9HGrMKOjzkSKSWNgTDVHj8uTwCAqflJXbZir5sxlMKMeO54aTVXPPEFAB5/iBHpDh64eCxXTsltsbe/MCLdyc8uGNPXZhxXtPBr+ozyeh+Pfri1nf/2cOJtJk4bkcasUWmcUZjOkJS4To8F8IWUG+XZz3dTWu9jdGY8f7xG+VnNRgPhiKTa46fsUKdfvZc6b7BlFEVD84iKBl+QOIuRSyZmMyzN0dyx6CQv2Y7ZaEBKiccforYxSE1TgNrGAJsO1vP88hKufnIFpwxP4QdnFXLqiFSEEJS4GvnuC1+zv6aJx+dO4uqitp2gNrORs8dmcvbYTMIRSYMv2GH5rCYjdoux+ze7BwghVEVoNTE0NfqFTqYPS+G9u07n4QWbcVrNXDt9CEVDk/ut22MwooVfc9zxh8I8t6yEv35WTCgiuXPOSE4dkYo/FMYfjOAPRfCHwviCEbaVu1m6w8VHm8sBKEiNY1ZhOqOz4nH7gtQ2BqhpDFLbFKCmMcD2sia8oS1MH5bCI1dOYPbotn5Wo0GQkWAjI8HGpCE9L4MQgnibmXibuaWzb86YDG6eNYxXVu7j6aW7+fYzK5man8Tlk3P50793APDSzTOYMTy1q6wxGgRJcZaeG9cPyE2y89T13XO/aI4fWvg1x5XPtlXw8IItlFQ3cd64TB64eFy7URJHIqVkt6uRz3dU8Xmxi7fWHKApoEZvxFmMJMdZSG72s5+cZeLuy6YzNT/5eBSnHXEWE7fMGs53ThnKG6sP8OSSXTz03maGpzt47oaTKUgbfEsEavofWvg1XVJc0cCuqkbqvQHqmoLUNgVbXjf4QjQGQjT5w2obCNPoDxEMR3BaTSTYzSTYzCTYTSTalV/8y901DE938K/vTu/UH30kQqiRFSPSndx42jACoQjVjX6S4ywtk2UOsWTJkj4T/cOxmY1cf8pQvjltCJ8XVzGtIIVEu/noJ2o0xwEt/Jp2HKht4r31pby3rpRt5Q1tPjMbBYl2C0lxZuKbO0Mz4q04LCbirEYcFhNmowGPP4TbG8TtC+L2hthb3UQgFOEXF43lhpkFUQ1Z7AyLyUB2Yu9PaokFFpOBs8f2zVhtjaYztPBrAKj2+PlgUznz1x5k1V61IFrR0GQevvwkpuYnk+ywkGQ3E2cx6k46jWaAo4V/ENLgC7K51M3GA/VsOFjPxgN1lFQ3AWpM+E/PH81lk3KOOoJGo9EMTLTwn6D4w5Liigb2Vqu4Jftqmthf06Smolc3cmip5dwkOxPzEpk7bQhnjclgbHb7CTIajebEQgv/CcbOygb+9+PtLNrchPxkact+h8XIkJQ4CjOdXDEll4l5iUzITex3k2k0Gk3siZnwCyFGA68dtms48EvgX837C1ArcF0jpayNlR0DnWqPn/95fyvhiOSSidmcOTodq6n9BJ6DdV7+75MdvLXmAHEWExcOM3P+jJNaYpfEIsStRqMZmMRM+KWU24HJAEIII3AQeAe4D/hUSvmoEOK+5vf3xsqOgcyGA3X894urcTUGcFiMvLe+lHibiQtOyuLSSTnMHJGK2xfi74t38q8v94KEm04bxvfnjGTD18uZPTm3r4ug0Wj6IcfL1XM2sEtKuVcIcTkwu3n/C8AStPC34/VV+3ng3U2kO6289d8zGZMdzxc7XSxYX8ZHm8p5Y/UBUh0WAqEIjYEQV03N4+5zR5F7lCBhGo1GI6RsH2iq1y8ixHPAGinl34QQdVLKpMM+q5VStptxI4S4DbgNIDMzs2jevHk9urbH48HpdB79wH5CKCJ5eWuAxftDjEs1cMckG/GWti6aQFiy0RVmZVkIg4BLh1vIjW87Ln6glbu3GKzlhsFbdl3uzpkzZ85qKWX72BlSypgmwAK4gMzm93VHfF57tDyKiopkT1m8eHGPzz3elNV55RVPLJND710of/fBVhkMhXuc10Aqd28yWMst5eAtuy535wCrZAeaejxcPReiWvsVze8rhBDZUsoyIUQ2UHkcbOjXSCl5b30pv1m4haZAmL9fN5WLBkBMb41GMzA5HsJ/LfDqYe/fA24AHm3ezj8ONvRbdlQ08OC7m1i5p4aJeYk8PncSozLj+9osjUZzAhNT4RdCxAHnArcftvtR4HUhxM3APmBuLG3or3j8If7yaTHPLduDw2rit1dO4JsnD4l6RSKNRqPpKTEVfillE5B6xL5q1CifQUkkIvlgUxm/WbiFCrefb508hJ9dMIYUx8COv67RaAYOeubucWJ/TRNvrTnAW2sOsL/Gy0k5Cfy/7xT1ixDCGo1mcKGFP4Y0+kN8uKmcN1fv58vdNQgBp41I4yfnjeaSiTnaraPRaPoELfwxQErJU0t385dPi2kKhClIjeMn543iyql5eoKVRqPpc7Tw9zKN/hA/fXM9H2ws59xxmdx2xnCm6YWmNRpNP0ILfy+yx9XI7S+uYmelh19cNJZbZg3Tgq/RaPodWvh7icXbK/nBq2sxGQT/+u4MTi9M62uTNBqNpkO08B8jUkqeWLyTP3yyg7FZCTx1fZFeuUqj0fRrtPAfA6FwhB+/vp731pdyxeQcfveNidgt7WPlazQaTX9CC38PCUck97yhRP+n54/me7NHaH++RqMZEGjh7wHhiOSnb6xn/rpS7r1gDHfMHtHXJmk0Gk3UGI5+iOZwIhHJfW9t4O21B/nJeaO06Gs0mgGHFv5uEIlIfvHuRt5YfYAfnl3InWcV9rVJGo1G02208EeJlJJfvreJV7/az51zRnL3OVr0NRrNwEQLf5Q88v5WXvpyH7efOZx7zhulO3I1Gs2ARQt/FOyoaOCZZXu4bkY+910wRou+RqMZ0MRU+IUQSUKIN4UQ24QQW4UQpwohfiWEOCiEWNecLoqlDb3B00t3Yzcb+cl5o7XoazSaAU+sh3P+GfhISnm1EMICxAHnA3+SUj4e42v3CuX1PuavO8i3p+eTrBdL0Wg0JwAxE34hRAJwBnAjgJQyAAQGWov5n8tLCEckt8wa3temaDQaTa8gpJSxyViIycDTwBZgErAa+CHwU1Rl4AZWAfdIKWs7OP824DaAzMzMonnz5vXIDo/Hg9Pp7NG53pDkx0uamJBm5HuTbT3Ko684lnIPZAZruWHwll2Xu3PmzJmzWko5rd0HUsqYJGAaEAJmNL//M/AbIBMwovoXHgGeO1peRUVFsqcsXry4x+f+Y+kuOfTehXL9/toe59FXHEu5BzKDtdxSDt6y63J3DrBKdqCpsezcPQAckFKubH7/JjBVSlkhpQxLKSPAP4DpMbShxwTDEZ5btocZw1KYmJfU1+ZoNBpNrxEz4ZdSlgP7hRCjm3edDWwRQmQfdtiVwKZY2XAsLNxQSmm9j9vP1L59jUZzYhHrUT13AS83j+jZDdwE/KXZ/y+BEuD2GNvQbaSUPL10D4UZTmaPyuhrc/otUsoTdnirlJLGYCOeoKd1G1Bbs8HM1MypJFoT+9pMjaZHxFT4pZTrUL7+w7k+ltfsDZbtdLG1zM1jV0/EYDgxha2nhCNhPt33Kc9sfIbi2mIyHZlkObLIdmST7cgmy5FFliOLRGsiiZZEEq2JxFviMRn6byDYam81O+t2srNuJ8W1xeys28muul14gp5OzzEIA+PTxjMzZyYzc2YyIW1Cvy6jRnM4+pfaAU8v3U1GvJXLJ+f0tSkxJyIjbK3eylflX5ERl8HUjKlkO7PbHReMBHl/9/s8u/FZStwlFCQUcN3Y63D5XJR5ylhTsYaKpgrCMtzhdeLN8aTYU5gzZA5XFl7J8MS+daHtc+9j/q75LNy1kNLG0pb9idZECpMKuWT4JeQ6c3FanDjNThxmB06L2jYEGlhRuoIVpSt4esPTPLn+SZxmJ9MypzE0YSjZzuw2lWGSNYk6fx176ve0JvceSupLsJvsDE8azojEEYxIGsHwpOEMiR+C2WA+5jJ6Q16212xvKVeiNZEES4KuoDRa+I9kc2k9nxe7+NkFo7GaTszVtNwBNytKV/D5gc9ZdnAZ1b7qNp9nObKYkjGFqRlTmZwxmbWVa3l+0/OUNZYxJmUMj5/5OOfkn4PR0Pb+hCNhqrxVVDRVUO+vp95fjzvgbnl9wHOAl7a8xD83/5OpGVO5atRVnDv0XOwme6+US0pJQ7iBQDiAxdh+sl1jsJGPSz5m/s75rKlcg0EYODXnVK4bex2FyYUUJheSakuNyn1VlFnEnVPupN5fz8qylSwvXc6ayjWsKFuBP+xvc6zJYCIUCbW8txgsDE0cyuiU0fhCPjZUbeDDPR+2OT7NnoZJmDAZTBiEAaPBiEmYsJvs5CfkMzRhKAUJBQxNGMqQ+CHYTDZqQ7V8VPIR6yvXs65yHdtqthGSIY7EaXaSaE1kVPIoLhx2IWfmnUmcuf8sFxqKhKjx1VDVpH5LVU1VNIWaOCn1JCakT+j09yKlpLiumKUHlrLs4DJMBhOFSYWMTBrJyOSRjEwaicPs6JYtUkrcATcVTRVUNFaobVMF5Y3lVHmriDPFkWZPa5NS7akMSxjWr+7pkcRsHH9vMm3aNLlq1aoenbtkyRJmz54d9fF3z1vLJ1sqWH7/2STaj73V1Zv4Qj621Wxjo2sjG10b2eTaRCgSYkLaBCamT2RS+iTGpo7FarS2lNsf9rO7bjc7anewvXY7m12bWV+1nrAMk2BJ4LSc05iVN4tTc07F5XWxpmINayvXsqZiDZXeypZrT8mYwq0TbuX03NOPya/v8rp4b9d7vF38Nnvde3GanVw8/GLOLzifyRmTe9TSLfWU8sGeD1i4ayG76ncBEGeKI9mWTJI1iSRbEmaDmZVlK/GGvBQkFHD5yMu5dPilZDoye1yWjpBSUuuvpayxjHJPOWWNZVQ2VSoxSBzGsMRh5Dhy2lWaTcEm9rj3sLtuNzvrduLyuojICOFImJAMEY6ECcswnqCHfe59VHmrWs4VCBKsCdT76wGwGW1MSJ/A5PTJTEyfiMlgaql86wP1uP1uanw1rCpfRaW3ErvJzpl5Z3LBsAuYlTurpdI8JHrljeUtAlwfqG9XqTcFmxiXOo6ZOTOZnj2deEt8t+5ZmaeMzw9+zucHP2ezazPVvmoiMtLhsSaDifGp4ynKLKIos4hxqeN4bclr1CTXsPTAUsoaywAYmzIWozCyq34X3pC35fwcRw7ZzmxsJht2ox27yY7NZMNmsiEQ1PpqqfHVUOOrodpbTY2/pk2lfeh+p9vTSYtLwxvy4vK6aAg0tLNzSsYUZubM5NScUxmbMhaDaDuW5tBvZZ97H6WeUmr9tdT566j1qW2dr45afy0PnvIgkzMmt7sX0WibEKLDcfxa+A+jrN7L6b9fzI0zC3jwknE9ul5vs71mO/N3zWdV+SqKa4tbWnCZcZnqTy1MbHBt4KDnIKB+cGOSx2DymmgwN1DiLmlxv1iNVgqTCjk151Rm5c3q0i8tpeSg5yDrqtaR48hhaubUXi2XlJLVFat5u/htFu1dhD/sJ94c32Lb6bmnk2ZP6/R8d8DNJyWfsHD3QlZVqN/G1IypDAkMIX9Yfsufp9ZfS52vDk/Qw/Ss6Vw+8nImpk0c8J3SjcFG9rr3ste9lxJ3CRWNFeCCuTPnMiplVFQVaDgSZk3lGj7a8xGL9i6izl9HvDmeUSmjcHldVDRW4Av72p1nMphIsia19OGYjWY2Vm2kKdSEURiZkDahRfCyHFkYhAGDMCAQLdviumIl9gc+Z2fdTgBynblMy5xGtjObdHs6GXEZpMelk2HPwGK0sKFqA6sqVrG6YjWbXZvbPM3YTXZOyT6FM/POZFbeLDLi1KCMiIxw0HOQnbWtfTiV3kr8IT++sA9vyIsv5MMX9hGREVJsKR2mTEcmmXGqPyvVntru/vrDfqq91bi8LqqaqljvWs/yg8vZXqtcbcnWZE7JOYVcZy77G/azz72P/Q372/UjCQSJ1kSSrEktDZfbJ97OSWkntfsetPB3QXeE/w+LtvO3xTtZ+tM5DEnpu8e0en89H+z5gHeK32FrzdaWUSQT0yYyPm0849PGt/ywD+HyuthYtZH1VevZ4NrAHtcexmeNZ1TKKEYlq5Qfn9+updkfaAw28mXply1CcOhJY1zqOIYmDFV/zOY/py+k/qylnlICkQAFCQVcOuJSLhp2EXnxed1+wjuROJayByNBvir7ig/2fMD+hv1kxGWQGafE7pDoZcRlkGRNwm6yt6s4g5EgG6o2sLx0OStKV7DJtQlJ19piEiaKMouYlTeLWXmzGJYwLOoKuSnYxEbXRjZXb8a738st592C1WjtUdljicvraukPWl66nDp/HbnOXIYkDCE/Pl+lhHxyHDmk2FNItCRG/R/Vwt8F0f4ZAqEIMx/9jEl5iTx748k9ulY0RGSEbTXbCIQDRGSEiIwgkURkBE/Qw8clH/Pp3k8JRAKMTh7NlYVXcvGwi0mydW8S2UAVQCklO2p3tPhpXV5Xm8dxu1G9zozL5MJhFzIudVwbsRio5e4N+lPZ63x1rKpYRb2/nggRpJQtv/eIjJDtyGZG9gyclmMPtdCfyt0VUkrCMtxrnevHIvy6c7eZjzaX4/L4uf7UoTHJX0rJitIV/GnNn9hWs63T4xIsCVw16iquHHklY1PHxsSW/owQgtEpoxmdMppbJ97a1+ZoekiSLYlzhp7T12b0K4QQmET/kNz+YUU/4MUVJQxNjeOMwvRez3uzazN/WvMnVpatJNeZy69n/pqsuCyEEG38nyaDqaVzVqPRaGKFFn5ga5mbr0tq+cVFY3t1wtY+9z7+uvavfFTyEcnWZO6bfh9zR83tcKihRqPRHC+6FP7muDrfBGYBOYAXFVvnfWCRHAgdBFHw4pd7sZoMzJ2Wd8x5hSNhVpavZP7O+SwqWYTZaOa2ibdx00k39Yo/U6PRaI6VToVfCPEPYDhK5P8MVAI2YBRwBfCQEOJnUsplx8PQWOH2BXl37UEum5RDUlzPW+J73XuZv3M+C3YvoLyxnHhLPNeMvoZbJtxCelzvu480Go2mp3TV4v+blHJ9B/vXAa8LIWxAfmzMOn68vfoATYFwjzp1q73VfLrvUxbuXsjayrUYhIGZOTO5Z9o9zBkyR/vqNRpNv6RT4e9I9IUQQ4E4KeVWKaUP2BFL42KNlJIXv9zLpCFJUcfcd3ld/Hvvv/lk7yesqlhFREYYljiMHxX9iEuGX9JufL1Go9H0N6Lu3BVC3IuKtBkRQnillDfGzKrjxIpd1eyqauTxuZO6PC4iI8zfOZ/5u+azpmINEsmwxGHcOuFWzh16LqOSRw34maAajWbw0JWP/w7gqeaVskCtnjW3+bMNx8O4WPPil3tJijNzycT20SgPUdlUyQPLHmBF2QpGJo3kjsl3cN7Q8xiRNOI4WqrRaDS9R1ctfi/wkRDiT1LKD4FPhRCfAQL4NJrMhRBJwDPAeNTCK98FtgOvAQWohViukR0sth5ryuq9LNpSwS2nD8Nm7niK9Gf7PuOh5Q/hC/n45am/5OrCq3XLXqPRDHg6XXpRSvlP1OidU4QQ7wDLgcuBq6WUP4oy/z8DH0kpxwCTgK3AfcCnUspCVAVyX8/N7zmvfrWfiJRcN6N9p25TsImHVzzMDxf/kGxHNq9d+hpzR83Voq/RaE4IjubjHwK8APiB/wF8wEPRZCyESADOAG4EkFIGgIAQ4nJgdvNhLwBLgHu7Z/axEQhFePWrfcwelU5+attgbFuqt3Dv0nvZ697LTeNv4q7Jd2E29q/wzBqNRnMsdBqkTQjxLOAA7MAWKeX9QohpwK+BZVLK33WZsVpX92lgC6q1vxr4IXBQSpl02HG1UsrkDs6/DbgNIDMzs2jevHk9KB54PB6czrYTp9ZWhvjzGj93T7UyOaO17nMFXTxS+ggOo4PrU69ntH30kdkNGDoq92Cg18sdCmGor8dYW4ehrhZjbR3C60XGxRFxxBFxOJBxDiKOOKTDQcThAGPfREDV3/ngIppyz5kzp9tB2qZJKScBCCHWAvdLKVcBFwshrorCLhMwFbhLSrlSCPFnuuHWkVI+jao4mDZtmuxp9L2OItgVL90NbOWmS89ss9jKE+ueIFwa5vUrXifHObCXXRwoEQt7goxEkD4fwt4+PA7mwDsAACAASURBVPCR5ZZSIr1ewnV1hFwuQpWVBCsrCVVWEqqsIlRZScTnhVAYGQ4jQyEIhZDhMOEGN2FXNXRngroQGJOSMKWlYkxJxZSagjE5BWEyIiMSIhFkJAzNr4XVisHpwOh0YnA6MTicGJwOhMUC4TAyFEaGQy2vCYeQwWD7FA6zu6qKsafOxJSehilNJUNc5+HFZThMpKGBcEMDYbdbvXa7IRLBmJKCKTUVY0oKxsREhKFTr3CfcyL/1rviWMrdlfD/u7kz14LqjG1BSvlWFHkfAA5IKVc2v38TJfwVQohsKWVZc0iIyk5ziBGVDT6sJgMJttbiSylZsGsBM7JnDHjRP9EIVVXh3bgR7/oN+DZuwLtxE5GGBjAaMcbHY4iPV9uEBJIaGyl55hnCdXWE6+uJ1NUjg8H2mRqNShzT0zE4HAiLFUxGhNGEMBnBaMLgdGDOzMKcnYUpMwtzViamrCwMDgcRj4dwfT3hunq1ra8jXFtHuKaGUE014eoaQjU1+LZsJVRbC+EwGAyqojIY1FOBABkIqrJEOl5xqjskAAfntfmrIuLiEGazyj+iwiO3vA4EosvYaMSYkowpNQ1TZgbmzCxMWZmYs7LVPcnMxJiYiCEhAYOl89nvUkoijU1EGtwIiwVjSoruN+sjuprAdY8QIgUISynru5uxlLJcCLFfCDFaSrkdOBvl9tkC3AA82ryd3zPTe05Vg5+MBGubH93ayrUc9Bzke5O/d7zNGbRIKQlVVOAv3knI5VLiWV9P5DBB9e/eTahMLaeH0Yh19CgSLroIc14ukcZGIm7VYo243YQbGjDU1SKyc7AOG44xKVEJUqLamtLTMWVkYM7IUKJzDC4ZY0ICxoQEGDKkV+6D9PlUZeLxEPE0IgN+hMkEzRWRMBpbX1ssCLO5TUIIPn//faaPGkWoykWoqkrd02qXelIwGBAGAcLQ8lrY7BgT4jHEJzRv41WZhFAVWHUN4Zrq1m1lFcHKSnwbNhKu7XggnrDZWipho9OJDAbbfD+HV3DCZsOcm4s5Nwdzbi6W3FwsBQXYJ03ClK7DnMSSrsbxfwt4rbNAbEKIAiBHSrm8i/zvAl4WQliA3cBNqJFErwshbgb2AXN7ZnrPqWzwk+5sG05hwe4F2E12zsnXMcR7GxmJEKqqIlCyF39xcZsUaWi7VikGgxLVxEQMSYnETZmC/cYbsE2YiG3sGAz2rhdmX7JkCRMH2GO/EAJht2Ow249J8CIJCdjGjIExvWhcZ9fy+5W7rLycYEUlYXe9End3A5GG1q0wW7CMGIExIQFDQjzG+AQM8U6kz0/w4MGW5Fu/gXB9a/vSPGQI9imT1fc/ZQrWwsJjqqg1benK1ZMLrBVCfIXqmK1CBWkbiRqV4+Yoo3GklOtQs32P5OyeGNtbVDX4GZHe2iniD/v5uORjzs4/mzhz3y252N+JeL0E9uzBv2sX/p27CNfUIOw2DDY7BrsN0bxFSgL7DxDYt5fg3n0E9u9H+lrXbjUkJmItHEnipZdgLSzEOnIkpqwsJfZOZ7/2J2sUBqsVy5AhWHrhiecQYY8Hf3Ex3rXr8K5dS+PyFbjfWwCAsFpb+huMiYkYk5JaXtvd9biDQcxZWZgyMzGlpenf0FHoytXzh+YO2XOB04DpqEldW4GbpZR7jo+JvU9lg59Thqe2vF96YCkNgQYuHX5pH1rV/wjX1VH7xht4V6/Bv2sXwQMHWjs6TSaMSUnKReH1Kh/2YQirFUv+EMxD8nGcdhqWofmY8/OxjizElJGufbuadhidTuKmTCFuyhTgJqSUBA8exLt2Lb4tW1v6bcL19fh37WzpY0kIBjn42uutGZlMmDMyMOVkY8nNbXYnHZYyM1Xn+SCmy3H8UsqQEGJF88zdEwJfMEy9N0hGfKurZ8GuBaTb05mRPaMPLes/BA4coOafL1D31ltIrxdr4UjsE8aTeMXlWEeMxDpyBJb8/DZ/HhkMEvH5kD4fMiIxpetWl+bYEEJgycvDkpdH4qUdN8qklCxduJAZI0YQLK8gVFGutuVlBEvLaPp6FcEFC9t1nhtTUjBlZGDKOKzfJzVV9U8cNrrK6HSq/oqkpBOqsRJNkLbVze6e56WUi2JtUKxxefwApDcLf52vjs8Pfs51Y66LenX7ExXvxo1UP/ccDR8vAqORxEsuIeXGG7GNHnXUc4XZjNFshvj442CpRqMQQiDj47GNG4dt3LgOj5HBIMGKCoIHDhI8eIBgeXnLUN5QZSX+rdsIVVd3ObLK4HBgKSjAMmyY2hYUYBk6FGNCPMJmQ1gsGKxWhNU6IPoiohH+QuB84FYhxBPAq8ALUspdMbUsRlQ2KOHPSFDC/1HJR4QiIS4dMXjcPDIQIHDgAIGSEgJ7SgiUlODbtg3fxo0Y4uNJvfm7JH/nO5gzM/vaVI3mmBFmc8uTA3T8VC9DIcJ1dc0jqxqJeDxEGj3qfV0dgX37CezZg3fNGtzvv9/13A6TCWNyEubsHMzZ2SrlZGPKzkaYzapDvLSMYFkZwbJSQqVlhOrqMFgsqpPfZmvtO7PZSL/7h9gnTuzVe3JU4W+Ozvkh8KEQYjbwMvCj5qeA+6WUX/WqRTGm6pDwx9sA5eYpTC5kdMrAnaUbDZGmJmpfe526118nsHdvm9aNMSUFS0EBGffdS9LVczE6HX1oqUZz/BEmE6a0NEhLO+qxEZ+PwL59BPftU0OK/X6kP4AM+NVrn59wbQ3Bg6X4d+zA85//tBncAKh+iKwszNnZxJ08DWNyCjIQaHaXeol4fUR8XiJNTd2bQBglRxX+5gib1wH/BdQCPwLeAYpQE7uG9bpVMeRQiz893kpJfQkbXBu4p+iePrYqdkQaG6l99VWqn3uecE0NcdOmEX/B+VgPPbYOHYoxMbGvzdRoBgwGmw3bqFHYRh3dBQqqHyJcV0ewtBSCQUzZOZjSUvvUJRSNq+dr4BVU+OS9h+3/snld3gFFVYMfISDVYeHJDQsxCAMXDb+or83qdYTXi+upp6l5/nnCdXU4TjuNtO/dQVxRUV+bptEMKoQQmJKTMSW3C0nWZ0Qj/KMPW4ylDVLK3/ayPTGnqsFHqsOC0SBYuHshM7JmnDDLJUZ8PppWrqRhyRLS5r9HVVMTjjPPIP2OO7BPntzX5mk0mn5CNML/gRDiW1LKOgAhRDLwkpTy4tiaFhsq3X7S420tIRq+P/n7fW3SMRGsqMCz5D94liyhccUKFbwsLo7AmDGMuf8+7BMm9LWJGo2mnxGN8GcdEn0AKWWtEGLARjGr8vhJj7e2hGg4O79PJxH3GH9xMRW/f4zGZcsAMOfmknTVVTjnzCFu+sksXb5ci75Go+mQaIQ/LITIk1IeABBC5MfYpphS6fYzIsM6YEM0hGprcf31r9TOew1DfDxpP7iLhHPPxTJy5Ak1wUSj0cSOaIT/l8AXzSGaAeYAd8TOpNgRiUhcHj8h63Ya6gZWiAYZCFDzyiu4nvg7kaYmkq+9lrQ7v9+vOow0Gs3AIJpx/O8LIaYDp6IWWr9XSnncY+j3BrVNAUIRiTCpkLJjUo9DGMNjJFRdTePyFbieeIJASQmO008n8757sY4c2demaTSaAUo0LX5Qa+3uozk6pxBi5FHCMfdLqprDNZjMajJFvKX/hRcIlpXRtGo1TV9/TdOqVQR27wbAMnw4Q55+CucZZ/SxhRqNZqATzQSu7wL3oMI0bwROBr6kdcH0AUOlWwm/MPiwm+yYDf1nEXXvunWU/uIBArtUJAyD04m9aCqJV15B3LRp2CdMUAtzaDQazTESjZL8CBVTf4WUcpYQ4iTggWgyF0KUAA1AGAhJKacJIX4F3IqK7w/wcynlB901vCccCtcQEU0kWBKOxyWPipSS2ldeoeLR32POyCDz5/cTN20a1tGjB0SwJ41GM/CIRvh9UkqvEAIhhEVKuVkI0R3n+BwppeuIfX+SUj7ejTx6hUPhGoI09Qs3T6SpibKHfoV7wQKcZ55JzmO/1+ETNBpNzIlG+Mua4/UsAD4WQtQAFbE1KzZUNvhwWIw0hRr6vMXv37OHgz/4Af6du0i/+4ek3nabjl+v0WiOC9GM6rms+eWDQoizgUTg/Sjzl8AiIYQEnpJSPt28/04hxH8Bq4B7pJQdr9zcy6hF1m24/W6yndnH45Id4l60iLL7f44wmxnyzD9wnnZan9mi0WgGH6KTtdTVh0IYgTVSykk9ylyIHCllqRAiA/gEtfj6dsCFqhR+A2RLKb/bwbm3AbcBZGZmFs2bN68nJuDxeHA61fq6v1vpRQL+3EcZaRvJ9WnX9yjPniJ8Ppxvv03c0s8JFhRQd9utRFJSYnKtw8s9mBis5YbBW3Zd7s6ZM2fOaill+3XPpZRdJtTCK7lHOy6KfH4F/OSIfQXApqOdW1RUJHvK4sWLW17P+d/F8nsvr5anvHyKfHTloz3Osyd4Vnwpi886W24ZM1aWP/p7Gfb7Y3q9w8s9mBis5ZZy8JZdl7tzgFWyA02NxsefBmwVQqwAGg+rML7R1UlCCAdgkFI2NL8+D3hYCJEtpSxrPuxKYFMUNvQKlQ1+ZjnMeOo8x61zN9LUROUf/kjtyy9jHprP0JdfIm7q1ONybY1Go+mIaIT/0R7mnQm80xw/xgS8IqX8SAjxohBiMsrVUwLc3sP8u0VTIITHHyLRGYI6jkvnbtPq1ZTe/3OC+/aRfP31ZPz4Rxjs9phfV6PRaLoims7dT3uSsZRyN9Cub0BKeXwd680cGsPvsAcBSLDGTvjDDQ1U/d+fqX3lFcy5ueT/6wUc06fH7HoajUbTHaKZuduAap0fOt4I+KWU/WMGVJQcEv44mxL+eHPvu3qklLgXvk/F739PuLqa5G9/W7XyHXoNW41G03+IpsXfopBCCAPwDTpoyfd3Dk3eslhUnJ7ebvH7d++m/OHf0PTll9jGj2fIk09iH39Sr15Do9FoeoNuBX+RagnGN4UQPwEejI1JsaHSrQTfaGoW/l7y8Ue8XlxPPkX1c89hsNvJ+tVDJM2dq8MtaI4vkQ5XR9VoOiQaV89lh701oOL2DLgVP6o8fowGgRRqYFJvjOoJexrZd8MN+DZvJvHyy8n46U8wpaUdc76aAUigCfYtB6MVbImtyZoAvTkjW0pwH4Tyjc1pA5RtgPoDTEyaAKl3wZiLwawHEWg6J5oW/9zDXodQI3Euj4k1MaTS7SfNaaEhWA4ce4tfBoMcvPtufNu2kffE34g/e2Au4djnRCKw6zPIngTO9Nhc4+AaeP/HkHcyTPsuZIztvbw9VfD1P+Crf4C3poMDBFjjwWQFg6k5GVtfZ02ASd+C4XPU/o6IRGDvF7DuFdjx0WHXEZA6AnKnwuiLiFv3Frx1M1gTYfw3YPJ1kDcN9Mps/ZdIGLa+B8X/Vg0Eo6U5mVtfj79Kfc+9SDQ+/j4ZhdPbVHn8ZMTbaAg0YBIm7Kaet4iklJQ9+Esaly0j+5H/0aLfE6SE4k/g04ehYiOkj4GbPoS4Xp7JvO19ePNmsDqhYjN89TTkz1QVwLjLlCD3BNdOWPE3WP8qhHww6kI4+WaVn6++fQoHIBJSf/RISKWQX92DjW+AMwsmzoVJ10Jmc99QbQmsexXWvwJ1+9TTw5iLIbdIVZQZ41S5mvnSdh6zC0yqgtjwGqx+HtJGwcRrYMJcSC445tsZFYEm2DAP1rwIiXnqXg8789iefMIhMPZhWPJQAGr3gGsHVG0HV7F6nzoShp2hUkI3liIPemHdy7D8byofe4oS+UgQwkH1ewn5Aakq9uMt/EKIZ1HxdOqa3ycDj0kpb+1VS2JMpdtPVqKK05NgTTim9Wmr/vIX6t99l7Q77yTpqqt60coTAL9H/TnSClVLtyP2fwX//pVqxSYXwJwHYOlj8Oq34Pp3wdIL6yBLCV/+P/j45+qPc+08EEb1Z1v1HLx9C3yUqlrFw86A+CyIz1EVz5G/jaAX6vYr8a3bq55Qtr2v/qiTvgWn3gnpo3pmZ8gPOz6G9fOUvcv/qp4CrAnq/iBg+Jlw1i+V6Hd1b4ShVYQu+l/YMl9VAp/9j0p501UFcNKVbZ+uImFVyVRugYotEGiA5GGQMlylxLzOn0YOp6FCPf18/ax6KskcDyXLVIs2ZTgU3QRTvtO9yr1sAyx+RN2j/FNg3OUw9jJIzO38HH+DquSTC9T32l38Deq6pWugdC2UrYeaPSDDrcck5EJSPmz/QP2mAFILW+9/xliwJ4MtCUyW1vOaatT9WfkkNLkgZyqc+2sYc0nH9zgSJhae9Wiq0KmHRB9ASlkrhCjqdUtiTJXHz8S8RNwB9zG5eWrnvUb1/3uSpLlXk/b97/WihQOYUECJ4cY31B8h2KT2pwxXIpY1AbImqj/Csv+D7e+DIwMuehym3qD+GGmF8MaN8OZN8M2XO2/dRcLqT3NwtRLC0Re192eHQ/DRfUqExl4GVz7VKpin/UAJ9Z7/wKpnYcUTsPwvrecaLar1HZ8FMqLEvvGIlUbtyXDGT2D6beDMOLZ7Z7KqJ49xl0FjNWx6Sz1FNLrgrAdg4rcgaUj387XGK5Gd8h1VaW16Cza+CR/+VN2b4bOV7ZVbVAs25Gs912iFsL/1vcGsRPSQkDozm1OG2goDrPkXbHxdtVZHXwSnfh+GzlQV29b3VGX7yYOqAjrpCphwDeTP6LxxULUdFv8Wtryr+kqm3QT7VirbP7pPVWLjLodRF5BQvxW+3KpEunStao0jlStt3BUw47+7dnk11UDxIti1uPn8HbSMYE8cop6uxl2hnp7SCts2aiIRqNgEe5aqtOE19bs6HItT/WbsSVC9G4KNUHgenPZDGHpa1664aCrcHhCN8BuEEIlSynpoafH3n6WroiAckVR7/GTEW9kRaOhxx27DZ4spf/hhHGeeQdZDDx3TU8OAJxKB/V8qsd/8rmrh2ZNVC7jgdPUDP9TxuGV+63nWBCVoM+5o46bgpCug6XF4/x5Y8EO4/G/t/xDVu+Dd76nrWhOVmFnilQBMvAYKTscY8sK8b0PxxzDzB3DOr9u7GAwGGDFHpUaXyrehDBrKD9uWAgJGX6BadklDm7f5Suxi8Yd0pMKM21TqTZKGwOl3q1S5VX1nm95Wop8xDk6epbYZYyF9NJjs6j7U7G6b6vYqkfNUtm39ApjjVCV+yh1t3RJmm/puJl6jWuGrnldPNxteU09g2ZNUBTH0NBh6KnhrYcnvVSVijoMzfqYqEXuSys9VrH5PW+bDol/Aol/QEgDFmaWe7ibMVU8be79Q7qZNb6qW9Yz/Vr8zk1VVhts/gG0LoeQLVR5HOuROUz713KmQPfno/U4GA2RPVGnmnariK12n7pW3Frx1zdta9R/JnqzuUWbfDvWORvj/D1ghhHgNVQ1+C3gsplb1MtUePxEJ6fFWVtX0rMXvXb+egz/+MbZx48j74x8H9zKITTXwxg2qhWOOUy28CXNhxFltH2sP4XOrP33dXtXS6exR/+RblKj85/eqNXnOQ2q/lKoVtehB1fq88il1vZJlsOH1ZpfGSxCfQ1FIgK8cLvmT8i0fDUeaSoOFjLFw9i9V6orEXJWGzWr/WSSiRMxToZKvXvnwj+bCyTwJLn4czn1YVd57l6v01T9UfwlCPT0YLeqp7LS7VWV4OGmF6mnrjJ+oymjXZ2zcW82E8/4LEo4ItT7mIph9v3qCWvkUvHMbLHpAPbWUb1DHpI9RFeKYi1XlcKyNOaMZhpysUj8mms7d54UQq4GzUM6mb0opN8bcsl7k0OSt9Hgb7nI3ec68bp0fdrvZ//07MaWnM+SpJwf3TFxXMbxyDdQfUK6aSde2bbl3hC1BteaGnnr0/Gffr8Rk2R+V+I+9FObfCbsXq4rlsr+1+neHn6nSxY/D9g9VJbB/HXz7dSg859jLqukYg6G1wuxJy9USp77LEWep90Gfct3tXQ4hr3KhReObb+6DqG5c0l70D2F1wvRbYdrNsPszVcn46tWT4JhLIG1k9+0/AYimc/dkYKuUckPz+3ghxDQp5aqYW9dLVLUIv5WGHrh6XE/8nXB1NUPefANTaurRTzhR2fUZvH6jatXf+D4MiUH8ISHg4j8qF8xH98Fnjyhf+8V/VC34jlpkZrsavjj+G3y1ZAmzC2f3vl2a2GG2QcFpKsUKgwFGnqOShmjGVz0NNB32vhF4KjbmxIYW4XdaWkb1RIt/1y5qXn6ZpKuvxn5SPwrBIKXyhS64W406iYZIWI2o8dV3/3pf/QNeulqN8Lj1s9iI/iEMRrjqWRh5rvK13rFMDZUczH0qGk0vElXnbnOoBkCFbRBCDKjO3coGNWIhPk4SkqGoW/xSSip++zsMdjvpP7o7liZ2j0gYFt6tRlKA6qS7dl7XPtamGjW5Z9dnyk8+fLZyo4y5uGsfdzgEH9+vxr+PugCueqbzkRi9idkG33kz9tfRaAYh0Qj/HiHEHaiWvwTuQM3eHTBUNfhJsJnwRzxA9LN2PZ99RuMXX5D58/sxxWiJxG4TCsA7t8Pmt+GMnyof69u3w7PnwnVvQsqw9ueUb1IjXRrKlG+zyQVb3oMFP1AVSP7M1jHijVXKzdJYpVLtXtUpO/Ou5hEyOgaRRjPQiUb4bweeQK2PK4HFwMCavHVokfWAG4hO+CN+PxWP/h7LyBEkX3ttrE2MjkCTGk1TvAjO/Y0akw5qGNu8a+GZc1THZt5h0yw2vgnv3aXGQt/4Qetog3N/o2K9bF2g0sf3t55jTWzuvEtXY/DPelDNKtVoNCcE0YzqqQCu7knmQogSoAEIAyEp5TQhRArwGmq93RLgGillbU/yj5aqBj/pTtWxC9EFaKv55wsE9+9nyLPPIMz9wLPlc6uZrXuXwyX/pya0HGLoqXDzJ/DSVfDPi+Hq5xARqxq6tvyvMOQUuOZfEJ/Zeo4QreOPz/qFGtcsmkdr9DSMgUajGRBEM6rHCtwInATYDu2XUkY7y2SOlNJ12Pv7gE+llI8KIe5rfn9v1Bb3gMoGP1Pyk3D71QTko3XuBisqcD31FM5zzsZ5WgxHGkRLYzW89A01eeaqZ2BCB/VwWiHc8m945Zvw2nVMdQwFzx41Nv7833U8vv5wejI7VKPRDEiiGdXzL1Tr/BJgJTAC8HV1wlG4HHih+fULwBXHkNdRkVJS2eAj3WltdfWYuxb+ysf/AKEQmffGtD6KjoZy+OdFULUNvvVKx6J/CGcG3LgQRl2Ao3G/GvN+8R+OLvoajWZQIaSUXR8gxFop5RQhxAYp5cTmET0fSynPOmrmQuwBalF9A09JKZ8WQtRJKZMOO6ZWSpncwbm3AbcBZGZmFs2bN697JWumqs7DT78UfHO0BXvqF7xV+xaP5j2Kw9jxJCzzzl2kPP44ngsvoPHyvo0+bfVVMWn9g1j9tWyc8AvqkidGd6KU+OqrsCUdYxyZAYjH48HpPMqEshOUwVp2Xe7OmTNnzmop5bQj90fTuRts3tYJIcYCFcDQKG07TUpZKoTIAD4RQmyL8jyklE+jRhIxbdo0OXv27GhPbcOrCz8DvJwyeSxlohhq4YI5F2DsYHSKDIcp+ctfCWVmUvTIIxjieiFKZJsLSNj5b9U5O+V65V/vjJo98MJdEGmEmxYwuZvj5pcsWUJP79lAZrCWGwZv2XW5u080wv9sc2C2h4CPgTjgKIE+FFLK0uZtpRDiHWA6UCGEyJZSlgkhsoHKLjM5RuoD6okmI97GjpoGnGZnh6IP0PDvT/Ft2ULOY7/vfdE/PBQxAr5+Rk0jP+sXKrjZ4VTtgH9dpiIm3vAe5EzpXVs0Gs2g5qg+finlU1LKWinlYillvpQyTUr596OdJ4RwCCHiD70GzgM2Ae8BNzQfdgMwv+Mceoc6/yHht3YZkllKSfUzz2DOzyfh4ot7z4DKrfDqt9U4e1exim/zk2LV6brqWfhrkZqIdWjN1IrNyqcfCamwCFr0NRpNLxPLEJOZwDvNoYtNwCtSyo+EEF8Drwshbgb20XZpx16nvln40+OtXYZraPr6a3wbN5L1q4d6Z6F0d6mKPb7+VRWP+8hQxBf9L0z9L/jgp2qc/ep/quBUH92nwuLe8J4aqaPRaDS9TMyEX0q5G5jUwf5q4LitVVjvl1iMBhLtZtwBd6dj+KuffRZjSgqJVxzjICMpVQt+0QNqEYpTvgez7uk4nELWBLXc4IbX1SIV79yu4r3/13sdz8DVaDSaXiCacfwmKWXoaPv6K3V+SXq8FSEE7oCboQnt+6V923fQ+J+lpP/wBxhstg5yiZLavSoMwu4lUDALLvuLCh3bFULApG/C6AvV08GYS7peVk6j0WiOkWha/F9B6yI3Xezrl9T7I6THq5monYVkrnnuOYTd3vPQDJGI8td/8lBrWOGim7q3uLQtAWbc3rPrazQaTTfoVPibh2BmA3YhxARaV/xNQI3sGRDU+yVjM5Xwd9S5Gywro/7990n+9rUYk5I6yqJravbA/O+r0TojzoJL/6zcNRqNRtNP6arFfzHwXSAPFaTtkPA3AA/G2K5eo94vyYi3EowE8Ya87YS/5oV/gZSk3nBDJzl0QcivQik0VsPlT8Dk63TMeI1G0+/pVPillM8DzwshrpFSvn4cbeo1AqEIDUE1hr+jAG3h+nrqXn+dhIsuwpzbA7/68r+qdT+vf6d1GTmNRqPp50TjhM4QQiQACCGeFEJ8JYQ4bqNyjoXqxtYlF93+5jg9hw3nrJ33GpGmJlJvjmJR7iOp2w9LH4exl2nR12g0A4pohP82KaVbCHEeyu1zB/BYbM3qHSrdSvgPTd6C1lj8Eb+fmhdfxHH66djGjOl+5oseUNvzH+kVWzUajeZ4EY3wH4ridiHwvJRydZTn9TlHLrIOrcJfP38+YZeL1Ftu7n7GuxbDlnfV+HzdkavRaAYY0Qj4eiHEB8ClwIdCCCetlUG/prJZ+DMS2rb4ZThMzXPPYzvpJOJm8qZiUAAAIABJREFUzOhepqEAfPgzSC5QyxFqNBrNACOacfw3AUXATvn/2zvz8Jqu9Y9/VgYhJNQQlyateU4kJMQQU9G0XEOVaNVw1e3EraHUcH+3aItqtbSlVa2Slho6qFZJKUIojSlCjCWhCCKGnIjM7++Pc7KbSCQniQTJ+jzPeZyz9lp7ve/O9u611/BdIglKqapAAZrJxU/GJutVyjtguvj34G58SAjJUVE8PPcDVH5n4YR+BldOwDOrzBuCazQazQOGNSJtaUAdzH37AOWsKXc/EGNKwskeytjZ/N3id3AmYdculIMDTo/lc4zadBGC34H6j0ND/yKwWKPRaIoeayQb5gP2QAdgBnATWAj4FK1phWdUl3rUtTGrPsclxeFg64CDrQMJ+w9Qzt0dVSafO1NtegPSksF/VhFYq9FoNMWDNS33tiLyIpbtFkXkKvBA7OVXo2I56lQ0K21mCLSl37pF4tGjlGuRT8WJM79D+Cpo+ypUqVsE1mo0Gk3xYE3gT1FK2WAZ0FVKVQHSi9SqIiBDruFW+CFITaVci3zo3KcmwS/jwdkV/MYVnZEajUZTDNwx8CulMrqBFgDfA9WUUtOBHcDsYrDtrpLR4r91YD8Ajp6e1hf+bRpcjoAec6BMznv1ajQazYNCbn38oUALEflKKbUP6IpZr6e/iBy2tgKllC2wFzgvIj2VUkuBjsANS5ZhIhJWIOvzgSnZRJWyVUjYv58y9epaL8h2fAPs/gRav2SWTtZoNJoHnNwCvzHPUUQigIgC1jEaOIpZ1TODCSLyXQHPVyDikuKo7VSLW2HBOD/e3bpCN87Dj6/APzyg25tFa6BGo9EUE7kF/mpKqTt2aIvIB3mdXCnlilnlcwZwTzvHTSkmHo5JJz0ujnJeVgzspqfBD/829+8/vQTsHIreSI1GoykGcgv8tkAFMrX8C8A84HXg9t1PZiil3gA2A5NEJOn2gkqpF4AXAKpXr05wcHCBDIiPj2fL1i3EJcVR9mAUAIdSUkjL43yPRq2k9pmdHG00hkuHzwHnClT/3UYpRfny5bHNY19gZ2dnDhw4UExW3T+UVr+h9Pqu/Ya0tDRu3ryJiHWiCrkF/mgRKXD/hlKqJ3BZRPYppTplOjQZuIh5SugiYCKQrR4RWWQ5jre3t3Tq1On2LFYRHBxMy7YtkbNCo2tgW7ky7Qf0z33FbtQO2LYKmj9D477TaVygmouGyMhInJycqFKlSq4+mEwmnJxy3l+4JFNa/YbS63tp91tEiI2NxWQyUbu2dXt15zads7A7irQDeimlooCVQBel1DIRiRYzScASoFUh68kTQ6Dt+AXKtfDKPejfjIXv/w0P1YYn5xS1afkmMTExz6Cv0WhKD0opqlSpQmJiotVlcgv8hdLcF5HJIuIqIrWAgcAWEXlOKVXDYqwC+gBWzxAqKHHJcVS8KZSJvopjbv37IuZtFBOuQP8l4FChqE0rEDroazSazOQ3JuS2A9fVQluTM8uVUtUwv1GEAS8VUT0GcUlxNDxn7vvKdeHWni/gxAbwnw01mhe1WRqNRnNPKBaxNREJFpGelu9dRMRdRJqJyHMiEl/U9ZuSTebAX8aesk2b5pwp5oR5c5V6XaH1i0Vt0gONra0tnp6eNGvWjP79+5OQkABAhQp5vyG1bdu2wPVOmzaNOXPM3W+JiYl069aN6dOnW1V2woQJNGrUCA8PD/r27cv169fzLKOU4rXXXjN+z5kzh2nTpllVX1hYGG3atKFp06Z4eHiwatUqq8ppNMXBA6GyWVjiks0tftsmDbHJSZgtNRl+GAH2juZN03VXSq6UK1eOsLAwDh8+TJkyZVi4cKHVZX///fdC15+cnEy/fv1o2bIlU6dOtapMt27dOHz4MOHh4TRo0IBZs/IW2nNwcOCHH37gypUr+bbR0dGRr776ioiICIKCghgzZoxVDxuNpjiwRo//gcdkisXrIjj636F/f9s7EH0QApaB0z+K17hCMP3nCI5ciMvxWFpaWp5TPnOiSU1npv7zDm9FOeDn50d4eHiWtPj4eHr37s21a9dISUnh7bffpnfv3oD5rSA+Pp7g4GCmTZtG1apVOXz4MC1btmTZsmV59lWmpqYycOBA6tevzzvvvGO1nd27/71oz9fXl+++y3v9oJ2dHS+88AJz585lxoz8bbHZoEED43vNmjVxcXEhJiaGStauGNdoipBSEfjVsVPYpYOzdw4TiM7sgh1zwes5aPzP4jfuASY1NZUNGzbg7591b4KyZcuyZs0anJ2duXLlCr6+vvTq1StbUD9w4AARERHUrFmTdu3asXPnTtq3b59rne+++y5du3Zl3rx5WdL9/PwwmUykp6djY/P3i+ycOXPo2rVrlrxffvklAQEBVvk4cuRIPDw8eP3117OkL1++nPfeey9b/nr16mV7qISGhpKcnEzdulrVVXN/UCoCf9kjUQDZZ/QkxsGaF8z75vpb33q8X8itZV6Uc5tv3bqFp0Xkzs/Pj+efz7ohm4gwZcoUtm/fjo2NDefPn+fSpUv84x9Z36ZatWqFq6srAJ6enkRFReUZ+Nu3b8+uXbs4ceJEllZ1SEgIkLffM2bMwM7OjkGDBlnlq7OzM0OGDOGjjz6iXLlyRvqgQYOsOkd0dDSDBw8mMDAwywNJo7mXlIrAX/F4NJeq2dP4oYeyHtgwEW6cg+G/gkPpWwBSUDL6+O/E8uXLiYmJYd++fdjb21OrVq0c5xg7OPwtg2Fra0tqamqedXfo0IGhQ4fyxBNPEBISQs2aNQHrWvyBgYGsW7eOzZs352v625gxY2jRogX/+te/sviYV4s/Li6OHj168Pbbb+Pr62t1fRpNUVPyA396OtVOxXKk2W2BPeJHOPgNdHgd3Ip8DVmp4saNG7i4uGBvb8/WrVs5c+ZMvspPnjyZVq1a0bdv3xyP9+vXj5iYGPz9/dm+fTuVKlXKs8UfFBTE7Nmz2bZtG46Ojkb6+fPnGTJkCJs3b76jPZUrV2bAgAEsXryY4cOHA3m3+JOTk+nbty9Dhgyhf//+Vvmt0RQXJf7d0/bSJcompBJTt8rfiaaLsG4M1GwBHV+/c2FNgRg0aBB79+7F29ub5cuX06hRo3yVP3ToULZuodt56aWXeOqpp+jVq5dVKxZHjRqFyWSiW7dueHp68tJL5uUj0dHR2Nnl3f557bXX8jW7Z/Xq1Wzfvp2lS5fi6emJp6dnrm9JGk2xIiL3/adly5ZSUHa/9ZYcadhIpq3499+JfywSmeoscjGiwOe9Vxw5csSqfHFxcUVsSdHRvXv3ApfNr98ff/yxrF27tsD13U88yH/zwqD9NpNTbAD2Sg4xtcR39difOo3JUSFuNf5OvHraPGff5X6SX9Nk8OuvvxZbXaNGjSq2ujSa+4US39Vjf+oUJ1xtcHao+Hfi1UizCJteqKXRaEohJTrwp8bGYnf5MkcfFpwdMm0AdvU0VLZOvlSj0WhKGiU68N+ybFJw3FXhZG+Z6ZGeDteidODXaDSllhId+BP2HyDdzpbT/+DvFr/pAqQlQeU699Y4jUajuUeU6MFdx1Y+nLt4ghS73TiXsQT+q6fN/z6kW/wajaZ0UqJb/E6dOvFnN7O0gFMZS1fP1Ujzv7rFX2AyZJmbN29OixYtsiluzp07l7Jly3Ljxo0s6aGhoXTo0IGGDRvSqFEjRowYYUg6b9iwAW9vbxo3bkyjRo0YP378XbHV39+f8+fPW5V32LBh1K5dG09PT1q0aMGuXbvuig3WMnr0aB5++GHS09ONtKVLlxozjxYuXMhXX3111+sdNmyYsdr46tWreHl5sWTJEqvKDho0iIYNG9KsWTOGDx9OSkpKrvmjoqJQSvHxxx8baaNGjWLp0qVW1bdp0yZatmyJu7s7LVu2ZMuWLVaVu52ffvopXyJ/RUVQUBANGzakXr16d7Rn+/bttGjRAjs7O6vEBa0ipzmed/ODedP2A8A6y+/awB/ASWAVUCavcxRmHv+7P78rzZY2k8jrkeaEjW+ITK8ikpZa4HPeS+6Hefzly5c3vgcFBUmHDh2yHPfx8ZH27dvLkiVLjLSLFy/KI488Ir///ruIiKSnp8u3334rFy9elEOHDkmdOnXk6NGjIiKSkpIiCxYsKJBtmf1OSEgQHx8fq8sOHTpUvv32WxER+fXXX8Xd3b1ANuSHlJQUERFJS0sTNzc3ad26tWzdutU4vmTJEhk5cqRV5yro3zzD7+vXr4u3t7d88sknVpf95ZdfJD09XdLT02XgwIF5lo2MjBQXFxepW7euJCUliYjIyJEjs9wrubF//345f/68iIgcOnRIatas+cDO409NTZU6derIqVOnJCkpSTw8PCQiIvvaosjISDl48KAMHjzYuD9FCjePvzha/KOBo5l+zwbmikh94BrwfI6l7hIJ6eYWpdHivxYJDz0KNvmXLL7v2DAJlvTI8VNu1dN3PJbrZ8OkfJkQFxfHQ5k0kE6dOkV8fDxvv/02K1asMNIXLFjA0KFDadOmDWDe5OTpp5+mevXqvPvuu/z3v/81Vvja2dnxyiuv5Frvk08+achBe3l58eabbwLw1ltv8cUXXwAQHBxMp06dANi8eTNeXl64u7szfPhwkpKScj1/hw4d+PPPPwHzpiq+vr7GJi7Xrl3j8uXLtGzZEoCDBw+ilOLs2bMA1K1bl4SEBGJiYujXrx8+Pj74+Piwc+dOwLyhzAsvvED37t0ZMmQIAFu3bqVZs2a8/PLLWa5bZjJvRLNnzx48PDxo06YNEyZMoFmzZoD5DeGpp57C39+f+vXrZ1MVvRPx8fE88cQTPPvss7z88stWlQHz30EphVKKVq1ace7cuTzLVKtWjccee4zAwECr68nAy8vL0Gdq2rQpiYmJef4tP/roI5o0aYKHhwcDBw4Esr5JnTp1Cl9fX3x8fHjjjTeMDYWCg4Pp2LEjAwYMoEGDBkyaNInly5fTqlUr3N3dOXXqFAA///wzrVu3xsvLi65du3Lp0iWrfAkNDaVevXrUqVOHMmXKMHDgQNauXZstX61atfDw8LirIn9FGviVUq5AD+ALy28FdAEy3lcCMe+7W2TcSr8FZBrcvXpad/MUkgx1zozumv/973/GsRUrVvDMM8/g5+fH8ePHuXz5MoChuZ8TuR27Ex06dCAkJIS4uDjs7OyMoLp79278/PwADMnoxMREhg0bxqpVqzh06BCpqal8+umnuZ7/559/xt3dHYAhQ4Ywe/ZswsPDcXd3Z/r06bi4uJCYmEhcXBwhISF4e3sTEhLCmTNncHFxwdHRkdGjRzN27Fj27NnD999/z4gRI4zz79u3j7Vr1/LNN99kuW59+/Zl3bp1eXaZ/Otf/2LhwoXs2rUr274LYWFhhq+rVq3ir7/+yvN6jhs3jvbt2zN27FgjzWQyGXITt3+OHDmSpXxKSgpff/11NonuOzFp0iTef/990tLSsqS/9957Odb36quvZjvH999/j5eXVxaxv5x45513OHDgAOHh4TluGjR69GhGjx7Nnj17jIdKBgcPHuTDDz/k0KFDfP3115w4cYLQ0FBGjBhhdFe1b9+e3bt3c+DAAQYOHMi7774LmB/mOfmSsQvd+fPncXNzM+pydXW1uluysBT14O484HUgQzWrCnBdRDJkGM8BDxelAQnpCZSzK4e9jb15M/WrkfBIwbf/u6944s59lLeKUJY5szrnrl27GDJkCIcPH0YpxcqVK1mzZg02NjY89dRTfPvtt4wcOfKu2+Dn58dHH31E7dq16dGjB5s2bSIhIYGzZ8/SsGFDAHbu3MmcOXM4evQotWvXNmSchw4dyoIFCxgzZky2806YMIG3336batWqsXjxYm7cuMH169fp2LGjUTZDdK1t27bs3LmT7du3M2XKFIKCghAR48Hz22+/ZQmQcXFxmEwmAHr16mXIPCcnJ7N+/Xrmzp2Lk5MTrVu3ZuPGjfTo0SNH369fv47JZDICyLPPPsu6deuM44899hgVK5oXLDZp0oQzZ85kCTA50aVLF9auXcv48eNxcXEBwMnJyWp9oVdeeYUOHToYvudF7dq1adWqlfHgy2DChAlMmDAhz/IRERFMnDiRjRs35pnXw8ODQYMG0adPH/r0yd7O3LVrFz/++CNgvpaZx5d8fHyoUcO86r9u3brGhj7u7u5s3boVgHPnzhEQEEB0dDTJycnUrm2eONK5c+dcr5+5JyYr+d00vaAUWeBXSvUELovIPqVUp4zkHLJm995c/gXgBYDq1asTHBxcIDvikuNwEAeCg4OxT75Ou+R4Tl5N43wBz3evqVixohE8ciMtLc2qfAUl49zNmjUjJiaGyMhILl26xMmTJw0Z5OTkZGrVqsWQIUOoV68ev//+O126dMl2rgYNGrBz507q1LH+TaxRo0aEhobi6upK586duXDhAh9//DHNmzfHZDIRGRlJjRo1SEpKIj4+Psv1SEhIIDU1Ndv1SUlJ4c0338wSHG7cuIGIGHnj4+NJT0/HZDLh7e3N5s2bOX36NF26dGHmzJmkpqby+OOPYzKZSEtLY+PGjVl0/AGSkpKoUKGCcc7169dz48YNo7smISEBe3t7OnToQGJiIsnJyZhMJpKSkrC3tycuLi6LTTdv3iQ9PZ20tDQSExNRShnHRCTLAycnUlJS6N27N97e3vj7+7Nu3TqcnJwwmUx3bMEvXrzY6JqbNWsW0dHRLF++PM97LvP1Gz16NIMHD6Zt27YkJiZiMpn48MMPWb16dbZybdu2NWSwz58/T+/evVm4cCEuLi553usrV65k586drF+/nunTpxMaGprlumZcSzs7O+M8JpOJhIQEbG1ts1zLjPsmMTHRsPmVV15h1KhRPPnkk4SEhDBr1ixMJhPbt29n8uTJ2ewpV64cv/32Gw899BCRkZHG+U+dOkWVKlXu6EtKSgq3bt0yjt/ud2JiovVxMqeO/7vxAWZhbtFHAReBBGA5cAWws+RpA/ya17kKM7j77Kpnpc+Pfcw/zuw2i7MdDyrw+e4199vg7tGjR6VKlSqSmpoqkyZNkpkzZ2bJW6tWLYmKijIGd3fv3m0c+/rrryU6OloOHjwodevWlePHj4uIeaDz/fffFxGRH374QSZNmpSjHR07dpQ6derIzZs3ZeXKleLq6irvvPOOiIjMnz9fPv30UxERuXXrlri5ucnJkydFxDyYOW/evGznyzy4mxkPDw/Zvn27iIhMnTpVxowZIyLmQTc3NzcZNGiQiIg88cQT4ubmJlevXhURkWeeeUbeffdd4zwHDhwwzvHee+8Z6QMHDpRvvvnG+B0fHy/VqlWTmzdvZhnczVyuadOmsmvXLhERmTx5sjRt2lTi4uKyDQb36NHDGCwePHiw/PHHH7n6/frrr8tjjz1mDLzmxeeffy5t2rSRhISELOl//PGHDB48OFv+yMhIadq0qfG7f//+4ubmZvXg7rVr18TDw0O+++47Iy3jXs/Jv7S0NImMjBQRkeTkZHFxcZFr165luU5PPvmkrFy5UkREPvvsM+P+3rp1q/To0cM4V8eOHWXPnj3Zjnl6esrevXtFRGTYsGHSsWNHq3xJSUmR2rVry+nTp43B3cOHD98x/+335305uCsik0XEVURqAQOBLSIyCNgKPG3JNhTIPppxF7mVfuvvOfzX9FTOu0FGH7+npycBAQEEBgZia2vLypUrs2no9+3bl5UrV1K9enVWrlzJ+PHjadiwIY0bNyYkJARnZ2c8PDyYN28ezzzzDI0bN6ZZs2ZER0cD5laQs7NzTmbg5+dH9erVcXR0xM/Pj3PnzhndH0FBQUZrtWzZsixZsoT+/fvj7u6OjY2NIctsDYGBgUyYMAEPDw/CwsJ44403APOgG5jHG8Dc11upUiVjsPujjz5i7969eHh40KRJkxz7lxMSEvj111+zdOuUL1+e9u3b8/PPP9/RpsWLF/PCCy/Qpk0bRMTo2smN8PBwo9viTsyePRs3NzcGDx6cZVrpnXjppZe4dOkSbdq0wdPT0xhkP3v2bLY3nZz473//a9WAcAbz58/nzz//5K233jLuwZiYGCBn/9LS0njuuedwd3fHy8uLsWPHZtv3eN68eXzwwQe0atWK6Ohoq65lZqZNm0b//v3x8/OjatWqVpezs7Nj/vz5PP744zRu3JgBAwbQtKl5V7033niDn376CTAP5Lu6uvLtt9/y4osvGnkKRU5Pg7v9ATrx93TOOkAo8CfwLeCQV/nCtPj9l/vLqN9GmX9smSEyrZJISmKBz3evuR9a/MXJoEGD5PLly1bnj4uLk8TERCnMPfMgYDKZjO+zZs2SV199Nde/+Y0bN+Tpp58uDtNERGT8+PFy8ODBYqkrLi6uUP7dvHlT0tPTRURkxYoV0qtXr7tpXpFx38syi0gwEGz5fhooti2vEtITss7ocXYFu9xnAWjuH5YtW5bvMg4ODuzdu7cIrLl/+OWXX5g1axapqak8+uijeS6AcnZ25ttvvy0e4yDHbSmLksL4t2/fPkaNGoWIUKlSJb788su7bN39R4mWbABz4M+yaleLs2lKAAEBAQQEBGRJK8rB/JKMn58fBw8evNdmFCslWrIhLT2NREnMqtOjA79GoynllOjAH58SD2AO/Leuw62remBXo9GUekp04I9LjgMscg0ZM3q0KqdGoynllIrA71zG+W85Zt3i12g0pZySHfiTLIHfwTmTHLNu8ReWkirLvHTpUqpVq4anpydNmjTh888/vys2WMuaNWtQSnHs2DEjLSoqyljRu3fv3hw1awpLZsGy9PR0hg4dyvDhw3OUFLidDz74wBBAe+yxxzhz5kyeZWrVqkW/fv2M39999x3Dhg2zyta//vqLzp0707hxY5o2bcqHH35oVbnbuXDhAk8//XTeGYuYyMhIWrduTf369QkICCA5OTlbntjYWDp37kyFChWMv1NhKdGB35RsnuXgVMbJHPgrVIcy5e+xVQ8+GVo9Bw8eZNasWdmWpa9YsQIfHx/WrFljpF26dIn+/fsze/Zsjh8/ztGjR/H398dkMnH48GFGjRrFsmXLOHr0KIcPH86XfMOduHXrFlevXuXhh62XgwoICCAsLIzg4GCmTJlitdJiQcksUrZixQrat2/PypUrc8zr7e3NRx99VGS2iAgvvfQSKSkpfPHFF1bpxnh5ebF3717Cw8N5+umnrVYD3bt3LxEREfm20c7Ojvfff5+jR4+ye/duFixYkOVBaS01a9a8e9r2hWDixImMHTuWkydP8tBDD7F48eJsecqWLctbb71lKLPeDUr0dM4sXT3XIktcN8/s0Nkcu5rzTZ+WlpZNtdEaGlVuxMRWE63OfydZ5vfee4+ZM2caLbk7yTKDWZirILLM77zzDh4eHnh5edG3b1/eeOMN3nrrLRo0aMCIESOyyDLnFxcXF+rWrcuZM2ewt7dn+PDhnD59GkdHRxYtWoSHhwfu7u6EhIRQsWJFqlatyty5cxkyZAiDBw9m6NChdO7cmUmTJhEcHExSUhIjR47kxRdfJDg4mOnTp1OjRg3CwsI4cuQI8fHx7Ny5k61bt9KrVy+mTZuWzabg4GDmzJnDunXriImJ4dlnnyU2NhYfHx+CgoLYtm0bsbGxPPHEE7Rv357ff/+dhx9+mLVr11q1inb06NHExsayatUqqyWAO3fubHz39fW1et3F+PHjmTlzJsuXL7cqfwY1atQwVuc6OTnRuHFjLly4kGuZbdu2MXr0aMB8323fvp3Y2Fh69uzJ4cOHSUhIYNiwYRw7dozGjRsTFRXFggUL8Pb2pkKFCowcOdLQ1pk5cyavv/46Z8+eZd68efTq1YuoqCgGDx7MzZs3AfPq4owV5LkhImzZssUQqhs6dCjTpk3LJoudsZI7Qyb8blCiW/zZ+vj1wO5d4UGTZS4Ip0+f5vTp09SrV4+pU6fi5eVFeHg4M2fONDT027Vrx86dO4mIiKBOnTqEhIQYNvj6+rJ48WIqVqzInj172LNnD59//jmRkeYux9DQUGbMmGGod/7444/4+/vToEEDKleuzP79+3O1b/r06XTp0oX9+/fTt29fYy8AgJMnTzJy5EgiIiKoVKkS33//fZ7+fvPNN+zbt4+VK1diZ/d3ezAgICBHaeGcdgJbvHgxTzzxRJ51AQwYMID9+/dnC2Z5SRlnJioqigMHDuDt7Z1rXXPmzGHBggWEhYUREhKS7SH4ySef8NBDDxEeHs7//vc/9u3bZxy7efMmnTp1Yt++fTg5OfF///d/bNq0iTVr1hjSHS4uLmzatIn9+/ezatUqozsuL1nr2NhYKlWqZFzvkiTLfE8xJZuwwYZy6QKm6BLX4s+tZW7SssyGLHN+WLVqFTt27MDBwYHPPvuMypUrs2PHDiN4dunShdjYWG7cuIGfnx/bt2/n0Ucf5eWXX2bRokWcP3+eypUrU6FCBTZu3Eh4eLjRpXDjxg1OnjxJmTJlaNWqlSHfC+YHZoZM9MCBA1mxYgUtWrS4o507duwwutL8/f2zvHVlbB8J0LJlS6KiovL0u0WLFhw7dozQ0FDatWuX5XpYw7Jly9i7dy/btm2zKr+trS0TJkxg1qxZWR4WeUkZZxAfH0+/fv2YN2/eHbWcMmjXrh3jxo1j0KBBPPXUU7i6umY5vmPHDuONoFmzZnh4eBjHypQpYzQe3N3dcXBwwN7eHnd3d+O6pqSkMGrUKMLCwrC1teXEiRNA3rLWGRpDmXngZZnvB+KS4nC0cURdtww46YHdu06bNm24cuUKMTExXLx4kZMnT9KtWzfALMtcp04dRo4cSdOmTdm3bx+9e/fOdo6MY82bN7e6Xh8fH/bu3UudOnXo1q0bV65c4fPPPzcC3unTp3Fzc6NMmTL58icgIID58+dnSctpkFMpRYcOHViwYAFnz55lxowZrFmzhu+++8544xARPv74Yx5//PEsZYPs4NraAAAS3UlEQVSDgylf/u+xptjYWLZs2WI8PNPS0lBKGRt65ERuA6+ZNyaxtbXl1q1buTuNWeb6zTffZMCAAfz666+GEFhAQADHjx/Pln/cuHHGm89vv/3GjBkz2LZtW56bomRm8ODBzJo1K4vo2NatW7NsBpOBo6OjMYkgJSWFfv36GYE8rxXLkyZNokePHqxfvx5fX19+++03ypYtaxzP7Vra29sbwdjGxsbwz8bGhtRU87Yic+fOpXr16hw8eJD09HTj3CaT6Y77E3zzzTc0btyY69evk5qaip2dHefOncu2EUxRUaK7ev7t8W9ednk501ROHfjvNseOHSMtLY0qVaqwYsUKpk2bRlRUFFFRUVy4cIHz589z5swZRo0aRWBgIH/88YdRdtmyZVy8eJEJEyYwc+ZMo6WUnp7OBx98AJhnuuSkaV6mTBnc3NxYvXo1vr6++Pn5MWfOHGMMoTDdPLfToUMHoy86ODiYqlWr4uzsjJubG1euXOHkyZPUqVOH9u3bM2fOHOM/++OPP86nn35q7KZ14sQJox84M9999x1DhgzhzJkzREVF8ddff1G7dm127NhxR5vat29v6NZv3LiRa9eu5enH/Pnzsz3UMtO2bVsWLlxIjx49jK6jVatWERYWlu2TEfQPHDjAiy++yE8//WRs4JJBxpjNnbC3t2fs2LHMmzfPSMto8d/+yQj6IsLzzz9P48aNGTdunFX+nTp1Cnd3dyZOnIi3t3e2weDM1/LIkSMcOnQoV7tv58aNG9SoUQMbGxu+/vprY8A+o8Wf06dJkyYopejcubPxRhgYGJhjw6goKNGB/x/l/8EjDo9oOea7zIMmy1xYpk2bZsgrT5o0Kctesa1btzZ29vLz8+P8+fO0b98egBEjRtCkSRNatGhBs2bNePHFF41WYmZWrFiR7br169cv2+5UmZk6dSobN26kRYsWbNiwgRo1ahh7xd6JY8eOUaVKlVzz9OzZk6lTp+Lv709sbGyuecE8MB8fH0///v3x9PSkV69eAFy5csWq6aDPP/98jtfkTuzcuZOvv/6aLVu2GPfgr7/+CtzZv3nz5tGsWTOaN29OuXLlso1DvPLKK8TExODh4cHs2bPx8PDIlzTzK6+8QmBgIL6+vpw4cSLL21xezJ49mw8++IB69eoRGxvL88+btyD/6aefjDEEME+BHTduHEuXLsXV1TXb1pf5JifJzvvtUxiJ3a1bt4r8PEZk1iMFPsf9hJZlzp3SIsucmJgoKSkpIiLy+++/S/PmzfP8m/fo0cPqDVYKy88//ywffvhhsdSV4XdB/UtNTZVbt26JiMiff/4pjz76aLFdp8Jw38sy33OulrypnKUFLcucM2fPnmXAgAGkp6dTpkwZqxabZd6Xt6jp2bNnsdWVQUH9S0hIoHPnzqSkpCAifPrpp/keG3rQKCWB/zS45j7lS6N5kKhfvz4HDhzIkqZlmQuGk5NTiW8o3E6R9fErpcoqpUKVUgeVUhFKqemW9KVKqUilVJjl41lUNgCo9BS48Zdu8Ws0Go2FomzxJwFdRCReKWUP7FBKbbAcmyAixbJeumxiDEi6DvwajUZjocgCv2VgId7y097yyXuY/y5T7pZ5dohetavRaDRmirSPXyllC+wD6gELROQPpdTLwAyl1BvAZmCSiCTlUPYF4AWA6tWrExwcXCAbqt4wL976/dhFkiMLdo77iYoVK1rVl5uWllYq+3xLq99Qen3XfptJTEy0Pk7mNNXnbn+ASsBWoBlQA1CAAxAIvJFX+cJMzftr0SCRt2uIpKcX+Bz3E/fDdE4bGxtp3ry5NGnSRDw8POT999+XtLQ0ETFPn3V2dhZPT09p1KiRTJs2Lc/zlS9f3vj+yy+/SL169eTMmTN5ljtw4ID4+vpKkyZNxN3dXVauXJmn31OnTpVy5crJpUuXcqw/L8aPHy8NGzYUd3d36dOnj1y7ds3qspl5/vnnJSIiokBl70R+/+bp6enyn//8R+rWrSvu7u6yb9++HPNNmTJFXF1d83WdipOSMnU5vxRmOmexLOASketAMOAvItEWm5KAJUCroqy73K1o84rdYtLAKA1kaPVERESwadMm1q9fz/Tp043jfn5+HDhwgL1797Js2bIsole5sXnzZv7zn/8QFBTEI488kmd+R0dHvvrqKyIiIggKCmLMmDFcv349z3JVq1bl/ffft8qm2+nWrRuHDx8mPDycBg0aMGvWrAKd54svvqBJkyYFKnu32LBhAydPnuTkyZMsWrQomypkBv/85z8JDQ0tZus0RUmRdfUopaoBKSJyXSlVDugKzFZK1RCRaGUWwOgDHC4qGwDKJl4ElyKdOHTPuDhzJklHc5ZlTk1L42oBZJkdGjfiH1OmWJ3fxcWFRYsW4ePjk01KuHz58rRs2ZJTp07lqb4ZEhLCv//9b9avX0/dunWtqjtjxSyY9dVdXFy4cuUKbm5uuZYbPnw4S5cuZeLEiVSuXNmqujLo3r278d3X1zdPTfebN28yYMAAzp07R1paGv/73/8ICAigU6dOzJkzB29vbxYvXszs2bOpWbMm9evXx8HBgfnz5zNs2DDKlSvHsWPHOHPmDEuWLCEwMJBdu3bRunVrli5dCsDLL7/Mnj17jLoyP4RzY+3atQwZMgSlFL6+vly/fp3o6GhD9jizn5qSRVH28dcAAi39/DbAahFZp5TaYnkoKCAMeKnILEhPo9yti3pGTxFTp04d0tPTDQnmDGJjY9m9e3cW2eacSEpKonfv3gQHB2fRd1m+fDnvvfdetvz16tXLFnBDQ0MNUbi8qFChAsOHD+fDDz/MFiT9/Pxy7C+eM2cOXbt2zZL25ZdfEhAQkGtdQUFB1KxZk19++QUg265kFy5c4K233mL//v04OTnRpUuXLGJ1165dY8uWLfz000/885//ZOfOnXzxxRf4+PgQFhaGp6cnM2bMoHLlyly/fp0+ffoQHh6Oh4cHY8eOZevWrdlsGjhwIJMmTeL8+fNZHpIZssC3B35NyaMoZ/WEA145pHcpqjqzEXcBG0ktsYE/t5Z5Ucoy54Rk0mUJCQnBy8sLGxsbJk2alEV9MSfs7e1p27YtixcvzrKV3qBBgxg0aFCedUdHRzN48GACAwOt3kDk1VdfxdPTk9deey1Leoamfl7MmDEDOzu7PO1zd3dn/PjxTJw4kZ49e2ZTawwNDaVjx47Gm0f//v0NsTowd7MopXB3d6d69eq4u7sDZkXTqKgoPD09Wb16NYsWLSI5OZlLly5x5MgRPDw8mDt3bq62Zf6bZVBcssCae0vJXrmrVTmLhdOnT2Nra4uLiwtHjx7Fz88vX8vnbWxsWL16NV27dmXmzJlMsTzQrGnxx8XF0aNHD95++218fX2tnt1RqVIlnn32WT755JMs6da0+AMDA1m3bh2bN2/OM1A2aNCAffv2sX79eiZPnkz37t2ziG/lFHwzk1kGOLPkcYYscGRkJHPmzGHPnj3Y2dnxn//8h8TERIA8W/yurq789ddfRnpxygJr7i2lJPCXzBb//UBMTAwvvfQSo0aNyjUInj9/niFDhrB58+Ycjzs6OrJu3TpDcfP555/Ps8WfnJxM3759GTJkCP37989ybPLkybRq1Sqb6mVmxo0bh4+PTxZ1yLxa/EFBQcyePZtt27bh6OiYp38XLlygcuXKPPfcc1SoUMHol8+gVatWjB07lmvXruHk5MT3339vtOqtIS4ujvLly1OxYkUiIyPZsGGDsd1kXi3+Xr16MX/+fAYOHMgff/xBxYoVdTdPKaFkB/5rkaQrO2ycrd9sW5M3GbLMKSkp2NnZMXjw4Gza6LcTHR2dZUu/nKhcuTJBQUF06NCBqlWr5qlNvnr1amP/1IyAumDBAtq1a8ehQ4cMieA7UbVqVfr27ZtngMzMqFGjSEpKMjab8fX1ZeHChXf079ChQ0yYMAEbGxvs7e359NNPsxx/+OGHmTJlCq1bt6ZmzZo0adIkX5LAzZs3x8vLi6ZNm/LII49k2T0rL5588knWr19PvXr1cHR0ZMmSJcYxT09PY/eo119/nW+++YaEhARcXV0ZMWJEjnsCax4gcprjeb99CjyPf+9SufDpUwUre59yP8zjLwgff/yxrF27tsjryfC7e/fuRV5XZgrjn8lkEhGRlJQU6dmzp/zwww8FOs/99jcvLrTfZrQscwYth3Lc9Cj65fXeM2rUqGKtL2NzjuKiMP5NmzaN3377jcTERLp3706fPn3uomUaTXZKduDXaB4A8rshvEZTWEr01oslFcljJohGoyld5Dcm6MD/gFG2bFliY2N18NdoNIA56MfGxlK2bFmry+iungcMV1dXzp07R0xMTK75EhMT83UjlBRKq99Qen3XfpsbhK6urlaX1YH/AcPe3p7atfNekBYcHIyXV7aF0yWe0uo3lF7ftd/5R3f1aDQaTSlDB36NRqMpZejAr9FoNKUM9SDMDlFKxQBnCli8KnDlLprzoKD9Ln2UVt+133fmURGpdnviAxH4C4NSaq+IeN9rO4ob7Xfpo7T6rv3OP7qrR6PRaEoZOvBrNBpNKaM0BP5F99qAe4T2u/RRWn3XfueTEt/Hr9FoNJqslIYWv0aj0WgyoQO/RqPRlDJKdOBXSvkrpY4rpf5USk261/YUFUqpL5VSl5VShzOlVVZKbVJKnbT8+9C9tLEoUEq5KaW2KqWOKqUilFKjLekl2nelVFmlVKhS6qDF7+mW9NpKqT8sfq9SSpW517YWBUopW6XUAaXUOsvvEu+3UipKKXVIKRWmlNprSSvwfV5iA79SyhZYADwBNAGeUUo1ubdWFRlLAf/b0iYBm0WkPrDZ8rukkQq8JiKNAV9gpOVvXNJ9TwK6iEhzwBPwV0r5ArOBuRa/rwHP30Mbi5LRwNFMv0uL351FxDPT3P0C3+clNvADrYA/ReS0iCQDK4Hcd+9+QBGR7cDV25J7A4GW74FAidvPT0SiRWS/5bsJczB4mBLuu2U71XjLT3vLR4AuwHeW9BLnN4BSyhXoAXxh+a0oBX7fgQLf5yU58D8M/JXp9zlLWmmhuohEgzlAAi732J4iRSlVC/AC/qAU+G7p7ggDLgObgFPAdRFJtWQpqff7POB1IN3yuwqlw28BNiql9imlXrCkFfg+L8l6/CqHND13tQSilKoAfA+MEZE4cyOwZCMiaYCnUqoSsAZonFO24rWqaFFK9QQui8g+pVSnjOQcspYovy20E5ELSikXYJNS6lhhTlaSW/znALdMv12BC/fIlnvBJaVUDQDLv5fvsT1FglLKHnPQXy4iP1iSS4XvACJyHQjGPMZRSSmV0Zgrifd7O6CXUioKc9dtF8xvACXdb0TkguXfy5gf9K0oxH1ekgP/HqC+ZcS/DDAQ+Oke21Sc/AQMtXwfCqy9h7YUCZb+3cXAURH5INOhEu27UqqapaWPUqoc0BXz+MZW4GlLthLnt4hMFhFXEamF+f/zFhEZRAn3WylVXinllPEd6A4cphD3eYleuauUehJzi8AW+FJEZtxjk4oEpdQKoBNmmdZLwFTgR2A18AhwFugvIrcPAD/QKKXaAyHAIf7u852CuZ+/xPqulPLAPJhni7nxtlpE3lRK1cHcEq4MHACeE5Gke2dp0WHp6hkvIj1Lut8W/9ZYftoB34jIDKVUFQp4n5fowK/RaDSa7JTkrh6NRqPR5IAO/BqNRlPK0IFfo9FoShk68Gs0Gk0pQwd+jUajKWXowK/RFAFKqU4Z6pEazf2GDvwajUZTytCBX1OqUUo9Z9G2D1NKfWYRP4tXSr2vlNqvlNqslKpmyeuplNqtlApXSq3J0D9XStVTSv1m0cffr5Sqazl9BaXUd0qpY0qp5ZaVxiil3lFKHbGcZ849cl1TitGBX1NqUUo1BgIwC2B5AmnAIKA8sF9EWgDbMK+EBvgKmCgiHphXC2ekLwcWWPTx2wLRlnQvYAzm/SDqAO2UUpWBvkBTy3neLlovNZrs6MCvKc08BrQE9lgkjh/DHKDTgVWWPMuA9kqpikAlEdlmSQ8EOlg0VB4WkTUAIpIoIgmWPKEick5E0oEwoBYQByQCXyilngIy8mo0xYYO/JrSjAICLbsaeYpIQxGZlkO+3HRNctOAzqwXkwbYWXTjW2FWFO0DBOXTZo2m0OjArynNbAaetmicZ+xh+ijm/xcZao/PAjtE5AZwTSnlZ0kfDGwTkTjgnFKqj+UcDkopxztVaNk7oKKIrMfcDeRZFI5pNLlRkjdi0WhyRUSOKKX+D/PORjZACjASuAk0VUrtA25gHgcAs/TtQktgPw38y5I+GPhMKfWm5Rz9c6nWCVirlCqL+W1h7F12S6PJE63OqdHchlIqXkQq3Gs7NJqiQnf1aDQaTSlDt/g1Go2mlKFb/BqNRlPK0IFfo9FoShk68Gs0Gk0pQwd+jUajKWXowK/RaDSljP8HFO6FNtAz7tMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lr_array = [0.1, 0.01, 0.005, 0.001,0.0005]\n",
    "\n",
    "plt.plot(acc_test_FedAvg, label='Plain, K=2, N=2')\n",
    "plt.plot(acc_test_arr_K2_G1_v3[0,0,0,0:50],label='BACC, w/o PowerAlign, K=2, N=2, sigma=0.1' )\n",
    "plt.plot(acc_test_arr_K2_G1_powerAlign[0,0,0,0:50],label='BACC, w/   PowerAlign, K=2, N=2, sigma=0.1' )\n",
    "plt.plot(acc_test_arr_K2_DP[0,0,0,0:50],label='DP, K=2, N=2, sigma=0.1' )\n",
    "\n",
    "# plt.plot(acc_test_arr_K2_G1_powerAlign[0,1,0,0:50],label='w/o Grouping, K=2, N=2, sigma=0.1, PowerAlignment' )\n",
    "# plt.plot(acc_test_arr_K4_G1[0,3,0,0:30],label='w/o Grouping, K=4, N=4, G=1, lr=0.001' )\n",
    "# plt.plot(acc_test_arr_K4_G1[0,4,0,0:30],label='w/o Grouping, K=4, N=4, G=1, lr=0.0005' )\n",
    "\n",
    "\n",
    "# plt.plot(acc_test_arr_uncoded,'r',label='uncoded')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "# plt.title(title_name)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Test accuracy(%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2. K=2, T=0 (without noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.58778525  0.58778525]\n",
      "[-0.81, 0.81]\n",
      "[-0.81]\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "# K=2\n",
    "# T=3\n",
    "# j_array = np.array(range(K+T))\n",
    "# alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(alpha_array)\n",
    "\n",
    "print(alpha_array[Signal_Alloc])\n",
    "\n",
    "print(z_array)\n",
    "\n",
    "print(dec_z_array)\n",
    "\n",
    "print(idxs_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate = 0.001\n",
      "\n",
      "\n",
      "\n",
      "z_array: [-0.81, 0.81]\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 0 25000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 0 25000 \n",
      "\n",
      "(T, sigma)= 0 1 )  0 -th Trial!!\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 5634018.6192 \n",
      "Accuracy: 861/10000 (8.61%)\n",
      "\n",
      "Round   0, Average loss 5634018.619 Test accuracy 8.610\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 57287201.1812 \n",
      "Accuracy: 1474/10000 (14.74%)\n",
      "\n",
      "Round   1, Average loss 57287201.181 Test accuracy 14.740\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 314855690.2992 \n",
      "Accuracy: 1928/10000 (19.28%)\n",
      "\n",
      "Round   2, Average loss 314855690.299 Test accuracy 19.280\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 1074226853.8176 \n",
      "Accuracy: 2011/10000 (20.11%)\n",
      "\n",
      "Round   3, Average loss 1074226853.818 Test accuracy 20.110\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 2806019532.7168 \n",
      "Accuracy: 2053/10000 (20.53%)\n",
      "\n",
      "Round   4, Average loss 2806019532.717 Test accuracy 20.530\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 6193538510.8480 \n",
      "Accuracy: 1914/10000 (19.14%)\n",
      "\n",
      "Round   5, Average loss 6193538510.848 Test accuracy 19.140\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 12129739192.1536 \n",
      "Accuracy: 1949/10000 (19.49%)\n",
      "\n",
      "Round   6, Average loss 12129739192.154 Test accuracy 19.490\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 21831574532.4032 \n",
      "Accuracy: 1977/10000 (19.77%)\n",
      "\n",
      "Round   7, Average loss 21831574532.403 Test accuracy 19.770\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 36814444650.4960 \n",
      "Accuracy: 1842/10000 (18.42%)\n",
      "\n",
      "Round   8, Average loss 36814444650.496 Test accuracy 18.420\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 58734183510.5536 \n",
      "Accuracy: 2018/10000 (20.18%)\n",
      "\n",
      "Round   9, Average loss 58734183510.554 Test accuracy 20.180\n",
      "Learning Rate = 0.0003\n",
      "\n",
      "\n",
      "\n",
      "z_array: [-0.81, 0.81]\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 0 25000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 0 25000 \n",
      "\n",
      "(T, sigma)= 0 1 )  0 -th Trial!!\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 1098.7915 \n",
      "Accuracy: 2173/10000 (21.73%)\n",
      "\n",
      "Round   0, Average loss 1098.792 Test accuracy 21.730\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 29951.1897 \n",
      "Accuracy: 1964/10000 (19.64%)\n",
      "\n",
      "Round   1, Average loss 29951.190 Test accuracy 19.640\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 201100.2640 \n",
      "Accuracy: 1931/10000 (19.31%)\n",
      "\n",
      "Round   2, Average loss 201100.264 Test accuracy 19.310\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 702837.2578 \n",
      "Accuracy: 1559/10000 (15.59%)\n",
      "\n",
      "Round   3, Average loss 702837.258 Test accuracy 15.590\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 1837671.2020 \n",
      "Accuracy: 2053/10000 (20.53%)\n",
      "\n",
      "Round   4, Average loss 1837671.202 Test accuracy 20.530\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 4034149.4798 \n",
      "Accuracy: 2194/10000 (21.94%)\n",
      "\n",
      "Round   5, Average loss 4034149.480 Test accuracy 21.940\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 7867041.9956 \n",
      "Accuracy: 2053/10000 (20.53%)\n",
      "\n",
      "Round   6, Average loss 7867041.996 Test accuracy 20.530\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 14160261.8787 \n",
      "Accuracy: 1965/10000 (19.65%)\n",
      "\n",
      "Round   7, Average loss 14160261.879 Test accuracy 19.650\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 23888037.4280 \n",
      "Accuracy: 2016/10000 (20.16%)\n",
      "\n",
      "Round   8, Average loss 23888037.428 Test accuracy 20.160\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 38257028.8201 \n",
      "Accuracy: 1915/10000 (19.15%)\n",
      "\n",
      "Round   9, Average loss 38257028.820 Test accuracy 19.150\n",
      "Learning Rate = 0.0001\n",
      "\n",
      "\n",
      "\n",
      "z_array: [-0.81, 0.81]\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 0 25000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 0 25000 \n",
      "\n",
      "(T, sigma)= 0 1 )  0 -th Trial!!\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 2.4614 \n",
      "Accuracy: 3675/10000 (36.75%)\n",
      "\n",
      "Round   0, Average loss 2.461 Test accuracy 36.750\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 14.4971 \n",
      "Accuracy: 3758/10000 (37.58%)\n",
      "\n",
      "Round   1, Average loss 14.497 Test accuracy 37.580\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 126.3403 \n",
      "Accuracy: 3399/10000 (33.99%)\n",
      "\n",
      "Round   2, Average loss 126.340 Test accuracy 33.990\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 612.5828 \n",
      "Accuracy: 3048/10000 (30.48%)\n",
      "\n",
      "Round   3, Average loss 612.583 Test accuracy 30.480\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 2030.1067 \n",
      "Accuracy: 2844/10000 (28.44%)\n",
      "\n",
      "Round   4, Average loss 2030.107 Test accuracy 28.440\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 5315.4736 \n",
      "Accuracy: 2716/10000 (27.16%)\n",
      "\n",
      "Round   5, Average loss 5315.474 Test accuracy 27.160\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 11968.0919 \n",
      "Accuracy: 2850/10000 (28.50%)\n",
      "\n",
      "Round   6, Average loss 11968.092 Test accuracy 28.500\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 23884.8934 \n",
      "Accuracy: 2434/10000 (24.34%)\n",
      "\n",
      "Round   7, Average loss 23884.893 Test accuracy 24.340\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 43964.1225 \n",
      "Accuracy: 2273/10000 (22.73%)\n",
      "\n",
      "Round   8, Average loss 43964.123 Test accuracy 22.730\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 75984.9236 \n",
      "Accuracy: 2484/10000 (24.84%)\n",
      "\n",
      "Round   9, Average loss 75984.924 Test accuracy 24.840\n",
      "Learning Rate = 3e-05\n",
      "\n",
      "\n",
      "\n",
      "z_array: [-0.81, 0.81]\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 0 25000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 0 25000 \n",
      "\n",
      "(T, sigma)= 0 1 )  0 -th Trial!!\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 1.8613 \n",
      "Accuracy: 3399/10000 (33.99%)\n",
      "\n",
      "Round   0, Average loss 1.861 Test accuracy 33.990\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 1.7457 \n",
      "Accuracy: 3916/10000 (39.16%)\n",
      "\n",
      "Round   1, Average loss 1.746 Test accuracy 39.160\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 1.7886 \n",
      "Accuracy: 4133/10000 (41.33%)\n",
      "\n",
      "Round   2, Average loss 1.789 Test accuracy 41.330\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 1.9903 \n",
      "Accuracy: 4185/10000 (41.85%)\n",
      "\n",
      "Round   3, Average loss 1.990 Test accuracy 41.850\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 2.4602 \n",
      "Accuracy: 4198/10000 (41.98%)\n",
      "\n",
      "Round   4, Average loss 2.460 Test accuracy 41.980\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 3.4327 \n",
      "Accuracy: 4168/10000 (41.68%)\n",
      "\n",
      "Round   5, Average loss 3.433 Test accuracy 41.680\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 5.2844 \n",
      "Accuracy: 4118/10000 (41.18%)\n",
      "\n",
      "Round   6, Average loss 5.284 Test accuracy 41.180\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 8.6784 \n",
      "Accuracy: 4085/10000 (40.85%)\n",
      "\n",
      "Round   7, Average loss 8.678 Test accuracy 40.850\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 14.4921 \n",
      "Accuracy: 4011/10000 (40.11%)\n",
      "\n",
      "Round   8, Average loss 14.492 Test accuracy 40.110\n",
      "selected users: [0 1]\n",
      "local update, idx= 0\n",
      "local update, idx= 1\n",
      "dec z_array= [-0.81, 0.81]\n",
      "\n",
      "Test set: Average loss: 24.2211 \n",
      "Accuracy: 4057/10000 (40.57%)\n",
      "\n",
      "Round   9, Average loss 24.221 Test accuracy 40.570\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "K = 2\n",
    "T = 0\n",
    "sigma = 1\n",
    "Noise_Alloc = []\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K+T))\n",
    "# alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "alpha_array = np.array([-5.87785252e-01, 5.87785252e-01])\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "\n",
    "N_array = [2]\n",
    "\n",
    "lr_array = [0.001, 0.0003, 0.0001, 0.00003] # 0.001 is the bset\n",
    "\n",
    "\n",
    "B_array = [0.5]\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 10\n",
    "\n",
    "\n",
    "\n",
    "loss_test_arr_K2_G1_T0_v1 = np.zeros((len(N_array),len(lr_array),N_trials,N_epochs))\n",
    "acc_test_arr_K2_G1_T0_v1  = np.zeros((len(N_array),len(lr_array),N_trials,N_epochs))\n",
    "\n",
    "for N_idx in range(len(N_array)):\n",
    "    \n",
    "    N = N_array[N_idx]\n",
    "    \n",
    "    \n",
    "           \n",
    "        \n",
    "    # print(\"alpha_array: \",alpha_array,'\\n')\n",
    "    \n",
    "    \n",
    "    for lr_idx in range(len(lr_array)):\n",
    "        \n",
    "        args.lr = lr_array[lr_idx]\n",
    "        \n",
    "        print('Learning Rate =',args.lr)\n",
    "        print('\\n\\n')\n",
    "        \n",
    "        \n",
    "        z_array = []\n",
    "#         while(len(z_array)<N):\n",
    "#             z_tmp = np.random.uniform(-1,1,1)\n",
    "#             MIS_tmp = MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_tmp], 1,sigma)\n",
    "#             if MIS_tmp < B and MIS_tmp > 0.1:\n",
    "#                 z_array.append(z_tmp[0])\n",
    "#         \n",
    "#         z_array = np.sort(z_array)\n",
    "#         print(N_idx,'!!!')\n",
    "#         if N_idx==0:\n",
    "# #             z_array = np.array([-0.94,-0.534,0.534, 0.94])\n",
    "# #         elif N_idx==1:\n",
    "# #             z_array = np.array([-0.94, -0.73, 0.73, 0.94])\n",
    "#         elif N_idx==1:\n",
    "#             z_array = np.array([-0.94, -0.125, 0.125, 0.94])\n",
    "#         else:\n",
    "# #             z_array = np.array([-0.94, -0.73, -0.534, -0.125, 0.125, 0.534, 0.73, 0.94])\n",
    "# #             z_array = np.array([-0.9, -0.81, -0.22, -0.20, 0.20, 0.22, 0.81, 0.9])\n",
    "        \n",
    "        i_array = np.array(range(N))\n",
    "#         z_array = np.cos(i_array*2*math.pi/N/2) # np.cos(i_array*2*math.pi/N/2)\n",
    "        z_array = [-0.81, 0.81]\n",
    "#         z_array = alpha_array - 0.05\n",
    "        \n",
    "        print('z_array:',z_array)\n",
    "#         for j in range(len(z_array)):\n",
    "#             print(MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_array[j]], 1,sigma))\n",
    "        \n",
    "        \n",
    "        _Noise_label = np.ones((15000*T,10)) * 0.1\n",
    "\n",
    "        X_tilde,a,b = BACC_Enc_Data_v3(encoding_input_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde,a,b = BACC_Enc_Data_v3(encoding_label_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "\n",
    "        m = N # m is the number of received result @ master\n",
    "    #     print('number of results:',m)\n",
    "\n",
    "        for trial_idx in range(N_trials):\n",
    "            print('(T, sigma)=',T,sigma,') ',trial_idx,'-th Trial!!')\n",
    "\n",
    "            net_glob = CNNCifar(args=args)\n",
    "            net_glob.cuda()\n",
    "            net_glob.train()\n",
    "\n",
    "            # copy weights\n",
    "            w_glob = net_glob.state_dict()\n",
    "\n",
    "            for iter in range(N_epochs): #args.epochs\n",
    "                w_locals, loss_locals = [], []\n",
    "                idxs_users = np.random.choice(range(N), m, replace=False)\n",
    "                idxs_users = np.sort(idxs_users)\n",
    "                print('selected users:',idxs_users)\n",
    "                \n",
    "#                 coded_net = BACC_Enc_Model_withNoise_v4(net_glob.cuda(), N, K, T, 1, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "                dec_z_array = []\n",
    "                for idx in idxs_users: #for idx in range(N):\n",
    "                    print('local update, idx=',idx)\n",
    "                    local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[idx,:,:], label=y_tilde[idx,:,:])\n",
    "#                     w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "                    w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                    w_locals.append(copy.deepcopy(w))\n",
    "                    loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                    dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "                # update global weights\n",
    "                #w_glob = FedAvg(w_locals)\n",
    "                print('dec z_array=',dec_z_array)\n",
    "                w_glob = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "\n",
    "                # copy weight to net_glob\n",
    "                net_glob.load_state_dict(w_glob)\n",
    "\n",
    "                # print loss\n",
    "            #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "            #     loss_train_arr.append(loss_train)\n",
    "\n",
    "                acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "                acc_test_arr_K2_G1_T0_v1[N_idx][lr_idx][trial_idx][iter] = acc_test\n",
    "                loss_test_arr_K2_G1_T0_v1[N_idx][lr_idx][trial_idx][iter] = loss_test\n",
    "                if iter % 1 ==0:\n",
    "                    print('Round {:3d}, Average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_test, acc_test))\n",
    "                #print(loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lr_array = [0.1, 0.01, 0.005, 0.001,0.0005]\n",
    "\n",
    "plt.plot(acc_test_arr_K4_G1[0,0,0,0:30],label='w/o Grouping, K=4, N=4, G=1, lr=0.1' )\n",
    "plt.plot(acc_test_arr_K4_G1[0,1,0,0:30],label='w/o Grouping, K=4, N=4, G=1, lr=0.01' )\n",
    "plt.plot(acc_test_arr_K4_G1[0,2,0,0:30],label='w/o Grouping, K=4, N=4, G=1, lr=0.005' )\n",
    "plt.plot(acc_test_arr_K4_G1[0,3,0,0:30],label='w/o Grouping, K=4, N=4, G=1, lr=0.001' )\n",
    "plt.plot(acc_test_arr_K4_G1[0,4,0,0:30],label='w/o Grouping, K=4, N=4, G=1, lr=0.0005' )\n",
    "\n",
    "\n",
    "# plt.plot(acc_test_arr_uncoded,'r',label='uncoded')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "# plt.title(title_name)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Test accuracy(%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(acc_test_arr_K4_G1_v1[0,0,0,0:30],label='w/o Grouping, K=4, N=4, G=1, sigma=0' )\n",
    "plt.plot(acc_test_arr_K4_G1_v1[1,0,0,0:30],label='w/o Grouping, K=4, N=4, G=1, sigma=0.1' )\n",
    "plt.plot(acc_test_arr_K4_G1_v1[2,0,0,0:30],label='w/o Grouping, K=4, N=4, G=1, sigma=0.3' )\n",
    "plt.plot(acc_test_arr_K4_G1_v1[3,0,0,0:30],label='w/o Grouping, K=4, N=4, G=1, sigma=0.5' )\n",
    "plt.plot(acc_test_arr_K4_G1_v1[4,0,0,0:30],label='w/o Grouping, K=4, N=4, G=1, sigma=1' )\n",
    "\n",
    "\n",
    "# plt.plot(acc_test_arr_uncoded,'r',label='uncoded')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "# plt.title(title_name)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Test accuracy(%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. K=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "class my_argument:    \n",
    "    epochs    = 16    #\"rounds of training\"\n",
    "    num_users = 4  # \"number of users: K\"\n",
    "    frac      = 0.5 #\"the fraction of clients: C\"\n",
    "    local_ep  = 5 #\"the number of local epochs: E\"\n",
    "    local_bs  = 50  #\"local batch size: B\"\n",
    "    bs        = 50 #\"test batch size\"\n",
    "    lr        = 0.0001 #\"learning rate\"\n",
    "    momentum  = 0.5 # \"SGD momentum (default: 0.5)\"\n",
    "    weight_decay = 5e-4\n",
    "    split     = 'user' # \"train-test split type, user or sample\"\n",
    "    opt='ADAM'\n",
    "    loss='Custom' # 'Custom' or 'Default'\n",
    "\n",
    "    # model arguments\n",
    "    model = 'cnn'\n",
    "    kernel_num=9 #, help='number of each kind of kernel')\n",
    "    kernel_sizes='3,4,5' #  help='comma-separated kernel size to use for convolution')\n",
    "    norm='batch_norm' #, help=\"batch_norm, layer_norm, or None\")\n",
    "    num_filters=32 #, help=\"number of filters for conv nets\")\n",
    "    max_pool='True' #help=\"Whether use max pooling rather than strided convolutions\")\n",
    "\n",
    "    # other arguments\n",
    "    dataset='cifar' #, help=\"name of dataset\")\n",
    "    iid=1\n",
    "    num_classes=10#, help=\"number of classes\")\n",
    "    num_channels=1#, help=\"number of channels of imges\")\n",
    "    gpu=1#, help=\"GPU ID, -1 for CPU\")\n",
    "    stopping_rounds=10#, help='rounds of early stopping')\n",
    "    verbose='False'#, help='verbose print')\n",
    "    seed=1#, help='random seed (default: 1)')\n",
    "    \n",
    "args = my_argument()\n",
    "\n",
    "args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)\n",
    "args.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(args.device)\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Normalize the test set same as training set without augmentation\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# trainset = torchvision.datasets.CIFAR10(\n",
    "#     root=\"./data/cifar\", train=True, download=True, transform=transform_train)\n",
    "# trainloader = torch.utils.data.DataLoader(\n",
    "#     trainset, batch_size=args.bs, shuffle=True, num_workers=2)\n",
    "\n",
    "# testset = torchvision.datasets.CIFAR10(\n",
    "#     root=\"./data/cifar\", train=False, download=True, transform=transform_test)\n",
    "# testloader = torch.utils.data.DataLoader(\n",
    "#     testset, batch_size=args.bs, shuffle=False, num_workers=2)\n",
    "\n",
    "dataset_train = datasets.CIFAR10('../data/cifar', train=True, download=True, transform=transform_train)\n",
    "dataset_test = datasets.CIFAR10('../data/cifar', train=False, download=True, transform=transform_test)\n",
    "if args.iid:\n",
    "    dict_users = cifar_iid(dataset_train, args.num_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of X: (50000, 3072)\n",
      "size of Y: (50000, 10)\n",
      "(50000, 10)\n",
      "2.071376214594275\n"
     ]
    }
   ],
   "source": [
    "encoding_input_array_np = np.empty((len(dataset_train),32*32*3))\n",
    "encoding_label_array_np = np.empty((len(dataset_train),args.num_classes))\n",
    "print(\"size of X:\" ,encoding_input_array_np.shape)\n",
    "print(\"size of Y:\" ,encoding_label_array_np.shape)\n",
    "\n",
    "Size_submatrices = int(50000/args.num_users)\n",
    "\n",
    "\n",
    "for i in range(args.num_users):\n",
    "    \n",
    "    stt_pos = i*Size_submatrices\n",
    "    end_pos = (i+1)*Size_submatrices\n",
    "#     print(i,stt_pos,end_pos)\n",
    "    Temp_train = DataLoader(DatasetSplit(dataset_train, dict_users[i]), batch_size=Size_submatrices, shuffle=True)\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(Temp_train):\n",
    "        \n",
    "        images_np = images.detach().cpu().numpy()\n",
    "        \n",
    "#         print(np.size(images_np))\n",
    "        encoding_input_array_np[stt_pos:end_pos,:] = np.reshape(images_np, (Size_submatrices,32*32*3))\n",
    "#         print(encoding_input_array_np[stt_pos:end_pos,:].shape)\n",
    "\n",
    "        onehot_labels = torch.nn.functional.one_hot(labels,num_classes=args.num_classes)\n",
    "        labels_np = onehot_labels.detach().cpu().numpy()\n",
    "#         print(labels_np.shape)\n",
    "        encoding_label_array_np[stt_pos:end_pos,:] = labels_np\n",
    "    \n",
    "print(np.shape(encoding_label_array_np))\n",
    "\n",
    "avg_power = np.sum(encoding_input_array_np*encoding_input_array_np, axis=1)/np.shape(encoding_input_array_np)[1]\n",
    "avg_power = np.sum(avg_power)/np.shape(encoding_input_array_np)[0]\n",
    "print(avg_power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate = 0.001\n",
      "\n",
      "\n",
      "\n",
      "z_array: [-0.94  -0.125  0.125  0.94 ]\n",
      "2.21641645756389\n",
      "2.3227453960929183\n",
      "2.3227453960929143\n",
      "2.2164164575638967\n",
      "@BACC_Enc: N,K,T, m_i= 4 4 5 12500 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 4 5 12500 \n",
      "\n",
      "Adjust the power of X_tilde\n",
      "0 0.37594425519342023\n",
      "power after adjusting = 2.073207572302197\n",
      "1 0.5520716062908136\n",
      "power after adjusting = 2.0732075723021977\n",
      "2 0.5522205126489026\n",
      "power after adjusting = 2.0732075723021977\n",
      "3 0.3753727500517572\n",
      "power after adjusting = 2.0732075723021977\n",
      "\n",
      "(T, sigma)= 5 0.1 )  0 -th Trial!!\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.2002 \n",
      "Accuracy: 2535/10000 (25.35%)\n",
      "\n",
      "Round   0, Average loss 2.200 Test accuracy 25.350\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8854 \n",
      "Accuracy: 4567/10000 (45.67%)\n",
      "\n",
      "Round   1, Average loss 1.885 Test accuracy 45.670\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.7508 \n",
      "Accuracy: 4995/10000 (49.95%)\n",
      "\n",
      "Round   2, Average loss 1.751 Test accuracy 49.950\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.7205 \n",
      "Accuracy: 5183/10000 (51.83%)\n",
      "\n",
      "Round   3, Average loss 1.721 Test accuracy 51.830\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6996 \n",
      "Accuracy: 5264/10000 (52.64%)\n",
      "\n",
      "Round   4, Average loss 1.700 Test accuracy 52.640\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6854 \n",
      "Accuracy: 5316/10000 (53.16%)\n",
      "\n",
      "Round   5, Average loss 1.685 Test accuracy 53.160\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6836 \n",
      "Accuracy: 5322/10000 (53.22%)\n",
      "\n",
      "Round   6, Average loss 1.684 Test accuracy 53.220\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6863 \n",
      "Accuracy: 5364/10000 (53.64%)\n",
      "\n",
      "Round   7, Average loss 1.686 Test accuracy 53.640\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6724 \n",
      "Accuracy: 5369/10000 (53.69%)\n",
      "\n",
      "Round   8, Average loss 1.672 Test accuracy 53.690\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6724 \n",
      "Accuracy: 5408/10000 (54.08%)\n",
      "\n",
      "Round   9, Average loss 1.672 Test accuracy 54.080\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6625 \n",
      "Accuracy: 5438/10000 (54.38%)\n",
      "\n",
      "Round  10, Average loss 1.662 Test accuracy 54.380\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6606 \n",
      "Accuracy: 5423/10000 (54.23%)\n",
      "\n",
      "Round  11, Average loss 1.661 Test accuracy 54.230\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6581 \n",
      "Accuracy: 5436/10000 (54.36%)\n",
      "\n",
      "Round  12, Average loss 1.658 Test accuracy 54.360\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6608 \n",
      "Accuracy: 5443/10000 (54.43%)\n",
      "\n",
      "Round  13, Average loss 1.661 Test accuracy 54.430\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6595 \n",
      "Accuracy: 5487/10000 (54.87%)\n",
      "\n",
      "Round  14, Average loss 1.659 Test accuracy 54.870\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6531 \n",
      "Accuracy: 5458/10000 (54.58%)\n",
      "\n",
      "Round  15, Average loss 1.653 Test accuracy 54.580\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6514 \n",
      "Accuracy: 5451/10000 (54.51%)\n",
      "\n",
      "Round  16, Average loss 1.651 Test accuracy 54.510\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6540 \n",
      "Accuracy: 5483/10000 (54.83%)\n",
      "\n",
      "Round  17, Average loss 1.654 Test accuracy 54.830\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6574 \n",
      "Accuracy: 5465/10000 (54.65%)\n",
      "\n",
      "Round  18, Average loss 1.657 Test accuracy 54.650\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6536 \n",
      "Accuracy: 5492/10000 (54.92%)\n",
      "\n",
      "Round  19, Average loss 1.654 Test accuracy 54.920\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6485 \n",
      "Accuracy: 5521/10000 (55.21%)\n",
      "\n",
      "Round  20, Average loss 1.648 Test accuracy 55.210\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6482 \n",
      "Accuracy: 5496/10000 (54.96%)\n",
      "\n",
      "Round  21, Average loss 1.648 Test accuracy 54.960\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6457 \n",
      "Accuracy: 5512/10000 (55.12%)\n",
      "\n",
      "Round  22, Average loss 1.646 Test accuracy 55.120\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6493 \n",
      "Accuracy: 5536/10000 (55.36%)\n",
      "\n",
      "Round  23, Average loss 1.649 Test accuracy 55.360\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6473 \n",
      "Accuracy: 5556/10000 (55.56%)\n",
      "\n",
      "Round  24, Average loss 1.647 Test accuracy 55.560\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6486 \n",
      "Accuracy: 5526/10000 (55.26%)\n",
      "\n",
      "Round  25, Average loss 1.649 Test accuracy 55.260\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6480 \n",
      "Accuracy: 5533/10000 (55.33%)\n",
      "\n",
      "Round  26, Average loss 1.648 Test accuracy 55.330\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6496 \n",
      "Accuracy: 5538/10000 (55.38%)\n",
      "\n",
      "Round  27, Average loss 1.650 Test accuracy 55.380\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6426 \n",
      "Accuracy: 5559/10000 (55.59%)\n",
      "\n",
      "Round  28, Average loss 1.643 Test accuracy 55.590\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6359 \n",
      "Accuracy: 5577/10000 (55.77%)\n",
      "\n",
      "Round  29, Average loss 1.636 Test accuracy 55.770\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6444 \n",
      "Accuracy: 5546/10000 (55.46%)\n",
      "\n",
      "Round  30, Average loss 1.644 Test accuracy 55.460\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6381 \n",
      "Accuracy: 5528/10000 (55.28%)\n",
      "\n",
      "Round  31, Average loss 1.638 Test accuracy 55.280\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6433 \n",
      "Accuracy: 5533/10000 (55.33%)\n",
      "\n",
      "Round  32, Average loss 1.643 Test accuracy 55.330\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6399 \n",
      "Accuracy: 5548/10000 (55.48%)\n",
      "\n",
      "Round  33, Average loss 1.640 Test accuracy 55.480\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6402 \n",
      "Accuracy: 5533/10000 (55.33%)\n",
      "\n",
      "Round  34, Average loss 1.640 Test accuracy 55.330\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6422 \n",
      "Accuracy: 5537/10000 (55.37%)\n",
      "\n",
      "Round  35, Average loss 1.642 Test accuracy 55.370\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6434 \n",
      "Accuracy: 5549/10000 (55.49%)\n",
      "\n",
      "Round  36, Average loss 1.643 Test accuracy 55.490\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6448 \n",
      "Accuracy: 5510/10000 (55.10%)\n",
      "\n",
      "Round  37, Average loss 1.645 Test accuracy 55.100\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6374 \n",
      "Accuracy: 5540/10000 (55.40%)\n",
      "\n",
      "Round  38, Average loss 1.637 Test accuracy 55.400\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6399 \n",
      "Accuracy: 5549/10000 (55.49%)\n",
      "\n",
      "Round  39, Average loss 1.640 Test accuracy 55.490\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6371 \n",
      "Accuracy: 5595/10000 (55.95%)\n",
      "\n",
      "Round  40, Average loss 1.637 Test accuracy 55.950\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6401 \n",
      "Accuracy: 5545/10000 (55.45%)\n",
      "\n",
      "Round  41, Average loss 1.640 Test accuracy 55.450\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6331 \n",
      "Accuracy: 5569/10000 (55.69%)\n",
      "\n",
      "Round  42, Average loss 1.633 Test accuracy 55.690\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6336 \n",
      "Accuracy: 5627/10000 (56.27%)\n",
      "\n",
      "Round  43, Average loss 1.634 Test accuracy 56.270\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6295 \n",
      "Accuracy: 5592/10000 (55.92%)\n",
      "\n",
      "Round  44, Average loss 1.629 Test accuracy 55.920\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6287 \n",
      "Accuracy: 5613/10000 (56.13%)\n",
      "\n",
      "Round  45, Average loss 1.629 Test accuracy 56.130\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6251 \n",
      "Accuracy: 5618/10000 (56.18%)\n",
      "\n",
      "Round  46, Average loss 1.625 Test accuracy 56.180\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6267 \n",
      "Accuracy: 5598/10000 (55.98%)\n",
      "\n",
      "Round  47, Average loss 1.627 Test accuracy 55.980\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6303 \n",
      "Accuracy: 5606/10000 (56.06%)\n",
      "\n",
      "Round  48, Average loss 1.630 Test accuracy 56.060\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6190 \n",
      "Accuracy: 5654/10000 (56.54%)\n",
      "\n",
      "Round  49, Average loss 1.619 Test accuracy 56.540\n",
      "Learning Rate = 0.001\n",
      "\n",
      "\n",
      "\n",
      "z_array: [-0.94  -0.125  0.125  0.94 ]\n",
      "1.1478225193847706\n",
      "1.222976255153871\n",
      "1.2229762551538679\n",
      "1.147822519384775\n",
      "@BACC_Enc: N,K,T, m_i= 4 4 5 12500 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 4 5 12500 \n",
      "\n",
      "Adjust the power of X_tilde\n",
      "0 0.41290316933628163\n",
      "power after adjusting = 2.0732075723021977\n",
      "1 0.610368753363087\n",
      "power after adjusting = 2.0732075723021977\n",
      "2 0.610481572856639\n",
      "power after adjusting = 2.0732075723021977\n",
      "3 0.4123819764120661\n",
      "power after adjusting = 2.0732075723021977\n",
      "\n",
      "(T, sigma)= 5 0.3 )  0 -th Trial!!\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.1711 \n",
      "Accuracy: 2830/10000 (28.30%)\n",
      "\n",
      "Round   0, Average loss 2.171 Test accuracy 28.300\n",
      "selected users: [0 1 2 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.8488 \n",
      "Accuracy: 4701/10000 (47.01%)\n",
      "\n",
      "Round   1, Average loss 1.849 Test accuracy 47.010\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.7441 \n",
      "Accuracy: 4980/10000 (49.80%)\n",
      "\n",
      "Round   2, Average loss 1.744 Test accuracy 49.800\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.7110 \n",
      "Accuracy: 5161/10000 (51.61%)\n",
      "\n",
      "Round   3, Average loss 1.711 Test accuracy 51.610\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6893 \n",
      "Accuracy: 5258/10000 (52.58%)\n",
      "\n",
      "Round   4, Average loss 1.689 Test accuracy 52.580\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6655 \n",
      "Accuracy: 5356/10000 (53.56%)\n",
      "\n",
      "Round   5, Average loss 1.666 Test accuracy 53.560\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6523 \n",
      "Accuracy: 5430/10000 (54.30%)\n",
      "\n",
      "Round   6, Average loss 1.652 Test accuracy 54.300\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6436 \n",
      "Accuracy: 5469/10000 (54.69%)\n",
      "\n",
      "Round   7, Average loss 1.644 Test accuracy 54.690\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6374 \n",
      "Accuracy: 5500/10000 (55.00%)\n",
      "\n",
      "Round   8, Average loss 1.637 Test accuracy 55.000\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6406 \n",
      "Accuracy: 5521/10000 (55.21%)\n",
      "\n",
      "Round   9, Average loss 1.641 Test accuracy 55.210\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6345 \n",
      "Accuracy: 5532/10000 (55.32%)\n",
      "\n",
      "Round  10, Average loss 1.634 Test accuracy 55.320\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6367 \n",
      "Accuracy: 5547/10000 (55.47%)\n",
      "\n",
      "Round  11, Average loss 1.637 Test accuracy 55.470\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6343 \n",
      "Accuracy: 5527/10000 (55.27%)\n",
      "\n",
      "Round  12, Average loss 1.634 Test accuracy 55.270\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6352 \n",
      "Accuracy: 5529/10000 (55.29%)\n",
      "\n",
      "Round  13, Average loss 1.635 Test accuracy 55.290\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6320 \n",
      "Accuracy: 5515/10000 (55.15%)\n",
      "\n",
      "Round  14, Average loss 1.632 Test accuracy 55.150\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6352 \n",
      "Accuracy: 5497/10000 (54.97%)\n",
      "\n",
      "Round  15, Average loss 1.635 Test accuracy 54.970\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6396 \n",
      "Accuracy: 5489/10000 (54.89%)\n",
      "\n",
      "Round  16, Average loss 1.640 Test accuracy 54.890\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6345 \n",
      "Accuracy: 5504/10000 (55.04%)\n",
      "\n",
      "Round  17, Average loss 1.634 Test accuracy 55.040\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6325 \n",
      "Accuracy: 5536/10000 (55.36%)\n",
      "\n",
      "Round  18, Average loss 1.633 Test accuracy 55.360\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6347 \n",
      "Accuracy: 5522/10000 (55.22%)\n",
      "\n",
      "Round  19, Average loss 1.635 Test accuracy 55.220\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6325 \n",
      "Accuracy: 5557/10000 (55.57%)\n",
      "\n",
      "Round  20, Average loss 1.632 Test accuracy 55.570\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6311 \n",
      "Accuracy: 5585/10000 (55.85%)\n",
      "\n",
      "Round  21, Average loss 1.631 Test accuracy 55.850\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6260 \n",
      "Accuracy: 5577/10000 (55.77%)\n",
      "\n",
      "Round  22, Average loss 1.626 Test accuracy 55.770\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6259 \n",
      "Accuracy: 5566/10000 (55.66%)\n",
      "\n",
      "Round  23, Average loss 1.626 Test accuracy 55.660\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6256 \n",
      "Accuracy: 5592/10000 (55.92%)\n",
      "\n",
      "Round  24, Average loss 1.626 Test accuracy 55.920\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6232 \n",
      "Accuracy: 5581/10000 (55.81%)\n",
      "\n",
      "Round  25, Average loss 1.623 Test accuracy 55.810\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6242 \n",
      "Accuracy: 5587/10000 (55.87%)\n",
      "\n",
      "Round  26, Average loss 1.624 Test accuracy 55.870\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6230 \n",
      "Accuracy: 5587/10000 (55.87%)\n",
      "\n",
      "Round  27, Average loss 1.623 Test accuracy 55.870\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6213 \n",
      "Accuracy: 5580/10000 (55.80%)\n",
      "\n",
      "Round  28, Average loss 1.621 Test accuracy 55.800\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6252 \n",
      "Accuracy: 5566/10000 (55.66%)\n",
      "\n",
      "Round  29, Average loss 1.625 Test accuracy 55.660\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6281 \n",
      "Accuracy: 5543/10000 (55.43%)\n",
      "\n",
      "Round  30, Average loss 1.628 Test accuracy 55.430\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6260 \n",
      "Accuracy: 5549/10000 (55.49%)\n",
      "\n",
      "Round  31, Average loss 1.626 Test accuracy 55.490\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6279 \n",
      "Accuracy: 5564/10000 (55.64%)\n",
      "\n",
      "Round  32, Average loss 1.628 Test accuracy 55.640\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6231 \n",
      "Accuracy: 5566/10000 (55.66%)\n",
      "\n",
      "Round  33, Average loss 1.623 Test accuracy 55.660\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6255 \n",
      "Accuracy: 5561/10000 (55.61%)\n",
      "\n",
      "Round  34, Average loss 1.626 Test accuracy 55.610\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6272 \n",
      "Accuracy: 5564/10000 (55.64%)\n",
      "\n",
      "Round  35, Average loss 1.627 Test accuracy 55.640\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6230 \n",
      "Accuracy: 5565/10000 (55.65%)\n",
      "\n",
      "Round  36, Average loss 1.623 Test accuracy 55.650\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6156 \n",
      "Accuracy: 5565/10000 (55.65%)\n",
      "\n",
      "Round  37, Average loss 1.616 Test accuracy 55.650\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6206 \n",
      "Accuracy: 5569/10000 (55.69%)\n",
      "\n",
      "Round  38, Average loss 1.621 Test accuracy 55.690\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6232 \n",
      "Accuracy: 5604/10000 (56.04%)\n",
      "\n",
      "Round  39, Average loss 1.623 Test accuracy 56.040\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6186 \n",
      "Accuracy: 5579/10000 (55.79%)\n",
      "\n",
      "Round  40, Average loss 1.619 Test accuracy 55.790\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6206 \n",
      "Accuracy: 5590/10000 (55.90%)\n",
      "\n",
      "Round  41, Average loss 1.621 Test accuracy 55.900\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6166 \n",
      "Accuracy: 5590/10000 (55.90%)\n",
      "\n",
      "Round  42, Average loss 1.617 Test accuracy 55.900\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6205 \n",
      "Accuracy: 5587/10000 (55.87%)\n",
      "\n",
      "Round  43, Average loss 1.620 Test accuracy 55.870\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6223 \n",
      "Accuracy: 5571/10000 (55.71%)\n",
      "\n",
      "Round  44, Average loss 1.622 Test accuracy 55.710\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6260 \n",
      "Accuracy: 5563/10000 (55.63%)\n",
      "\n",
      "Round  45, Average loss 1.626 Test accuracy 55.630\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6285 \n",
      "Accuracy: 5531/10000 (55.31%)\n",
      "\n",
      "Round  46, Average loss 1.628 Test accuracy 55.310\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6286 \n",
      "Accuracy: 5538/10000 (55.38%)\n",
      "\n",
      "Round  47, Average loss 1.629 Test accuracy 55.380\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6264 \n",
      "Accuracy: 5567/10000 (55.67%)\n",
      "\n",
      "Round  48, Average loss 1.626 Test accuracy 55.670\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6236 \n",
      "Accuracy: 5562/10000 (55.62%)\n",
      "\n",
      "Round  49, Average loss 1.624 Test accuracy 55.620\n",
      "Learning Rate = 0.001\n",
      "\n",
      "\n",
      "\n",
      "z_array: [-0.94  -0.125  0.125  0.94 ]\n",
      "0.4486236179368535\n",
      "0.4857187736123886\n",
      "0.48571877361238697\n",
      "0.4486236179368556\n",
      "@BACC_Enc: N,K,T, m_i= 4 4 5 12500 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 4 4 5 12500 \n",
      "\n",
      "Adjust the power of X_tilde\n",
      "0 0.8336726604550079\n",
      "power after adjusting = 2.0732075723021977\n",
      "1 1.273043591738011\n",
      "power after adjusting = 2.073207572302197\n",
      "2 1.2728835273939056\n",
      "power after adjusting = 2.0732075723021977\n",
      "3 0.8326453192585589\n",
      "power after adjusting = 2.0732075723021977\n",
      "\n",
      "(T, sigma)= 5 1 )  0 -th Trial!!\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 2.1785 \n",
      "Accuracy: 3165/10000 (31.65%)\n",
      "\n",
      "Round   0, Average loss 2.178 Test accuracy 31.650\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.8450 \n",
      "Accuracy: 3947/10000 (39.47%)\n",
      "\n",
      "Round   1, Average loss 1.845 Test accuracy 39.470\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.7789 \n",
      "Accuracy: 4034/10000 (40.34%)\n",
      "\n",
      "Round   2, Average loss 1.779 Test accuracy 40.340\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.7576 \n",
      "Accuracy: 4168/10000 (41.68%)\n",
      "\n",
      "Round   3, Average loss 1.758 Test accuracy 41.680\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.7381 \n",
      "Accuracy: 4256/10000 (42.56%)\n",
      "\n",
      "Round   4, Average loss 1.738 Test accuracy 42.560\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.7252 \n",
      "Accuracy: 4293/10000 (42.93%)\n",
      "\n",
      "Round   5, Average loss 1.725 Test accuracy 42.930\n",
      "selected users: [0 1 2 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.7174 \n",
      "Accuracy: 4312/10000 (43.12%)\n",
      "\n",
      "Round   6, Average loss 1.717 Test accuracy 43.120\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.7104 \n",
      "Accuracy: 4382/10000 (43.82%)\n",
      "\n",
      "Round   7, Average loss 1.710 Test accuracy 43.820\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.7012 \n",
      "Accuracy: 4393/10000 (43.93%)\n",
      "\n",
      "Round   8, Average loss 1.701 Test accuracy 43.930\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6972 \n",
      "Accuracy: 4359/10000 (43.59%)\n",
      "\n",
      "Round   9, Average loss 1.697 Test accuracy 43.590\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6956 \n",
      "Accuracy: 4351/10000 (43.51%)\n",
      "\n",
      "Round  10, Average loss 1.696 Test accuracy 43.510\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6969 \n",
      "Accuracy: 4350/10000 (43.50%)\n",
      "\n",
      "Round  11, Average loss 1.697 Test accuracy 43.500\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6926 \n",
      "Accuracy: 4376/10000 (43.76%)\n",
      "\n",
      "Round  12, Average loss 1.693 Test accuracy 43.760\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6907 \n",
      "Accuracy: 4379/10000 (43.79%)\n",
      "\n",
      "Round  13, Average loss 1.691 Test accuracy 43.790\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6876 \n",
      "Accuracy: 4372/10000 (43.72%)\n",
      "\n",
      "Round  14, Average loss 1.688 Test accuracy 43.720\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6861 \n",
      "Accuracy: 4385/10000 (43.85%)\n",
      "\n",
      "Round  15, Average loss 1.686 Test accuracy 43.850\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6785 \n",
      "Accuracy: 4406/10000 (44.06%)\n",
      "\n",
      "Round  16, Average loss 1.679 Test accuracy 44.060\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6765 \n",
      "Accuracy: 4447/10000 (44.47%)\n",
      "\n",
      "Round  17, Average loss 1.677 Test accuracy 44.470\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6754 \n",
      "Accuracy: 4434/10000 (44.34%)\n",
      "\n",
      "Round  18, Average loss 1.675 Test accuracy 44.340\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6792 \n",
      "Accuracy: 4450/10000 (44.50%)\n",
      "\n",
      "Round  19, Average loss 1.679 Test accuracy 44.500\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6795 \n",
      "Accuracy: 4437/10000 (44.37%)\n",
      "\n",
      "Round  20, Average loss 1.679 Test accuracy 44.370\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6777 \n",
      "Accuracy: 4435/10000 (44.35%)\n",
      "\n",
      "Round  21, Average loss 1.678 Test accuracy 44.350\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6774 \n",
      "Accuracy: 4444/10000 (44.44%)\n",
      "\n",
      "Round  22, Average loss 1.677 Test accuracy 44.440\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6824 \n",
      "Accuracy: 4402/10000 (44.02%)\n",
      "\n",
      "Round  23, Average loss 1.682 Test accuracy 44.020\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6847 \n",
      "Accuracy: 4394/10000 (43.94%)\n",
      "\n",
      "Round  24, Average loss 1.685 Test accuracy 43.940\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6835 \n",
      "Accuracy: 4369/10000 (43.69%)\n",
      "\n",
      "Round  25, Average loss 1.684 Test accuracy 43.690\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6885 \n",
      "Accuracy: 4367/10000 (43.67%)\n",
      "\n",
      "Round  26, Average loss 1.688 Test accuracy 43.670\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6862 \n",
      "Accuracy: 4384/10000 (43.84%)\n",
      "\n",
      "Round  27, Average loss 1.686 Test accuracy 43.840\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6871 \n",
      "Accuracy: 4396/10000 (43.96%)\n",
      "\n",
      "Round  28, Average loss 1.687 Test accuracy 43.960\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6853 \n",
      "Accuracy: 4409/10000 (44.09%)\n",
      "\n",
      "Round  29, Average loss 1.685 Test accuracy 44.090\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6805 \n",
      "Accuracy: 4392/10000 (43.92%)\n",
      "\n",
      "Round  30, Average loss 1.681 Test accuracy 43.920\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6821 \n",
      "Accuracy: 4367/10000 (43.67%)\n",
      "\n",
      "Round  31, Average loss 1.682 Test accuracy 43.670\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6711 \n",
      "Accuracy: 4432/10000 (44.32%)\n",
      "\n",
      "Round  32, Average loss 1.671 Test accuracy 44.320\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6711 \n",
      "Accuracy: 4453/10000 (44.53%)\n",
      "\n",
      "Round  33, Average loss 1.671 Test accuracy 44.530\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6731 \n",
      "Accuracy: 4421/10000 (44.21%)\n",
      "\n",
      "Round  34, Average loss 1.673 Test accuracy 44.210\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6775 \n",
      "Accuracy: 4458/10000 (44.58%)\n",
      "\n",
      "Round  35, Average loss 1.677 Test accuracy 44.580\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6831 \n",
      "Accuracy: 4415/10000 (44.15%)\n",
      "\n",
      "Round  36, Average loss 1.683 Test accuracy 44.150\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6740 \n",
      "Accuracy: 4483/10000 (44.83%)\n",
      "\n",
      "Round  37, Average loss 1.674 Test accuracy 44.830\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6760 \n",
      "Accuracy: 4436/10000 (44.36%)\n",
      "\n",
      "Round  38, Average loss 1.676 Test accuracy 44.360\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6718 \n",
      "Accuracy: 4486/10000 (44.86%)\n",
      "\n",
      "Round  39, Average loss 1.672 Test accuracy 44.860\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6713 \n",
      "Accuracy: 4481/10000 (44.81%)\n",
      "\n",
      "Round  40, Average loss 1.671 Test accuracy 44.810\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6725 \n",
      "Accuracy: 4466/10000 (44.66%)\n",
      "\n",
      "Round  41, Average loss 1.673 Test accuracy 44.660\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6755 \n",
      "Accuracy: 4435/10000 (44.35%)\n",
      "\n",
      "Round  42, Average loss 1.675 Test accuracy 44.350\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6825 \n",
      "Accuracy: 4419/10000 (44.19%)\n",
      "\n",
      "Round  43, Average loss 1.682 Test accuracy 44.190\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6826 \n",
      "Accuracy: 4372/10000 (43.72%)\n",
      "\n",
      "Round  44, Average loss 1.683 Test accuracy 43.720\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6856 \n",
      "Accuracy: 4359/10000 (43.59%)\n",
      "\n",
      "Round  45, Average loss 1.686 Test accuracy 43.590\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6821 \n",
      "Accuracy: 4414/10000 (44.14%)\n",
      "\n",
      "Round  46, Average loss 1.682 Test accuracy 44.140\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6882 \n",
      "Accuracy: 4394/10000 (43.94%)\n",
      "\n",
      "Round  47, Average loss 1.688 Test accuracy 43.940\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6804 \n",
      "Accuracy: 4446/10000 (44.46%)\n",
      "\n",
      "Round  48, Average loss 1.680 Test accuracy 44.460\n",
      "selected users: [0 1 2 3]\n",
      "\n",
      "Test set: Average loss: 1.6779 \n",
      "Accuracy: 4390/10000 (43.90%)\n",
      "\n",
      "Round  49, Average loss 1.678 Test accuracy 43.900\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "K = 4\n",
    "T = 5\n",
    "# sigma = 0.1\n",
    "Noise_Alloc = [0,2,4,6,8]\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "\n",
    "N_array = [4]\n",
    "lr_array = [0.001]\n",
    "sigma_array = [0.1, 0.3, 1]\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 50\n",
    "\n",
    "N = 4\n",
    "\n",
    "loss_test_arr_K4_G1 = np.zeros((len(sigma_array),len(lr_array),N_trials,N_epochs))\n",
    "acc_test_arr_K4_G1  = np.zeros((len(sigma_array),len(lr_array),N_trials,N_epochs))\n",
    "\n",
    "for sigma_idx in range(len(sigma_array)):\n",
    "    \n",
    "    \n",
    "    \n",
    "    sigma = sigma_array[sigma_idx]\n",
    "    \n",
    "    \n",
    "           \n",
    "        \n",
    "    # print(\"alpha_array: \",alpha_array,'\\n')\n",
    "    \n",
    "    \n",
    "    for lr_idx in range(len(lr_array)):\n",
    "        \n",
    "        args.lr = lr_array[lr_idx]\n",
    "        \n",
    "        print('Learning Rate =',args.lr)\n",
    "        print('\\n\\n')\n",
    "        \n",
    "        z_array = []\n",
    "#         while(len(z_array)<N):\n",
    "#             z_tmp = np.random.uniform(-1,1,1)\n",
    "#             MIS_tmp = MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_tmp], 1,sigma)\n",
    "#             if MIS_tmp < B and MIS_tmp > 0.1:\n",
    "#                 z_array.append(z_tmp[0])\n",
    "#         \n",
    "#         z_array = np.sort(z_array)\n",
    "#         print(N_idx,'!!!')\n",
    "#         if N_idx==0:\n",
    "# #             z_array = np.array([-0.94,-0.534,0.534, 0.94])\n",
    "# #         elif N_idx==1:\n",
    "# #             z_array = np.array([-0.94, -0.73, 0.73, 0.94])\n",
    "#         elif N_idx==1:\n",
    "#             z_array = np.array([-0.94, -0.125, 0.125, 0.94])\n",
    "#         else:\n",
    "# #             z_array = np.array([-0.94, -0.73, -0.534, -0.125, 0.125, 0.534, 0.73, 0.94])\n",
    "# #             z_array = np.array([-0.9, -0.81, -0.22, -0.20, 0.20, 0.22, 0.81, 0.9])\n",
    "        \n",
    "        z_array = np.array([-0.94, -0.125, 0.125, 0.94])\n",
    "        \n",
    "        print('z_array:',z_array)\n",
    "        for j in range(len(z_array)):\n",
    "            print(MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_array[j]], 1,sigma))\n",
    "        \n",
    "        \n",
    "        _Noise_label = np.ones((15000*T,10)) * 0.1\n",
    "\n",
    "        X_tilde,a,b = BACC_Enc_Data_v3(encoding_input_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde,a,b = BACC_Enc_Data_v3(encoding_label_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "        \n",
    "        \n",
    "        print(\"Adjust the power of X_tilde\")\n",
    "        \n",
    "        for p_idx in range(N):\n",
    "            tmp_power = np.sum(X_tilde[p_idx,:,:] * X_tilde[p_idx,:,:], axis=1)/np.shape(X_tilde)[2]\n",
    "            tmp_power = np.sum(tmp_power)/np.shape(X_tilde)[1]\n",
    "            print(p_idx, tmp_power)\n",
    "            \n",
    "            X_tilde[p_idx,:,:] = X_tilde[p_idx,:,:] * np.sqrt(avg_power/tmp_power)\n",
    "            tmp_power = np.sum(X_tilde[p_idx,:,:] * X_tilde[p_idx,:,:], axis=1)/np.shape(X_tilde)[2]\n",
    "            tmp_power = np.sum(tmp_power)/np.shape(X_tilde)[1]\n",
    "            print('power after adjusting =',tmp_power)\n",
    "        print()\n",
    "        \n",
    "        m = N # m is the number of received result @ master\n",
    "    #     print('number of results:',m)\n",
    "\n",
    "        for trial_idx in range(N_trials):\n",
    "            print('(T, sigma)=',T,sigma,') ',trial_idx,'-th Trial!!')\n",
    "\n",
    "            net_glob = CNNCifar(args=args)\n",
    "            net_glob.cuda()\n",
    "            net_glob.train()\n",
    "\n",
    "            # copy weights\n",
    "            w_glob = net_glob.state_dict()\n",
    "\n",
    "            for iter in range(N_epochs): #args.epochs\n",
    "                w_locals, loss_locals = [], []\n",
    "                idxs_users = np.random.choice(range(N), m, replace=False)\n",
    "                idxs_users = np.sort(idxs_users)\n",
    "                print('selected users:',idxs_users)\n",
    "                \n",
    "                coded_net = BACC_Enc_Model_withNoise_v4(net_glob.cuda(), N, K, T, sigma, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "                dec_z_array = []\n",
    "                for idx in idxs_users: #for idx in range(N):\n",
    "            #         print(idx)\n",
    "                    local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[idx,:,:], label=y_tilde[idx,:,:])\n",
    "                    w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "#                     w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                    w_locals.append(copy.deepcopy(w))\n",
    "                    loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                    dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "                # update global weights\n",
    "                #w_glob = FedAvg(w_locals)\n",
    "                w_glob = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "\n",
    "                # copy weight to net_glob\n",
    "                net_glob.load_state_dict(w_glob)\n",
    "\n",
    "                # print loss\n",
    "            #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "            #     loss_train_arr.append(loss_train)\n",
    "\n",
    "                acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "                acc_test_arr_K4_G1[sigma_idx][lr_idx][trial_idx][iter] = acc_test\n",
    "                loss_test_arr_K4_G1[sigma_idx][lr_idx][trial_idx][iter] = loss_test\n",
    "                if iter % 1 ==0:\n",
    "                    print('Round {:3d}, Average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_test, acc_test))\n",
    "                #print(loss_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3. K=4, N=4, T=3, G=2, with grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2 3\n",
      "##########################################\n",
      "Sigma = 0.1\n",
      "\n",
      "\n",
      "\n",
      "##########################################\n",
      "Learning Rate = 0.0005\n",
      "\n",
      "\n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 12500 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 12500 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 12500 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 3 12500 \n",
      "\n",
      "Adjust the power of X_tilde\n",
      "0 0.40958203262854803\n",
      "power after adjusting = 2.071376214594275\n",
      "1 0.41082296921406597\n",
      "power after adjusting = 2.0713762145942756\n",
      "2 0.4104500250991252\n",
      "power after adjusting = 2.0713762145942756\n",
      "3 0.4118166059656703\n",
      "power after adjusting = 2.0713762145942747\n",
      "\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 2.0329 \n",
      "Accuracy: 3907/10000 (39.07%)\n",
      "\n",
      "0 0 0\n",
      "Round   0, Average loss 2.033 Test accuracy 39.070\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7706 \n",
      "Accuracy: 4676/10000 (46.76%)\n",
      "\n",
      "0 0 1\n",
      "Round   1, Average loss 1.771 Test accuracy 46.760\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.7172 \n",
      "Accuracy: 5021/10000 (50.21%)\n",
      "\n",
      "0 0 2\n",
      "Round   2, Average loss 1.717 Test accuracy 50.210\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6898 \n",
      "Accuracy: 5139/10000 (51.39%)\n",
      "\n",
      "0 0 3\n",
      "Round   3, Average loss 1.690 Test accuracy 51.390\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6724 \n",
      "Accuracy: 5229/10000 (52.29%)\n",
      "\n",
      "0 0 4\n",
      "Round   4, Average loss 1.672 Test accuracy 52.290\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6593 \n",
      "Accuracy: 5297/10000 (52.97%)\n",
      "\n",
      "0 0 5\n",
      "Round   5, Average loss 1.659 Test accuracy 52.970\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6453 \n",
      "Accuracy: 5375/10000 (53.75%)\n",
      "\n",
      "0 0 6\n",
      "Round   6, Average loss 1.645 Test accuracy 53.750\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6347 \n",
      "Accuracy: 5428/10000 (54.28%)\n",
      "\n",
      "0 0 7\n",
      "Round   7, Average loss 1.635 Test accuracy 54.280\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6239 \n",
      "Accuracy: 5522/10000 (55.22%)\n",
      "\n",
      "0 0 8\n",
      "Round   8, Average loss 1.624 Test accuracy 55.220\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6170 \n",
      "Accuracy: 5571/10000 (55.71%)\n",
      "\n",
      "0 0 9\n",
      "Round   9, Average loss 1.617 Test accuracy 55.710\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6094 \n",
      "Accuracy: 5623/10000 (56.23%)\n",
      "\n",
      "0 0 10\n",
      "Round  10, Average loss 1.609 Test accuracy 56.230\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.6028 \n",
      "Accuracy: 5664/10000 (56.64%)\n",
      "\n",
      "0 0 11\n",
      "Round  11, Average loss 1.603 Test accuracy 56.640\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5982 \n",
      "Accuracy: 5695/10000 (56.95%)\n",
      "\n",
      "0 0 12\n",
      "Round  12, Average loss 1.598 Test accuracy 56.950\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5947 \n",
      "Accuracy: 5725/10000 (57.25%)\n",
      "\n",
      "0 0 13\n",
      "Round  13, Average loss 1.595 Test accuracy 57.250\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5911 \n",
      "Accuracy: 5742/10000 (57.42%)\n",
      "\n",
      "0 0 14\n",
      "Round  14, Average loss 1.591 Test accuracy 57.420\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5873 \n",
      "Accuracy: 5764/10000 (57.64%)\n",
      "\n",
      "0 0 15\n",
      "Round  15, Average loss 1.587 Test accuracy 57.640\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5855 \n",
      "Accuracy: 5769/10000 (57.69%)\n",
      "\n",
      "0 0 16\n",
      "Round  16, Average loss 1.585 Test accuracy 57.690\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5827 \n",
      "Accuracy: 5772/10000 (57.72%)\n",
      "\n",
      "0 0 17\n",
      "Round  17, Average loss 1.583 Test accuracy 57.720\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5806 \n",
      "Accuracy: 5800/10000 (58.00%)\n",
      "\n",
      "0 0 18\n",
      "Round  18, Average loss 1.581 Test accuracy 58.000\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5768 \n",
      "Accuracy: 5811/10000 (58.11%)\n",
      "\n",
      "0 0 19\n",
      "Round  19, Average loss 1.577 Test accuracy 58.110\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5750 \n",
      "Accuracy: 5807/10000 (58.07%)\n",
      "\n",
      "0 0 20\n",
      "Round  20, Average loss 1.575 Test accuracy 58.070\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5739 \n",
      "Accuracy: 5796/10000 (57.96%)\n",
      "\n",
      "0 0 21\n",
      "Round  21, Average loss 1.574 Test accuracy 57.960\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5713 \n",
      "Accuracy: 5798/10000 (57.98%)\n",
      "\n",
      "0 0 22\n",
      "Round  22, Average loss 1.571 Test accuracy 57.980\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5710 \n",
      "Accuracy: 5807/10000 (58.07%)\n",
      "\n",
      "0 0 23\n",
      "Round  23, Average loss 1.571 Test accuracy 58.070\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5693 \n",
      "Accuracy: 5809/10000 (58.09%)\n",
      "\n",
      "0 0 24\n",
      "Round  24, Average loss 1.569 Test accuracy 58.090\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5698 \n",
      "Accuracy: 5816/10000 (58.16%)\n",
      "\n",
      "0 0 25\n",
      "Round  25, Average loss 1.570 Test accuracy 58.160\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5675 \n",
      "Accuracy: 5812/10000 (58.12%)\n",
      "\n",
      "0 0 26\n",
      "Round  26, Average loss 1.567 Test accuracy 58.120\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5683 \n",
      "Accuracy: 5821/10000 (58.21%)\n",
      "\n",
      "0 0 27\n",
      "Round  27, Average loss 1.568 Test accuracy 58.210\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5672 \n",
      "Accuracy: 5829/10000 (58.29%)\n",
      "\n",
      "0 0 28\n",
      "Round  28, Average loss 1.567 Test accuracy 58.290\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5675 \n",
      "Accuracy: 5844/10000 (58.44%)\n",
      "\n",
      "0 0 29\n",
      "Round  29, Average loss 1.568 Test accuracy 58.440\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5666 \n",
      "Accuracy: 5851/10000 (58.51%)\n",
      "\n",
      "0 0 30\n",
      "Round  30, Average loss 1.567 Test accuracy 58.510\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5675 \n",
      "Accuracy: 5846/10000 (58.46%)\n",
      "\n",
      "0 0 31\n",
      "Round  31, Average loss 1.567 Test accuracy 58.460\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5657 \n",
      "Accuracy: 5851/10000 (58.51%)\n",
      "\n",
      "0 0 32\n",
      "Round  32, Average loss 1.566 Test accuracy 58.510\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5653 \n",
      "Accuracy: 5884/10000 (58.84%)\n",
      "\n",
      "0 0 33\n",
      "Round  33, Average loss 1.565 Test accuracy 58.840\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5653 \n",
      "Accuracy: 5874/10000 (58.74%)\n",
      "\n",
      "0 0 34\n",
      "Round  34, Average loss 1.565 Test accuracy 58.740\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5651 \n",
      "Accuracy: 5883/10000 (58.83%)\n",
      "\n",
      "0 0 35\n",
      "Round  35, Average loss 1.565 Test accuracy 58.830\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5653 \n",
      "Accuracy: 5878/10000 (58.78%)\n",
      "\n",
      "0 0 36\n",
      "Round  36, Average loss 1.565 Test accuracy 58.780\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5648 \n",
      "Accuracy: 5897/10000 (58.97%)\n",
      "\n",
      "0 0 37\n",
      "Round  37, Average loss 1.565 Test accuracy 58.970\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5639 \n",
      "Accuracy: 5890/10000 (58.90%)\n",
      "\n",
      "0 0 38\n",
      "Round  38, Average loss 1.564 Test accuracy 58.900\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5621 \n",
      "Accuracy: 5909/10000 (59.09%)\n",
      "\n",
      "0 0 39\n",
      "Round  39, Average loss 1.562 Test accuracy 59.090\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5622 \n",
      "Accuracy: 5900/10000 (59.00%)\n",
      "\n",
      "0 0 40\n",
      "Round  40, Average loss 1.562 Test accuracy 59.000\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5619 \n",
      "Accuracy: 5915/10000 (59.15%)\n",
      "\n",
      "0 0 41\n",
      "Round  41, Average loss 1.562 Test accuracy 59.150\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5625 \n",
      "Accuracy: 5917/10000 (59.17%)\n",
      "\n",
      "0 0 42\n",
      "Round  42, Average loss 1.563 Test accuracy 59.170\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5628 \n",
      "Accuracy: 5912/10000 (59.12%)\n",
      "\n",
      "0 0 43\n",
      "Round  43, Average loss 1.563 Test accuracy 59.120\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5603 \n",
      "Accuracy: 5915/10000 (59.15%)\n",
      "\n",
      "0 0 44\n",
      "Round  44, Average loss 1.560 Test accuracy 59.150\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5605 \n",
      "Accuracy: 5920/10000 (59.20%)\n",
      "\n",
      "0 0 45\n",
      "Round  45, Average loss 1.561 Test accuracy 59.200\n",
      "selected users: [0 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5606 \n",
      "Accuracy: 5925/10000 (59.25%)\n",
      "\n",
      "0 0 46\n",
      "Round  46, Average loss 1.561 Test accuracy 59.250\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5605 \n",
      "Accuracy: 5922/10000 (59.22%)\n",
      "\n",
      "0 0 47\n",
      "Round  47, Average loss 1.561 Test accuracy 59.220\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5586 \n",
      "Accuracy: 5914/10000 (59.14%)\n",
      "\n",
      "0 0 48\n",
      "Round  48, Average loss 1.559 Test accuracy 59.140\n",
      "selected users: [0 1]\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 1.5585 \n",
      "Accuracy: 5919/10000 (59.19%)\n",
      "\n",
      "0 0 49\n",
      "Round  49, Average loss 1.558 Test accuracy 59.190\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "G = 2\n",
    "N = 4 # N should be divisible by G\n",
    "K = 4 # N should be divisible by G\n",
    "\n",
    "size_per_group = int(50000/G)\n",
    "\n",
    "X_group = np.reshape(encoding_input_array_np, (G,size_per_group,32*32*3))\n",
    "y_group = np.reshape(encoding_label_array_np, (G,size_per_group,args.num_classes)) \n",
    "\n",
    "N_i = int(N/G) # = 2\n",
    "K_i = int(K/G) # = 2\n",
    "T = 3\n",
    "Noise_Alloc = [0,2,4]\n",
    "m = N_i # number of selected workers (if there is no straggler, m=N_i)\n",
    "\n",
    "print(N_i,K_i,T)\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K_i+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K_i+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K_i+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "z_array = np.array([-0.81, 0.81])\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "N_trials = 1\n",
    "\n",
    "N_epochs = 50\n",
    "\n",
    "lr_array = [0.0005]\n",
    "\n",
    "sigma_array = [0.1]\n",
    "\n",
    "loss_test_arr_K4_G2_alignment_v1 = np.zeros((len(sigma_array),len(lr_array),N_epochs))\n",
    "acc_test_arr_K4_G2_alignment_v1  = np.zeros((len(sigma_array),len(lr_array),N_epochs))\n",
    "\n",
    "\n",
    "for sigma_idx in range(len(sigma_array)):\n",
    "    \n",
    "    sigma = sigma_array[sigma_idx]\n",
    "    print('##########################################')\n",
    "    print('Sigma =',sigma)\n",
    "    print('\\n\\n')\n",
    "    \n",
    "    for lr_idx in range(len(lr_array)):\n",
    "\n",
    "        args.lr = lr_array[lr_idx]\n",
    "\n",
    "        print('##########################################')\n",
    "        print('Learning Rate =',args.lr)\n",
    "        print('\\n\\n')\n",
    "\n",
    "    #     print('##########################################')\n",
    "    #     print('######',trial_idx,'-th Trial!! ###########')\n",
    "\n",
    "        net_glob = CNNCifar(args=args)\n",
    "        net_glob.cuda()\n",
    "        net_glob.train()\n",
    "\n",
    "        # copy weights\n",
    "        w_glob = net_glob.state_dict()\n",
    "\n",
    "        X_tilde = np.empty((N,Size_submatrices,32*32*3))\n",
    "        y_tilde = np.empty((N,Size_submatrices,10))\n",
    "\n",
    "        for G_idx in range(G):\n",
    "\n",
    "            _Noise_label = np.ones((size_per_group*T,10)) * 0.1\n",
    "\n",
    "            X_tilde_tmp,a,b = BACC_Enc_Data_v3(X_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "            y_tilde_tmp,a,b = BACC_Enc_Data_v3(y_group[G_idx,:,:], N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "\n",
    "            stt_pos = G_idx * N_i\n",
    "            end_pos = (G_idx+1) * N_i\n",
    "\n",
    "            X_tilde[stt_pos:end_pos,:,:] = X_tilde_tmp\n",
    "            y_tilde[stt_pos:end_pos,:,:] = y_tilde_tmp\n",
    "\n",
    "\n",
    "        print(\"Adjust the power of X_tilde\")\n",
    "\n",
    "        for p_idx in range(N):\n",
    "            tmp_power = np.sum(X_tilde[p_idx,:,:] * X_tilde[p_idx,:,:], axis=1)/np.shape(X_tilde)[2]\n",
    "            tmp_power = np.sum(tmp_power)/np.shape(X_tilde)[1]\n",
    "            print(p_idx, tmp_power)\n",
    "\n",
    "            X_tilde[p_idx,:,:] = X_tilde[p_idx,:,:] * np.sqrt(avg_power/tmp_power)\n",
    "            tmp_power = np.sum(X_tilde[p_idx,:,:] * X_tilde[p_idx,:,:], axis=1)/np.shape(X_tilde)[2]\n",
    "            tmp_power = np.sum(tmp_power)/np.shape(X_tilde)[1]\n",
    "            print('power after adjusting =',tmp_power)\n",
    "        print()\n",
    "\n",
    "\n",
    "\n",
    "        for iter in range(N_epochs): #args.epochs\n",
    "\n",
    "            w_group_array = []\n",
    "            for G_idx in range(G):\n",
    "                w_locals, loss_locals = [], []\n",
    "                idxs_users = np.random.choice(range(N_i), m, replace=False)\n",
    "                idxs_users = np.sort(idxs_users)\n",
    "                print('selected users:',idxs_users)\n",
    "\n",
    "                coded_net = BACC_Enc_Model_withNoise_v4(net_glob.cuda(), N_i, K_i, T, sigma, alpha_array, z_array, _Noise_Alloc=Noise_Alloc)\n",
    "\n",
    "                dec_z_array = []\n",
    "                for idx in idxs_users: #for idx in range(N):\n",
    "            #         print(idx)\n",
    "                    local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[G_idx*N_i+idx,:,:], label=y_tilde[G_idx*N_i+idx,:,:])\n",
    "                    w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "    #                 w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                    w_locals.append(copy.deepcopy(w))\n",
    "                    loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                    dec_z_array.append(z_array[idx])\n",
    "\n",
    "\n",
    "                # update global weights\n",
    "                #w_glob = FedAvg(w_locals)\n",
    "                w_group = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "\n",
    "                w_group_array.append(copy.deepcopy(w_group))\n",
    "\n",
    "            w_glob = copy.deepcopy(w_group_array[0])\n",
    "            for k in w_glob.keys():\n",
    "                for G_idx in range(1,G):\n",
    "                    w_glob[k] += w_group_array[G_idx][k]\n",
    "                w_glob[k] = torch.div(w_glob[k], len(w_group_array))\n",
    "\n",
    "            # copy weight to net_glob\n",
    "            net_glob.load_state_dict(w_glob)\n",
    "\n",
    "            # print loss\n",
    "        #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "        #     loss_train_arr.append(loss_train)\n",
    "\n",
    "            acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "            \n",
    "            print(sigma_idx,lr_idx,iter)\n",
    "            acc_test_arr_K4_G2_alignment_v1[sigma_idx][lr_idx][iter] = acc_test\n",
    "            loss_test_arr_K4_G2_alignment_v1[sigma_idx][lr_idx][iter] = loss_test\n",
    "\n",
    "            if iter % 1 ==0:\n",
    "                print('Round {:3d}, Average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_test, acc_test))\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1, 1, 50)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(acc_test_arr_K4_G2_alignment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(acc_test_arr_K4_G1[0,0,0,0:30],label='BACC, w/o Grouping, K=4, N=4, G=1, sigma=0.1' )\n",
    "plt.plot(acc_test_arr_K4_G2_alignment[0,0,0:30],label='BACC, w/   Grouping, K=4, N=4, G=2, sigma=0.1' )\n",
    "plt.plot(acc_test_arr_K4_DP[0,0,0,0:30],label='DP, K=4, N=4, G=1, sigma=0.1' )\n",
    "\n",
    "\n",
    "# plt.plot(acc_test_arr_uncoded,'r',label='uncoded')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "# plt.title(title_name)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Test accuracy(%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4. DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12500"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Size_submatrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "sigma = 0.1\n",
      "\n",
      "\n",
      "\n",
      "Learning Rate = 0.0003\n",
      "\n",
      "\n",
      "\n",
      "(sigma, lr)= 0.1 0.0003 )  0 -th Trial!!\n",
      "\n",
      "Test set: Average loss: 1.5887 \n",
      "Accuracy: 4313/10000 (43.13%)\n",
      "\n",
      "Round   0, Average loss 1.589 Test accuracy 43.130\n",
      "\n",
      "Test set: Average loss: 1.4099 \n",
      "Accuracy: 4933/10000 (49.33%)\n",
      "\n",
      "Round   1, Average loss 1.410 Test accuracy 49.330\n",
      "\n",
      "Test set: Average loss: 1.3399 \n",
      "Accuracy: 5184/10000 (51.84%)\n",
      "\n",
      "Round   2, Average loss 1.340 Test accuracy 51.840\n",
      "\n",
      "Test set: Average loss: 1.2889 \n",
      "Accuracy: 5394/10000 (53.94%)\n",
      "\n",
      "Round   3, Average loss 1.289 Test accuracy 53.940\n",
      "\n",
      "Test set: Average loss: 1.2524 \n",
      "Accuracy: 5512/10000 (55.12%)\n",
      "\n",
      "Round   4, Average loss 1.252 Test accuracy 55.120\n",
      "\n",
      "Test set: Average loss: 1.2267 \n",
      "Accuracy: 5593/10000 (55.93%)\n",
      "\n",
      "Round   5, Average loss 1.227 Test accuracy 55.930\n",
      "\n",
      "Test set: Average loss: 1.2107 \n",
      "Accuracy: 5654/10000 (56.54%)\n",
      "\n",
      "Round   6, Average loss 1.211 Test accuracy 56.540\n",
      "\n",
      "Test set: Average loss: 1.2037 \n",
      "Accuracy: 5715/10000 (57.15%)\n",
      "\n",
      "Round   7, Average loss 1.204 Test accuracy 57.150\n",
      "\n",
      "Test set: Average loss: 1.2014 \n",
      "Accuracy: 5750/10000 (57.50%)\n",
      "\n",
      "Round   8, Average loss 1.201 Test accuracy 57.500\n",
      "\n",
      "Test set: Average loss: 1.2033 \n",
      "Accuracy: 5767/10000 (57.67%)\n",
      "\n",
      "Round   9, Average loss 1.203 Test accuracy 57.670\n",
      "\n",
      "Test set: Average loss: 1.2126 \n",
      "Accuracy: 5798/10000 (57.98%)\n",
      "\n",
      "Round  10, Average loss 1.213 Test accuracy 57.980\n",
      "\n",
      "Test set: Average loss: 1.2226 \n",
      "Accuracy: 5810/10000 (58.10%)\n",
      "\n",
      "Round  11, Average loss 1.223 Test accuracy 58.100\n",
      "\n",
      "Test set: Average loss: 1.2364 \n",
      "Accuracy: 5811/10000 (58.11%)\n",
      "\n",
      "Round  12, Average loss 1.236 Test accuracy 58.110\n",
      "\n",
      "Test set: Average loss: 1.2558 \n",
      "Accuracy: 5825/10000 (58.25%)\n",
      "\n",
      "Round  13, Average loss 1.256 Test accuracy 58.250\n",
      "\n",
      "Test set: Average loss: 1.2787 \n",
      "Accuracy: 5817/10000 (58.17%)\n",
      "\n",
      "Round  14, Average loss 1.279 Test accuracy 58.170\n",
      "\n",
      "Test set: Average loss: 1.3023 \n",
      "Accuracy: 5816/10000 (58.16%)\n",
      "\n",
      "Round  15, Average loss 1.302 Test accuracy 58.160\n",
      "\n",
      "Test set: Average loss: 1.3282 \n",
      "Accuracy: 5808/10000 (58.08%)\n",
      "\n",
      "Round  16, Average loss 1.328 Test accuracy 58.080\n",
      "\n",
      "Test set: Average loss: 1.3597 \n",
      "Accuracy: 5806/10000 (58.06%)\n",
      "\n",
      "Round  17, Average loss 1.360 Test accuracy 58.060\n",
      "\n",
      "Test set: Average loss: 1.3939 \n",
      "Accuracy: 5785/10000 (57.85%)\n",
      "\n",
      "Round  18, Average loss 1.394 Test accuracy 57.850\n",
      "\n",
      "Test set: Average loss: 1.4321 \n",
      "Accuracy: 5749/10000 (57.49%)\n",
      "\n",
      "Round  19, Average loss 1.432 Test accuracy 57.490\n",
      "\n",
      "Test set: Average loss: 1.4664 \n",
      "Accuracy: 5750/10000 (57.50%)\n",
      "\n",
      "Round  20, Average loss 1.466 Test accuracy 57.500\n",
      "\n",
      "Test set: Average loss: 1.5074 \n",
      "Accuracy: 5710/10000 (57.10%)\n",
      "\n",
      "Round  21, Average loss 1.507 Test accuracy 57.100\n",
      "\n",
      "Test set: Average loss: 1.5470 \n",
      "Accuracy: 5689/10000 (56.89%)\n",
      "\n",
      "Round  22, Average loss 1.547 Test accuracy 56.890\n",
      "\n",
      "Test set: Average loss: 1.5883 \n",
      "Accuracy: 5712/10000 (57.12%)\n",
      "\n",
      "Round  23, Average loss 1.588 Test accuracy 57.120\n",
      "\n",
      "Test set: Average loss: 1.6297 \n",
      "Accuracy: 5697/10000 (56.97%)\n",
      "\n",
      "Round  24, Average loss 1.630 Test accuracy 56.970\n",
      "\n",
      "Test set: Average loss: 1.6783 \n",
      "Accuracy: 5706/10000 (57.06%)\n",
      "\n",
      "Round  25, Average loss 1.678 Test accuracy 57.060\n",
      "\n",
      "Test set: Average loss: 1.7278 \n",
      "Accuracy: 5680/10000 (56.80%)\n",
      "\n",
      "Round  26, Average loss 1.728 Test accuracy 56.800\n",
      "\n",
      "Test set: Average loss: 1.7801 \n",
      "Accuracy: 5632/10000 (56.32%)\n",
      "\n",
      "Round  27, Average loss 1.780 Test accuracy 56.320\n",
      "\n",
      "Test set: Average loss: 1.8314 \n",
      "Accuracy: 5624/10000 (56.24%)\n",
      "\n",
      "Round  28, Average loss 1.831 Test accuracy 56.240\n",
      "\n",
      "Test set: Average loss: 1.8843 \n",
      "Accuracy: 5587/10000 (55.87%)\n",
      "\n",
      "Round  29, Average loss 1.884 Test accuracy 55.870\n",
      "\n",
      "Test set: Average loss: 1.9340 \n",
      "Accuracy: 5605/10000 (56.05%)\n",
      "\n",
      "Round  30, Average loss 1.934 Test accuracy 56.050\n",
      "\n",
      "Test set: Average loss: 1.9880 \n",
      "Accuracy: 5571/10000 (55.71%)\n",
      "\n",
      "Round  31, Average loss 1.988 Test accuracy 55.710\n",
      "\n",
      "Test set: Average loss: 2.0476 \n",
      "Accuracy: 5524/10000 (55.24%)\n",
      "\n",
      "Round  32, Average loss 2.048 Test accuracy 55.240\n",
      "\n",
      "Test set: Average loss: 2.1027 \n",
      "Accuracy: 5548/10000 (55.48%)\n",
      "\n",
      "Round  33, Average loss 2.103 Test accuracy 55.480\n",
      "\n",
      "Test set: Average loss: 2.1617 \n",
      "Accuracy: 5534/10000 (55.34%)\n",
      "\n",
      "Round  34, Average loss 2.162 Test accuracy 55.340\n",
      "\n",
      "Test set: Average loss: 2.2228 \n",
      "Accuracy: 5514/10000 (55.14%)\n",
      "\n",
      "Round  35, Average loss 2.223 Test accuracy 55.140\n",
      "\n",
      "Test set: Average loss: 2.2781 \n",
      "Accuracy: 5515/10000 (55.15%)\n",
      "\n",
      "Round  36, Average loss 2.278 Test accuracy 55.150\n",
      "\n",
      "Test set: Average loss: 2.3336 \n",
      "Accuracy: 5515/10000 (55.15%)\n",
      "\n",
      "Round  37, Average loss 2.334 Test accuracy 55.150\n",
      "\n",
      "Test set: Average loss: 2.3862 \n",
      "Accuracy: 5491/10000 (54.91%)\n",
      "\n",
      "Round  38, Average loss 2.386 Test accuracy 54.910\n",
      "\n",
      "Test set: Average loss: 2.4369 \n",
      "Accuracy: 5488/10000 (54.88%)\n",
      "\n",
      "Round  39, Average loss 2.437 Test accuracy 54.880\n",
      "\n",
      "Test set: Average loss: 2.4844 \n",
      "Accuracy: 5457/10000 (54.57%)\n",
      "\n",
      "Round  40, Average loss 2.484 Test accuracy 54.570\n",
      "\n",
      "Test set: Average loss: 2.5416 \n",
      "Accuracy: 5484/10000 (54.84%)\n",
      "\n",
      "Round  41, Average loss 2.542 Test accuracy 54.840\n",
      "\n",
      "Test set: Average loss: 2.6143 \n",
      "Accuracy: 5451/10000 (54.51%)\n",
      "\n",
      "Round  42, Average loss 2.614 Test accuracy 54.510\n",
      "\n",
      "Test set: Average loss: 2.6460 \n",
      "Accuracy: 5444/10000 (54.44%)\n",
      "\n",
      "Round  43, Average loss 2.646 Test accuracy 54.440\n",
      "\n",
      "Test set: Average loss: 2.7039 \n",
      "Accuracy: 5443/10000 (54.43%)\n",
      "\n",
      "Round  44, Average loss 2.704 Test accuracy 54.430\n",
      "\n",
      "Test set: Average loss: 2.7569 \n",
      "Accuracy: 5463/10000 (54.63%)\n",
      "\n",
      "Round  45, Average loss 2.757 Test accuracy 54.630\n",
      "\n",
      "Test set: Average loss: 2.8223 \n",
      "Accuracy: 5440/10000 (54.40%)\n",
      "\n",
      "Round  46, Average loss 2.822 Test accuracy 54.400\n",
      "\n",
      "Test set: Average loss: 2.8759 \n",
      "Accuracy: 5431/10000 (54.31%)\n",
      "\n",
      "Round  47, Average loss 2.876 Test accuracy 54.310\n",
      "\n",
      "Test set: Average loss: 2.9219 \n",
      "Accuracy: 5395/10000 (53.95%)\n",
      "\n",
      "Round  48, Average loss 2.922 Test accuracy 53.950\n",
      "\n",
      "Test set: Average loss: 2.9627 \n",
      "Accuracy: 5397/10000 (53.97%)\n",
      "\n",
      "Round  49, Average loss 2.963 Test accuracy 53.970\n",
      "\n",
      "\n",
      "\n",
      "sigma = 0.3\n",
      "\n",
      "\n",
      "\n",
      "Learning Rate = 0.0003\n",
      "\n",
      "\n",
      "\n",
      "(sigma, lr)= 0.3 0.0003 )  0 -th Trial!!\n",
      "\n",
      "Test set: Average loss: 1.5911 \n",
      "Accuracy: 4242/10000 (42.42%)\n",
      "\n",
      "Round   0, Average loss 1.591 Test accuracy 42.420\n",
      "\n",
      "Test set: Average loss: 1.4284 \n",
      "Accuracy: 4819/10000 (48.19%)\n",
      "\n",
      "Round   1, Average loss 1.428 Test accuracy 48.190\n",
      "\n",
      "Test set: Average loss: 1.3442 \n",
      "Accuracy: 5141/10000 (51.41%)\n",
      "\n",
      "Round   2, Average loss 1.344 Test accuracy 51.410\n",
      "\n",
      "Test set: Average loss: 1.2877 \n",
      "Accuracy: 5371/10000 (53.71%)\n",
      "\n",
      "Round   3, Average loss 1.288 Test accuracy 53.710\n",
      "\n",
      "Test set: Average loss: 1.2501 \n",
      "Accuracy: 5528/10000 (55.28%)\n",
      "\n",
      "Round   4, Average loss 1.250 Test accuracy 55.280\n",
      "\n",
      "Test set: Average loss: 1.2255 \n",
      "Accuracy: 5636/10000 (56.36%)\n",
      "\n",
      "Round   5, Average loss 1.226 Test accuracy 56.360\n",
      "\n",
      "Test set: Average loss: 1.2091 \n",
      "Accuracy: 5755/10000 (57.55%)\n",
      "\n",
      "Round   6, Average loss 1.209 Test accuracy 57.550\n",
      "\n",
      "Test set: Average loss: 1.2023 \n",
      "Accuracy: 5821/10000 (58.21%)\n",
      "\n",
      "Round   7, Average loss 1.202 Test accuracy 58.210\n",
      "\n",
      "Test set: Average loss: 1.2037 \n",
      "Accuracy: 5823/10000 (58.23%)\n",
      "\n",
      "Round   8, Average loss 1.204 Test accuracy 58.230\n",
      "\n",
      "Test set: Average loss: 1.2094 \n",
      "Accuracy: 5838/10000 (58.38%)\n",
      "\n",
      "Round   9, Average loss 1.209 Test accuracy 58.380\n",
      "\n",
      "Test set: Average loss: 1.2205 \n",
      "Accuracy: 5852/10000 (58.52%)\n",
      "\n",
      "Round  10, Average loss 1.221 Test accuracy 58.520\n",
      "\n",
      "Test set: Average loss: 1.2365 \n",
      "Accuracy: 5838/10000 (58.38%)\n",
      "\n",
      "Round  11, Average loss 1.236 Test accuracy 58.380\n",
      "\n",
      "Test set: Average loss: 1.2602 \n",
      "Accuracy: 5821/10000 (58.21%)\n",
      "\n",
      "Round  12, Average loss 1.260 Test accuracy 58.210\n",
      "\n",
      "Test set: Average loss: 1.2908 \n",
      "Accuracy: 5768/10000 (57.68%)\n",
      "\n",
      "Round  13, Average loss 1.291 Test accuracy 57.680\n",
      "\n",
      "Test set: Average loss: 1.3208 \n",
      "Accuracy: 5782/10000 (57.82%)\n",
      "\n",
      "Round  14, Average loss 1.321 Test accuracy 57.820\n",
      "\n",
      "Test set: Average loss: 1.3532 \n",
      "Accuracy: 5750/10000 (57.50%)\n",
      "\n",
      "Round  15, Average loss 1.353 Test accuracy 57.500\n",
      "\n",
      "Test set: Average loss: 1.3908 \n",
      "Accuracy: 5739/10000 (57.39%)\n",
      "\n",
      "Round  16, Average loss 1.391 Test accuracy 57.390\n",
      "\n",
      "Test set: Average loss: 1.4316 \n",
      "Accuracy: 5712/10000 (57.12%)\n",
      "\n",
      "Round  17, Average loss 1.432 Test accuracy 57.120\n",
      "\n",
      "Test set: Average loss: 1.4738 \n",
      "Accuracy: 5692/10000 (56.92%)\n",
      "\n",
      "Round  18, Average loss 1.474 Test accuracy 56.920\n",
      "\n",
      "Test set: Average loss: 1.5227 \n",
      "Accuracy: 5665/10000 (56.65%)\n",
      "\n",
      "Round  19, Average loss 1.523 Test accuracy 56.650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.5798 \n",
      "Accuracy: 5646/10000 (56.46%)\n",
      "\n",
      "Round  20, Average loss 1.580 Test accuracy 56.460\n",
      "\n",
      "Test set: Average loss: 1.6431 \n",
      "Accuracy: 5645/10000 (56.45%)\n",
      "\n",
      "Round  21, Average loss 1.643 Test accuracy 56.450\n",
      "\n",
      "Test set: Average loss: 1.7001 \n",
      "Accuracy: 5619/10000 (56.19%)\n",
      "\n",
      "Round  22, Average loss 1.700 Test accuracy 56.190\n",
      "\n",
      "Test set: Average loss: 1.7627 \n",
      "Accuracy: 5577/10000 (55.77%)\n",
      "\n",
      "Round  23, Average loss 1.763 Test accuracy 55.770\n",
      "\n",
      "Test set: Average loss: 1.8168 \n",
      "Accuracy: 5561/10000 (55.61%)\n",
      "\n",
      "Round  24, Average loss 1.817 Test accuracy 55.610\n",
      "\n",
      "Test set: Average loss: 1.8846 \n",
      "Accuracy: 5532/10000 (55.32%)\n",
      "\n",
      "Round  25, Average loss 1.885 Test accuracy 55.320\n",
      "\n",
      "Test set: Average loss: 1.9452 \n",
      "Accuracy: 5528/10000 (55.28%)\n",
      "\n",
      "Round  26, Average loss 1.945 Test accuracy 55.280\n",
      "\n",
      "Test set: Average loss: 2.0059 \n",
      "Accuracy: 5500/10000 (55.00%)\n",
      "\n",
      "Round  27, Average loss 2.006 Test accuracy 55.000\n",
      "\n",
      "Test set: Average loss: 2.0463 \n",
      "Accuracy: 5476/10000 (54.76%)\n",
      "\n",
      "Round  28, Average loss 2.046 Test accuracy 54.760\n",
      "\n",
      "Test set: Average loss: 2.1093 \n",
      "Accuracy: 5454/10000 (54.54%)\n",
      "\n",
      "Round  29, Average loss 2.109 Test accuracy 54.540\n",
      "\n",
      "Test set: Average loss: 2.1606 \n",
      "Accuracy: 5458/10000 (54.58%)\n",
      "\n",
      "Round  30, Average loss 2.161 Test accuracy 54.580\n",
      "\n",
      "Test set: Average loss: 2.2206 \n",
      "Accuracy: 5435/10000 (54.35%)\n",
      "\n",
      "Round  31, Average loss 2.221 Test accuracy 54.350\n",
      "\n",
      "Test set: Average loss: 2.2868 \n",
      "Accuracy: 5424/10000 (54.24%)\n",
      "\n",
      "Round  32, Average loss 2.287 Test accuracy 54.240\n",
      "\n",
      "Test set: Average loss: 2.3268 \n",
      "Accuracy: 5425/10000 (54.25%)\n",
      "\n",
      "Round  33, Average loss 2.327 Test accuracy 54.250\n",
      "\n",
      "Test set: Average loss: 2.3802 \n",
      "Accuracy: 5403/10000 (54.03%)\n",
      "\n",
      "Round  34, Average loss 2.380 Test accuracy 54.030\n",
      "\n",
      "Test set: Average loss: 2.4261 \n",
      "Accuracy: 5393/10000 (53.93%)\n",
      "\n",
      "Round  35, Average loss 2.426 Test accuracy 53.930\n",
      "\n",
      "Test set: Average loss: 2.4504 \n",
      "Accuracy: 5403/10000 (54.03%)\n",
      "\n",
      "Round  36, Average loss 2.450 Test accuracy 54.030\n",
      "\n",
      "Test set: Average loss: 2.5145 \n",
      "Accuracy: 5404/10000 (54.04%)\n",
      "\n",
      "Round  37, Average loss 2.515 Test accuracy 54.040\n",
      "\n",
      "Test set: Average loss: 2.5578 \n",
      "Accuracy: 5396/10000 (53.96%)\n",
      "\n",
      "Round  38, Average loss 2.558 Test accuracy 53.960\n",
      "\n",
      "Test set: Average loss: 2.6020 \n",
      "Accuracy: 5375/10000 (53.75%)\n",
      "\n",
      "Round  39, Average loss 2.602 Test accuracy 53.750\n",
      "\n",
      "Test set: Average loss: 2.6406 \n",
      "Accuracy: 5385/10000 (53.85%)\n",
      "\n",
      "Round  40, Average loss 2.641 Test accuracy 53.850\n",
      "\n",
      "Test set: Average loss: 2.7103 \n",
      "Accuracy: 5385/10000 (53.85%)\n",
      "\n",
      "Round  41, Average loss 2.710 Test accuracy 53.850\n",
      "\n",
      "Test set: Average loss: 2.7459 \n",
      "Accuracy: 5380/10000 (53.80%)\n",
      "\n",
      "Round  42, Average loss 2.746 Test accuracy 53.800\n",
      "\n",
      "Test set: Average loss: 2.8063 \n",
      "Accuracy: 5328/10000 (53.28%)\n",
      "\n",
      "Round  43, Average loss 2.806 Test accuracy 53.280\n",
      "\n",
      "Test set: Average loss: 2.8548 \n",
      "Accuracy: 5346/10000 (53.46%)\n",
      "\n",
      "Round  44, Average loss 2.855 Test accuracy 53.460\n",
      "\n",
      "Test set: Average loss: 2.9177 \n",
      "Accuracy: 5319/10000 (53.19%)\n",
      "\n",
      "Round  45, Average loss 2.918 Test accuracy 53.190\n",
      "\n",
      "Test set: Average loss: 2.9488 \n",
      "Accuracy: 5324/10000 (53.24%)\n",
      "\n",
      "Round  46, Average loss 2.949 Test accuracy 53.240\n",
      "\n",
      "Test set: Average loss: 3.0039 \n",
      "Accuracy: 5327/10000 (53.27%)\n",
      "\n",
      "Round  47, Average loss 3.004 Test accuracy 53.270\n",
      "\n",
      "Test set: Average loss: 2.9954 \n",
      "Accuracy: 5307/10000 (53.07%)\n",
      "\n",
      "Round  48, Average loss 2.995 Test accuracy 53.070\n",
      "\n",
      "Test set: Average loss: 3.0952 \n",
      "Accuracy: 5298/10000 (52.98%)\n",
      "\n",
      "Round  49, Average loss 3.095 Test accuracy 52.980\n",
      "\n",
      "\n",
      "\n",
      "sigma = 1\n",
      "\n",
      "\n",
      "\n",
      "Learning Rate = 0.0003\n",
      "\n",
      "\n",
      "\n",
      "(sigma, lr)= 1 0.0003 )  0 -th Trial!!\n",
      "\n",
      "Test set: Average loss: 1.7816 \n",
      "Accuracy: 3646/10000 (36.46%)\n",
      "\n",
      "Round   0, Average loss 1.782 Test accuracy 36.460\n",
      "\n",
      "Test set: Average loss: 1.6535 \n",
      "Accuracy: 4122/10000 (41.22%)\n",
      "\n",
      "Round   1, Average loss 1.653 Test accuracy 41.220\n",
      "\n",
      "Test set: Average loss: 1.5964 \n",
      "Accuracy: 4283/10000 (42.83%)\n",
      "\n",
      "Round   2, Average loss 1.596 Test accuracy 42.830\n",
      "\n",
      "Test set: Average loss: 1.5632 \n",
      "Accuracy: 4411/10000 (44.11%)\n",
      "\n",
      "Round   3, Average loss 1.563 Test accuracy 44.110\n",
      "\n",
      "Test set: Average loss: 1.5282 \n",
      "Accuracy: 4554/10000 (45.54%)\n",
      "\n",
      "Round   4, Average loss 1.528 Test accuracy 45.540\n",
      "\n",
      "Test set: Average loss: 1.5099 \n",
      "Accuracy: 4633/10000 (46.33%)\n",
      "\n",
      "Round   5, Average loss 1.510 Test accuracy 46.330\n",
      "\n",
      "Test set: Average loss: 1.4931 \n",
      "Accuracy: 4714/10000 (47.14%)\n",
      "\n",
      "Round   6, Average loss 1.493 Test accuracy 47.140\n",
      "\n",
      "Test set: Average loss: 1.4782 \n",
      "Accuracy: 4769/10000 (47.69%)\n",
      "\n",
      "Round   7, Average loss 1.478 Test accuracy 47.690\n",
      "\n",
      "Test set: Average loss: 1.4759 \n",
      "Accuracy: 4812/10000 (48.12%)\n",
      "\n",
      "Round   8, Average loss 1.476 Test accuracy 48.120\n",
      "\n",
      "Test set: Average loss: 1.4734 \n",
      "Accuracy: 4842/10000 (48.42%)\n",
      "\n",
      "Round   9, Average loss 1.473 Test accuracy 48.420\n",
      "\n",
      "Test set: Average loss: 1.4799 \n",
      "Accuracy: 4870/10000 (48.70%)\n",
      "\n",
      "Round  10, Average loss 1.480 Test accuracy 48.700\n",
      "\n",
      "Test set: Average loss: 1.4913 \n",
      "Accuracy: 4898/10000 (48.98%)\n",
      "\n",
      "Round  11, Average loss 1.491 Test accuracy 48.980\n",
      "\n",
      "Test set: Average loss: 1.5008 \n",
      "Accuracy: 4907/10000 (49.07%)\n",
      "\n",
      "Round  12, Average loss 1.501 Test accuracy 49.070\n",
      "\n",
      "Test set: Average loss: 1.5187 \n",
      "Accuracy: 4931/10000 (49.31%)\n",
      "\n",
      "Round  13, Average loss 1.519 Test accuracy 49.310\n",
      "\n",
      "Test set: Average loss: 1.5344 \n",
      "Accuracy: 4933/10000 (49.33%)\n",
      "\n",
      "Round  14, Average loss 1.534 Test accuracy 49.330\n",
      "\n",
      "Test set: Average loss: 1.5593 \n",
      "Accuracy: 4921/10000 (49.21%)\n",
      "\n",
      "Round  15, Average loss 1.559 Test accuracy 49.210\n",
      "\n",
      "Test set: Average loss: 1.5936 \n",
      "Accuracy: 4923/10000 (49.23%)\n",
      "\n",
      "Round  16, Average loss 1.594 Test accuracy 49.230\n",
      "\n",
      "Test set: Average loss: 1.6256 \n",
      "Accuracy: 4914/10000 (49.14%)\n",
      "\n",
      "Round  17, Average loss 1.626 Test accuracy 49.140\n",
      "\n",
      "Test set: Average loss: 1.6640 \n",
      "Accuracy: 4886/10000 (48.86%)\n",
      "\n",
      "Round  18, Average loss 1.664 Test accuracy 48.860\n",
      "\n",
      "Test set: Average loss: 1.6994 \n",
      "Accuracy: 4884/10000 (48.84%)\n",
      "\n",
      "Round  19, Average loss 1.699 Test accuracy 48.840\n",
      "\n",
      "Test set: Average loss: 1.7433 \n",
      "Accuracy: 4872/10000 (48.72%)\n",
      "\n",
      "Round  20, Average loss 1.743 Test accuracy 48.720\n",
      "\n",
      "Test set: Average loss: 1.7795 \n",
      "Accuracy: 4869/10000 (48.69%)\n",
      "\n",
      "Round  21, Average loss 1.779 Test accuracy 48.690\n",
      "\n",
      "Test set: Average loss: 1.8202 \n",
      "Accuracy: 4879/10000 (48.79%)\n",
      "\n",
      "Round  22, Average loss 1.820 Test accuracy 48.790\n",
      "\n",
      "Test set: Average loss: 1.8785 \n",
      "Accuracy: 4827/10000 (48.27%)\n",
      "\n",
      "Round  23, Average loss 1.878 Test accuracy 48.270\n",
      "\n",
      "Test set: Average loss: 1.9183 \n",
      "Accuracy: 4832/10000 (48.32%)\n",
      "\n",
      "Round  24, Average loss 1.918 Test accuracy 48.320\n",
      "\n",
      "Test set: Average loss: 1.9821 \n",
      "Accuracy: 4804/10000 (48.04%)\n",
      "\n",
      "Round  25, Average loss 1.982 Test accuracy 48.040\n",
      "\n",
      "Test set: Average loss: 2.0368 \n",
      "Accuracy: 4773/10000 (47.73%)\n",
      "\n",
      "Round  26, Average loss 2.037 Test accuracy 47.730\n",
      "\n",
      "Test set: Average loss: 2.0880 \n",
      "Accuracy: 4767/10000 (47.67%)\n",
      "\n",
      "Round  27, Average loss 2.088 Test accuracy 47.670\n",
      "\n",
      "Test set: Average loss: 2.1433 \n",
      "Accuracy: 4762/10000 (47.62%)\n",
      "\n",
      "Round  28, Average loss 2.143 Test accuracy 47.620\n",
      "\n",
      "Test set: Average loss: 2.2197 \n",
      "Accuracy: 4720/10000 (47.20%)\n",
      "\n",
      "Round  29, Average loss 2.220 Test accuracy 47.200\n",
      "\n",
      "Test set: Average loss: 2.2712 \n",
      "Accuracy: 4705/10000 (47.05%)\n",
      "\n",
      "Round  30, Average loss 2.271 Test accuracy 47.050\n",
      "\n",
      "Test set: Average loss: 2.3211 \n",
      "Accuracy: 4704/10000 (47.04%)\n",
      "\n",
      "Round  31, Average loss 2.321 Test accuracy 47.040\n",
      "\n",
      "Test set: Average loss: 2.3806 \n",
      "Accuracy: 4701/10000 (47.01%)\n",
      "\n",
      "Round  32, Average loss 2.381 Test accuracy 47.010\n",
      "\n",
      "Test set: Average loss: 2.4154 \n",
      "Accuracy: 4720/10000 (47.20%)\n",
      "\n",
      "Round  33, Average loss 2.415 Test accuracy 47.200\n",
      "\n",
      "Test set: Average loss: 2.4651 \n",
      "Accuracy: 4725/10000 (47.25%)\n",
      "\n",
      "Round  34, Average loss 2.465 Test accuracy 47.250\n",
      "\n",
      "Test set: Average loss: 2.5245 \n",
      "Accuracy: 4740/10000 (47.40%)\n",
      "\n",
      "Round  35, Average loss 2.524 Test accuracy 47.400\n",
      "\n",
      "Test set: Average loss: 2.5798 \n",
      "Accuracy: 4728/10000 (47.28%)\n",
      "\n",
      "Round  36, Average loss 2.580 Test accuracy 47.280\n",
      "\n",
      "Test set: Average loss: 2.6309 \n",
      "Accuracy: 4708/10000 (47.08%)\n",
      "\n",
      "Round  37, Average loss 2.631 Test accuracy 47.080\n",
      "\n",
      "Test set: Average loss: 2.6770 \n",
      "Accuracy: 4734/10000 (47.34%)\n",
      "\n",
      "Round  38, Average loss 2.677 Test accuracy 47.340\n",
      "\n",
      "Test set: Average loss: 2.7467 \n",
      "Accuracy: 4731/10000 (47.31%)\n",
      "\n",
      "Round  39, Average loss 2.747 Test accuracy 47.310\n",
      "\n",
      "Test set: Average loss: 2.8104 \n",
      "Accuracy: 4722/10000 (47.22%)\n",
      "\n",
      "Round  40, Average loss 2.810 Test accuracy 47.220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.8710 \n",
      "Accuracy: 4693/10000 (46.93%)\n",
      "\n",
      "Round  41, Average loss 2.871 Test accuracy 46.930\n",
      "\n",
      "Test set: Average loss: 2.9543 \n",
      "Accuracy: 4661/10000 (46.61%)\n",
      "\n",
      "Round  42, Average loss 2.954 Test accuracy 46.610\n",
      "\n",
      "Test set: Average loss: 2.9567 \n",
      "Accuracy: 4703/10000 (47.03%)\n",
      "\n",
      "Round  43, Average loss 2.957 Test accuracy 47.030\n",
      "\n",
      "Test set: Average loss: 3.0076 \n",
      "Accuracy: 4697/10000 (46.97%)\n",
      "\n",
      "Round  44, Average loss 3.008 Test accuracy 46.970\n",
      "\n",
      "Test set: Average loss: 3.0940 \n",
      "Accuracy: 4687/10000 (46.87%)\n",
      "\n",
      "Round  45, Average loss 3.094 Test accuracy 46.870\n",
      "\n",
      "Test set: Average loss: 3.1386 \n",
      "Accuracy: 4652/10000 (46.52%)\n",
      "\n",
      "Round  46, Average loss 3.139 Test accuracy 46.520\n",
      "\n",
      "Test set: Average loss: 3.1243 \n",
      "Accuracy: 4719/10000 (47.19%)\n",
      "\n",
      "Round  47, Average loss 3.124 Test accuracy 47.190\n",
      "\n",
      "Test set: Average loss: 3.1903 \n",
      "Accuracy: 4680/10000 (46.80%)\n",
      "\n",
      "Round  48, Average loss 3.190 Test accuracy 46.800\n",
      "\n",
      "Test set: Average loss: 3.2656 \n",
      "Accuracy: 4675/10000 (46.75%)\n",
      "\n",
      "Round  49, Average loss 3.266 Test accuracy 46.750\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "N = 4\n",
    "N_trials = 1\n",
    "N_epochs = 50\n",
    "\n",
    "lr_array = [0.0003] # 0.001 is the bset\n",
    "\n",
    "sigma_array = [0.1, 0.3, 1]\n",
    "# lr_array = [0.1, 0.01, 0.005, 0.001,0.0005]\n",
    "\n",
    "\n",
    "loss_test_arr_K4_DP = np.zeros((len(sigma_array),len(lr_array),N_trials,N_epochs))\n",
    "acc_test_arr_K4_DP  = np.zeros((len(sigma_array),len(lr_array),N_trials,N_epochs))\n",
    "\n",
    "\n",
    "for sigma_idx in range(len(sigma_array)):\n",
    "    \n",
    "    sigma = sigma_array[sigma_idx]\n",
    "    \n",
    "    print('\\n\\n')\n",
    "    print('sigma =',sigma)\n",
    "    print('\\n\\n')\n",
    "           \n",
    "        \n",
    "    # print(\"alpha_array: \",alpha_array,'\\n')\n",
    "    \n",
    "    \n",
    "    for lr_idx in range(len(lr_array)):\n",
    "        \n",
    "        args.lr = lr_array[lr_idx]\n",
    "        \n",
    "        print('Learning Rate =',args.lr)\n",
    "        print('\\n\\n')\n",
    "        \n",
    "        X_tilde = np.reshape(encoding_input_array_np, (N,Size_submatrices, 32*32*3)) + np.random.normal(0,sigma,size=(N,Size_submatrices, 32*32*3))\n",
    "        y_tilde = np.reshape(encoding_label_array_np, (N,Size_submatrices, 10))\n",
    "\n",
    "#         X_tilde,a,b = BACC_Enc_Data_v3(encoding_input_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "#         y_tilde,a,b = BACC_Enc_Data_v3(encoding_label_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "        \n",
    "#         print(\"Adjust the power of X_tilde\")\n",
    "        \n",
    "#         for p_idx in range(N):\n",
    "#             tmp_power = np.sum(X_tilde[p_idx,:,:] * X_tilde[p_idx,:,:], axis=1)/np.shape(X_tilde)[2]\n",
    "#             tmp_power = np.sum(tmp_power)/np.shape(X_tilde)[1]\n",
    "#             print(p_idx, tmp_power)\n",
    "            \n",
    "#             X_tilde[p_idx,:,:] = X_tilde[p_idx,:,:] * np.sqrt(avg_power/tmp_power)\n",
    "#             tmp_power = np.sum(X_tilde[p_idx,:,:] * X_tilde[p_idx,:,:], axis=1)/np.shape(X_tilde)[2]\n",
    "#             tmp_power = np.sum(tmp_power)/np.shape(X_tilde)[1]\n",
    "#             print('power after adjusting =',tmp_power)\n",
    "#         print()\n",
    "        \n",
    "        m = N # m is the number of received result @ master\n",
    "    #     print('number of results:',m)\n",
    "\n",
    "        for trial_idx in range(N_trials):\n",
    "            print('(sigma, lr)=',sigma,args.lr,') ',trial_idx,'-th Trial!!')\n",
    "\n",
    "            net_glob = CNNCifar(args=args)\n",
    "            net_glob.cuda()\n",
    "            net_glob.train()\n",
    "\n",
    "            # copy weights\n",
    "            w_glob = net_glob.state_dict()\n",
    "\n",
    "            for iter in range(N_epochs): #args.epochs\n",
    "                w_locals, loss_locals = [], []\n",
    "                idxs_users = np.random.choice(range(N), m, replace=False)\n",
    "                idxs_users = np.sort(idxs_users)\n",
    "                \n",
    "                for idx in idxs_users: #for idx in range(N):\n",
    "            #         print(idx)\n",
    "                    local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[idx,:,:], label=y_tilde[idx,:,:])\n",
    "#                     w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "                    w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "                    w_locals.append(copy.deepcopy(w))\n",
    "                    loss_locals.append(copy.deepcopy(loss))\n",
    "            \n",
    "                                \n",
    "                w_glob = copy.deepcopy(w_locals[0])\n",
    "                for k in w_glob.keys():\n",
    "                    for G_idx in range(1,N):\n",
    "                        w_glob[k] += w_locals[G_idx][k]\n",
    "                    w_glob[k] = torch.div(w_glob[k], N)\n",
    "\n",
    "                # copy weight to net_glob\n",
    "                net_glob.load_state_dict(w_glob)\n",
    "\n",
    "                # print loss\n",
    "            #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "            #     loss_train_arr.append(loss_train)\n",
    "\n",
    "                acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "                acc_test_arr_K4_DP[sigma_idx][lr_idx][trial_idx][iter] = acc_test\n",
    "                loss_test_arr_K4_DP[sigma_idx][lr_idx][trial_idx][iter] = loss_test\n",
    "                \n",
    "#                 PATH = \"./save_models/CIFAR10_LeNet_BACC_PowerAlign_K2_sigma_E50_iter\"+str(iter)+\".pt\"\n",
    "#                 torch.save(net_glob.state_dict(), PATH)\n",
    "                \n",
    "                if iter % 1 ==0:\n",
    "                    print('Round {:3d}, Average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_test, acc_test))\n",
    "                #print(loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9.51056516e-01  5.87785252e-01  6.12323400e-17 -5.87785252e-01\n",
      " -9.51056516e-01]\n"
     ]
    }
   ],
   "source": [
    "K = 2\n",
    "T = 3\n",
    "sigma = 0.1\n",
    "Noise_Alloc = [0,2,4]\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "j_array = np.array(range(K+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "\n",
    "print(alpha_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
