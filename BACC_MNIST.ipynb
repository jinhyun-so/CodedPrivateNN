{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "\n",
    "from utils.sampling import mnist_iid, mnist_noniid, cifar_iid\n",
    "from utils.options import args_parser\n",
    "from models.Update import LocalUpdate\n",
    "from models.Nets import MLP, CNNMnist, CNNCifar, LeNet, CNNMnist2\n",
    "from models.Fed import FedAvg\n",
    "from models.Fed import FedQAvg, Quantization, Quantization_Finite, my_score, my_score_Finite\n",
    "from models.test import test_img\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Define the Loss function\n",
    "\n",
    "As we encode the labels as well, cross entropy function should take the one-hot vector with softed value as an input.\n",
    "However, cross entorpy function supported by pytorch only takes one dimensional label (e.g [1,0,9,...] where entries presents class labels).\n",
    "Hence, now I define my cross entropy function here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_cross_entropy(input, target, size_average=True):\n",
    "    \"\"\" Cross entropy that accepts soft targets\n",
    "    Args:\n",
    "         pred: predictions for neural network\n",
    "         targets: targets, can be soft\n",
    "         size_average: if false, sum is returned instead of mean\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        input = torch.FloatTensor([[1.1, 2.8, 1.3], [1.1, 2.1, 4.8]])\n",
    "        input = torch.autograd.Variable(out, requires_grad=True)\n",
    "\n",
    "        target = torch.FloatTensor([[0.05, 0.9, 0.05], [0.05, 0.05, 0.9]])\n",
    "        target = torch.autograd.Variable(y1)\n",
    "        loss = cross_entropy(input, target)\n",
    "        loss.backward()\n",
    "    \"\"\"\n",
    "    logsoftmax = nn.LogSoftmax(dim=1)\n",
    "    if size_average:\n",
    "        return torch.mean(torch.sum(-target * logsoftmax(input) , dim=1))\n",
    "    else:\n",
    "        return torch.sum(torch.sum(-target * logsoftmax(input), dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1. Test my_cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "tensor([2, 4, 0])\n",
      "tensor([[0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 1],\n",
      "        [1, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn, autograd\n",
    "\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "# target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "target = torch.LongTensor([2,4,0])\n",
    "\n",
    "one_hot = torch.nn.functional.one_hot(target,num_classes=5)\n",
    "\n",
    "print(input.dim())\n",
    "print(target)\n",
    "print(one_hot)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot assign 'torch.FloatTensor' as parameter 'weight' (torch.nn.Parameter or None expected)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-117-54db371e0781>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m    611\u001b[0m                 raise TypeError(\"cannot assign '{}' as parameter '{}' \"\n\u001b[0;32m    612\u001b[0m                                 \u001b[1;34m\"(torch.nn.Parameter or None expected)\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 613\u001b[1;33m                                 .format(torch.typename(value), name))\n\u001b[0m\u001b[0;32m    614\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister_parameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    615\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot assign 'torch.FloatTensor' as parameter 'weight' (torch.nn.Parameter or None expected)"
     ]
    }
   ],
   "source": [
    "model = nn.Linear(2, 2)\n",
    "model.weight = torch.FloatTensor([[1,0],[0,1]])\n",
    "print(model.weight)\n",
    "print(model.bias)\n",
    "x = torch.randn(1, 2)\n",
    "# target = torch.randn(1, 2)\n",
    "output = model(x)\n",
    "print(x)\n",
    "print(output)\n",
    "# loss = my_loss(output, target)\n",
    "# loss.backward()\n",
    "# print(model.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "following outputs should be same.\n",
      "tensor(2.6336, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6336, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6336, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6336, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nfollowing outputs should be same.\")\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "loss_defalut = loss(input, target)\n",
    "print(loss_defalut)\n",
    "loss_defalut.backward()\n",
    "print(loss_defalut)\n",
    "\n",
    "\n",
    "print(loss(input, target))\n",
    "print(my_cross_entropy(input, one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [1., 0., 0.]])\n",
      "tensor(0.6178)\n",
      "tensor(0.5927)\n"
     ]
    }
   ],
   "source": [
    "out = torch.FloatTensor([[0.05, 0.9, 0.05], [0.05, 0.05, 0.9], [0.9, 0.05, 0.05]])\n",
    "out = torch.autograd.Variable(out)\n",
    "\n",
    "# Categorical targets\n",
    "y = torch.LongTensor([1, 2, 0])\n",
    "y = torch.autograd.Variable(y)\n",
    "\n",
    "# One-hot encoded targets\n",
    "y1 = torch.FloatTensor([[0, 1, 0], [0, 0, 1], [1, 0, 0]])\n",
    "y1 = torch.autograd.Variable(y1)\n",
    "\n",
    "print(y1)\n",
    "\n",
    "# Calculating the loss\n",
    "loss_val = nn.CrossEntropyLoss()(out, y)\n",
    "loss_val1 = nn.BCEWithLogitsLoss()(out, y1)\n",
    "\n",
    "print(loss_val)\n",
    "print(loss_val1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check whether my_cross_entropy function works properly with soft-valued one-hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 1],\n",
      "        [1, 0, 0, 0, 0]])\n",
      "tensor(2.6336, grad_fn=<MeanBackward0>)\n",
      "\n",
      "check the soft valued one-hot vector\n",
      "tensor([[ 0.0000,  0.1000,  0.9000,  0.1000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.1000,  0.9000],\n",
      "        [ 1.1000, -0.1000,  0.0000,  0.0000,  0.0000]])\n",
      "tensor(2.6009, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "target = torch.LongTensor([2,4,0])\n",
    "one_hot = torch.nn.functional.one_hot(target,num_classes=5)\n",
    "print(one_hot)\n",
    "print(my_cross_entropy(input, one_hot))\n",
    "print()\n",
    "\n",
    "print(\"check the soft valued one-hot vector\")\n",
    "one_hot = torch.FloatTensor([[0,0.1, 0.9, 0.1, 0],[0, 0, 0, 0.1, 0.9],[1.1, -0.1, 0,0,0]])\n",
    "print(one_hot)\n",
    "print(my_cross_entropy(input, one_hot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_argument:    \n",
    "    epochs = 200    #\"rounds of training\"\n",
    "    num_users = 15  # \"number of users: K\"\n",
    "    frac = 0.5 #\"the fraction of clients: C\"\n",
    "    local_ep = 1 #\"the number of local epochs: E\"\n",
    "    local_bs = 200 #\"local batch size: B\"\n",
    "    bs=128 #\"test batch size\"\n",
    "    lr=0.01 #\"learning rate\"\n",
    "    momentum=0.5 # \"SGD momentum (default: 0.5)\"\n",
    "    split='user' # \"train-test split type, user or sample\"\n",
    "    opt='ADAM'\n",
    "    loss='Default' # 'Custom' or 'Default'\n",
    "\n",
    "    # model arguments\n",
    "    model = 'cnn'\n",
    "    kernel_num=9 #, help='number of each kind of kernel')\n",
    "    kernel_sizes='3,4,5' #  help='comma-separated kernel size to use for convolution')\n",
    "    norm='None' #, help=\"batch_norm, layer_norm, or None\")\n",
    "    num_filters=32 #, help=\"number of filters for conv nets\")\n",
    "    max_pool='True' #help=\"Whether use max pooling rather than strided convolutions\")\n",
    "\n",
    "    # other arguments\n",
    "    dataset='mnist' #, help=\"name of dataset\")\n",
    "    iid=1\n",
    "    num_classes=10#, help=\"number of classes\")\n",
    "    num_channels=1#, help=\"number of channels of imges\")\n",
    "    gpu=1#, help=\"GPU ID, -1 for CPU\")\n",
    "    stopping_rounds=10#, help='rounds of early stopping')\n",
    "    verbose='False'#, help='verbose print')\n",
    "    seed=1#, help='random seed (default: 1)')\n",
    "    \n",
    "args = my_argument()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "# load dataset and split users\n",
    "if args.dataset == 'mnist':\n",
    "    trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    dataset_train = datasets.MNIST('../data/mnist/', train=True, download=True, transform=trans_mnist)\n",
    "    dataset_test = datasets.MNIST('../data/mnist/', train=False, download=True, transform=trans_mnist)\n",
    "    # sample users\n",
    "    if args.iid:\n",
    "        dict_users = mnist_iid(dataset_train, args.num_users)\n",
    "    else:\n",
    "        dict_users = mnist_noniid(dataset_train, args.num_users)\n",
    "elif args.dataset == 'cifar':\n",
    "    trans_cifar = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    dataset_train = datasets.CIFAR10('../data/cifar', train=True, download=True, transform=trans_cifar)\n",
    "    dataset_test = datasets.CIFAR10('../data/cifar', train=False, download=True, transform=trans_cifar)\n",
    "    if args.iid:\n",
    "        dict_users = cifar_iid(dataset_train, args.num_users)\n",
    "    else:\n",
    "        exit('Error: only consider IID setting in CIFAR10')\n",
    "else:\n",
    "    exit('Error: unrecognized dataset')\n",
    "img_size = dataset_train[0][0].shape\n",
    "\n",
    "# print(dataset_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train CNN with Torch's CrossEntropy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNMnist2(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=1024, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_glob = CNNMnist2(args=args)\n",
    "net_glob.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNMnist2(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=1024, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.343 Test accuracy 9.800\n",
      "\n",
      "Test set: Average loss: 2.3013 \n",
      "Accuracy: 1823/10000 (18.23%)\n",
      "\n",
      "Round   1, Average loss 2.098 Test accuracy 18.230\n",
      "\n",
      "Test set: Average loss: 0.7712 \n",
      "Accuracy: 8988/10000 (89.88%)\n",
      "\n",
      "Round   2, Average loss 1.264 Test accuracy 89.880\n",
      "\n",
      "Test set: Average loss: 0.1878 \n",
      "Accuracy: 9523/10000 (95.23%)\n",
      "\n",
      "Round   3, Average loss 0.661 Test accuracy 95.230\n",
      "\n",
      "Test set: Average loss: 0.1428 \n",
      "Accuracy: 9633/10000 (96.33%)\n",
      "\n",
      "Round   4, Average loss 0.478 Test accuracy 96.330\n",
      "\n",
      "Test set: Average loss: 0.1150 \n",
      "Accuracy: 9677/10000 (96.77%)\n",
      "\n",
      "Round   5, Average loss 0.403 Test accuracy 96.770\n",
      "\n",
      "Test set: Average loss: 0.1033 \n",
      "Accuracy: 9703/10000 (97.03%)\n",
      "\n",
      "Round   6, Average loss 0.367 Test accuracy 97.030\n",
      "\n",
      "Test set: Average loss: 0.0901 \n",
      "Accuracy: 9733/10000 (97.33%)\n",
      "\n",
      "Round   7, Average loss 0.343 Test accuracy 97.330\n",
      "\n",
      "Test set: Average loss: 0.0845 \n",
      "Accuracy: 9754/10000 (97.54%)\n",
      "\n",
      "Round   8, Average loss 0.318 Test accuracy 97.540\n",
      "\n",
      "Test set: Average loss: 0.0821 \n",
      "Accuracy: 9754/10000 (97.54%)\n",
      "\n",
      "Round   9, Average loss 0.308 Test accuracy 97.540\n"
     ]
    }
   ],
   "source": [
    "print(net_glob)\n",
    "net_glob.train()\n",
    "\n",
    "# copy weights\n",
    "w_glob = net_glob.state_dict()\n",
    "\n",
    "# training\n",
    "loss_train = []\n",
    "loss_test_arr = []\n",
    "acc_test_arr = []\n",
    "cv_loss, cv_acc = [], []\n",
    "val_loss_pre, counter = 0, 0\n",
    "net_best = None\n",
    "best_loss = None\n",
    "val_acc_list, net_list = [], []\n",
    "\n",
    "for iter in range(10): #args.epochs\n",
    "    w_locals, loss_locals = [], []\n",
    "    m = 15\n",
    "    idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "    for idx in idxs_users:\n",
    "#         print(idx)\n",
    "        local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])\n",
    "        w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "        w_locals.append(copy.deepcopy(w))\n",
    "        loss_locals.append(copy.deepcopy(loss))\n",
    "    # update global weights\n",
    "    w_glob = FedAvg(w_locals)\n",
    "\n",
    "    # copy weight to net_glob\n",
    "    net_glob.load_state_dict(w_glob)\n",
    "\n",
    "    # print loss\n",
    "    loss_avg = sum(loss_locals) / len(loss_locals)\n",
    "    \n",
    "    loss_train.append(loss_avg)\n",
    "    \n",
    "    acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "    acc_test_arr.append(acc_test)\n",
    "    loss_test_arr.append(loss_test)\n",
    "    if iter % 1 ==0:\n",
    "        print('Round {:3d}, Average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_avg,acc_test))\n",
    "    #print(loss_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train CNN with Customized Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.loss='Custom' # 'Custom' or 'Default'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.3026 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   0, Average loss 2.309 Test accuracy 9.800\n",
      "\n",
      "Test set: Average loss: 1.9062 \n",
      "Accuracy: 8132/10000 (81.32%)\n",
      "\n",
      "Round   1, Average loss 1.646 Test accuracy 81.320\n",
      "\n",
      "Test set: Average loss: 0.1939 \n",
      "Accuracy: 9448/10000 (94.48%)\n",
      "\n",
      "Round   2, Average loss 0.732 Test accuracy 94.480\n",
      "\n",
      "Test set: Average loss: 0.1276 \n",
      "Accuracy: 9642/10000 (96.42%)\n",
      "\n",
      "Round   3, Average loss 0.396 Test accuracy 96.420\n",
      "\n",
      "Test set: Average loss: 0.1027 \n",
      "Accuracy: 9705/10000 (97.05%)\n",
      "\n",
      "Round   4, Average loss 0.320 Test accuracy 97.050\n",
      "\n",
      "Test set: Average loss: 0.0857 \n",
      "Accuracy: 9746/10000 (97.46%)\n",
      "\n",
      "Round   5, Average loss 0.277 Test accuracy 97.460\n",
      "\n",
      "Test set: Average loss: 0.0766 \n",
      "Accuracy: 9767/10000 (97.67%)\n",
      "\n",
      "Round   6, Average loss 0.249 Test accuracy 97.670\n",
      "\n",
      "Test set: Average loss: 0.0694 \n",
      "Accuracy: 9793/10000 (97.93%)\n",
      "\n",
      "Round   7, Average loss 0.235 Test accuracy 97.930\n",
      "\n",
      "Test set: Average loss: 0.0643 \n",
      "Accuracy: 9805/10000 (98.05%)\n",
      "\n",
      "Round   8, Average loss 0.222 Test accuracy 98.050\n",
      "\n",
      "Test set: Average loss: 0.0613 \n",
      "Accuracy: 9830/10000 (98.30%)\n",
      "\n",
      "Round   9, Average loss 0.213 Test accuracy 98.300\n"
     ]
    }
   ],
   "source": [
    "net_glob = CNNMnist2(args=args)\n",
    "net_glob.cuda()\n",
    "net_glob.train()\n",
    "\n",
    "# copy weights\n",
    "w_glob = net_glob.state_dict()\n",
    "\n",
    "# training\n",
    "loss_train = []\n",
    "loss_test_arr = []\n",
    "acc_test_arr = []\n",
    "cv_loss, cv_acc = [], []\n",
    "val_loss_pre, counter = 0, 0\n",
    "net_best = None\n",
    "best_loss = None\n",
    "val_acc_list, net_list = [], []\n",
    "\n",
    "for iter in range(10): #args.epochs\n",
    "    w_locals, loss_locals = [], []\n",
    "    m = 15\n",
    "    idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "    for idx in range(args.num_users):\n",
    "#         print(idx)\n",
    "        local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])\n",
    "        w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "        w_locals.append(copy.deepcopy(w))\n",
    "        loss_locals.append(copy.deepcopy(loss))\n",
    "    # update global weights\n",
    "    w_glob = FedAvg(w_locals)\n",
    "\n",
    "    # copy weight to net_glob\n",
    "    net_glob.load_state_dict(w_glob)\n",
    "\n",
    "    # print loss\n",
    "    loss_avg = sum(loss_locals) / len(loss_locals)\n",
    "    \n",
    "    loss_train.append(loss_avg)\n",
    "    \n",
    "    acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "    acc_test_arr.append(acc_test)\n",
    "    loss_test_arr.append(loss_test)\n",
    "    if iter % 1 ==0:\n",
    "        print('Round {:3d}, Average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_avg,acc_test))\n",
    "    #print(loss_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train CNN by utilizing BACC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. BACC encoding for MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_argument:    \n",
    "    epochs = 200    #\"rounds of training\"\n",
    "    num_users = 6  # \"number of users: K\"\n",
    "    frac = 0.5 #\"the fraction of clients: C\"\n",
    "    local_ep = 1 #\"the number of local epochs: E\"\n",
    "    local_bs = 200 #\"local batch size: B\"\n",
    "    bs=200 #\"test batch size\"\n",
    "    lr=0.01 #\"learning rate\"\n",
    "    momentum=0.5 # \"SGD momentum (default: 0.5)\"\n",
    "    split='user' # \"train-test split type, user or sample\"\n",
    "    opt='ADAM'\n",
    "    loss='Custom' # 'Custom' or 'Default'\n",
    "\n",
    "    # model arguments\n",
    "    model = 'cnn'\n",
    "    kernel_num=9 #, help='number of each kind of kernel')\n",
    "    kernel_sizes='3,4,5' #  help='comma-separated kernel size to use for convolution')\n",
    "    norm='None' #, help=\"batch_norm, layer_norm, or None\")\n",
    "    num_filters=32 #, help=\"number of filters for conv nets\")\n",
    "    max_pool='True' #help=\"Whether use max pooling rather than strided convolutions\")\n",
    "\n",
    "    # other arguments\n",
    "    dataset='mnist' #, help=\"name of dataset\")\n",
    "    iid=1\n",
    "    num_classes=10#, help=\"number of classes\")\n",
    "    num_channels=1#, help=\"number of channels of imges\")\n",
    "    gpu=1#, help=\"GPU ID, -1 for CPU\")\n",
    "    stopping_rounds=10#, help='rounds of early stopping')\n",
    "    verbose='False'#, help='verbose print')\n",
    "    seed=1#, help='random seed (default: 1)')\n",
    "    \n",
    "args = my_argument()\n",
    "\n",
    "args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "# load dataset and split users\n",
    "trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "dataset_train = datasets.MNIST('../data/mnist/', train=True, download=True, transform=trans_mnist)\n",
    "dataset_test = datasets.MNIST('../data/mnist/', train=False, download=True, transform=trans_mnist)\n",
    "\n",
    "dict_users = mnist_iid(dataset_train, args.num_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of X: (60000, 784)\n",
      "size of Y: (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "encoding_input_array_np = np.empty((len(dataset_train),28*28))\n",
    "encoding_label_array_np = np.empty((len(dataset_train),args.num_classes))\n",
    "print(\"size of X:\" ,encoding_input_array_np.shape)\n",
    "print(\"size of Y:\" ,encoding_label_array_np.shape)\n",
    "\n",
    "Size_submatrices = int(60000/args.num_users)\n",
    "\n",
    "for i in range(args.num_users):\n",
    "    \n",
    "    stt_pos = i*Size_submatrices\n",
    "    end_pos = (i+1)*Size_submatrices\n",
    "#     print(i,stt_pos,end_pos)\n",
    "    Temp_train = DataLoader(DatasetSplit(dataset_train, dict_users[i]), batch_size=Size_submatrices, shuffle=True)\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(Temp_train):\n",
    "        \n",
    "        images_np = images.detach().cpu().numpy()\n",
    "        encoding_input_array_np[stt_pos:end_pos,:] = np.reshape(images_np, (Size_submatrices,28*28))\n",
    "#         print(encoding_input_array_np[stt_pos:end_pos,:].shape)\n",
    "\n",
    "        onehot_labels = torch.nn.functional.one_hot(labels,num_classes=args.num_classes)\n",
    "        labels_np = onehot_labels.detach().cpu().numpy()\n",
    "#         print(labels_np.shape)\n",
    "        encoding_label_array_np[stt_pos:end_pos,:] = labels_np\n",
    "\n",
    "\n",
    "# print(labels_np[0:10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j:  [0.26179939 0.78539816 1.30899694 1.83259571 2.35619449 2.87979327] \n",
      "\n",
      "alpha_array:  [ 0.96592583  0.70710678  0.25881905 -0.25881905 -0.70710678 -0.96592583] \n",
      "\n",
      "z_array:  [ 1.          0.9781476   0.91354546  0.80901699  0.66913061  0.5\n",
      "  0.30901699  0.10452846 -0.10452846 -0.30901699 -0.5        -0.66913061\n",
      " -0.80901699 -0.91354546 -0.9781476 ] \n",
      "\n",
      "@BACC_Enc: N,K, m_i= 15 6 10000 \n",
      "\n",
      "@BACC_Enc: N,K, m_i= 15 6 10000 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "N= 15\n",
    "K= args.num_users\n",
    "\n",
    "\n",
    "j_array = np.array(range(K))\n",
    "print(\"j: \",(2*j_array+1)*math.pi/2/K,'\\n')\n",
    "\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*K)) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "i_array = np.array(range(N))\n",
    "z_array = np.cos(i_array*2*math.pi/N/2) # np.cos(i_array*2*math.pi/N/2)\n",
    "print(\"z_array: \",z_array,'\\n')\n",
    "\n",
    "X_tilde = BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "y_tilde = BACC_Enc(encoding_label_array_np, alpha_array, z_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 10000, 784)\n",
      "(15, 10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X_tilde.shape)\n",
    "print(y_tilde.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.2958 \n",
      "Accuracy: 1855/10000 (18.55%)\n",
      "\n",
      "Round   0, Average loss 2.296 Test accuracy 18.550\n",
      "\n",
      "Test set: Average loss: 57.1782 \n",
      "Accuracy: 3785/10000 (37.85%)\n",
      "\n",
      "Round   1, Average loss 57.178 Test accuracy 37.850\n",
      "\n",
      "Test set: Average loss: 2.2479 \n",
      "Accuracy: 980/10000 (9.80%)\n",
      "\n",
      "Round   2, Average loss 2.248 Test accuracy 9.800\n",
      "\n",
      "Test set: Average loss: 1.3262 \n",
      "Accuracy: 8581/10000 (85.81%)\n",
      "\n",
      "Round   3, Average loss 1.326 Test accuracy 85.810\n",
      "\n",
      "Test set: Average loss: 0.1843 \n",
      "Accuracy: 9470/10000 (94.70%)\n",
      "\n",
      "Round   4, Average loss 0.184 Test accuracy 94.700\n",
      "\n",
      "Test set: Average loss: 0.3227 \n",
      "Accuracy: 9526/10000 (95.26%)\n",
      "\n",
      "Round   5, Average loss 0.323 Test accuracy 95.260\n",
      "\n",
      "Test set: Average loss: 2.4643 \n",
      "Accuracy: 9387/10000 (93.87%)\n",
      "\n",
      "Round   6, Average loss 2.464 Test accuracy 93.870\n",
      "\n",
      "Test set: Average loss: 1.5965 \n",
      "Accuracy: 9532/10000 (95.32%)\n",
      "\n",
      "Round   7, Average loss 1.597 Test accuracy 95.320\n",
      "\n",
      "Test set: Average loss: 19.5999 \n",
      "Accuracy: 9297/10000 (92.97%)\n",
      "\n",
      "Round   8, Average loss 19.600 Test accuracy 92.970\n",
      "\n",
      "Test set: Average loss: 2.6050 \n",
      "Accuracy: 9506/10000 (95.06%)\n",
      "\n",
      "Round   9, Average loss 2.605 Test accuracy 95.060\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec\n",
    "\n",
    "net_glob = CNNMnist2(args=args)\n",
    "net_glob.cuda()\n",
    "net_glob.train()\n",
    "\n",
    "# copy weights\n",
    "w_glob = net_glob.state_dict()\n",
    "\n",
    "# training\n",
    "loss_train_arr = []\n",
    "loss_test_arr = []\n",
    "acc_test_arr = []\n",
    "cv_loss, cv_acc = [], []\n",
    "val_loss_pre, counter = 0, 0\n",
    "net_best = None\n",
    "best_loss = None\n",
    "val_acc_list, net_list = [], []\n",
    "\n",
    "for iter in range(10): #args.epochs\n",
    "    w_locals, loss_locals = [], []\n",
    "    m = 15\n",
    "    idxs_users = np.random.choice(range(N), m, replace=False)\n",
    "    dec_z_array = []\n",
    "    for idx in idxs_users: #for idx in range(N):\n",
    "#         print(idx)\n",
    "        local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[idx,:,:], label=y_tilde[idx,:,:])\n",
    "        w, loss = local.train(net=copy.deepcopy(net_glob).cuda())\n",
    "        w_locals.append(copy.deepcopy(w))\n",
    "        loss_locals.append(copy.deepcopy(loss))\n",
    "        \n",
    "        dec_z_array.append(z_array[idx])\n",
    "    \n",
    "    \n",
    "    # update global weights\n",
    "    #w_glob = FedAvg(w_locals)\n",
    "    w_glob = FedAvg_with_BACC_Dec(w_locals, alpha_array, dec_z_array)\n",
    "\n",
    "    # copy weight to net_glob\n",
    "    net_glob.load_state_dict(w_glob)\n",
    "\n",
    "    # print loss\n",
    "#     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "    \n",
    "#     loss_train_arr.append(loss_train)\n",
    "    \n",
    "    acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "    acc_test_arr.append(acc_test)\n",
    "    loss_test_arr.append(loss_test)\n",
    "    if iter % 1 ==0:\n",
    "        print('Round {:3d}, Average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_test, acc_test))\n",
    "    #print(loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
