{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "size of X: (60000, 784)\n",
      "size of Y: (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "\n",
    "import math\n",
    "\n",
    "from utils.sampling import mnist_iid, mnist_noniid, cifar_iid\n",
    "from utils.options import args_parser\n",
    "from models.Update import LocalUpdate\n",
    "from models.Nets import MLP, CNNMnist, CNNCifar, LeNet, CNNMnist2\n",
    "from models.Fed import FedAvg\n",
    "from models.Fed import FedQAvg, Quantization, Quantization_Finite, my_score, my_score_Finite\n",
    "from models.test import test_img\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "\n",
    "from utils.sampling import mnist_iid, mnist_noniid, cifar_iid\n",
    "from utils.options import args_parser\n",
    "from models.Update import LocalUpdate\n",
    "from models.Nets import MLP, CNNMnist, CNNCifar, LeNet, CNNMnist2, CNNMnist3\n",
    "from models.Fed import FedAvg\n",
    "from models.Fed import FedQAvg, Quantization, Quantization_Finite, my_score, my_score_Finite\n",
    "from models.test import test_img\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from utils.functions import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "class DatasetSplit(Dataset):\n",
    "    def __init__(self, dataset, idxs):\n",
    "        self.dataset = dataset\n",
    "        self.idxs = list(idxs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image, label = self.dataset[self.idxs[item]]\n",
    "        return image, label\n",
    "\n",
    "class my_argument:    \n",
    "    epochs = 200    #\"rounds of training\"\n",
    "    num_users = 15  # \"number of users: N\"\n",
    "    num_partition = 2 # \"number of users: K\"\n",
    "    frac = 0.5 #\"the fraction of clients: C\"\n",
    "    local_ep = 1 #\"the number of local epochs: E\"\n",
    "    local_bs = 200 #\"local batch size: B\"\n",
    "    bs=200 #\"test batch size\"\n",
    "    lr=0.01 #\"learning rate\"\n",
    "    momentum=0.5 # \"SGD momentum (default: 0.5)\"\n",
    "    split='user' # \"train-test split type, user or sample\"\n",
    "    opt='ADAM'\n",
    "    loss='Custom' # 'Custom' or 'Default'\n",
    "\n",
    "    # model arguments\n",
    "    model = 'cnn'\n",
    "    kernel_num=9 #, help='number of each kind of kernel')\n",
    "    kernel_sizes='3,4,5' #  help='comma-separated kernel size to use for convolution')\n",
    "    norm='None' #, help=\"batch_norm, layer_norm, or None\")\n",
    "    num_filters=32 #, help=\"number of filters for conv nets\")\n",
    "    max_pool='True' #help=\"Whether use max pooling rather than strided convolutions\")\n",
    "\n",
    "    # other arguments\n",
    "    dataset='mnist' #, help=\"name of dataset\")\n",
    "    iid=1\n",
    "    num_classes=10#, help=\"number of classes\")\n",
    "    num_channels=1#, help=\"number of channels of imges\")\n",
    "    gpu=1#, help=\"GPU ID, -1 for CPU\")\n",
    "    stopping_rounds=10#, help='rounds of early stopping')\n",
    "    verbose='False'#, help='verbose print')\n",
    "    seed=1#, help='random seed (default: 1)')\n",
    "    \n",
    "args = my_argument()\n",
    "\n",
    "args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "# load dataset and split users\n",
    "trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "dataset_train = datasets.MNIST('../data/mnist/', train=True, download=True, transform=trans_mnist)\n",
    "dataset_test = datasets.MNIST('../data/mnist/', train=False, download=True, transform=trans_mnist)\n",
    "\n",
    "dict_users = mnist_iid(dataset_train, args.num_partition)\n",
    "\n",
    "encoding_input_array_np = np.empty((len(dataset_train),28*28))\n",
    "encoding_label_array_np = np.empty((len(dataset_train),args.num_classes))\n",
    "print(\"size of X:\" ,encoding_input_array_np.shape)\n",
    "print(\"size of Y:\" ,encoding_label_array_np.shape)\n",
    "\n",
    "Size_submatrices = int(60000/args.num_partition)\n",
    "\n",
    "for i in range(args.num_partition):\n",
    "    \n",
    "    stt_pos = i*Size_submatrices\n",
    "    end_pos = (i+1)*Size_submatrices\n",
    "#     print(i,stt_pos,end_pos)\n",
    "    Temp_train = DataLoader(DatasetSplit(dataset_train, dict_users[i]), batch_size=Size_submatrices, shuffle=True)\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(Temp_train):\n",
    "        \n",
    "        images_np = images.detach().cpu().numpy()\n",
    "        encoding_input_array_np[stt_pos:end_pos,:] = np.reshape(images_np, (Size_submatrices,28*28))\n",
    "#         print(encoding_input_array_np[stt_pos:end_pos,:].shape)\n",
    "\n",
    "        onehot_labels = torch.nn.functional.one_hot(labels,num_classes=args.num_classes)\n",
    "        labels_np = onehot_labels.detach().cpu().numpy()\n",
    "#         print(labels_np.shape)\n",
    "        encoding_label_array_np[stt_pos:end_pos,:] = labels_np\n",
    "\n",
    "\n",
    "# print(labels_np[0:10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha_array:  [ 0.70710678 -0.70710678] \n",
      "\n",
      "z_array:  [ 1.00000000e+00  8.66025404e-01  5.00000000e-01  6.12323400e-17\n",
      " -5.00000000e-01 -8.66025404e-01] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-1.2, 1.2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "K=2\n",
    "j_array = np.array(range(K))\n",
    "# print(\"j: \",(2*j_array+1)*math.pi/2/K,'\\n')\n",
    "\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*K)) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "N = 6\n",
    "i_array = np.array(range(N))\n",
    "z_array = np.cos(i_array*2*math.pi/N/2) # np.cos(i_array*2*math.pi/N/2)\n",
    "print(\"z_array: \",z_array,'\\n')\n",
    "\n",
    "plt.plot(alpha_array, np.zeros(K),'g*',label='alpha')\n",
    "plt.plot(z_array, np.zeros(N),'b.',label='beta')\n",
    "plt.xlim([-1.2, 1.2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha_array:  [ 0.70710678 -0.70710678] \n",
      "\n",
      "z_array:  [1.000000e+00 6.123234e-17] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-1.2, 1.2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD4CAYAAADhNOGaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAPYElEQVR4nO3cf6zddX3H8edrvaHIHL+LVEpXDU1MnYnOExxzi0QEyhIpU7YAJnYJpiGR7IdZshqyoMUFWbawmDGTDo2dyRRHstjFLQ0WO+OCjFt10+qwtc5xpUJdCRlTZMX3/rhfzPV6bu+5PYdz7u3n+UhOvuf7+b7v9/vmc863r/v9nnNJVSFJatfPTboBSdJkGQSS1DiDQJIaZxBIUuMMAklq3NSkGzgZ559/fm3YsGHSbUjSirJ///7vV9Wa+eMrMgg2bNjA9PT0pNuQpBUlyXf6jXtrSJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaN5IgSLI5yaNJDiXZ3mf76iT3ddsfTrJh3vb1SZ5J8oej6EeSNLihgyDJKuAe4BpgE3Bjkk3zym4GnqqqS4C7gbvmbb8b+Kdhe5EkLd0orgguBQ5V1eGqeg74JLBlXs0WYFf3/H7giiQBSHIdcBg4MIJeJElLNIoguAh4bM76TDfWt6aqjgNPA+cl+Xngj4D3L3aQJNuSTCeZPnr06AjaliTBaIIgfcZqwJr3A3dX1TOLHaSqdlZVr6p6a9asOYk2JUn9TI1gHzPAxXPW1wGPL1Azk2QKOAs4BrwBuD7JnwJnAz9O8mxV/eUI+pIkDWAUQfAIsDHJK4DvAjcAN82r2Q1sBR4CrgcerKoCfv2FgiTvA54xBCRpvIYOgqo6nuRWYA+wCvhoVR1IsgOYrqrdwEeAjyc5xOyVwA3DHleSNBqZ/cV8Zen1ejU9PT3pNiRpRUmyv6p688f9y2JJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUuJEEQZLNSR5NcijJ9j7bVye5r9v+cJIN3fiVSfYn+Wq3fPMo+pEkDW7oIEiyCrgHuAbYBNyYZNO8spuBp6rqEuBu4K5u/PvAW6vqNcBW4OPD9iNJWppRXBFcChyqqsNV9RzwSWDLvJotwK7u+f3AFUlSVV+uqse78QPA6UlWj6AnSdKARhEEFwGPzVmf6cb61lTVceBp4Lx5NW8HvlxVPxpBT5KkAU2NYB/pM1ZLqUnyamZvF1214EGSbcA2gPXr1y+9S0lSX6O4IpgBLp6zvg54fKGaJFPAWcCxbn0d8PfAO6vqWwsdpKp2VlWvqnpr1qwZQduSJBhNEDwCbEzyiiSnATcAu+fV7Gb2w2CA64EHq6qSnA18BnhvVf3LCHqRJC3R0EHQ3fO/FdgDfAP4VFUdSLIjybVd2UeA85IcAt4DvPAV01uBS4A/TvKV7nHBsD1JkgaXqvm385e/Xq9X09PTk25DklaUJPurqjd/3L8slqTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcSMJgiSbkzya5FCS7X22r05yX7f94SQb5mx7bzf+aJKrR9FPS478zxHe9LE38b1nvjfpVqRlw/NiaYYOgiSrgHuAa4BNwI1JNs0ruxl4qqouAe4G7up+dhNwA/BqYDPwV93+NKA7Pn8HX/ivL7Djn3dMupXmPfQQ3Hnn7FKTtRLOi+X0fklVDbeD5DLgfVV1dbf+XoCqunNOzZ6u5qEkU8D3gDXA9rm1c+tOdMxer1fT09ND9b3SveRPXsKzx5/9mfHTp07nh7f9cAIdte2hh+CKK+C55+C002DvXrjsskl31Z6Vcl5M6v2SZH9V9eaPj+LW0EXAY3PWZ7qxvjVVdRx4GjhvwJ8FIMm2JNNJpo8ePTqCtle2w797mJt+6SbOmDoDgDOmzuAdr3kH3/69b0+4szbt2zd7Uj///Oxy375Jd9SmlXJeLLf3yyiCIH3G5l9mLFQzyM/ODlbtrKpeVfXWrFmzxBZPPWt/YS1nrj6TZ59/ltOnTufZ55/lzNVncuFLL5x0a026/PLZ3+xWrZpdXn75pDtq00o5L5bb+2VqBPuYAS6es74OeHyBmpnu1tBZwLEBf1YLeOJ/n+CW19/CttdvY+f+nRx55sikW2rWZZfNXt7v2zd7UntbaHJWwnmx3N4vo/iMYAr4JnAF8F3gEeCmqjowp+bdwGuq6pYkNwBvq6rfTvJq4G+BS4GXA3uBjVX1/ImO6WcEkrR0C31GMPQVQVUdT3IrsAdYBXy0qg4k2QFMV9Vu4CPAx5McYvZK4IbuZw8k+RTwdeA48O7FQkCSNFpDXxFMglcEkrR0L+a3hiRJK5hBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUuKGCIMm5SR5IcrBbnrNA3dau5mCSrd3YGUk+k+Q/khxI8sFhepEknZxhrwi2A3uraiOwt1v/KUnOBW4H3gBcCtw+JzD+rKpeBbwOeGOSa4bsR5K0RMMGwRZgV/d8F3Bdn5qrgQeq6lhVPQU8AGyuqh9U1ecAquo54EvAuiH7kSQt0bBB8LKqOgLQLS/oU3MR8Nic9Zlu7CeSnA28ldmrCknSGE0tVpDks8CFfTbdNuAx0mes5ux/CvgE8KGqOnyCPrYB2wDWr18/4KElSYtZNAiq6i0LbUvyRJK1VXUkyVrgyT5lM8Dlc9bXAfvmrO8EDlbVXyzSx86ull6vVyeqlSQNbthbQ7uBrd3zrcCn+9TsAa5Kck73IfFV3RhJPgCcBfz+kH1Ikk7SsEHwQeDKJAeBK7t1kvSS3AtQVceAO4BHuseOqjqWZB2zt5c2AV9K8pUk7xqyH0nSEqVq5d1l6fV6NT09Pek2JGlFSbK/qnrzx/3LYklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGjdUECQ5N8kDSQ52y3MWqNva1RxMsrXP9t1JvjZML5KkkzPsFcF2YG9VbQT2dus/Jcm5wO3AG4BLgdvnBkaStwHPDNmHJOkkDRsEW4Bd3fNdwHV9aq4GHqiqY1X1FPAAsBkgyUuB9wAfGLIPSdJJGjYIXlZVRwC65QV9ai4CHpuzPtONAdwB/Dnwg8UOlGRbkukk00ePHh2ua0nST0wtVpDks8CFfTbdNuAx0meskrwWuKSq/iDJhsV2UlU7gZ0AvV6vBjy2JGkRiwZBVb1loW1JnkiytqqOJFkLPNmnbAa4fM76OmAfcBnw+iT/2fVxQZJ9VXU5kqSxGfbW0G7ghW8BbQU+3admD3BVknO6D4mvAvZU1Yer6uVVtQH4NeCbhoAkjd+wQfBB4MokB4Eru3WS9JLcC1BVx5j9LOCR7rGjG5MkLQOpWnm323u9Xk1PT0+6DUlaUZLsr6re/HH/sliSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktS4VNWke1iyJEeB74zxkOcD3x/j8VYC56Q/56U/56W/cc/LL1bVmvmDKzIIxi3JdFX1Jt3HcuKc9Oe89Oe89Ldc5sVbQ5LUOINAkhpnEAxm56QbWIack/6cl/6cl/6Wxbz4GYEkNc4rAklqnEEgSY0zCPpI8ltJDiT5cZIFv9qVZHOSR5McSrJ9nD2OW5JzkzyQ5GC3PGeBuueTfKV77B53n+Oy2GufZHWS+7rtDyfZMP4ux2+AefmdJEfnvEfeNYk+xynJR5M8meRrC2xPkg91c/bvSX553D0aBP19DXgb8PmFCpKsAu4BrgE2ATcm2TSe9iZiO7C3qjYCe7v1fn5YVa/tHteOr73xGfC1vxl4qqouAe4G7hpvl+O3hHPivjnvkXvH2uRkfAzYfILt1wAbu8c24MNj6OmnGAR9VNU3qurRRcouBQ5V1eGqeg74JLDlxe9uYrYAu7rnu4DrJtjLpA3y2s+dr/uBK5JkjD1OQmvnxECq6vPAsROUbAH+pmZ9ETg7ydrxdDfLIDh5FwGPzVmf6cZOVS+rqiMA3fKCBepOTzKd5ItJTtWwGOS1/0lNVR0HngbOG0t3kzPoOfH27hbI/UkuHk9ry9rE/y2ZGufBlpMknwUu7LPptqr69CC76DO2or+Le6I5WcJu1lfV40leCTyY5KtV9a3RdLhsDPLan3LvjwEM8t/8D8AnqupHSW5h9qrpzS96Z8vbxN8rzQZBVb1lyF3MAHN/m1kHPD7kPifqRHOS5Ikka6vqSHfZ+uQC+3i8Wx5Osg94HXCqBcEgr/0LNTNJpoCzOPHtgVPBovNSVf89Z/WvaeCzkwFM/N8Sbw2dvEeAjUlekeQ04AbglP2WDLP/bVu751uBn7lqSnJOktXd8/OBNwJfH1uH4zPIaz93vq4HHqxT/683F52Xefe+rwW+Mcb+lqvdwDu7bw/9CvD0C7dhx6aqfMx7AL/JbEr/CHgC2NONvxz4xzl1vwF8k9nfeG+bdN8v8pycx+y3hQ52y3O78R5wb/f8V4GvAv/WLW+edN8v4nz8zGsP7ACu7Z6fDvwdcAj4V+CVk+55mczLncCB7j3yOeBVk+55DHPyCeAI8H/dvys3A7cAt3Tbw+y3rb7VnTe9cffo/2JCkhrnrSFJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhr3/1ZOFylv6N9VAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "K=2\n",
    "j_array = np.array(range(K))\n",
    "# print(\"j: \",(2*j_array+1)*math.pi/2/K,'\\n')\n",
    "\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*K)) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "N = 2\n",
    "i_array = np.array(range(N))\n",
    "z_array = np.cos(i_array*2*math.pi/N/2) # np.cos(i_array*2*math.pi/N/2)\n",
    "print(\"z_array: \",z_array,'\\n')\n",
    "\n",
    "plt.plot(alpha_array, np.zeros(K),'g*',label='alpha')\n",
    "plt.plot(z_array, np.zeros(N),'b.',label='beta')\n",
    "plt.xlim([-1.2, 1.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "z_array: [1.000000e+00 6.123234e-17]\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 0 30000 \n",
      "\n",
      "@BACC_Enc: N,K,T, m_i= 2 2 0 30000 \n",
      "\n",
      "(T, sigma)= 0 1 )  0 -th Trial!!\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.3506 \n",
      "Accuracy: 8777/10000 (87.77%)\n",
      "\n",
      "Round   0, Average loss 0.351 Test accuracy 87.770\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1202 \n",
      "Accuracy: 9655/10000 (96.55%)\n",
      "\n",
      "Round   1, Average loss 0.120 Test accuracy 96.550\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1123 \n",
      "Accuracy: 9704/10000 (97.04%)\n",
      "\n",
      "Round   2, Average loss 0.112 Test accuracy 97.040\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1000 \n",
      "Accuracy: 9724/10000 (97.24%)\n",
      "\n",
      "Round   3, Average loss 0.100 Test accuracy 97.240\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1109 \n",
      "Accuracy: 9692/10000 (96.92%)\n",
      "\n",
      "Round   4, Average loss 0.111 Test accuracy 96.920\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1045 \n",
      "Accuracy: 9697/10000 (96.97%)\n",
      "\n",
      "Round   5, Average loss 0.104 Test accuracy 96.970\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1040 \n",
      "Accuracy: 9701/10000 (97.01%)\n",
      "\n",
      "Round   6, Average loss 0.104 Test accuracy 97.010\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1127 \n",
      "Accuracy: 9661/10000 (96.61%)\n",
      "\n",
      "Round   7, Average loss 0.113 Test accuracy 96.610\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.0897 \n",
      "Accuracy: 9759/10000 (97.59%)\n",
      "\n",
      "Round   8, Average loss 0.090 Test accuracy 97.590\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.0965 \n",
      "Accuracy: 9730/10000 (97.30%)\n",
      "\n",
      "Round   9, Average loss 0.097 Test accuracy 97.300\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.0829 \n",
      "Accuracy: 9779/10000 (97.79%)\n",
      "\n",
      "Round  10, Average loss 0.083 Test accuracy 97.790\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.0937 \n",
      "Accuracy: 9744/10000 (97.44%)\n",
      "\n",
      "Round  11, Average loss 0.094 Test accuracy 97.440\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.0948 \n",
      "Accuracy: 9759/10000 (97.59%)\n",
      "\n",
      "Round  12, Average loss 0.095 Test accuracy 97.590\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1075 \n",
      "Accuracy: 9722/10000 (97.22%)\n",
      "\n",
      "Round  13, Average loss 0.108 Test accuracy 97.220\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.0904 \n",
      "Accuracy: 9763/10000 (97.63%)\n",
      "\n",
      "Round  14, Average loss 0.090 Test accuracy 97.630\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1018 \n",
      "Accuracy: 9746/10000 (97.46%)\n",
      "\n",
      "Round  15, Average loss 0.102 Test accuracy 97.460\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1053 \n",
      "Accuracy: 9760/10000 (97.60%)\n",
      "\n",
      "Round  16, Average loss 0.105 Test accuracy 97.600\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1103 \n",
      "Accuracy: 9721/10000 (97.21%)\n",
      "\n",
      "Round  17, Average loss 0.110 Test accuracy 97.210\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1086 \n",
      "Accuracy: 9736/10000 (97.36%)\n",
      "\n",
      "Round  18, Average loss 0.109 Test accuracy 97.360\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1089 \n",
      "Accuracy: 9746/10000 (97.46%)\n",
      "\n",
      "Round  19, Average loss 0.109 Test accuracy 97.460\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1145 \n",
      "Accuracy: 9723/10000 (97.23%)\n",
      "\n",
      "Round  20, Average loss 0.115 Test accuracy 97.230\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1157 \n",
      "Accuracy: 9727/10000 (97.27%)\n",
      "\n",
      "Round  21, Average loss 0.116 Test accuracy 97.270\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1217 \n",
      "Accuracy: 9706/10000 (97.06%)\n",
      "\n",
      "Round  22, Average loss 0.122 Test accuracy 97.060\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1077 \n",
      "Accuracy: 9732/10000 (97.32%)\n",
      "\n",
      "Round  23, Average loss 0.108 Test accuracy 97.320\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1144 \n",
      "Accuracy: 9739/10000 (97.39%)\n",
      "\n",
      "Round  24, Average loss 0.114 Test accuracy 97.390\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1123 \n",
      "Accuracy: 9741/10000 (97.41%)\n",
      "\n",
      "Round  25, Average loss 0.112 Test accuracy 97.410\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1269 \n",
      "Accuracy: 9712/10000 (97.12%)\n",
      "\n",
      "Round  26, Average loss 0.127 Test accuracy 97.120\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1280 \n",
      "Accuracy: 9714/10000 (97.14%)\n",
      "\n",
      "Round  27, Average loss 0.128 Test accuracy 97.140\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1196 \n",
      "Accuracy: 9730/10000 (97.30%)\n",
      "\n",
      "Round  28, Average loss 0.120 Test accuracy 97.300\n",
      "selected users: [0 1]\n",
      "\n",
      "Test set: Average loss: 0.1291 \n",
      "Accuracy: 9701/10000 (97.01%)\n",
      "\n",
      "Round  29, Average loss 0.129 Test accuracy 97.010\n"
     ]
    }
   ],
   "source": [
    "from models.Update import LocalUpdate_with_BACC\n",
    "from models.Fed import FedAvg_with_BACC_Dec, FedAvg_with_LCC_Dec\n",
    "\n",
    "from utils.functions import *\n",
    "import math\n",
    "\n",
    "K = 2\n",
    "T = 0\n",
    "sigma = 1\n",
    "Noise_Alloc = []\n",
    "\n",
    "Signal_Alloc = []\n",
    "for i in range(K+T):\n",
    "    if i not in Noise_Alloc:\n",
    "        Signal_Alloc.append(i)\n",
    "\n",
    "\n",
    "# j_array = np.array(range(K+T))\n",
    "alpha_array = np.cos((2*j_array+1)*math.pi/(2*(K+T))) #np.cos((2*j_array+1)*math.pi/(2*K))\n",
    "\n",
    "# alpha_array = np.array([-5.87785252e-01, 5.87785252e-01])\n",
    "# print(\"alpha_array: \",alpha_array,'\\n')\n",
    "\n",
    "B = 0.5\n",
    "\n",
    "\n",
    "N_array = [2]\n",
    "alloc_case = 3\n",
    "B_array = [0.5]\n",
    "\n",
    "N_trials = 1\n",
    "N_epochs = 30\n",
    "\n",
    "\n",
    "\n",
    "loss_test_arr_v0_2 = np.zeros((len(N_array),len(B_array),N_trials,N_epochs))\n",
    "acc_test_arr_v0_2  = np.zeros((len(N_array),len(B_array),N_trials,N_epochs))\n",
    "\n",
    "for N_idx in range(len(N_array)):\n",
    "    \n",
    "    N = N_array[N_idx]\n",
    "    \n",
    "    \n",
    "           \n",
    "        \n",
    "    # print(\"alpha_array: \",alpha_array,'\\n')\n",
    "    \n",
    "    \n",
    "    for B_idx in range(len(B_array)):\n",
    "        \n",
    "        B = B_array[B_idx]\n",
    "        z_array = []\n",
    "#         while(len(z_array)<N):\n",
    "#             z_tmp = np.random.uniform(-1,1,1)\n",
    "#             MIS_tmp = MutualInformationSecurity(alpha_array[Signal_Alloc], alpha_array[Noise_Alloc],[z_tmp], 1,sigma)\n",
    "#             if MIS_tmp < B and MIS_tmp > 0.1:\n",
    "#                 z_array.append(z_tmp[0])\n",
    "#         \n",
    "#         z_array = np.sort(z_array)\n",
    "        print(N_idx)\n",
    "        i_array = np.array(range(N))\n",
    "        z_array = np.cos(i_array*2*math.pi/N/2) # np.cos(i_array*2*math.pi/N/2)\n",
    "#         if N==2:\n",
    "#             z_array = np.array([-0.88, 0.88])\n",
    "#         elif N ==4:\n",
    "#             z_array = np.array([-0.88, -0.25, 0.25, 0.88])\n",
    "#         elif N ==5:\n",
    "#             z_array = np.array([-0.88, -0.25, -0.2, 0.25, 0.88])\n",
    "#         elif N ==6:\n",
    "#             z_array = np.array([-0.94, -0.88, -0.25, 0.25, 0.88, 0.94])\n",
    "#         elif N ==7:\n",
    "#             z_array = np.array([-0.94, -0.88, -0.25, 0, 0.25, 0.88, 0.94])\n",
    "#         else:\n",
    "#             z_array = np.array([-0.94, -0.88, -0.25, -0.2, 0.2, 0.25, 0.88, 0.94])\n",
    "\n",
    "            \n",
    "        print('z_array:',z_array)        \n",
    "        \n",
    "        _Noise_label = np.ones((30000*T,10)) * 0.1\n",
    "\n",
    "        X_tilde,a,b = BACC_Enc_Data_v3(encoding_input_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _is_LCC=False) #BACC_Enc(encoding_input_array_np, alpha_array, z_array)\n",
    "        y_tilde,a,b = BACC_Enc_Data_v3(encoding_label_array_np, N, K, T, sigma, alpha_array, z_array, _Noise_Alloc = Noise_Alloc, _Noise = _Noise_label, is_predefined_noise=True, _is_LCC=False) #BACC_Enc(encoding_label_array_np, alpha_array, z_array)\n",
    "\n",
    "        m = N # m is the number of received result @ master\n",
    "    #     print('number of results:',m)\n",
    "\n",
    "        for trial_idx in range(N_trials):\n",
    "            print('(T, sigma)=',T,sigma,') ',trial_idx,'-th Trial!!')\n",
    "\n",
    "            net_glob = CNNMnist2(args=args)\n",
    "            net_glob.cuda()\n",
    "            net_glob.train()\n",
    "            \n",
    "            w_locals_array = []\n",
    "            w_dec_array = []\n",
    "            loss_locals_array = []\n",
    "            w_glob_array = []\n",
    "            \n",
    "            # copy weights\n",
    "            w_glob = net_glob.state_dict()\n",
    "\n",
    "            for iter in range(N_epochs): #args.epochs\n",
    "                w_locals, loss_locals = [], []\n",
    "                idxs_users = np.random.choice(range(N), m, replace=False)\n",
    "                idxs_users = np.sort(idxs_users)\n",
    "                print('selected users:',idxs_users)\n",
    "                \n",
    "                coded_net = BACC_Enc_Model_withNoise_v3(net_glob.cuda(), N, K, T, 0, alpha_array, z_array, _Noise_Alloc=Noise_Alloc, _is_LCC=False)\n",
    "\n",
    "                dec_z_array = []\n",
    "                for idx in idxs_users: #for idx in range(N):\n",
    "            #         print(idx)\n",
    "                    local = LocalUpdate_with_BACC(args=args, dataset=X_tilde[idx,:,:], label=y_tilde[idx,:,:])\n",
    "                    w, loss = local.train(net=copy.deepcopy(coded_net[idx]).cuda())\n",
    "                    w_locals.append(copy.deepcopy(w))\n",
    "                    loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "                    dec_z_array.append(z_array[idx])\n",
    "\n",
    "                \n",
    "                # update global weights\n",
    "                \n",
    "                w_glob, w_dec = FedAvg_with_BACC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array, is_debug=True)\n",
    "#                 w_glob = FedAvg_with_LCC_Dec(w_locals, alpha_array[Signal_Alloc], dec_z_array)\n",
    "\n",
    "                # copy weight to net_glob\n",
    "                net_glob.load_state_dict(w_glob)\n",
    "        \n",
    "                w_locals_array.append(w_locals)\n",
    "                loss_locals_array.append(loss_locals)\n",
    "                w_dec_array.append(w_dec)\n",
    "                w_glob_array.append(w_glob)\n",
    "\n",
    "                # print loss\n",
    "            #     acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "\n",
    "            #     loss_train_arr.append(loss_train)\n",
    "\n",
    "                acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "                acc_test_arr_v0_2[N_idx][B_idx][trial_idx][iter] = acc_test\n",
    "                loss_test_arr_v0_2[N_idx][B_idx][trial_idx][iter] = loss_test\n",
    "                if iter % 1 ==0:\n",
    "                    print('Round {:3d}, Average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_test, acc_test))\n",
    "                #print(loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N,iter =  0 0\n",
      "\n",
      "Test set: Average loss: 95.4018 \n",
      "Accuracy: 2443/10000 (24.43%)\n",
      "\n",
      "N,iter =  0 1\n",
      "\n",
      "Test set: Average loss: 51432.1536 \n",
      "Accuracy: 8807/10000 (88.07%)\n",
      "\n",
      "N,iter =  0 2\n",
      "\n",
      "Test set: Average loss: 31476.6765 \n",
      "Accuracy: 8829/10000 (88.29%)\n",
      "\n",
      "N,iter =  0 3\n",
      "\n",
      "Test set: Average loss: 55654.3670 \n",
      "Accuracy: 8825/10000 (88.25%)\n",
      "\n",
      "N,iter =  0 4\n",
      "\n",
      "Test set: Average loss: 48775.0508 \n",
      "Accuracy: 8822/10000 (88.22%)\n",
      "\n",
      "N,iter =  0 5\n",
      "\n",
      "Test set: Average loss: 56902.6558 \n",
      "Accuracy: 8810/10000 (88.10%)\n",
      "\n",
      "N,iter =  0 6\n",
      "\n",
      "Test set: Average loss: 71389.2056 \n",
      "Accuracy: 8757/10000 (87.57%)\n",
      "\n",
      "N,iter =  0 7\n",
      "\n",
      "Test set: Average loss: 102885.6166 \n",
      "Accuracy: 8875/10000 (88.75%)\n",
      "\n",
      "N,iter =  0 8\n",
      "\n",
      "Test set: Average loss: 68095.1705 \n",
      "Accuracy: 8795/10000 (87.95%)\n",
      "\n",
      "N,iter =  0 9\n",
      "\n",
      "Test set: Average loss: 74933.0274 \n",
      "Accuracy: 8790/10000 (87.90%)\n",
      "\n",
      "N,iter =  0 10\n",
      "\n",
      "Test set: Average loss: 65209.4529 \n",
      "Accuracy: 8824/10000 (88.24%)\n",
      "\n",
      "N,iter =  0 11\n",
      "\n",
      "Test set: Average loss: 80611.8838 \n",
      "Accuracy: 8812/10000 (88.12%)\n",
      "\n",
      "N,iter =  0 12\n",
      "\n",
      "Test set: Average loss: 101975.8568 \n",
      "Accuracy: 8763/10000 (87.63%)\n",
      "\n",
      "N,iter =  0 13\n",
      "\n",
      "Test set: Average loss: 82549.4867 \n",
      "Accuracy: 8766/10000 (87.66%)\n",
      "\n",
      "N,iter =  0 14\n",
      "\n",
      "Test set: Average loss: 87040.6866 \n",
      "Accuracy: 8802/10000 (88.02%)\n",
      "\n",
      "N,iter =  0 15\n",
      "\n",
      "Test set: Average loss: 83397.0303 \n",
      "Accuracy: 8804/10000 (88.04%)\n",
      "\n",
      "N,iter =  0 16\n",
      "\n",
      "Test set: Average loss: 81373.4964 \n",
      "Accuracy: 8803/10000 (88.03%)\n",
      "\n",
      "N,iter =  0 17\n",
      "\n",
      "Test set: Average loss: 92076.3237 \n",
      "Accuracy: 8819/10000 (88.19%)\n",
      "\n",
      "N,iter =  0 18\n",
      "\n",
      "Test set: Average loss: 69833.3964 \n",
      "Accuracy: 8847/10000 (88.47%)\n",
      "\n",
      "N,iter =  0 19\n",
      "\n",
      "Test set: Average loss: 91462.7489 \n",
      "Accuracy: 8831/10000 (88.31%)\n",
      "\n",
      "N,iter =  0 20\n",
      "\n",
      "Test set: Average loss: 83277.4031 \n",
      "Accuracy: 8798/10000 (87.98%)\n",
      "\n",
      "N,iter =  0 21\n",
      "\n",
      "Test set: Average loss: 64244.4417 \n",
      "Accuracy: 8838/10000 (88.38%)\n",
      "\n",
      "N,iter =  0 22\n",
      "\n",
      "Test set: Average loss: 95286.0621 \n",
      "Accuracy: 8795/10000 (87.95%)\n",
      "\n",
      "N,iter =  0 23\n",
      "\n",
      "Test set: Average loss: 91527.9564 \n",
      "Accuracy: 8792/10000 (87.92%)\n",
      "\n",
      "N,iter =  0 24\n",
      "\n",
      "Test set: Average loss: 76230.7317 \n",
      "Accuracy: 8884/10000 (88.84%)\n",
      "\n",
      "N,iter =  0 25\n",
      "\n",
      "Test set: Average loss: 87773.3954 \n",
      "Accuracy: 8805/10000 (88.05%)\n",
      "\n",
      "N,iter =  0 26\n",
      "\n",
      "Test set: Average loss: 105008.7085 \n",
      "Accuracy: 8837/10000 (88.37%)\n",
      "\n",
      "N,iter =  0 27\n",
      "\n",
      "Test set: Average loss: 69471.8865 \n",
      "Accuracy: 8814/10000 (88.14%)\n",
      "\n",
      "N,iter =  0 28\n",
      "\n",
      "Test set: Average loss: 93927.6414 \n",
      "Accuracy: 8824/10000 (88.24%)\n",
      "\n",
      "N,iter =  0 29\n",
      "\n",
      "Test set: Average loss: 81735.6609 \n",
      "Accuracy: 8792/10000 (87.92%)\n",
      "\n",
      "N,iter =  1 0\n",
      "\n",
      "Test set: Average loss: 0.3506 \n",
      "Accuracy: 8777/10000 (87.77%)\n",
      "\n",
      "N,iter =  1 1\n",
      "\n",
      "Test set: Average loss: 0.1202 \n",
      "Accuracy: 9655/10000 (96.55%)\n",
      "\n",
      "N,iter =  1 2\n",
      "\n",
      "Test set: Average loss: 0.1123 \n",
      "Accuracy: 9704/10000 (97.04%)\n",
      "\n",
      "N,iter =  1 3\n",
      "\n",
      "Test set: Average loss: 0.1000 \n",
      "Accuracy: 9724/10000 (97.24%)\n",
      "\n",
      "N,iter =  1 4\n",
      "\n",
      "Test set: Average loss: 0.1109 \n",
      "Accuracy: 9692/10000 (96.92%)\n",
      "\n",
      "N,iter =  1 5\n",
      "\n",
      "Test set: Average loss: 0.1045 \n",
      "Accuracy: 9697/10000 (96.97%)\n",
      "\n",
      "N,iter =  1 6\n",
      "\n",
      "Test set: Average loss: 0.1040 \n",
      "Accuracy: 9701/10000 (97.01%)\n",
      "\n",
      "N,iter =  1 7\n",
      "\n",
      "Test set: Average loss: 0.1127 \n",
      "Accuracy: 9661/10000 (96.61%)\n",
      "\n",
      "N,iter =  1 8\n",
      "\n",
      "Test set: Average loss: 0.0897 \n",
      "Accuracy: 9759/10000 (97.59%)\n",
      "\n",
      "N,iter =  1 9\n",
      "\n",
      "Test set: Average loss: 0.0965 \n",
      "Accuracy: 9730/10000 (97.30%)\n",
      "\n",
      "N,iter =  1 10\n",
      "\n",
      "Test set: Average loss: 0.0829 \n",
      "Accuracy: 9779/10000 (97.79%)\n",
      "\n",
      "N,iter =  1 11\n",
      "\n",
      "Test set: Average loss: 0.0937 \n",
      "Accuracy: 9744/10000 (97.44%)\n",
      "\n",
      "N,iter =  1 12\n",
      "\n",
      "Test set: Average loss: 0.0948 \n",
      "Accuracy: 9759/10000 (97.59%)\n",
      "\n",
      "N,iter =  1 13\n",
      "\n",
      "Test set: Average loss: 0.1075 \n",
      "Accuracy: 9722/10000 (97.22%)\n",
      "\n",
      "N,iter =  1 14\n",
      "\n",
      "Test set: Average loss: 0.0904 \n",
      "Accuracy: 9763/10000 (97.63%)\n",
      "\n",
      "N,iter =  1 15\n",
      "\n",
      "Test set: Average loss: 0.1018 \n",
      "Accuracy: 9746/10000 (97.46%)\n",
      "\n",
      "N,iter =  1 16\n",
      "\n",
      "Test set: Average loss: 0.1053 \n",
      "Accuracy: 9760/10000 (97.60%)\n",
      "\n",
      "N,iter =  1 17\n",
      "\n",
      "Test set: Average loss: 0.1103 \n",
      "Accuracy: 9721/10000 (97.21%)\n",
      "\n",
      "N,iter =  1 18\n",
      "\n",
      "Test set: Average loss: 0.1086 \n",
      "Accuracy: 9736/10000 (97.36%)\n",
      "\n",
      "N,iter =  1 19\n",
      "\n",
      "Test set: Average loss: 0.1089 \n",
      "Accuracy: 9746/10000 (97.46%)\n",
      "\n",
      "N,iter =  1 20\n",
      "\n",
      "Test set: Average loss: 0.1145 \n",
      "Accuracy: 9723/10000 (97.23%)\n",
      "\n",
      "N,iter =  1 21\n",
      "\n",
      "Test set: Average loss: 0.1157 \n",
      "Accuracy: 9727/10000 (97.27%)\n",
      "\n",
      "N,iter =  1 22\n",
      "\n",
      "Test set: Average loss: 0.1217 \n",
      "Accuracy: 9706/10000 (97.06%)\n",
      "\n",
      "N,iter =  1 23\n",
      "\n",
      "Test set: Average loss: 0.1077 \n",
      "Accuracy: 9732/10000 (97.32%)\n",
      "\n",
      "N,iter =  1 24\n",
      "\n",
      "Test set: Average loss: 0.1144 \n",
      "Accuracy: 9739/10000 (97.39%)\n",
      "\n",
      "N,iter =  1 25\n",
      "\n",
      "Test set: Average loss: 0.1123 \n",
      "Accuracy: 9741/10000 (97.41%)\n",
      "\n",
      "N,iter =  1 26\n",
      "\n",
      "Test set: Average loss: 0.1269 \n",
      "Accuracy: 9712/10000 (97.12%)\n",
      "\n",
      "N,iter =  1 27\n",
      "\n",
      "Test set: Average loss: 0.1280 \n",
      "Accuracy: 9714/10000 (97.14%)\n",
      "\n",
      "N,iter =  1 28\n",
      "\n",
      "Test set: Average loss: 0.1196 \n",
      "Accuracy: 9730/10000 (97.30%)\n",
      "\n",
      "N,iter =  1 29\n",
      "\n",
      "Test set: Average loss: 0.1291 \n",
      "Accuracy: 9701/10000 (97.01%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_test_arr_w_locals = np.zeros((N,N_epochs))\n",
    "acc_test_arr_w_locals = np.zeros((N,N_epochs))\n",
    "\n",
    "\n",
    "\n",
    "for i in range(N):\n",
    "    for iter in range(N_epochs):\n",
    "        net_glob = CNNMnist2(args=args)\n",
    "        net_glob.cuda()\n",
    "#             net_glob.train()\n",
    "        net_glob.load_state_dict(w_locals_array[iter][i])\n",
    "        print('N,iter = ',i,iter)\n",
    "        acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "        \n",
    "        loss_test_arr_w_locals[i][iter] = loss_test\n",
    "        acc_test_arr_w_locals[i][iter] = acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N,iter =  1 0\n",
      "\n",
      "Test set: Average loss: 525.9844 \n",
      "Accuracy: 3008/10000 (30.08%)\n",
      "\n",
      "N,iter =  1 1\n",
      "\n",
      "Test set: Average loss: 59310.4036 \n",
      "Accuracy: 8670/10000 (86.70%)\n",
      "\n",
      "N,iter =  1 2\n",
      "\n",
      "Test set: Average loss: 32767.6623 \n",
      "Accuracy: 8767/10000 (87.67%)\n",
      "\n",
      "N,iter =  1 3\n",
      "\n",
      "Test set: Average loss: 58781.7546 \n",
      "Accuracy: 8800/10000 (88.00%)\n",
      "\n",
      "N,iter =  1 4\n",
      "\n",
      "Test set: Average loss: 52069.2806 \n",
      "Accuracy: 8788/10000 (87.88%)\n",
      "\n",
      "N,iter =  1 5\n",
      "\n",
      "Test set: Average loss: 58463.7772 \n",
      "Accuracy: 8794/10000 (87.94%)\n",
      "\n",
      "N,iter =  1 6\n",
      "\n",
      "Test set: Average loss: 71874.3899 \n",
      "Accuracy: 8794/10000 (87.94%)\n",
      "\n",
      "N,iter =  1 7\n",
      "\n",
      "Test set: Average loss: 112662.3921 \n",
      "Accuracy: 8779/10000 (87.79%)\n",
      "\n",
      "N,iter =  1 8\n",
      "\n",
      "Test set: Average loss: 75484.7694 \n",
      "Accuracy: 8730/10000 (87.30%)\n",
      "\n",
      "N,iter =  1 9\n",
      "\n",
      "Test set: Average loss: 78559.7654 \n",
      "Accuracy: 8808/10000 (88.08%)\n",
      "\n",
      "N,iter =  1 10\n",
      "\n",
      "Test set: Average loss: 67343.1108 \n",
      "Accuracy: 8844/10000 (88.44%)\n",
      "\n",
      "N,iter =  1 11\n",
      "\n",
      "Test set: Average loss: 91139.5181 \n",
      "Accuracy: 8724/10000 (87.24%)\n",
      "\n",
      "N,iter =  1 12\n",
      "\n",
      "Test set: Average loss: 107984.2863 \n",
      "Accuracy: 8737/10000 (87.37%)\n",
      "\n",
      "N,iter =  1 13\n",
      "\n",
      "Test set: Average loss: 87938.8282 \n",
      "Accuracy: 8752/10000 (87.52%)\n",
      "\n",
      "N,iter =  1 14\n",
      "\n",
      "Test set: Average loss: 92207.9519 \n",
      "Accuracy: 8784/10000 (87.84%)\n",
      "\n",
      "N,iter =  1 15\n",
      "\n",
      "Test set: Average loss: 91112.1831 \n",
      "Accuracy: 8756/10000 (87.56%)\n",
      "\n",
      "N,iter =  1 16\n",
      "\n",
      "Test set: Average loss: 87031.4537 \n",
      "Accuracy: 8765/10000 (87.65%)\n",
      "\n",
      "N,iter =  1 17\n",
      "\n",
      "Test set: Average loss: 97481.7783 \n",
      "Accuracy: 8795/10000 (87.95%)\n",
      "\n",
      "N,iter =  1 18\n",
      "\n",
      "Test set: Average loss: 74951.2688 \n",
      "Accuracy: 8805/10000 (88.05%)\n",
      "\n",
      "N,iter =  1 19\n",
      "\n",
      "Test set: Average loss: 98584.3228 \n",
      "Accuracy: 8782/10000 (87.82%)\n",
      "\n",
      "N,iter =  1 20\n",
      "\n",
      "Test set: Average loss: 90742.3057 \n",
      "Accuracy: 8763/10000 (87.63%)\n",
      "\n",
      "N,iter =  1 21\n",
      "\n",
      "Test set: Average loss: 71159.6941 \n",
      "Accuracy: 8747/10000 (87.47%)\n",
      "\n",
      "N,iter =  1 22\n",
      "\n",
      "Test set: Average loss: 107534.6460 \n",
      "Accuracy: 8655/10000 (86.55%)\n",
      "\n",
      "N,iter =  1 23\n",
      "\n",
      "Test set: Average loss: 94591.9794 \n",
      "Accuracy: 8752/10000 (87.52%)\n",
      "\n",
      "N,iter =  1 24\n",
      "\n",
      "Test set: Average loss: 79424.5804 \n",
      "Accuracy: 8877/10000 (88.77%)\n",
      "\n",
      "N,iter =  1 25\n",
      "\n",
      "Test set: Average loss: 89574.1061 \n",
      "Accuracy: 8827/10000 (88.27%)\n",
      "\n",
      "N,iter =  1 26\n",
      "\n",
      "Test set: Average loss: 107277.5097 \n",
      "Accuracy: 8823/10000 (88.23%)\n",
      "\n",
      "N,iter =  1 27\n",
      "\n",
      "Test set: Average loss: 71168.0243 \n",
      "Accuracy: 8822/10000 (88.22%)\n",
      "\n",
      "N,iter =  1 28\n",
      "\n",
      "Test set: Average loss: 98494.3975 \n",
      "Accuracy: 8794/10000 (87.94%)\n",
      "\n",
      "N,iter =  1 29\n",
      "\n",
      "Test set: Average loss: 88234.6926 \n",
      "Accuracy: 8743/10000 (87.43%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "acc_test_arr_w_locals_sum = np.zeros((N_epochs))\n",
    "\n",
    "for iter in range(N_epochs):\n",
    "    net_glob = CNNMnist2(args=args)\n",
    "    net_glob.cuda()\n",
    "#             net_glob.train()\n",
    "    \n",
    "    w_tmp = copy.deepcopy(w_locals_array[iter][0])\n",
    "    for k in w_tmp.keys():\n",
    "        for G_idx in range(1,N):\n",
    "            w_tmp[k] += w_locals_array[iter][G_idx][k]\n",
    "        w_tmp[k] = torch.div(w_tmp[k], 1)\n",
    "    \n",
    "    net_glob.load_state_dict(w_tmp)\n",
    "    \n",
    "    print('N,iter = ',i,iter)\n",
    "    acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "\n",
    "    acc_test_arr_w_locals_sum[iter] = acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K,iter =  0 0\n",
      "\n",
      "Test set: Average loss: 34.0099 \n",
      "Accuracy: 2381/10000 (23.81%)\n",
      "\n",
      "K,iter =  0 1\n",
      "\n",
      "Test set: Average loss: 19116.4474 \n",
      "Accuracy: 8769/10000 (87.69%)\n",
      "\n",
      "K,iter =  0 2\n",
      "\n",
      "Test set: Average loss: 11181.6078 \n",
      "Accuracy: 8822/10000 (88.22%)\n",
      "\n",
      "K,iter =  0 3\n",
      "\n",
      "Test set: Average loss: 20059.0201 \n",
      "Accuracy: 8822/10000 (88.22%)\n",
      "\n",
      "K,iter =  0 4\n",
      "\n",
      "Test set: Average loss: 17645.9341 \n",
      "Accuracy: 8812/10000 (88.12%)\n",
      "\n",
      "K,iter =  0 5\n",
      "\n",
      "Test set: Average loss: 20259.4790 \n",
      "Accuracy: 8801/10000 (88.01%)\n",
      "\n",
      "K,iter =  0 6\n",
      "\n",
      "Test set: Average loss: 25105.1147 \n",
      "Accuracy: 8787/10000 (87.87%)\n",
      "\n",
      "K,iter =  0 7\n",
      "\n",
      "Test set: Average loss: 37249.0274 \n",
      "Accuracy: 8862/10000 (88.62%)\n",
      "\n",
      "K,iter =  0 8\n",
      "\n",
      "Test set: Average loss: 24838.6108 \n",
      "Accuracy: 8796/10000 (87.96%)\n",
      "\n",
      "K,iter =  0 9\n",
      "\n",
      "Test set: Average loss: 26744.5995 \n",
      "Accuracy: 8816/10000 (88.16%)\n",
      "\n",
      "K,iter =  0 10\n",
      "\n",
      "Test set: Average loss: 23302.5614 \n",
      "Accuracy: 8831/10000 (88.31%)\n",
      "\n",
      "K,iter =  0 11\n",
      "\n",
      "Test set: Average loss: 29369.4527 \n",
      "Accuracy: 8813/10000 (88.13%)\n",
      "\n",
      "K,iter =  0 12\n",
      "\n",
      "Test set: Average loss: 36856.8425 \n",
      "Accuracy: 8759/10000 (87.59%)\n",
      "\n",
      "K,iter =  0 13\n",
      "\n",
      "Test set: Average loss: 29686.8463 \n",
      "Accuracy: 8760/10000 (87.60%)\n",
      "\n",
      "K,iter =  0 14\n",
      "\n",
      "Test set: Average loss: 31288.1517 \n",
      "Accuracy: 8815/10000 (88.15%)\n",
      "\n",
      "K,iter =  0 15\n",
      "\n",
      "Test set: Average loss: 30193.2796 \n",
      "Accuracy: 8804/10000 (88.04%)\n",
      "\n",
      "K,iter =  0 16\n",
      "\n",
      "Test set: Average loss: 29311.0840 \n",
      "Accuracy: 8806/10000 (88.06%)\n",
      "\n",
      "K,iter =  0 17\n",
      "\n",
      "Test set: Average loss: 32913.1027 \n",
      "Accuracy: 8845/10000 (88.45%)\n",
      "\n",
      "K,iter =  0 18\n",
      "\n",
      "Test set: Average loss: 25263.7167 \n",
      "Accuracy: 8835/10000 (88.35%)\n",
      "\n",
      "K,iter =  0 19\n",
      "\n",
      "Test set: Average loss: 33013.3069 \n",
      "Accuracy: 8835/10000 (88.35%)\n",
      "\n",
      "K,iter =  0 20\n",
      "\n",
      "Test set: Average loss: 29932.0516 \n",
      "Accuracy: 8825/10000 (88.25%)\n",
      "\n",
      "K,iter =  0 21\n",
      "\n",
      "Test set: Average loss: 23421.3903 \n",
      "Accuracy: 8809/10000 (88.09%)\n",
      "\n",
      "K,iter =  0 22\n",
      "\n",
      "Test set: Average loss: 34659.6423 \n",
      "Accuracy: 8766/10000 (87.66%)\n",
      "\n",
      "K,iter =  0 23\n",
      "\n",
      "Test set: Average loss: 32732.7255 \n",
      "Accuracy: 8775/10000 (87.75%)\n",
      "\n",
      "K,iter =  0 24\n",
      "\n",
      "Test set: Average loss: 27271.4238 \n",
      "Accuracy: 8875/10000 (88.75%)\n",
      "\n",
      "K,iter =  0 25\n",
      "\n",
      "Test set: Average loss: 31215.9680 \n",
      "Accuracy: 8820/10000 (88.20%)\n",
      "\n",
      "K,iter =  0 26\n",
      "\n",
      "Test set: Average loss: 37252.9605 \n",
      "Accuracy: 8843/10000 (88.43%)\n",
      "\n",
      "K,iter =  0 27\n",
      "\n",
      "Test set: Average loss: 24754.0117 \n",
      "Accuracy: 8829/10000 (88.29%)\n",
      "\n",
      "K,iter =  0 28\n",
      "\n",
      "Test set: Average loss: 33700.5756 \n",
      "Accuracy: 8829/10000 (88.29%)\n",
      "\n",
      "K,iter =  0 29\n",
      "\n",
      "Test set: Average loss: 29659.0036 \n",
      "Accuracy: 8773/10000 (87.73%)\n",
      "\n",
      "K,iter =  1 0\n",
      "\n",
      "Test set: Average loss: 4794.0946 \n",
      "Accuracy: 298/10000 (2.98%)\n",
      "\n",
      "K,iter =  1 1\n",
      "\n",
      "Test set: Average loss: 115025.2160 \n",
      "Accuracy: 11/10000 (0.11%)\n",
      "\n",
      "K,iter =  1 2\n",
      "\n",
      "Test set: Average loss: 66452.4855 \n",
      "Accuracy: 14/10000 (0.14%)\n",
      "\n",
      "K,iter =  1 3\n",
      "\n",
      "Test set: Average loss: 120673.0273 \n",
      "Accuracy: 12/10000 (0.12%)\n",
      "\n",
      "K,iter =  1 4\n",
      "\n",
      "Test set: Average loss: 108002.8800 \n",
      "Accuracy: 16/10000 (0.16%)\n",
      "\n",
      "K,iter =  1 5\n",
      "\n",
      "Test set: Average loss: 110032.4975 \n",
      "Accuracy: 18/10000 (0.18%)\n",
      "\n",
      "K,iter =  1 6\n",
      "\n",
      "Test set: Average loss: 121159.1493 \n",
      "Accuracy: 28/10000 (0.28%)\n",
      "\n",
      "K,iter =  1 7\n",
      "\n",
      "Test set: Average loss: 113338.1254 \n",
      "Accuracy: 19/10000 (0.19%)\n",
      "\n",
      "K,iter =  1 8\n",
      "\n",
      "Test set: Average loss: 73400.6907 \n",
      "Accuracy: 48/10000 (0.48%)\n",
      "\n",
      "K,iter =  1 9\n",
      "\n",
      "Test set: Average loss: 82434.7029 \n",
      "Accuracy: 32/10000 (0.32%)\n",
      "\n",
      "K,iter =  1 10\n",
      "\n",
      "Test set: Average loss: 94552.2286 \n",
      "Accuracy: 18/10000 (0.18%)\n",
      "\n",
      "K,iter =  1 11\n",
      "\n",
      "Test set: Average loss: 135066.2874 \n",
      "Accuracy: 14/10000 (0.14%)\n",
      "\n",
      "K,iter =  1 12\n",
      "\n",
      "Test set: Average loss: 209229.5276 \n",
      "Accuracy: 9/10000 (0.09%)\n",
      "\n",
      "K,iter =  1 13\n",
      "\n",
      "Test set: Average loss: 162902.3424 \n",
      "Accuracy: 6/10000 (0.06%)\n",
      "\n",
      "K,iter =  1 14\n",
      "\n",
      "Test set: Average loss: 166922.5020 \n",
      "Accuracy: 8/10000 (0.08%)\n",
      "\n",
      "K,iter =  1 15\n",
      "\n",
      "Test set: Average loss: 249345.7572 \n",
      "Accuracy: 4/10000 (0.04%)\n",
      "\n",
      "K,iter =  1 16\n",
      "\n",
      "Test set: Average loss: 126427.7946 \n",
      "Accuracy: 8/10000 (0.08%)\n",
      "\n",
      "K,iter =  1 17\n",
      "\n",
      "Test set: Average loss: 208533.6188 \n",
      "Accuracy: 4/10000 (0.04%)\n",
      "\n",
      "K,iter =  1 18\n",
      "\n",
      "Test set: Average loss: 212161.9332 \n",
      "Accuracy: 10/10000 (0.10%)\n",
      "\n",
      "K,iter =  1 19\n",
      "\n",
      "Test set: Average loss: 211367.0362 \n",
      "Accuracy: 15/10000 (0.15%)\n",
      "\n",
      "K,iter =  1 20\n",
      "\n",
      "Test set: Average loss: 165722.7114 \n",
      "Accuracy: 10/10000 (0.10%)\n",
      "\n",
      "K,iter =  1 21\n",
      "\n",
      "Test set: Average loss: 236092.1752 \n",
      "Accuracy: 16/10000 (0.16%)\n",
      "\n",
      "K,iter =  1 22\n",
      "\n",
      "Test set: Average loss: 236521.4976 \n",
      "Accuracy: 5/10000 (0.05%)\n",
      "\n",
      "K,iter =  1 23\n",
      "\n",
      "Test set: Average loss: 189745.5570 \n",
      "Accuracy: 5/10000 (0.05%)\n",
      "\n",
      "K,iter =  1 24\n",
      "\n",
      "Test set: Average loss: 166219.2534 \n",
      "Accuracy: 6/10000 (0.06%)\n",
      "\n",
      "K,iter =  1 25\n",
      "\n",
      "Test set: Average loss: 250481.2612 \n",
      "Accuracy: 5/10000 (0.05%)\n",
      "\n",
      "K,iter =  1 26\n",
      "\n",
      "Test set: Average loss: 222081.8116 \n",
      "Accuracy: 6/10000 (0.06%)\n",
      "\n",
      "K,iter =  1 27\n",
      "\n",
      "Test set: Average loss: 145770.2204 \n",
      "Accuracy: 11/10000 (0.11%)\n",
      "\n",
      "K,iter =  1 28\n",
      "\n",
      "Test set: Average loss: 229391.9056 \n",
      "Accuracy: 4/10000 (0.04%)\n",
      "\n",
      "K,iter =  1 29\n",
      "\n",
      "Test set: Average loss: 127571.3988 \n",
      "Accuracy: 11/10000 (0.11%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_test_arr_w_dec = np.zeros((K,N_epochs))\n",
    "acc_test_arr_w_dec = np.zeros((K,N_epochs))\n",
    "\n",
    "for i in range(K):\n",
    "    for iter in range(N_epochs):\n",
    "        net_glob = CNNMnist2(args=args)\n",
    "        net_glob.cuda()\n",
    "#             net_glob.train()\n",
    "        net_glob.load_state_dict(w_dec_array[iter][i])\n",
    "        print('K,iter = ',i,iter)\n",
    "        acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "        \n",
    "        loss_test_arr_w_dec[i][iter] = loss_test\n",
    "        acc_test_arr_w_dec[i][iter] = acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(K)\n",
    "# N_array = [2,4,7,10,13]\n",
    "\n",
    "plt.plot(acc_test_arr_w_locals[0,0:30],  label='w_tilde_i, i=1 (out of N=2 working nodes)')\n",
    "plt.plot(acc_test_arr_w_locals[1,0:30],  label='w_tilde_i, i=2 (out of N=2 working nodes)')\n",
    "plt.ylim([0,100])\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(acc_test_arr_w_dec[0,0:30],  label='w_hat_k, k=1')\n",
    "plt.plot(acc_test_arr_w_dec[1,0:30],  label='w_hat_k, k=2')\n",
    "plt.ylim([0,100])\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0132, -0.0007,  0.0376, -0.0167,  0.0405],\n",
      "         [-0.0100, -0.2157, -0.6273, -0.7034, -0.4362],\n",
      "         [-0.2113, -0.4471,  0.4227,  0.5216, -0.4417],\n",
      "         [-0.1872, -0.0997,  0.2947,  0.2044, -0.3192],\n",
      "         [-0.2438, -0.0140,  0.2971, -0.0087, -0.0929]]], device='cuda:0')\n",
      "\n",
      "tensor([[[-0.0132, -0.0007,  0.0376, -0.0167,  0.0405],\n",
      "         [-0.0100, -0.2157, -0.6273, -0.7034, -0.4362],\n",
      "         [-0.2113, -0.4471,  0.4227,  0.5216, -0.4417],\n",
      "         [-0.1872, -0.0997,  0.2947,  0.2044, -0.3192],\n",
      "         [-0.2438, -0.0140,  0.2971, -0.0087, -0.0929]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print((w_dec_array[28][0]['conv1.weight'][0]+w_dec_array[28][1]['conv1.weight'][0])/2)\n",
    "# print(w_dec_array[0][1]['conv1.weight'][0])\n",
    "print()\n",
    "print(w_glob_array[28]['conv1.weight'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
